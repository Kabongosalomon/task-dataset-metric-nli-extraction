<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-hand Global 3D Pose Estimation using Monocular RGB</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanqing</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University</orgName>
								<address>
									<postCode>84602</postCode>
									<settlement>Provo</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Wilhelm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University</orgName>
								<address>
									<postCode>84602</postCode>
									<settlement>Provo</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University</orgName>
								<address>
									<postCode>84602</postCode>
									<settlement>Provo</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Two-hand Global 3D Pose Estimation using Monocular RGB</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the challenging task of estimating global 3D joint locations for both hands via only monocular RGB input images. We propose a novel multi-stage convolutional neural network based pipeline that accurately segments and locates the hands despite occlusion between two hands and complex background noise and estimates the 2D and 3D canonical joint locations without any depth information. Global joint locations with respect to the camera origin are computed using the hand pose estimations and the actual length of the key bone with a novel projection algorithm. To train the CNNs for this new task, we introduce a largescale synthetic 3D hand pose dataset. We demonstrate that our system outperforms previous works on 3D canonical hand pose estimation benchmark datasets with RGB-only information. Additionally, we present the first work that achieves accurate global 3D hand tracking on both hands using RGB-only inputs and provide extensive quantitative and qualitative evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the primary operating tool for human activities, the hands play a significant role in applications such as gesture control, action recognition, human-computer interaction and VR/AR. As the field of computer vision advances, commercial systems [3, 1, 2, 4] are shifting from marker/glovebased methods to vision-based hand tracking and pose estimation. However, accurate hand pose estimation from camera inputs remains challenging due to the possible heavy occlusion from the hand itself, the other hand or objects, complex background noise and the large pose space.</p><p>Most contemporary vision-based markerless works tackling the task of 3D hand pose estimation rely on depth information, requiring either multi-view setup or depth cameras. However, such hardware requirements add severe limitations to the possible applications by significantly increasing the setup overhead and cost. Depth cameras also only work in indoor scenes and have relatively high-power consumption. To circumvent this problem, some recent approaches <ref type="figure" target="#fig_8">Figure 1</ref>: We present an approach to estimate the global 3D hand poses for both hands from a single RGB image, a new task that is particularly challenging not only due to complex background noise and various types of occlusion, but also the lack of depth information for estimating the distances. Given a RGB image (top), we show the side view of our estimation of global 3D hand poses (bottom). tackle 3D canonical one-hand pose estimation using deep CNNs with only RGB-based inputs and show good results.</p><p>In this paper, we present the first algorithm that simultaneously estimates the 3D global joint locations of both hands with respect to the camera origin using monocular RGB inputs ( <ref type="figure" target="#fig_8">Fig. 1)</ref>, which is an essential step towards the next generation gesture control and pose recognition systems. Our pipeline consists of 4 major components: <ref type="bibr" target="#b0">(1)</ref> hand segmentation and detection, (2) 2D hand pose estimation, (3) 3D canonical hand pose estimation and (4) 3D global hand pose estimation.</p><p>The first challenge of training our pipeline is the lack of annotated data from existing benchmark datasets. Real-world markerless 3D hand tracking data collected using multiple cameras or RGB-D camera setup inevitably has tracking error. Manual annotation is time-consuming and infeasible for large-scale data collection. Consequently, many recent benchmark datasets provide synthetic data with perfect ground truth annotation of the joint locations. However, synthetic images have different statistical distributions than real-world images and knowledge learned on synthetic data does not always transfer to the real-world domain, since CNNs are sensitive to textural information.</p><p>Since it is currently infeasible to collect real-world twohand pose data with accurate 2D and 3D joint annotations at a large-scale with sufficient variety, we create a novel highquality synthetic 3D hand dataset suitable for training and evaluating the networks and focus on the challenging task of two-hand global 3D pose estimation using RGB.</p><p>In summary, our main contributions include:</p><p>• The first system capable of estimating 3D global joint locations for both hands using monocular RGB inputs. We introduce a viewpoint-invariant global projection algorithm capable of perfectly reconstructing the absolute 3D joint locations and the evaluation protocol for the new task. • A novel egocentric RGB-D two-part (static + dynamic) synthetic dataset for the task of two-hand 3D global pose estimation, which introduces unique challenges and also benefits researches in hand segmentation and detection, 2D and 3D canonical pose estimation. • Extensive evaluation on both two-hand 3D global and single-hand 3D canonical hand pose estimation on 4 target datasets. Our networks outperform the current state-of-the-art canonical methods with less information (only RGB) and additionally achieve promising results for global pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hand pose estimation is a long-standing research area due to its wide range of applications. Compared to the popular task of body pose estimation, vision-based 3D hand pose estimation has more complex articulation, heavier occlusion and more restricted availability of data. We first review the most relevant previous methods that utilize depth information, then shift our emphasis to approaches that use RGB-only data. Depth-based methods. Oberweger et al. <ref type="bibr" target="#b13">[16]</ref> and Zhou et al. <ref type="bibr" target="#b33">[36]</ref> introduced CNN architectures that regressed 3D joint locations from depth images directly. Ge et al. <ref type="bibr" target="#b5">[8]</ref> proposed to project the depth image onto three orthogonal planes and fuse the corresponding 2D joint locations for the final 3D joint locations. Cai et al. <ref type="bibr" target="#b3">[6]</ref> and Iqbal et al. <ref type="bibr" target="#b7">[10]</ref> proposed models capable of training on RGB-D data and evaluating on RGB inputs.</p><p>Multiple-camera methods. Many methods use multiple RGB cameras to gain additional information from different viewpoints that can help with resolving the depth ambiguity and heavy occlusion. Wang et al. <ref type="bibr" target="#b27">[30]</ref> used 2 cameras and estimated 3D hand pose by matching with instances in a hand database. Oikonomidis et al. <ref type="bibr" target="#b14">[17]</ref> demonstrated the tracking of both the hand and an interacting object in 3D with 8 surrounding fixed cameras. Sridhar et al. <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b21">24]</ref> estimated the hand pose by using generative approach on inputs of multiple RGB cameras and a depth sensor. Zhang et al. <ref type="bibr" target="#b32">[35]</ref> introduced 3D hand pose estimation using matching algorithm on inputs from stereo cameras. Single-camera methods. Due to the significantly higher setup overhead and costs introduced by depth sensors and multiple calibrated cameras, some methods use a single RGB image to estimate the 3D hand poses. Zimmermann and Brox <ref type="bibr" target="#b34">[37]</ref> proposed a CNN-based pipeline that estimates the 2D joint locations and lifts the 2D heatmaps to 3D canonical joint locations. Mueller et al. <ref type="bibr" target="#b10">[13]</ref> introduced a model that estimates both 2D and 3D canonical joint locations with kinematic skeleton fitting to better address physical constraints and temporal smoothness. Spurr et al. <ref type="bibr" target="#b18">[21]</ref> and Yang et al. <ref type="bibr" target="#b30">[33]</ref> proposed to use variational autoencoders to learn a latent space for hand poses, which are capable of estimating 3D hand poses from RGB inputs. Some methods <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b6">9]</ref> estimated the low-dimensional parameters for a 3D deformable hand model <ref type="bibr" target="#b16">[19]</ref> to fit the RGB inputs in order to retrieve the 3D canonical hand poses. Two-hand methods. It is more natural yet difficult to estimate poses for two interacting hands due to inter-hand occlusion. Tzionas et al. <ref type="bibr" target="#b26">[29]</ref>, Taylor et al. <ref type="bibr" target="#b24">[27]</ref> and Mueller et al. <ref type="bibr" target="#b11">[14]</ref> achieved promising results using energy optimization to fit parametric hand models using depth data. Note that 3D pose estimation using RGB data is much more challenging due to the lack of depth information and the additional noise from images in the wild. To the best of our knowledge, our work is the first to address two-hand global pose estimation using RGB data from a single camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets for Hand Pose Estimation</head><p>For depth-based hand pose estimation, <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b4">7]</ref> presented large-scale datasets consisting of real depth images with estimated ground-truth joint locations. For RGB-D hand pose estimation, due to the need to manually annotate joint locations, small-scale datasets <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b32">35]</ref> with limited variation were presented with real RGB and depth data. For RGB-based hand pose estimation, <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b35">38]</ref> performed extensive manual annotation and provided a decent amount of labeled real-world instances. Note that accurately annotated hand data with sufficient variation is necessary for learning-based approaches, and datasets with real-world RGB/depth data can only provide estimation of the joint locations as the ground truth and some severely lacks sufficient variety. Consequently, synthetic datasets <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b34">37]</ref> with large amount of color, depth images and perfect annotation are introduced for advancing research in the field. It is worth mentioning that existing RGB-based datasets and methods are designed to estimate the 3D hand poses in a canonical (localized) frame for a single hand. Therefore, it is necessary to generate a new dataset suitable for the task of RGB-based two-hand global pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ego3DHands Dataset</head><p>We introduce the first dataset for the task of two-hand global 3D pose estimation from an egocentric view. Following <ref type="bibr" target="#b34">[37]</ref>, the dataset is generated using rendering from Blender 1 , which enables us to obtain the segmentation masks of hand parts as well as the annotated 2D and 3D joint locations (infeasible to obtain on real hand data at large-scale with variety). We utilize a single character from Mixamo 2 to keep the bone ratios of the hands consistent for global 3D pose reconstruction. The dataset includes two versions for static and dynamic hand pose estimation respectively. Despite the domain gap between synthetic and real-world data, this dataset enables training for learningbased approaches and quantitative analysis for a new task. Data Representation. As illustrated in <ref type="figure" target="#fig_9">Fig. 2</ref>, the dataset provides 7 segmentation masks for each hand. The 2D joint locations are normalized values ranging from (0, 0) at the top left to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref> at the bottom right. The global 3D coordinates are represented in the camera space. We scale the 3D coordinates so that the bone length from the wrist to the middle metacarpophalangeal (mMCP) is 10 cm. Each hand consists of 21 joints: wrist and 4 joints for each finger. The Depth map is also provided but not used in this work. Static Ego3DHands. To capture the images for static hand poses, we set the camera to be between the eyes of the character facing forward. We keep the hands inside a fix-sized bounding box in front of the character so the targets stay in sight for estimation. Each hand has a 10% drop rate for single-hand scenarios. The rotational angles for arm and hand joints are randomized within reasonable rotational ranges to obtain vast variety in the pose space. We include 4 light sources with slightly randomized color, brightness and position for illumination. Additionally, for the background of the hand pose images, we selected 100 unique scene topics and collected 20,000 images from online sources, on which we further applied random color augmentation and horizontal flips. We create 50,000 instances for the training set and 5,000 instances for the test set. Dynamic Ego3DHands. For global dynamic two-hand 3D tracking, we introduce an additional dataset with 100 sequences for the training set, and 10 sequences for the test set. Each sequence consists of 500 frames where we 1 www.blender.org 2 www.mixamo.com <ref type="figure" target="#fig_9">Figure 2</ref>: Our dataset provides a total of 14 segmentation masks (right) for the fingers, palm and arm along with the 2D and 3D joint annotations. randomize independent motion for both hands. For background sequences, we selected 110 short videos with variety from Pexels 3 , so each hand pose sequence has a unique corresponding background sequence. This dataset enables researchers to explore methods for 3D global hand pose estimation that utilize temporal consistency. We report our baseline results in Section 5 for future comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this paper, we present the first algorithm capable of estimating the global 3D poses of both hands from a monocular RGB image. The overall system is demonstrated in <ref type="figure" target="#fig_0">Fig.  3</ref>. Given a single RGB image as input, we use HandSeg-Net to simultaneously obtain the segmentation masks and the heatmap energy of both hands. The hand heatmap energy indicates the approximate locations of the hands despite occlusion, which are used to detect and provide a cropped image for each hand. The cropped RGB hand images are then processed using the corresponding segmentation masks for the next stage. To estimate the 2D joint locations, we present P oseN et 2D that estimates and refines the 2D heatmaps of the joints in multiple stages. To lift the 2D heatmaps to a 3D pose estimation, we present P oseN et 3D that takes the heatmaps as input and regresses the 3D canonical joint locations which we define in detail in Section 4.3. Finally, we present a novel algorithm that accurately estimates the 3D global hand joint locations in the spherical coordinate system using the obtained 2D and 3D canonical information, the actual length of key bone and the camera intrinsics. Our method can theoretically be applied to estimate the global location and pose of any object given the aforementioned information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Two-hand Segmentation and Detection</head><p>Unlike existing methods that perform pose estimation on a single cropped hand, we need to first distinguish between left and right hand by estimating the individual hand locations. For the task of hand segmentation and detection, we use a deep convolutional neural network trained to predict both the segmentation masks and the location of hands in form of heatmap energy. We show in Section 4.2 that the accurate segmentation of hands is necessary information for For the architecture of HandSegNet, we use a residual network for the task of semantic segmentation and hand detection. It consists of 4 downsampling and 4 upsampling layers comprised of 16 residual blocks. For the output layer, we have 3 channels for the task of segmentation (2 objects and background) and 2 additional channels for estimating the heatmap energy of the left and right hand. The heatmap energy is capable of providing high activation at locations of partially or even completely occluded hands. To generate the cropped bounding boxes, we apply Otsu Thresholding on the hand energy for selecting the high activation area. In the case of very low activation, we classify the corresponding hand(s) as being absent and drop the absent hand(s) in the subsequent stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">2D Canonical Hand Pose Estimation</head><p>The goal of P oseN et 2D is to estimate the 2D joint locations given a cropped hand image. We use a variant of Convolutional Pose Machines (CPM) <ref type="bibr" target="#b28">[31]</ref> as our base model, with batch normalization layers inserted after the convolutional layers for better adaptation to the vast RGB image space. The 2D joint locations are represented as heatmaps and CPM refines the output heatmaps in progressive stages. Since the left and right hand have different articulation, we horizontally flip the cropped images of the right hand so the learned articulation remains consistent for the model. We resize the cropped input hand images from HandSegNet to 256x256. The output of the CPM consists of 21 heatmaps with size of 32x32. We generate stronger heatmap energy for closer joints so that depth information is encoded into the 2D heatmaps. We show that this heatmap generation technique (we refer to as z-heatmaps) improves accuracy for 3D canonical hand pose estimation in Section 5.</p><p>Previous work <ref type="bibr" target="#b10">[13]</ref> showed good performance on RGBbased 2D hand pose estimation with occlusion introduced by an interacting object. However, the method fails when the background has similar appearance as the hand. With the presence of both hands, accurate 2D hand pose estimation becomes more challenging due to the similar-object occlusion introduced by the secondary hand. We show in <ref type="figure" target="#fig_1">Fig.  4</ref> that we successfully address this issue by providing the segmentation information necessary for distinguishing the left and right hand. By using the segmentation masks of the two hands, we simplify the input image space by removing the background noise; additionally, we differentiate the color space between two hands by reducing the brightness of the secondary hand by a factor of 0.5. As a result, it is very important for HandSegNet to produce accurate segmentation masks for the two hands. The output heatmaps are used as inputs for P oseN et 3D and we retrieve the resulting 2D global joint locations using the bounding box coordinates from HandSegNet. For a set of 2D global joint locations, we use p j = (ρ r j , ρ c j ), where ρ r and ρ c represent the corresponding row and column position of the j th joint in form of percentages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D Canonical Hand Joints Regression</head><p>The 3D canonical frame for a single hand is defined such that the middle metacarpophalangeal (mMCP) joint is at the origin and the distance between wrist and mMCP is 1 <ref type="bibr" target="#b10">[13]</ref>. The defined canonical frame requires the target hand to be in the middle of the cropped image so the z-axis aligns with the camera direction. Therefore, we generate the 3D canonical joint annotation for training by rotating the original global 3D joint locations of both hands to the center of the image; zero-centering on the mMCP and normalization is applied afterwards. Thus, for a set of annotated canonical 3D Cartesian coordinates represented as w j = (x j , y j , z j ),</p><formula xml:id="formula_0">w center = R · w glob w can = (w center − w center mmcp )/d<label>(1)</label></formula><p>where R is the 3D rotational matrix for centering w can mmcp and d is the distance between the wrist joint and the mMCP. As a result, our 3D canonical hand poses are consistent with the visual representation of the hands, which is necessary for estimating the global joint locations and will be explained in Section 4.4.</p><p>For the architecture of P oseN et 3D , we use a small residual network comprised of 8 residual blocks with 2 fully connected layers before the output layer. The input heatmaps are upscaled by a factor of 2 to the size of 64x64 for better performance. The model estimates the root-relative 3D coordinates of 21 joints for each hand.</p><p>To enforce physical constraints and encourage 2D pose consistency between w can and p, we employ the following loss function for training P oseN et 3D ,</p><formula xml:id="formula_1">L 3d = L j + L bone + L proj<label>(2)</label></formula><p>where L j is the Mean Squared Error (MSE) loss for joint regression. In addition, we introduce L bone that indicates the MSE between the ground truth and the predicted bone lengths. L proj indicates the MSE between the (x, y) component of w can and p projected into the same canonical frame. The overall L 3d aims to produce physically plausible 3D canonical hand poses consistent with the 2D poses. For the task of 3D canonical hand pose estimation only (Section 5.1), we expect P oseN et 3D to refine the potentially inaccurate 2D pose estimations and drop L proj . However, for the task of 3D global hand pose estimation in the next stage, we replace p from P oseN et 2D with the (x, y) component of w can projected back into the pixel space since the consistency between 2D and 3D poses is important for our projection algorithm (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">3D Global Hand Pose Estimation</head><p>The problem of 3D global hand pose estimation in Cartesian coordinate system introduces different challenges compared to conventional 3D canonical hand pose estimation. First, the same canonical hand pose in different global positions is visually rotated, thus introducing rotational ambiguity. Second, the size of the hand correlates not to the z-value of its global Cartesian 3D position, but to its absolute distance to the camera origin, which introduces The bone plane view (bottom) illustrates how we compute the absolute distance r of the centered root joint and subsequently return the pose to its original global 3D position by rotation in spherical coordinate system. depth ambiguity. As a result, we propose a novel algorithm for global 3D hand pose estimation using the spherical coordinate system. As illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>, in order to transform the 3D canonical hand pose back to its original 3D global position, we first scale it by the known actual key bone length L to the real-world size, then translate it by r cm in the positive direction along the z-axis, and finally apply a 3D global rotation with θ mmcp and φ mmcp respectively in the spherical coordinate system. To compute a set of global 3D Cartesian coordinates w glob given w can in 3D and p in 2D, we find the absolute spherical coordinate of the mMCP v mmcp = (r mmcp , θ mmcp , φ mmcp ). For the rotational angles,</p><formula xml:id="formula_2">θ mmcp = atan(((p r mmcp · H) − H/2)/pxcm, foc) φ mmcp = atan(((p c mmcp · W) − W/2)/pxcm, foc)<label>(3)</label></formula><p>where H and W represent the height and width of the RGB input image, pxcm is a constant conversion factor for converting from image pixels to centimeters and foc is the camera focal length. In order to compute r mmcp , we need to apply the Side Splitter Theorem on the right-angled similar triangles shown on the key bone plane <ref type="figure" target="#fig_2">(Fig. 5 (2)</ref>),  where (x can s , y can s , z can s ) is the 3D canonical position for joint s. Finally, we compute the spherical radius of the mMCP,</p><formula xml:id="formula_3">z 3d = z 2d · h 3d /h 2d<label>(4)</label></formula><formula xml:id="formula_4">r mmcp = z 3d − z can s · L<label>(6)</label></formula><p>with z can s being positive if the key bone extends away from the camera.</p><p>Since the key bone needs to have sufficient length in 2D images for accurate projection and estimation, we use mMCP as the primary joint for the key bone, and select either the wrist or the pinky MCP as the secondary joint. By selecting the longer one of the two "bones" as the key bone, we guarantee its validity since the two "bones" can never be parallel and therefore can never both point in the direction of z-axis in 2D images.</p><p>For tracking both hands in 3D global space in video settings with temporal smoothness, we apply polynomial regression for the estimation of r mmcp . We use two queues (for left and right hands) to store the most recent r mmcp values to estimate the current r mmcp .</p><p>Our 3D global pose estimation algorithm can be applied generally to estimate the 3D global location of any objects given the 2D, 3D canonical information, actual key bone length and camera intrinsics. Unlike other methods <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b15">18]</ref> that attempt to estimate the approximate global poses, our algorithm is, to our knowledge, the first capable of perfectly reconstructing the global 3D poses given accurate input information. We show its effectiveness in Section 5.2 on several hand pose datasets (both synthetic and real) with annotated 3D global joint locations and different viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first compare the performance of P oseN et 2D and P oseN et 3D for single-hand 3D canonical pose estimation with the current state-of-the-art methods on two popular benchmark datasets: Stereo Tracking Benchmark Dataset (STB) <ref type="bibr" target="#b32">[35]</ref> and Rendered Hand Pose Dataset (RHP) <ref type="bibr" target="#b34">[37]</ref>. For two-hand global 3D pose estimation, we evaluate our method on the test sets of both the static (Ego3D s ) and the dynamic (Ego3D d ) versions of Ego3DHands. To demonstrate the effectiveness of our global pose estimation algorithm, we show quantitative and qualitative results for global hand pose estimation on all 4 target datasets. Training details are included in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single-hand Canonical Pose Estimation</head><p>Stereo Tracking Benchmark Dataset (STB) consists of 12 sequences (1500 frames per sequence) of captured single-hand motion of 1 subject with 6 different backgrounds and lighting. Stereo and depth images are provided, but only the RGB images from the left camera are used in this work. We follow the same evaluation protocol as <ref type="bibr" target="#b34">[37]</ref>, training on 10 and testing on the other 2 sequences. For evaluation of the 3D pose estimation, the 3D canonical pose needs to be scaled to its actual size and transformed to its global position using the ground truth root joint. Other methods scale w can by L and simply translate w can mmcp to w glob mmcp (Cartesian alignment), which assumes that there is no rotational discrepancy since the hand is relatively close to the center of the camera. We align our canonical hand poses to global hand poses by spherical alignment. Specifically, we scale w can by L, apply translation in the z-axis and rotation in the spherical coordinate system to align w can mmcp with w glob mmcp . In <ref type="figure">Fig. 6a</ref>, we perform various experiments for self-comparisons to justify our design choices, reporting the Area Under Curve (AUC) computed using the Percentage of Correct Keypoints (PCK), where a predicted keypoint is correct if its location is within the threshold radius around the ground truth. We also report the End Point Error (EPE) in mm. For comparison with other methods, we show  0.940 11.33 13.41 Baek et al. <ref type="bibr" target="#b2">[5]</ref> 0.926 --Ge et al. <ref type="bibr" target="#b6">[9]</ref> 0.920 --Cai <ref type="bibr" target="#b3">[6]</ref> 0.887 --Yang et al. <ref type="bibr" target="#b30">[33]</ref> 0. in <ref type="figure">Fig. 6b</ref> that we outperform most state-of-the-art methods with an AUC = 0.995. It is worth mentioning that fair comparison cannot be made since many methods utilized depth information <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b3">6,</ref><ref type="bibr" target="#b7">10]</ref>, deformable hand model (guaranteed physical plausibility) <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b2">5]</ref> or additional datasets <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b29">32]</ref> during training while we simply trained on the left RGB images in the training set of STB with color augmentation. Rendered Hand Pose Dataset (RHP) provides 41258 images for training and 2728 images for evaluation. Each rendered image contains a single character performing 1 of 39 gestures and the view is focused on one of the two hands. The training and test sets contain 31 and 8 distinct gestures respectively. We use the same setting as our method for the STB dataset. Since the pose variation in this dataset is very limited, we perform data augmentation by rotating the images with random angles. As shown in Tab. 1, we achieve an AUC of 0.929 and a top AUC of 0.942 with utilization of the segmentation information. Note that <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b3">6]</ref> leverage on depth data (more information than segmentation since the background has infinite depth) for training and <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b2">5]</ref> utilize a deformable hand model and additional datasets. Our RGB-only method outperforms other state-of-the-art meth-ods that utilize various additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Two-hand Global Pose Estimation</head><p>We first provide quantitative results on Ego3D s for all relevant subparts using the ground truth inputs for isolated analysis, then provide evaluation on the complete cascaded pipeline on both Ego3D s and Ego3D d . Results for the two hands are combined by taking the average for simplicity. Extensive ablation studies are also performed. HandSegNet. For the segmentation of the two hands, we report mean Intersection over Union (mIoU) of 0.955 and 0.962 on Ego3D s -test and Ego3D d -test respectively. For hand detection, the ground truth bounding boxes are determined by the annotated 2D joint locations. We report 2 metrics for the task of hand detection: the hand detection accuracy for how well the model correctly classifies the presence of the left and right hand; the bounding box detection accuracy for how accurately the model determines the location of the left and right hands. We report a hand detection accuracy of 1.00 and bounding box detection accuracy of 0.965 and 0.982 for Ego3D s -test and Ego3D d -test respectively, where a positive is scored when the IoU between the ground truth and the predicted bounding boxes is greater than 0.5. PoseNet 2D . We compute 2D PCK using the ground truth and the predicted global 2D joint pixel locations. <ref type="figure" target="#fig_6">Fig. 7a</ref> shows the 2D PCK of P oseN et 2D on Ego3D s -test and Ego3D d -test. Additionally, we perform ablation studies by comparing the 2D PCK of P oseN et 2D on various settings. We show that the hand segmentation and inserted batch normalization layers both lead to noticeable improvement. PoseNet 3D . We show the canonical 3D PCK of P oseN et 3D on Ego3D s -test and Ego3D d -test in <ref type="figure" target="#fig_6">Fig. 7b</ref>. We transform the 3D canonical hand poses to the global 3D space with spherical alignment for evaluation. We also perform ablation studies by comparing the results of training using different training losses and heatmaps. We point out that L bone decreases the average bone length error from 3.7mm to 1.8mm despite having slightly worse AUC and EPE. 3D Global Pose Estimation. For this new task, it is nec- essary that we evaluate the global pose estimation accuracy using PCK in the spherical coordinate system for more intuitive results. Specifically, the spherical PCK evaluates directional accuracy and distance accuracy of the root joint (mMCP). We claim that the spherical PCK on the root joint and PCK for the 3D canonical pose estimation together produce comprehensive evaluation results for the task of 3D global hand pose estimation. We skip the spherical PCK plot for isolated study since our 3D global pose estimation algorithm perfectly reconstructs the global 3D poses given the ground truth w can in 3D and p in 2D. The task for the complete cascaded pipeline is particularly difficult not only due to each module being dependent on the accuracy of the prior estimation, but also the fact that any error on the 2D or 3D canonical pose estimation can directly impact the global projection and decrease the final accuracy. <ref type="figure" target="#fig_6">Fig. 7c</ref> shows the spherical PCK of our complete pipeline on Ego3D s -test and Ego3D d -test. Note that L proj improved the overall spherical PCK for 3D global pose estimation despite leading to slightly worse performance in 3D canonical pose estimation.</p><p>We demonstrate that global hand pose estimation through monocular RGB input is achievable and we show promising results. For Ego3D s and Ego3D d , our method achieves a directional and distance accuracy of 0.90 approximately at an angular threshold of 3 degrees and a radius threshold of 7 cm respectively. Note that hand poses with differences of 7 cm in distance with respect to the camera origin show little difference visually in 2D images but there is definitely room for improvement.</p><p>Our global estimation algorithm also generalizes for other datasets. We report the spherical PCK on STB in <ref type="figure">Fig.  6c</ref> for global pose estimation without accessing the ground truth location of the root joint. Our results on STB indicates that our method is capable of accurate global pose estimation on real-world data as well if sufficient training data is available. For RHP, we report an AUC of 0.960, 0.958 and 0.690 for the spherical PCK of θ, φ and radius respectively. This dataset is challenging for accurate distance estimation due to its low image resolution. We show qualitative results for 3D global hand pose estimation on 4 datasets in <ref type="figure" target="#fig_7">Fig.  8</ref>. We provide additional qualitative results and preliminary evaluation on real-world data in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work we present the first method that estimates the 3D global poses for both hands given only a single RGB image. We contribute a large-scale synthetic egocentric hand pose dataset for training and evaluation of the networks. We show that our approach outperforms methods that utilize additional information for single-hand 3D canonical hand pose estimation and further achieves promising results for two-hand 3D global hand pose estimation. Evaluation on real-world data remains as necessary future work, which requires sufficiently annotated training data in the real-world domain.</p><p>Supplementary Document: Two-hand Global 3D Pose Estimation using Monocular RGB</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Training Details</head><p>To obtain the experimental results for all targeting datasets, we use the same training schedule for HandSegNet, P oseN et 2D and P oseN et 3D . Specifically, we use the Adam optimizer with an initial learning rate of 0.001, β 1 = 0.9 and β 2 = 0.999. We use cross entropy (CE) loss for the segmentation loss of HandSegNet and mean squared error (MSE) loss for the training of all other parts of the networks. We set the batch size to 4 and trained HandSegNet for 30,000 iterations. P oseN et 2D and P oseN et 3D are trained for 15,000 iterations since each iteration consists of training of two separate hands. The learning rates decrease with a rate of 0.5 every 5,000 iterations.</p><p>For results obtained without segmentation and batch normalization layers using P oseN et 2D , we used standard stochastic gradient descent and an initial learning rate of 0.000001 for better convergence. We also set the weight decay to 0.0005 for P oseN et 3D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Additional Qualitative Results</head><p>For evaluation on real-world data, we manually annotated a small dataset to train the networks and provide preliminary qualitative results in <ref type="figure" target="#fig_8">Fig. 1</ref>. We evaluate on simple poses due to the limited variety in our annotated data. Our preliminary results indicate that our method is capable of evaluation on real-world data when sufficient training data in the real-world domain becomes available in the future. Note that evaluation on the RGB data in the real-world domain is extremely challenging due to factors such as the vast color space, different skin color/texture, complex background noise, motion blur, lighting, shadow features, etc. As a result, evaluation on videos in the wild is beyond the scope of this paper and will be addressed in our future works. It is worth mentioning that since it is currently infeasible to quantitatively evaluate on the task of RGB-based two-hand global 3D pose estimation using real-world data due to the lack of ground truth data, Ego3DHands serves as a necessary benchmark dataset for this new task. We also provide additional qualitative results for the 4 target datasets in <ref type="figure" target="#fig_9">Fig. 2</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Overview of proposed pipeline for two-hand global pose estimation from monocular RGB. Given a RGB image, we segment and detect the hands, crop and process the hand images for 2D canonical pose estimation despite inter-hand occlusion, and use the 2D heatmaps to estimate 3D canonical poses. For the final step, We introduce a novel algorithm for computing the 3D global hand poses using the 2D and 3D canonical estimation as well as the actual bone lengths.2D hand pose estimation in the presence of occlusion from the other hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>We show that 2D canonical hand pose estimation with segmentation information (top) better resolves interhand occlusion from the secondary hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The camera view (top) shows how we treat the hand as if it is in the center of the image and generate the corresponding 3D canonical pose using Eq. (1) for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Self-comparisons.(b) Comparison with the state-of-the-art.(c) Spherical PCK for global estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 : 2</head><label>62</label><figDesc>Self-comparisons (left)  and comparison with the state-of-the-art (middle) for 3D canonical hand pose estimation on the STB dataset. + indicates that the feature is applied incrementally. Spherical PCK (right) without alignment with the ground truth root joint is also reported.where z 2d and h 2d are computed by rotating p s (the secondary joint s that forms the key bone with mMCP) with −θ mmcp and −φ mmcp . For h 3d , the segment of key bone perpendicular to the z-axis ish 3d = x can s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) 2D Pose Estimation.(b) 3D Canonical Pose Estimation.(c) Spherical PCK for global estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Quantitative results on the Ego3D s and Ego3D d for 2D (a), 3D canonical (b) and 3D global (c) hand pose estimation. 2D PCK is computed using image size of (270x480). Results are reported given the ground truth input for isolated studies unless marked as "*complete" in (a) and (b). "dynamic" indicates that experiments are performed on Ego3D d . Method 3D Hand Pose Estimation AUC ↑ EPE (mm) median ↓ mean ↓ Iqbal et al. [10]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results for 3D global hand pose estimation on 4 datasets. Top row visualizes the 3D global hand poses from the center camera view. Middle and bottom rows show the top and side views respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Preliminary qualitative results on real-world data. We collected sample test sequences using 3 different background environments. Top row visualizes the 3D global hand poses from the center camera view. Middle and bottom rows show the top and side views respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :</head><label>2</label><figDesc>(a) Ego3Ds(img1)(b) Ego3D d (img1) (c) STB (img1) (d) RHP (img1) (a) Ego3Ds(img2) (b) Ego3D d (img2) (c) STB (img2) (d) RHP (img2)Additional qualitative results for 3D global two-hand pose estimation on Ego3D s , Ego3D d , STB and RHP.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">www.pexels.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hololens2</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/hololens" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Htc Vive</surname></persName>
		</author>
		<ptr target="https://developer.vive.com/us/viveport" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pushing the Envelope for RGB-Based Dense 3D Hand Pose Estimation via Neural Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-Supervised 3D Hand Pose Estimation from Monocular RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Firstperson Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View CNN to Multi-View CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D Hand Shape and Pose Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hand Pose Estimation via Latent 2.5D Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Normalized Diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wangni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camera Distanceaware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verschoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Otaduy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands Deep in Deep Learning for Hand Pose Estimation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Full DOF Tracking of a Hand Interacting with an Object by Modeling Occlusions and Physical Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2088" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embodied Hands: Modeling and Capturing Hands and Bodies Together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hand Keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-modal Deep Variational Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2456" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time Hand Tracking Using a Sum of Anisotropic Gaussians Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="319" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascaded Hand Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Articulated Distance Fields for Ultra-Fast Tracking of Hands Interacting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>SIG-GRAPH Asia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<title level="m">Capturing Hands in Action Using Discriminative Salient Points and Physics Simulation. IJCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">6D Hands: Markerless Hand-tracking for Computer Aided Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popoviât</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
		<meeting>of UIST</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular Total Capture: Posing Face, Body, and Hands in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling Latent Hands for Image Synthesis and Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Big-Hand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07214</idno>
		<title level="m">3D Hand Pose Tracking and Estimation Using Stereo Matching</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06854</idno>
		<title level="m">Model-based Deep Hand Pose Estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Hand Pose from Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
