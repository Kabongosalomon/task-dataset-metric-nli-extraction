<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantisation and Pruning for Neural Network Compression and Regularisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimessha</forename><surname>Paupamah</surname></persName>
							<email>kimessha.paupamah1@students.wits.ac.za</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of the Witwatersrand Johannesburg</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>James</surname></persName>
							<email>steven.james@wits.ac.za</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of the Witwatersrand Johannesburg</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Klein</surname></persName>
							<email>richard.klein@wits.ac.za</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">University of the Witwatersrand Johannesburg</orgName>
								<address>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantisation and Pruning for Neural Network Compression and Regularisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-deep learning</term>
					<term>neural networks</term>
					<term>compression</term>
					<term>regularisation</term>
					<term>pruning</term>
					<term>quantisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are typically too computationally expensive to run in real-time on consumer-grade hardware and low-powered devices. In this paper, we investigate reducing the computational and memory requirements of neural networks through network pruning and quantisation. We examine their efficacy on large networks like AlexNet compared to recent compact architectures: ShuffleNet and MobileNet. Our results show that pruning and quantisation compresses these networks to less than half their original size and improves their efficiency, particularly on MobileNet with a 7× speedup. We also demonstrate that pruning, in addition to reducing the number of parameters in a network, can aid in the correction of overfitting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Designing deep and complex neural networks is common practice for effective performance on various applications, particularly visual tasks like image classification <ref type="bibr" target="#b0">[1]</ref>. As neural networks become larger and deeper, more computational resources are required to train and store them, making it increasingly more difficult to deploy these networks on the consumergrade hardware, mobile and embedded devices in use today. In addition to requiring a large amount of computational resources, deep neural networks take up tremendous amounts of energy, leaving a large carbon footprint. For example, <ref type="bibr" target="#b1">[2]</ref> show that training certain deep natural language processing (NLP) models can result in as much CO 2 emissions as five cars in their lifetime. Small, powerful neural networks would help overcome these problems.</p><p>We can employ methods of neural network compression to obtain small and efficient neural networks which consume much less energy and can consequently be deployed to devices with limited computing capabilities. Large neural networks often contain many redundant parameters that have no impact on the network <ref type="bibr" target="#b2">[3]</ref>. Removing or pruning these redundant parameters result in networks with lower complexity. Quantisation is a further approach to reduce the size of neural networks by lowering the number of bits required to represent parameters. Another approach to obtain small networks is to This work is based on the research supported in part by the National Research Foundation of South Africa (Grant Numbers: 118075 and 117808). directly build smaller, efficient network architectures. These compact architectures, like MobileNet <ref type="bibr" target="#b3">[4]</ref> and ShuffleNet <ref type="bibr" target="#b4">[5]</ref>, perform computationally efficient operations and produce networks that are small in size, which makes them easy to deploy on mobile and embedded devices.</p><p>These approaches of obtaining smaller networks fall into two categories: either reduce the size of large networks, or directly train small, compact networks. The aim of this work is to compare these two approaches and examine their sensitivity to compression techniques in terms of accuracy, size, and inference time. Furthermore, we examine the effects of pruning as a means of correcting overfitting. We conduct our experiments on the CIFAR-10 <ref type="bibr" target="#b5">[6]</ref> and FashionMNIST <ref type="bibr" target="#b6">[7]</ref> datasets. We find that overfitted networks benefit from pruning, and that compact architectures outperform large, compressed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>This section aims to provide the background necessary for understanding neural network compression and the methods thereof. We give a brief overview of neural networks and convolutional neural networks, followed by a discussion on separable convolutions then network pruning and quantisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural Networks</head><p>A typical feedforward neural network is composed of a number of artificial neurons which are organised into layers, the first being the input layer while the last being the output layer. The layers between are the hidden layers, which form the capacity of the network. Neurons that reside in a layer are linked to neurons in the subsequent layer by weighted connections. These weights form the parameters of the neural network.</p><p>An artificial neuron performs some mathematical operation, usually by taking the dot product of the input connections and passing it through some activation function to fire an output through an output connection, consequently weighting the connection <ref type="bibr" target="#b7">[8]</ref>. These output connections form the input connections for neurons in the subsequent layer and so the output is propagated to other connected neurons to give the final output of the network. This process is called a forward pass or forward propagation.</p><p>A neural network is trained by learning its parameters. A common method for learning the parameters of a neural network is through backpropagation <ref type="bibr" target="#b8">[9]</ref>. First, a forward pass of the network occurs to predict the final output, and a loss function is used to measure the error of the prediction. Optimisation techniques like gradient descent are used to minimise the loss and find optimal weights for the connections; however, the partial derivatives of the loss function are required. Backpropagation is a method used to compute these partial derivatives by propagating the loss from the output layer back through the network to the input layer and computing how each neuron contributes to the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Networks</head><p>Convolutional neural networks are similar to feedforward neural networks, except their layers are composed of convolutional layers which have a height, width and depth. A convolutional layer performs convolutions with input to extract features, and so has of a set of filters (or weights) which are learnt during training <ref type="bibr" target="#b7">[8]</ref>. Mathematically, a convolution operation with an image can be described by</p><formula xml:id="formula_0">(I * K)[i, j] = m−1 p=0 n−1 q=0 I[i − p, j − q]K[p, q]<label>(1)</label></formula><p>for an image I, of size M × N , and kernel K, of size m × n.</p><p>The image is convolved with a kernel by sliding the kernel across each pixel in the image and taking the dot product of the kernel elements and the pixel values aligned with the kernel. We convolve a stack of kernels, or filter, of the same depth as the number of colour channels in the image. A number of filters can be convolved with the image, each producing an output channel. The convolutional layer arranges neurons in a threedimensional grid. Neurons in the convolutional layer are only connected to a local region of the input. This region is called the receptive field of a neuron and is the same size as the filter used. These neurons work similarly to neurons in feedforward networks by convolving the filter with a local region of the input, then passing it through an activation function to give output channels which are stacked together to form a feature map. Other popular layers in a convolutional neural networks include Batch Normalisation <ref type="bibr" target="#b9">[10]</ref> and Dropout <ref type="bibr" target="#b10">[11]</ref> layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Separable Convolutions</head><p>Consider an input size of w in × h in × d, convolved with a filter of size N ×k×k×d, to give a feature map of size w out × h out × N . This standard convolution has a computational cost of w in × h in × d × N × k × k. We look at reducing this computational cost with depthwise separable convolutions and group convolutions.</p><p>1) Depthwise Separable Convolutions: Depthwise separable convolutions <ref type="bibr" target="#b11">[12]</ref> first perform a depthwise convolution followed by a pointwise convolution. Depthwise convolutions perform convolutions along the input channels separately. Each filter is sliced into d separate k × k kernels, and each kernel is then convolved with its own input channel. The output of each convolution is stacked together to form an output layer of size w out × h out × d × N . To transform this output layer to a single feature map of size w out × h out × N , a pointwise convolution is performed by convolving this layer with a 1 × 1 × d filter N times. This process is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> using one filter (N = 1). The total computation cost can be calculated as</p><formula xml:id="formula_1">(w in × h in × d × k × k) + (w in × h in × d × N )</formula><p>which results in a significant reduction in computation.</p><p>2) Group Convolutions: Group convolutions [13] operate by dividing filters into different groups, with each filter group being convolved with a different part of the input layer of a certain depth. A filter can be divided into g separate groups along its depth. This results in g groups, with each group consisting of N/g separate filters of size k × k × d/g. The same is done to the input layer to get g separate layer groups of w in ×h in ×d/g. Each N/g group is convolved with an input layer group of the same depth, to get an output layer of size w out × h out × N/g. These resulting output layers from each group convolution are stacked together to obtain the resultant feature map. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates this with two groups (g = 2). The filters and input layer are divided into two groups: the first filter group convolves with the first half of the input, while the second filter group convolves with the second half of the input. Grouped convolutions reduces the computational cost to g×(w in ×h in ×d/g×N/g). Since the convolutions are divided, group convolutions also allow for efficient computation as each convolution can be handled in parallel, on a separate GPU for instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Pruning</head><p>The general procedure for pruning a trained neural network is locating which parameters have no significant impact on  the network and then removing those redundant parameters. The network is then retrained after this pruning process so that the remaining parameters in the network are adjusted to compensate for those removed. In this work, we focus on iterative pruning as introduced in <ref type="bibr" target="#b13">[14]</ref>. This iterative procedure is a three-step method. The first step of the method fully trains the neural network to learn the parameters (or connections). Once the network is fully trained, the second step of the method is to learn which connections in the network are important. These important connections are learnt iteratively, where an iteration involves pruning connections with weights below a threshold and then retraining the network. The threshold value is found manually, by determining which layers are sensitive to pruning. After this pruning stage, neurons which have no input or output connections are also removed, and hence all further connections to and from the pruned neuron are removed. This results in a sparse neural network, with the unimportant connections pruned away and important connections preserved. The final step of this method is to retrain the resulting sparse network. During the retraining stages, the weights are not re-initialised to ensure that gradient descent finds a good solution. This is also computationally cheaper since there is no need to backpropagate through the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Quantisation</head><p>Network quantisation is a method of reducing the precision of weights and activations in neural networks by lowering the number of bits to represent these quantities <ref type="bibr" target="#b14">[15]</ref>. This is a quick and efficient way to reduce a network's size without the need for retraining. We can quantise a parameter x according to the mapping</p><formula xml:id="formula_2">Q(x, ∆, z) = round( x ∆ + z).<label>(2)</label></formula><p>This maps floating-point values to integers, which in turn lowers the number of bits required to represent a parameter. The scale (∆) indicates the step size of the quantiser, while a floating-point zero maps to an integer z, the zero-point, so that zero can be quantised with no error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>Early pruning methods like Optimal Brain Damage <ref type="bibr" target="#b15">[16]</ref> and Optimal Brain Surgeon <ref type="bibr" target="#b16">[17]</ref> pruned shallow, fully-connected networks based on saliency values, which indicate the effect of a parameter on the training error. These values are computed by approximating the Hessian matrix which is computationally expensive for the large, deep networks in practice today. Recent works have studied pruning larger and deeper neural networks. An iterative pruning method which results in sparse networks is introduced in <ref type="bibr" target="#b13">[14]</ref>. ThiNet <ref type="bibr" target="#b17">[18]</ref> introduces a filter pruning technique which removes entire filters in convolutional neural networks. The authors propose a method using a least-squares approach to find channels corresponding to unimportant filters. These weak channels in the next layer of a network are found and their corresponding filters in the current layer are pruned away. This thins down the original wider network and reduces the computational cost compared to magnitude pruning. Additionally, no sparsity is introduced into the network, and so the original network structure remains intact. Bayesian Compression <ref type="bibr" target="#b18">[19]</ref> introduce a Bayesian approach to pruning. Sparsity inducing priors are used to prune entire weight structures rather than individual parameters. This results in very sparse networks with high compression rates.</p><p>Other means of reducing the number of weights or parameters in neural networks are methods of weight sharing and quantisation. Weight sharing methods allow weights within layers of the network to be shared, while quantisation simply represents weights with a lower number of bits. This leads to a more accelerated network with reduced complexity. HashNets <ref type="bibr" target="#b19">[20]</ref> is a neural network architecture that operates by grouping weights together into hash buckets using a hash function. The assignment of weights to connections are determined by a hash function so that all connections grouped to the same hash bucket share the same weight. Deep Compression <ref type="bibr" target="#b20">[21]</ref> employs both weight sharing and quantisation as an additional compression method after pruning networks. This is done by clustering all the weights within a layer, then approximating each weight to the closest cluster centroid, lowering the number of bits needed to store each weight.</p><p>There has also been interest in building efficient architectures. Like MobileNet and ShuffleNet, these compact architectures perform several varieties of convolutions to reduce computational cost. GoogLeNet <ref type="bibr" target="#b21">[22]</ref> allows for the increase in depth and width of a network while maintaining a constant computational cost. The architecture uses Inception modules which combines convolutions at different scales to help with dimensionality reduction. SqueezeNet <ref type="bibr" target="#b22">[23]</ref> designs very small networks with bottleneck layers that squeeze input and expands it afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>We have outlined two approaches of obtaining small networks: reducing the size of large networks through network compression or directly building small and efficient compact architectures. We compare these two opposing ideas to obtain an understanding of each method and which to employ in practice. Our comparison also examines the response of compression on small, compact architectures in an effort to understand how they are impacted. Additionally, we investigate the effects of correcting overfitted networks with pruning, to determine whether pruning can be used as an effective regularisation technique. This section outlines our approach to neural network compression and the overfitting of networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Network Architectures</head><p>We test our experiments on the CIFAR-10 <ref type="bibr" target="#b5">[6]</ref> and Fashion-MNIST <ref type="bibr" target="#b6">[7]</ref> datasets. We choose to use AlexNet <ref type="bibr" target="#b12">[13]</ref> as our large network as it contains tens of million parameters and can fit onto the hardware available to us. AlexNet is an eight-layer deep network, containing five convolutional layers followed by three fully-connected layers. For our compact architectures we use the improved state-of-the-art MobileNetV2 <ref type="bibr" target="#b23">[24]</ref> and Shuf-fleNetV2 <ref type="bibr" target="#b24">[25]</ref> architectures. The MobileNetV2 architecture falls under the class of MobileNet architectures, and similarly the ShuffleNetV2 architecture falls under the class of Shuf-fleNet architectures, and so shall be referred to as MobileNet and ShuffleNet respectively. MobileNet's input layer is fully convolutional, followed by eighteen inverted residual block hidden layers, which perform depthwise separable convolutions. The output layer is a single fully connected layer to perform classification. ShuffleNet's architecture is composed of three stages, each having a repeated stack of inverted residual blocks, which perform pointwise group convolutions with channel shuffling followed by a depthwise convolution, and then another pointwise convolution. The input layer of the network is fully convolutional, while the output layer is a single fully-connected layer. These networks were trained from scratch and used as our reference networks in our experiments. They were trained with a batch size of 50 and optimised using stochastic gradient descent with a momentum of 0.9 on both datasets. The learning rate decayed during training, with AlexNet starting with a learning rate of 0.001 while MobileNet and ShuffleNet started with a learning rate of 0.01. Our experiments implemented in PyTorch <ref type="bibr" target="#b25">[26]</ref> and run them on Nvidia GeForce GTX 1060 Ti and 1080 Ti GPUs. The source code is available online. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Compression</head><p>To compress our networks, we first iteratively prune them. A pruning iteration consists of pruning parameters that have no impact on the network then retraining the resulting sparse network. The pruned parameters are those with the smallest weights, and the number of parameters removed is determined by the network's sensitivity to pruning.</p><p>Our pruned models are further compressed by applying perchannel quantisation <ref type="bibr" target="#b14">[15]</ref>. Per-channel quantisation lowers the bits used to represent parameters along the depth (or channels) of a layer. Applying quantisation results in minor accuracy loss with a smaller network size, and a speedup in terms of training and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Overfitting</head><p>To overfit our networks, we remove all regularisation layers and train our networks until we completely learn our training data. In particular, we remove the Dropout layers from all 1 https://github.com/kpaupamah/compression-and-regularisation three networks and remove the BatchNorm layers from both MobileNet and ShuffleNet. Training our networks until we see a decrease in our validation accuracy, and an increase in train accuracy (of at least 99.9%), we can declare our networks as overfitted. We attempt to correct overfitting by pruning these networks for a better test accuracy than that of the overfitted networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NEURAL NETWORK COMPRESSION</head><p>We tested our compression experiments on both the CIFAR-10 and FashionMNIST datasets. The number of parameters to remove from each network were determined through sensitivity scans, as illustrated in <ref type="figure" target="#fig_0">Fig. 3 (CIFAR-10)</ref> and <ref type="figure" target="#fig_3">Fig. 4  (FashionMNIST)</ref>. We note that the sensitivity scans were done to find how many of the smallest weights could be removed without negatively impacting the network's performance, and so the networks were retrained with early-stopping and not fully retrained. MobileNet, ShuffleNet and AlexNet performed best with sensitivities of 0.4, 0.3 and 0.5 respectively, on CIFAR-10. With FashionMNIST, MobileNet responded well to a sensitivity of 0.7, while ShuffleNet could be pruned with a sensitivity of 0.35, and AlexNet performed best with a sensitivity of 0.9. These sensitivities were used in the final experiments.</p><p>Iteratively pruning each network on both datasets give our results in <ref type="table" target="#tab_0">Table I</ref>. We then quantise the resulting pruned networks to get our final compressed networks shown in <ref type="table" target="#tab_0">Table II</ref>. We retrained the pruned networks with uniformly re-initialising the parameter weights and with fine-tuning (retraining from the pruned parameter weights) the parameters. AlexNet did not respond well to re-initialisation and so the remaining parameters had to be fine-tuned. The compact architectures performed better with re-initialisation compared to fine-tuning, and so we retrained them with uniform reinitialisation.</p><p>We find that MobileNet and ShuffleNet are quite sensitive to pruning as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. These compact networks are    AlexNet, on the other hand, is a much larger network and proves to be less sensitive to pruning, indicating many redundant parameters. We removed 90% of AlexNet's parameters when trained on FashionMNIST and 50% of its parameters when trained on CIFAR-10, without a significant reduction in accuracy. We find that we get better compression rates on networks trained on FashionMNIST compared to CIFAR-10 largely due to FashionMNIST containing grayscale images of smaller size, and so network complexity can significantly be reduced. Quantising the pruned versions of MobileNet and AlexNet trained on CIFAR-10 resulted in a considerable reduction of in physical size and inference time, also without a significant loss in accuracy as shown in <ref type="table" target="#tab_0">Table II</ref>. Quanti-sation worked particularly well on MobileNet, leading to a 7.3× speedup from 34.80ms to 4.74ms on CIFAR-10 and a 5.7× speedup from 1.70ms to 0.30ms on FashionMNIST. Surprisingly, ShuffleNet trained on CIFAR-10 did not respond well to quantisation: while its size decreases, it suffers a very large accuracy loss with an increase in its inference time. This is possibly due to its more complex architecture and small size, leading quantisation to add more overhead overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. NEURAL NETWORK REGULARISATION</head><p>We trained our networks to overfit FashionMNIST as it required significantly less training time and computational resources compared to CIFAR-10. Once our networks completely overfitted the training data with over 99.9% training accuracy, we pruned each network until the test accuracy started to decrease. The results are from overfitting and then pruning each network are shown in <ref type="table" target="#tab_0">Table III</ref>. MobileNet, ShuffleNet and AlexNet were pruned with sensitivities 0.1, 0.15 and 0.6 respectively. <ref type="table" target="#tab_0">Table III</ref> demonstrates that pruning is a means to correct overfitting. The parameters pruned away results in a higher test accuracy than that of the overfitted network. This allows the network to generalise better and obtain a higher test accuracy overall. An advantage of using pruning as a regularisation technique is that it can be applied after training, rather than during training as there could be uncertainty as to whether regularisation is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Across both compact and large networks we demonstrated that pruning is an effective regularisation technique to correct overfitting. We have shown that the compact networks Mo-bileNet and ShuffleNet are still receptive to network pruning as a means of compression and the correction of overfitting, despite having relatively few parameters compared to larger networks such as AlexNet. We found that quantisation significantly decreased the size and computational requirements of AlexNet and MobileNet, but negatively impacted the performance of a more complex ShuffleNet architecture. Compared to a large, compressed network, we find that compact architectures consume less memory and storage, has better accuracy with faster training and inference times, and yet can still benefit from compression techniques with relatively few parameters. Our results suggest that employing compact architectures are more promising to compressing large networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. FUTURE WORK</head><p>This work focused on an iterative pruning method which introduced sparsity into networks. Pruning techniques like filter pruning do not introduce sparsity into the networks allowing them to maintain their original structure, leading to a smaller model size with faster inference times. Filter pruning removes entire filters from convolutional layers but leaves the fully-connected layers untouched. A future direction of research would be to explore the compression rates between these two methods and examine the trade-offs between sparse networks and non-sparse networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Example of a depthwise separable convolution on an RGB image with N = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Example of a group convolution with g = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Sensitivity of pruning networks trained on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Sensitivity of pruning networks trained on FashionM-NIST small and their parameters are less likely to be redundant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Network pruning on CIFAR-10 and FashionMNIST</figDesc><table><row><cell>Network</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Total Parameters</cell><cell cols="2">Compression Rate</cell></row><row><cell></cell><cell>CIFAR10</cell><cell>FashionMNIST</cell><cell>CIFAR10</cell><cell>FashionMNIST</cell><cell>CIFAR10</cell><cell>FashionMNIST</cell></row><row><cell>MobileNet -Reference</cell><cell>91.31</cell><cell>90.75</cell><cell>2.2M</cell><cell>2.2M</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNet -Pruned</cell><cell>91.53</cell><cell>90.43</cell><cell>671K</cell><cell>1.1M</cell><cell>1.6×</cell><cell>3.3×</cell></row><row><cell>ShuffleNet -Reference</cell><cell>93.36</cell><cell>90.36</cell><cell>1.2M</cell><cell>1.2M</cell><cell>-</cell><cell>-</cell></row><row><cell>ShuffleNet -Pruned</cell><cell>93.05</cell><cell>90.09</cell><cell>879K</cell><cell>815K</cell><cell>1.4×</cell><cell>1.5×</cell></row><row><cell>AlexNet -Reference</cell><cell>93.54</cell><cell>91.61</cell><cell>57M</cell><cell>57M</cell><cell>-</cell><cell>-</cell></row><row><cell>AlexNet -Pruned</cell><cell>90.91</cell><cell>90.34</cell><cell>28M</cell><cell>5M</cell><cell>2×</cell><cell>10×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Network quantisation of pruned models trained on CIFAR10 and FashionMNIST</figDesc><table><row><cell>Network</cell><cell cols="2">Accuracy (%)</cell><cell></cell><cell>Size (MB)</cell><cell cols="2">Inference Time (ms)</cell></row><row><cell></cell><cell>CIFAR10</cell><cell>FashionMNIST</cell><cell>CIFAR10</cell><cell>FashionMNIST</cell><cell>CIFAR10</cell><cell>FashionMNIST</cell></row><row><cell>MobileNet -Reference</cell><cell>91.31</cell><cell>90.75</cell><cell>8.7</cell><cell>8.7</cell><cell>34.80</cell><cell>1.70</cell></row><row><cell>MobileNet -Quantised</cell><cell>90.59</cell><cell>90.07</cell><cell>2.9</cell><cell>2.9</cell><cell>4.74</cell><cell>0.30</cell></row><row><cell>ShuffleNet -Reference</cell><cell>93.36</cell><cell>90.36</cell><cell>4.9</cell><cell>4.9</cell><cell>11.67</cell><cell>0.73</cell></row><row><cell>ShuffleNet -Quantised</cell><cell>81.29</cell><cell>89.78</cell><cell>1.8</cell><cell>1.8</cell><cell>23.15</cell><cell>0.61</cell></row><row><cell>AlexNet -Reference</cell><cell>93.54</cell><cell>91.61</cell><cell>217.6</cell><cell>217.6</cell><cell>22.13</cell><cell>6.70</cell></row><row><cell>AlexNet -Quantised</cell><cell>90.06</cell><cell>90.27</cell><cell>54.6</cell><cell>54.6</cell><cell>5.23</cell><cell>4.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Overfitted and pruned networks trained on Fash-ionMNIST</figDesc><table><row><cell>Network</cell><cell>Accuracy (%)</cell><cell>Total Parameters</cell></row><row><cell>MobileNet -Reference</cell><cell>90.75</cell><cell>2.2M</cell></row><row><cell>MobileNet -Overfitted</cell><cell>90.55</cell><cell>2.2M</cell></row><row><cell>MobileNet -Pruned</cell><cell>91.26</cell><cell>1.76M</cell></row><row><cell>ShuffleNet -Reference</cell><cell>90.36</cell><cell>1.2M</cell></row><row><cell>ShuffleNet -Overfitted</cell><cell>89.71</cell><cell>1.2M</cell></row><row><cell>ShuffleNet -Pruned</cell><cell>91.01</cell><cell>1M</cell></row><row><cell>AlexNet -Reference</cell><cell>91.61</cell><cell>57M</cell></row><row><cell>AlexNet -Overfitted</cell><cell>91.32</cell><cell>57M</cell></row><row><cell>AlexNet -Pruned</cell><cell>92.11</cell><cell>22M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
	</analytic>
	<monogr>
		<title level="j">NLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<title level="m">The CIFAR-10 dataset</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08342</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ThiNet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through L 0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡ 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PyTorch: Tensors and dynamic neural networks in Python with strong GPU acceleration</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
