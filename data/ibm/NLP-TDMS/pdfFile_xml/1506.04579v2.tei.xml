<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARSENET: LOOKING WIDER TO SEE BETTER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>wliu@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
							<email>arabinovich@magicleap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">MagicLeap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
							<email>aberg@cs.unc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">UNC</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARSENET: LOOKING WIDER TO SEE BETTER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a technique for adding global context to fully convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN Long et al. <ref type="formula">(2014)</ref>). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic segmentation, largely studied in the last 10 years, merges image segmentation with object recognition to produce per-pixel labeling of image content. The currently most successful techniques for semantic segmentation are based on fully convolution networks (FCN) <ref type="bibr" target="#b15">Long et al. (2014)</ref>. These are adapted from networks designed to classify whole images <ref type="bibr" target="#b11">Krizhevsky et al. (2012)</ref>; <ref type="bibr" target="#b23">Szegedy et al. (2014a)</ref>; <ref type="bibr" target="#b22">Simonyan &amp; Zisserman (2014)</ref>, and have demonstrated impressive level of performance. The FCN approach can be thought of as sliding an classification network around an input image, and processes each sliding window area independently. In particular, FCN disregards global information about an image, thus ignoring potentially useful scene-level semantic context. In order to integrate more context, several approaches <ref type="bibr" target="#b0">Chen et al. (2014)</ref>; <ref type="bibr" target="#b20">Schwing &amp; Urtasun (2015)</ref>; <ref type="bibr" target="#b13">Lin et al. (2015)</ref>; <ref type="bibr" target="#b27">Zheng et al. (2015)</ref>, propose using techniques from graphical models such as conditional random field (CRF), to introduce global context and structured information into a FCN. Although powerful, these architectures can be complex, combining both the challenges of tuning a deep neural network and a CRF, and require a fair amount of experience in managing the idiosyncrasies of training methodology and parameters. At the least, this leads to time-consuming training and inference.</p><p>In this work, we propose ParseNet, an end-to-end simple and effective convolutional neural network, for semantic segmentation. One of our main contributions, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, is to use global context to help clarify local confusions. Looking back at previous work, adding global context for semantic segmentation is not a new idea, but has so far been pursued in patch-based frameworks <ref type="bibr" target="#b16">Lucchi et al. (2011)</ref>. Such patch-based approaches have much in common with detection and segmentation work that have also shown benefits from integrating global context into classifying regions or objects in an image <ref type="bibr" target="#b24">Szegedy et al. (2014b)</ref>; <ref type="bibr" target="#b17">Mostajabi et al. (2014)</ref>. Our approach allows integrating global context in an end-to-end fully convolutional network (as opposed to a patch-based approach) for semantic segmentation with small computational overhead. In our setting, the image is not divided into regions or objects, instead the network makes a joint prediction of all pixel values. Previous work on fully convolutional networks did not include global features, and there were limits in the pixel distance across which consistency in labeling was maintained.</p><p>The key "widget" that allows adding global context to the FCN framework is simple, but has several important consequences in addition to improving the accuracy of FCN. First, the entire end-to-end process is a single deep network, making training relatively straightforward compared to combining deep networks and CRFs. In addition, the way we add global context does not introduce much computational overhead versus training and evaluating a standard FCN, while improving performance  significantly. In our approach, the feature map for a layer is pooled over the whole image to result in a context vector. This is appended to each of the features sent on to the subsequent layer of the network. In implementation, this is accomplished by unpooling the context vector and appending the resulting feature map with the standard feature map. The process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. This technique can be applied selectively to feature maps within a network, and can be used to combine information from multiple feature maps, as desired. Notice that the scale of features from different layers may be quite different, making it difficult to directly combine them for prediction. We find that L 2 normalizing features for each layer and combining them using a scaling factor learned through backpropagation works well to address this potential difficulty.</p><p>In section 4, we demonstrate that these operations, appending global context pooled from a feature map along with an appropriate scaling, are sufficient to significantly improve performance over the basic FCN, resulting in accuracy on par with the method of <ref type="bibr" target="#b0">Chen et al. (2014)</ref> that uses detailed structure information for post processing. That said, we do not advocate ignoring the structure information. Instead, we posit that adding the global feature is a simple and robust method to improve FCN performance by considering contextual information. In fact, our network can be combined with explicit structure output prediction, e.g. a CRF, to potentially further increase performance.</p><p>The rest of the paper is organized as follows. In Section 2 we review the related work. Our proposed approach is described in Section 3 followed by extensive experimental validation in Section 4. We conclude our work and describe future directions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep convolutional neural networks (CNN) <ref type="bibr" target="#b11">Krizhevsky et al. (2012)</ref>; <ref type="bibr" target="#b23">Szegedy et al. (2014a)</ref>; Simonyan &amp; Zisserman (2014) have become powerful tools not only for whole image classification, but also for object detection and semantic segmentation <ref type="bibr" target="#b2">Girshick et al. (2014)</ref>; <ref type="bibr" target="#b24">Szegedy et al. (2014b)</ref>; <ref type="bibr" target="#b4">Gupta et al. (2014)</ref>. This success has been attributed to both the large capacity and effective training of the CNN. Following the proposal + post-classification scheme <ref type="bibr" target="#b26">Uijlings et al. (2013)</ref>, CNNs achieve state-of-the-art results on object detection and segmentation tasks. As a caveat, even though a single pass through the networks used in these systems is approaching or already past video frame rate for individual patch, these approaches require classifying hundreds or thousands of patches per image, and thus are still slow. <ref type="bibr" target="#b7">He et al. (2014)</ref>; <ref type="bibr" target="#b15">Long et al. (2014)</ref> improve the computation by applying convolution to the whole image once, and then pool features from the final feature map of the network for each region proposal or pixel to achieve comparable or even better results. Yet, these methods still fall short of including whole image context and only classify patches or pixels locally. Our ParseNet is built upon the fully convolutional network architecture <ref type="bibr" target="#b15">Long et al. (2014)</ref> with a strong emphasis on including contextual information in a simple approach.</p><p>For semantic segmentation, using context information <ref type="bibr" target="#b19">Rabinovich et al. (2007)</ref>; <ref type="bibr" target="#b21">Shotton et al. (2009);</ref><ref type="bibr" target="#b25">Torralba (2003)</ref> from the whole image can significantly help classifying local patches. <ref type="bibr" target="#b16">Lucchi et al. (2011)</ref> shows that by concatenating features from the whole image to the local patch, the inclusion of post processing (i.e. CRF smoothing) becomes unnecessary because the image level features already encode the smoothness. <ref type="bibr" target="#b17">Mostajabi et al. (2014)</ref> demonstrate that by using the "zoom-out" features, which is a combination of features for each super pixel, region surrounding it, and the whole image, they can achieve impressive performance for the semantic segmentation task. These approaches pool features differently for local patches and the whole image, making it difficult to train the whole system end-to-end. Exploiting the FCN architecture, ParsetNet can directly use global average pooling from the final (or any) feature map, resulting in the feature of the whole image, and use it as context. Experiments results confirm that ParseNet can capture the context of the image and thus improve local patch prediction results.</p><p>There is another line of work that attempts to combine graphical models with CNNs to incorporate both context and smoothness priors. <ref type="bibr" target="#b0">Chen et al. (2014)</ref> first uses a FCN to estimate the unary potential, then applies a fully connected CRF to smooth the predictions spatially. As this approach consists of two decoupled stages, it is difficult to train the FCN properly to minimize the final objective of smooth and accurate semantic segments. A more unified and principled approach is to incorporate the structure information during training directly. <ref type="bibr" target="#b20">Schwing &amp; Urtasun (2015)</ref> propagates the marginals computed from the structured loss to update the network parameters, <ref type="bibr" target="#b13">Lin et al. (2015)</ref> uses piece-wise training to make learning more efficient by adding a few extra piece-wise networks, while <ref type="bibr" target="#b27">Zheng et al. (2015)</ref> convert CRF learning to recurrent neural network (RNN) and use message passing to do the learning and inference. However, we show that our method can achieve comparable accuracy, with a simpler -hence more robust -structure, while requiring only a small amount of additional training/inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARSENET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GLOBAL CONTEXT</head><p>Context is known to be very useful for improving performance on detection and segmentation tasks using deep learning. <ref type="bibr" target="#b17">Mostajabi et al. (2014)</ref>; <ref type="bibr" target="#b24">Szegedy et al. (2014b)</ref> and references therein illustrate how context can be used to help in different tasks. As for semantic segmentation, per pixel classification, is often ambiguous in the presence of only local information. However, the task becomes much simpler if contextual information, from the whole image, is available. Although theoretically, features from the top layers of a network have very large receptive fields (e.g. fc7 in FCN with VGG has a 404 × 404 pixels receptive field), we argue that in practice, the empirical size of the receptive fields is much smaller, and is not enough to capture the global context. To identify the effective receptive field, we slide a small patch of random noise across the input image, and measure the change in the activation of the desired layer. If the activation does not vary significantly, that suggests the given random patch is outside of the empirical receptive field, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The effective receptive field at the last layer of this network barely covers 1 4 of the entire image. Such an effect of difference between empirical and theoretical receptive field sizes was also observed in <ref type="bibr" target="#b28">Zhou et al. (2014)</ref>. Fortunately, it is rather straightforward to get the context within the FCN architecture.</p><p>Specifically, we use global average pooling and pool the context features from the last layer or any layer if that is desired. The quality of semantic segmentation is greatly improved by adding the global feature to local feature map, either with early fusion 1 or late fusion as discussed in Sec. 3.2. For example, <ref type="figure" target="#fig_0">Fig 1 has</ref> misclassified a large portion of the image as bird since it only used local information, however, adding contextual information in the loop, which might contain strong signal of cat, corrects the mistake. Experiment results on VOC2012 and PASCAL-Context dataset also verify our assumption. Compared with <ref type="bibr" target="#b0">Chen et al. (2014)</ref>, the improvement is similar as of using CRF to post-process the output of FCN.</p><p>In addition, we also tried to follow the spatial pyramid idea <ref type="bibr" target="#b12">Lazebnik et al. (2006)</ref> to pool features from increasingly finer sub-regions and attach them to local features in the sub-regions, however, we did not observe significant improvements. We conjecture that it is because the (empirical) receptive field of high-level feature maps is larger than or similar as those sub-regions. However features pooled from the whole image are still beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EARLY FUSION AND LATE FUSION</head><p>Once we get the global context feature, there are two general standard paradigms of using it with the local feature map. First, the early fusion, illustrated in in <ref type="figure" target="#fig_0">Fig. 1</ref> where we unpool (replicate) global feature to the same size as of local feature map spatially and then concatenate them, and use the combined feature to learn the classifier. The alternative approach, is late fusion, where each feature is used to learn its own classifier, followed by merging the two predictions into a single classification score <ref type="bibr" target="#b15">Long et al. (2014)</ref>; <ref type="bibr" target="#b0">Chen et al. (2014)</ref>. There are cons and pros for both fusion methods. If there is no additional processing on combined features, early fusion is quite similar to late fusion as pointed out in <ref type="bibr" target="#b6">Hariharan et al. (2014)</ref>. With late fusion, there might be a case where individual features cannot recognize something but combining them may and there is no way to recover from independent predictions. Our experiments show that both method works more or less the same if we normalize the feature properly for early fusion case.</p><p>When merging the features, one must be careful to normalize each individual feature to make the combined feature work well; in classical computer vision this is referred as the cue combination problem. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we extract a feature vector at a position combined from increasing higher level layers (from left to right), with lower level feature having a significantly larger scale than higher level layers. As we show in Sec. 4.2, by naively combining features, the resultant feature will not be discriminative, and heavy parameter tuning will be required to achieve sufficient accuracy. Instead, we can first L 2 normalize each feature and also possibly learn the scale parameter, which makes the learning more stable. We will describe more details in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">L 2 NORMALIZATION LAYER</head><p>As discussed above and shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we need to combine two (or more) feature vectors, which generally have different scale and norm. Naively concatenating features leads to poor performance as the "larger" features dominate the "smaller" ones. Although during training, the weight might adjust accordingly, it requires very careful tuning of parameters and depends on dataset, thus goes against the robust principle. We find that by normalizing each individual feature first, and also learn to scale each differently, it makes the training more stable and improves performance.</p><p>L 2 norm layer is not only useful for feature combination. As was pointed out above, in some cases late fusion also works equally well, but only with the help of L 2 normalization. For example, if we want to use lower level feature to learn classifier, as demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, some of the features will have very large norm. It is not trivial to learn with it without careful weight initialization and parameter tuning. A work around strategy is to apply an additional convolutional layer <ref type="bibr" target="#b0">Chen et al. (2014)</ref>; <ref type="bibr" target="#b6">Hariharan et al. (2014)</ref> and use several stages of finetuning <ref type="bibr" target="#b15">Long et al. (2014)</ref> with much lower learning rate for lower layer. This again goes against the principle of simply and robustness. In our work, we apply L 2 -norm and learn the scale parameter for each channel before using the feature for classification, which leads to more stable training. Formally, let be the loss we want to minimize. Here we use the summed softmax loss. For a layer with d-dimensional input x = (x 1 · · · x d ), we will normalize it using L</p><formula xml:id="formula_0">2 -norm 2 withx = x ||x||2 where ||x|| 2 = d i=1 |x i | 2 1/2 is the L 2 norm of x.</formula><p>Note that simply normalizing each input of a layer changes the scale of the layer and will slow down the learning if we do not scale it accordingly. For example, we tried to normalize a feature s.t. L 2 -norm is 1, yet we can hardly train the network because the features become very small. However, if we normalize it to e.g. 10 or 20, the network begins to learn well. Motivated by batch normalization <ref type="bibr" target="#b9">Ioffe &amp; Szegedy (2015)</ref> and PReLU <ref type="bibr" target="#b8">He et al. (2015)</ref>, we introduce a scaling parameter γ i , for each channel, which scales the normalized value by y i = γ ixi .</p><p>The number of extra parameters is equal to total number of channels, and are negligible and can be learned with backprogation. Indeed, by setting γ i = ||x|| 2 , we could recover the L 2 normalized feature, if that was optimal. Notice that this is simple to implement as the normalization and scale parameter learning only depend on each input feature vector and do not need to aggregate information from other samples as batch normalization does. During training, we use backpropagation and chain rule to compute derivatives with respect to scaling factor γ and input data x</p><formula xml:id="formula_1">∂ ∂x = ∂ ∂y · γ ∂ ∂x = ∂ ∂x I ||x|| 2 − xx T ||x|| 3 2 ∂ ∂γ i = yi ∂ ∂y ix i<label>(1)</label></formula><p>For our case, we need to do L 2 -norm per each pixel in a feature map instead of the whole. We can easily extend the equations by doing it elemental wise as it is efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we mainly report results on three benchmark datasets: VOC2012 Everingham et al. All the results we describe below use the training images to train, and most of the results are on the validation set. We also report results on VOC2012 test set. We use Caffe <ref type="bibr" target="#b10">Jia (2013)</ref> and fine-tune ParseNet from VGG-16 network <ref type="bibr" target="#b22">Simonyan &amp; Zisserman (2014)</ref> for different dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BEST PRACTICE OF FINETUNING</head><p>As we know parameters are important for training/finetuning network, we try to reproduce the stateof-the-art systems' results by exploring the parameter space and achieve better baseline performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-Context</head><p>We start from the public system FCN-32s PASCAL-Context. Notice that it uses the accumulated gradient and affine transformation tricks that were introduced in <ref type="bibr" target="#b15">Long et al. (2014)</ref>. As such, it can deal with any input image of various sizes without warping or cropping it to fixed size, which can distort the image and affect the final segmentation result. <ref type="table">Table 1</ref> shows our different versions of reproduced baseline results. Baseline A uses the exactly same protocol, and our result is 1.5% lower. In Baseline B, we tried more iteration (160k vs. 80k) of finetuning and achieved similar performance to the reported one. Then, we modified the network a bit, i.e. we used "xavier" initialization <ref type="bibr" target="#b3">Glorot &amp; Bengio (2010)</ref>, higher base learning rate (1e-9 vs. 1e-10), and lower momentum (0.9 vs. 0.99), and we achieved 1% higher accuracy as shown in Baseline C. What's more, we also remove the 100 padding in the first convolution layer and observed no significant difference but network trained slightly faster. Furthermore, we also used "poly" learning rate policy (base lr × (1 − iter max iter ) power , where power is set to 0.9.) as it is proved to converge faster than normal "step" policy, and thus can achieve 1.5% better performance with the same iterations (80k). All experimental results on PASCAL-Context are shown in table 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-Context Mean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC2012</head><p>We carry over the parameters we found on PASCAL-Context to VOC2012. We tried both FCN-32s and DeepLab-LargeFOV 4 . <ref type="table" target="#tab_2">Table 2</ref> shows the reproduced baseline results. DeepLab is very similar to FCN-32s, and our reproduced result is 5% better (64.96 vs. 59.80) using the parameters we found in PASCAL-Context. DeepLab-LargeFOV uses the filter rarefication technique (atrous algorithm) that has much less parameters and is faster. We also use the same parameters on this architecture and can achieve 3.5% improvements. The gap between these two models is not significant anymore as reported in <ref type="bibr" target="#b0">Chen et al. (2014)</ref>. Later on, we renamed DeepLab-LargeFOV Baseline as ParseNet Baseline, and ParseNet is ParseNet Baseline plus global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VOC2012</head><p>Mean IoU DeepLab <ref type="bibr" target="#b0">Chen et al. (2014)</ref> 59.80 DeepLab-LargeFOV <ref type="bibr" target="#b0">Chen et al. (2014)</ref> 62.25 DeepLab Baseline 64.96 DeepLab-LargeFOV Baseline 65.82 Until now, we see that parameters and details are important to get best performance using FCN models. Below, we report all our results with the reproduced baseline networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMBINING LOCAL AND GLOBAL FEATURES</head><p>In this section, we report results of combining global and local feature on three dataset: SiftFlow <ref type="bibr" target="#b14">Liu et al. (2011)</ref>, PASCAL-Context, and PASCAL VOC2012. For simplicity, we use pool6 as the global context feature, conv5 as conv5 3, conv4 as conv4 3, and conv3 as conv3 3 through the rest of paper.</p><p>SiftFlow is a relatively small dataset that only has 2,688 images with 33 semantic categories. We do not use the geometric categories during training. We use the FCN-32s network with the parameters found in PASCAL-Context. Instead of using two stages of learning as done in <ref type="bibr" target="#b15">Long et al. (2014)</ref>, we combine the feature directly from different layers for learning. As shown in <ref type="table">Table 3</ref>, adding more layers can normally improve the performance as lower level layers have more detailed information. We also notice that adding global context feature does not help much. This is perhaps due to the small image size (256 × 256), as we know even the empirical receptive field of fc7 (e.g. <ref type="figure" target="#fig_1">Fig. 2)</ref>   <ref type="table">Table 3</ref>: Results on SiftFlow. Early fusion can work equally well as late fusion as used in <ref type="bibr" target="#b15">Long et al. (2014)</ref>. Adding more layers of feature generally increase the performance. Global feature is not that helpful as receptive field size of fc7 is large enough to cover most of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL-Context</head><p>We then apply the same model on PASCAL-Context by concatenating features from different layers of the network. As shown in <ref type="table">Table 4</ref>, by adding global context pool6, it instantly helps improve by about 1.6%, which means that context is useful here as opposed to the observation in SiftFlow. Context becomes more important proportionally to the image size. Another interesting observation from the table is that, without normalization, the performance keep increasing until we add conv5. However, if we naively keep adding conv4, it starts decreasing the performance a bit; and if we add conv3, the network collapses. Interestingly, if we normalize all the features before we combine them, we don't see such a drop, instead, adding all the feature together can achieve the state-of-the-art result on PASCAL-Context as far as we know.  <ref type="table">Table 4</ref>: Results on PASCAL-Context. Adding more layers helps if we L 2 normalize them.</p><p>PASCAL VOC2012 Since we have reproduced both network architecture on VOC2012, we want to see how does global context, normalization, and early or late fusion affect performance.</p><p>We start with using DeepLab Baseline, and try to add pool6 to it. It improves from 64.92% to 67.49% by adding pool6 with normalization. Interestingly, without normalizing fc7 and pool6, we don't see any improvements. As opposed to what we observed from SiftFlow and PASCAL-Context. We hypothesize this is due to images in VOC2012 mostly have one or two objects in the image versus the other two dataset who have multiple labels per image, and we need to adjust the weight more carefully to make the context feature more useful.</p><p>ParseNet Baseline performance is higher than DeepLab Baseline and it is faster, thus we switch to use it for most of the experimental comparison for VOC2012. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we observe a similar pattern as of DeepLab Baseline that if we add pool6, it is helping improve the performance by 3.8%. However, we also notice that if we do not normalize them and learn the scaling factors, its effect is diminished. Furthermore, we notice that early fusion and late fusion both work very similar. <ref type="figure">Figure 4</ref> illustrates some examples of how global context helps. We can clearly see that without using context feature, the network will make many mistakes by confusing between similar categories as well as making spurious predictions. Two similar looking patches are indistinguishable by the network if considered in isolation. However, adding context solves this issue as the global context helps discriminate the local patches more accurately. On the other hand, sometimes context also brings confusion for prediction as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. For example, in the first row, the global context feature definitely captured the spotty dog information that it used to help discriminate sheep from dog. However, it also added bias to classify the spotty horse as a dog. The other three examples have the same issue. Overall, by learning to weight pool6 and fc7 after L 2 normalization helps improve the performance greatly.  We also tried to combine lower level feature as was done with PASCAL-Context and SiftFlow, but no significant improvements using either early fusion or late fusion were observed. We believe it is because the fc7 of ParseNet Baseline is the same size as of conv4, and including lower level feature will not help much as they are not sufficiently discriminative. Besides, we also tried the idea similar to spatial pyramid pooling where we pool 1 × 1 global feature, 2 × 2 subregion feature, and 4 × 4 subregion feature, and tried both early fusion and late fusion. However, we observed no improvements. We conjecture that the receptive field of the high level feature map (e.g. fc7) is sufficiently large that sub-region global feature does not help much.  Finally, we test two models, ParseNet Baseline and ParseNet, on VOC2012 test set. As shown in <ref type="table" target="#tab_8">Table 6</ref>, we can see that our baseline result is already higher than many of the existing methods due to proper finetuning. By adding the global context feature, we achieve performance that is within the standard deviation of the one <ref type="bibr" target="#b0">Chen et al. (2014)</ref> using fully connect CRF to smooth the outputs and perform better on more than half of categories. Again, our approach is much simpler to implement and train, hence is more robust. Using late fusion has almost no extra training/inference cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we presented ParseNet, a simple fully convolutional neural network architecture that allows for direct inclusion of global context for the task of semantic segmentation. We have explicitly demonstrated that relying on the largest receptive field of FCN network does not provide sufficient global context, and the largest empirical receptive field is not sufficient to capture global context -modeling global context directly in required. On PASCAL VOC2012 test set, segmentation results of ParseNet are within the standard deviation of the DeepLab-LargeFOV-CRF, which suggests that adding a global feature has a similar effect of post processing FCN predictions with a graphical model. As part of developing and analyzing this approach we provided analysis of many  architectural choices for the network, discussing best practices for training, and demonstrated the importance of normalization and learning weights when combining features from multiple layers of a network. By themselves, our practices for training significantly improve the baselines we use before adding global context. The guiding principle in the design of ParseNet is simplicity and robustness of learning. Results are presented on three benchmark dataset, and are state of the art on SiftFlow and PASCAL-Context, and near the state of the art on PASCAL VOC2012. Given the simplicity and ease of training, we find these results very encouraging. In our on going work, we are exploring combining our technique with structure training/inference as done in <ref type="bibr" target="#b20">Schwing &amp; Urtasun (2015)</ref>; <ref type="bibr" target="#b13">Lin et al. (2015)</ref>; <ref type="bibr" target="#b27">Zheng et al. (2015)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ParseNet uses extra global context to clarify local confusion and smooth segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Receptive field (RF) size for last layer. (a) original image; (b) activation map on bicycle from a channel of the last layer of a network; (c) theoretical receptive field of the maximum activation (marked by red cross) is defined by the network structure; (d) empirical receptive field affecting the activation. Clearly empirical receptive field is not large enough to capture the global context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Features from 4 different layers have activations that are of drastically different scales. Each color corresponds to a different layers' feature. While blue and cyan are on a comparable scale, red and green features are of a scale 2 orders of magnitude less.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(2014)  and PASCAL-Context<ref type="bibr" target="#b18">Mottaghi et al. (2014)</ref>. VOC2012 has 20 object classes and one background class. Following<ref type="bibr" target="#b15">Long et al. (2014)</ref>;<ref type="bibr" target="#b0">Chen et al. (2014)</ref>, we augment it with extra annotations from Hariharan et allet@tokeneonedot<ref type="bibr" target="#b5">Hariharan et al. (2011)</ref> that leads to 10,582, 1,449, and 1,456 images for training, validation, and testing. PASCAL-Context<ref type="bibr" target="#b18">Mottaghi et al. (2014)</ref> fully labeled all scene classes appeared in VOC2010. We follow the same training + validation split as defined and used in<ref type="bibr" target="#b18">Mottaghi et al. (2014)</ref>;<ref type="bibr" target="#b15">Long et al. (2014)</ref>, resulting in 59 object + stuff classes and one background classes with 4,998 and 5105 training and validation images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Global context confuse local patch predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Reproduce DeepLab and DeepLab-LargeFOV on PASCAL VOC2012.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Add context for ParseNet Baseline on VOC2012.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>System bkg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean</figDesc><table><row><cell>FCN-8s</cell><cell>-76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5</cell><cell>73.9 45.2 72.4 37.4 70.9 55.1 62.2</cell></row><row><cell>Hypercolumn</cell><cell>-68.7 33.5 69.8 51.3 70.2 81.1 71.9 74.9 23.9 60.6 46.9 72.1 68.3 74.5</cell><cell>72.9 52.6 64.4 45.4 64.9 57.4 62.6</cell></row><row><cell>TTI-Zoomout-16</cell><cell>89.8 81.9 35.1 78.2 57.4 56.5 80.5 74.0 79.8 22.4 69.6 53.7 74.0 76.0 76.6</cell><cell>68.8 44.3 70.2 40.2 68.9 55.3 64.4</cell></row><row><cell cols="3">DeepLab-LargeFOV 92.6 83.5 36.6 82.5 62.3 66.5 85.4 78.5 83.7 30.4 72.9 60.4 78.5 75.5 82.1 79.7 58.2 82.0 48.8 73.7 63.3 70.3</cell></row><row><cell>ParseNet Baseline 5</cell><cell>92.3 82.6 36.1 76.1 59.3 62.3 81.6 79.5 81.4 28.1 70.0 53.0 73.2 70.6 78.8</cell><cell>78.6 51.9 77.4 45.5 71.7 62.6 67.3</cell></row><row><cell>ParseNet 6</cell><cell>92.4 84.1 37.0 77.0 62.8 64.0 85.8 79.7 83.7 27.7 74.8 57.6 77.1 78.3 81.0</cell><cell>78.2 52.6 80.4 49.9 75.7 65.0 69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>PASCAL VOC2012 test Segmentation results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Global context helps for classifying local patches.</figDesc><table><row><cell></cell><cell>cow</cell><cell>bird</cell><cell>cow</cell></row><row><cell></cell><cell></cell><cell>chair</cell><cell>person</cell></row><row><cell></cell><cell></cell><cell>cow</cell><cell></cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell></cell></row><row><cell></cell><cell></cell><cell>horse</cell><cell></cell></row><row><cell></cell><cell></cell><cell>person</cell><cell></cell></row><row><cell></cell><cell></cell><cell>sheep</cell><cell></cell></row><row><cell></cell><cell>cat</cell><cell>cat</cell><cell>cat</cell></row><row><cell></cell><cell>pottedplant</cell><cell>diningtable</cell><cell>pottedplant</cell></row><row><cell></cell><cell></cell><cell>person</cell><cell></cell></row><row><cell></cell><cell></cell><cell>pottedplant</cell><cell></cell></row><row><cell></cell><cell></cell><cell>tvmonitor</cell><cell></cell></row><row><cell></cell><cell>bird</cell><cell>bird</cell><cell>bird</cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell>dog</cell></row><row><cell></cell><cell>horse</cell><cell>cow</cell><cell>horse</cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell></cell></row><row><cell></cell><cell></cell><cell>horse</cell><cell></cell></row><row><cell></cell><cell></cell><cell>sheep</cell><cell></cell></row><row><cell>(a) Original Image</cell><cell>(b) Ground truth</cell><cell>(c) ParseNet Baseline</cell><cell>(d) ParseNet</cell></row><row><cell></cell><cell>Figure 4: dog</cell><cell>cow</cell><cell>dog</cell></row><row><cell></cell><cell>horse</cell><cell>dog</cell><cell>horse</cell></row><row><cell></cell><cell>person</cell><cell>horse</cell><cell>person</cell></row><row><cell></cell><cell></cell><cell>person</cell><cell>train</cell></row><row><cell></cell><cell></cell><cell>sheep</cell><cell></cell></row><row><cell></cell><cell></cell><cell>train</cell><cell></cell></row><row><cell></cell><cell>cat</cell><cell>cat</cell><cell>cat</cell></row><row><cell></cell><cell></cell><cell>chair</cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell>dog</cell></row><row><cell></cell><cell></cell><cell>person</cell><cell>person</cell></row><row><cell></cell><cell></cell><cell>sofa</cell><cell>sofa</cell></row><row><cell></cell><cell>dog</cell><cell>cow</cell><cell>dog</cell></row><row><cell></cell><cell>sheep</cell><cell>dog</cell><cell>sheep</cell></row><row><cell></cell><cell></cell><cell>sheep</cell><cell></cell></row><row><cell></cell><cell>cow</cell><cell>cow</cell><cell>cow</cell></row><row><cell></cell><cell></cell><cell>horse</cell><cell>horse</cell></row><row><cell>(a) Original Image</cell><cell>(b) Ground truth</cell><cell>(c) ParseNet Baseline</cell><cell>(d) ParseNet</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">we use unpool operation by simply replicating the global feature horizontally and vertically to have the same size as the local feature map.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We have only tried L2 norm, but can also potentially try other lp norms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://gist.github.com/shelhamer/80667189b218ad570e82#file-readme-md 4 https://bitbucket.org/deeplab/deeplab-public/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://host.robots.ox.ac.uk:8080/anonymous/LGOLRG.html 6 http://host.robots.ox.ac.uk:8080/anonymous/56QLXU.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iasonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5752</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4729</idno>
		<title level="m">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chunhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are spatial and global constraints really necessary for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yunpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0774</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xianjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaobai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">-</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seong-Whan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><forename type="middle">Rr</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vibhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhizhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dalong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
