<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
							<email>phchencs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
							<email>im.huangdeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
							<email>runhaozeng.cs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang1990@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study unsupervised video representation learning that seeks to learn both motion and appearance features from unlabeled video only, which can be reused for downstream tasks such as action recognition. This task, however, is extremely challenging due to: 1) the highly complex spatial-temporal information in videos; and 2) the lack of labeled data for training. Unlike the representation learning for static images, it is difficult to construct a suitable self-supervised task to well model both motion and appearance features. More recently, several attempts have been made to learn video representation through video playback speed prediction. However, it is non-trivial to obtain precise speed labels for the videos. More critically, the learnt models may tend to focus on motion pattern and thus may not learn appearance features well. In this paper, we observe that the relative playback speed is more consistent with motion pattern, and thus provide more effective and stable supervision for representation learning. Therefore, we propose a new way to perceive the playback speed and exploit the relative speed between two video clips as labels. In this way, we are able to well perceive speed and learn better motion features. Moreover, to ensure the learning of appearance features, we further propose an appearance-focused task, where we enforce the model to perceive the appearance difference between two video clips. We show that optimizing the two tasks jointly consistently improves the performance on two downstream tasks. Remarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy without the use of labeled data for pre-training. Code and pre-trained models can be found at https://github.com/PeihaoChen/RSPNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. An illustrative example of content-label inconsistency. In existing speed perception-based methods <ref type="bibr" target="#b0">[1]</ref>, 1) both video clips (a) and (b) are labeled as 1x speed, i.e., sampled consecutively, but the content of these two clips are dissimilar. The left player shoots more slowly and the middle player has finished shooting in the same time period. 2) Although clip (c) is labeled as 2x speed, i.e., the sampling interval is set to 2 frames, it looks similar to the middle clip with different speed labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video analysis <ref type="bibr" target="#b23">[24]</ref> has been a prominent research topic in computer vision due to its vast potential applications, such as action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>, event detection <ref type="bibr" target="#b13">[14]</ref>, action localization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, audio-visual scene analysis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>, etc. Compared with static images, videos often contain more complex spatial-temporal contents and have a larger data volume, making it very challenging to annotate and analyze. How to learn effective video representations with a few annotations even without annotations is an important yet challenging task <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Recently, unsupervised video representation learning, which seeks to learn appearance and motion features from unlabeled videos, has attracted great attention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref>. This task, however, is very difficult due to several challenges: 1) The downstream video understanding tasks, such as action recognition, rely on both appearance features (e.g., texture and shape of objects, background scene) and motion features (e.g., the movement of objects). It is diffi-cult to learn representation for both appearance and motion simultaneously because of the complex spatial-temporal information in videos.</p><p>2) It is difficult to mine effective supervision from unlabeled video data for representation learning.</p><p>Existing methods attempt to solve these challenges by designing pretext tasks to obtain pseudo labels for video representation learning. The pretext tasks include context prediction <ref type="bibr" target="#b19">[20]</ref>, playback rate perception <ref type="bibr" target="#b0">[1]</ref>, temporal clip orders prediction <ref type="bibr" target="#b45">[46]</ref>, etc. Among them, training models using playback speeds perception task achieves great success because models have to focus on the moving objects to perceive the playback speed <ref type="bibr" target="#b42">[43]</ref>. This helps models to learn representative motion features. Specifically, Benaim et al. <ref type="bibr" target="#b0">[1]</ref> train a model to determine whether videos are sped up or not. Some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43]</ref> try to predict the specific playback speed for each video.</p><p>However, these works suffer from two limitations. First, the playback speed labels used for pretext task can be imprecise because it may be inconsistent with motion content in videos. As shown in <ref type="figure">Figure 1</ref>, the clips with different labels (i.e., different playback speeds) may look similar to each other. The underlying reason is that different people often implement the same action at different speeds. Using such inconsistent speed labels for training may make it difficult to learn discriminative features. Second, perceiving speed mainly relies on motion content. It does not explicitly encourage models to explore appearance features which, however, are also important for video understanding. Recently, instance discrimination task <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref> has shown its effectiveness for learning appearance features in image domain. However, how to extend it to video domain and combine it well with motion features learning is non-trivial.</p><p>To address the imprecise label issue in the above methods, we observe that the relative playback speed can provide more precise supervision for training. To this end, we propose a new pretext task that exploits relative playback speed as labels for perceiving speed, namely Relative Speed Perception (RSP). Specifically, we sample two clips from the same video and train a neural network to identify their relative playback speed instead of predicting the specific playback speed of each video clip. The relative playback speed label is obtained through the comparison between playback speeds of two clips from the same video (e.g., 2x is faster than 1x). We observe that for the same video, the higher the playback speed, the faster the objects will move. Consequently, such labels are independent of the original speed of objects in a video and can reveal the precise motion distinction between two clips. In this sense, the labels are more consistent with the motion content and can provide more effective supervision for representation learning.</p><p>Moreover, to encourage models to pay attention to learning appearance features, we follow the spirit of in-stance discrimination task in image domain and design an Appearance-focused Video Instance Discrimination (A-VID) task. In this task, we require model to find out two clips sampled from the same video against a bunch of clips from other videos. Considering that different clips in the same video are often at the same speed, we propose a speed augmentation strategy, i.e., randomizing the playback speed of each clip. Consequently, models cannot finish this task by simply learning speed information. Instead, models tend to learn appearance features, such as background scene and the texture of objects, because these features are consistent along a video but vary among different videos. We train models to finish RSP and A-VID tasks jointly using a two-branch architecture such that models are expected to learn both motion and appearance features simultaneously. We name our model as RSPNet. Experimental results on three datasets show that the learnt features perform well on two downstream tasks, i.e., action recognition and video retrieval.</p><p>To sum up, our contributions are as follows:</p><p>• We propose a relative speed perception task for unsupervised video representation learning. In this sense, the labels are more consistent with the motion content and can provide more effective supervision for representation learning.</p><p>• We extend instance discrimination task to video domain and propose a speed augmentation strategy to make it focus more on exploring appearance content.</p><p>In this way, we can combine it well with relative speed perception task to learn representation for both motion and appearance contents simultaneously.</p><p>• We verify the effectiveness of RSP and A-VID tasks for learning video representation on two downstream tasks and three datasets. Remarkably, without the need of annotation for pre-training, the action recognition accuracy on UCF-101 significantly outperforms the models supervised pre-trained on ImageNet (93.7% v.s 86.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Unsupervised video representation learning. In recent years, unsupervised video representation learning, which uses video itself as supervision, has become a popular topic <ref type="bibr" target="#b24">[25]</ref>. The existing methods learn representation through various carefully designed pretext tasks. Xu et al. <ref type="bibr" target="#b45">[46]</ref> proposed video clip order prediction task for leveraging the temporal order of image sequences. Luo et al. <ref type="bibr" target="#b32">[33]</ref> proposed video cloze procedure task by prediction the spatiotemporal operation applied on the video clips.  <ref type="figure">Figure 2</ref>. Illustration of the proposed self-supervised video representation learning scheme. Given a set of video clips with different playback speeds, we use a spatial-temporal encoder f (·; θ) followed by two projection heads (i.e., gm and ga) to extract clip features for two pretext tasks. In the relative speed perception (RSP) task, we identify the relative playback speed between clips instead of predicting their specific playback speeds. In the appearance-focused video instance discrimination (A-VID) task, we distinguish video clips relying on the appearance contents. We formulate two tasks as a metric learning problem and use triplet loss Lm and InfoNCE loss La for training. learn motion representation. Since the video contains multiple frames, predicting future frames in latent space <ref type="bibr" target="#b40">[41]</ref> is also a effective task to learn visual representation.</p><p>More recently, many works have been proposed to learn features through discriminating playback speeds. Epstein et al. <ref type="bibr" target="#b8">[9]</ref> try to predict whether a clip is sped up or not. Some works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48]</ref> attempt to predict the specific playback speed of one clip. However, these works suffer from the imprecise speed label issue. Cho et al. <ref type="bibr" target="#b6">[7]</ref> design a method to sort video clips according to their playback speeds. However, they do not explicitly encourage model to learn appearance features. Our method makes use of relative speed to resolve the imprecise label issue. Moreover, we extent instance discrimination task <ref type="bibr" target="#b43">[44]</ref> to video domain to encourage appearance learning.</p><p>Metric Learning. Metric learning <ref type="bibr" target="#b44">[45]</ref> aims to automatically construct task-specific distance metrics that compare two samples from a specific aspect. Based on such metric, the similar pairs of samples are pulled together and the dissimilar pairs of samples are pushed apart. It has achieved great success in many areas, e.g., face recognition <ref type="bibr" target="#b36">[37]</ref>, music recommendation <ref type="bibr" target="#b33">[34]</ref>, and person reidentification <ref type="bibr" target="#b46">[47]</ref>. Recently, many works have successfully adopted metric learning for self-supervised representation learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. They usually generate positive pairs by creating multiple views of each data and generate negative pairs by randomly choosing images/patches/videos. In this work, we aim to learn video representation by comparing two video clips using metric learning. Unlike the existing works, we propose to identify their speed distinction and appearance distinction to learn motion and appearance features from unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><formula xml:id="formula_0">Problem Definition. Let V = {v i } N i=1</formula><p>be a video set containing N videos. We sample a clip c i from a video with s i playback speed. Unsupervised video representation learning aims to learn a spatial-temporal encoder f (·; θ) to map video clip c i to their corresponding features x i that best describe the content in c i . This task is very challenging because of the complex spatial-temporal information in videos and the lack of annotations. It is difficult to construct supervision information from unlabeled videos V to train a model to learn representation for both appearance and motion contents. Recently, some existing unsupervised learning methods attempt to learn video representation through playback speed perception. However, most of them suffer from imprecise speed label issue and do not explicitly encourage models to learn appearance features. Consequently, the learnt features may not be suitable for downstream video understanding tasks such as action recognition and video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General scheme of RSPNet</head><p>In this paper, we observe that relative playback speed can provide more effective labels for representation learning. Thus, we propose a relative speed perception task, i.e., predicting whether two clips are with the same speed or not, to resolve imprecise label issues and learn motion features. Moreover, we extend the instance discrimination task to video domain and propose a speed augmentation strategy to explicitly make models pay attention on exploring appearance features. Considering the success of metric learning on representation learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>, we formulate these two tasks as metric learning, in which we seek to maximize the similarity of two clip features in positive pairs while minimizing the one in negative pairs.</p><p>Formally, for relative speed perception task, instead of directly predicting playback speed s i for clip c i , we propose to compare the speeds of two clips c i and c j that are sampled from the same video. Since the actions in c i (or c j ) are often implemented by the same subject, the motions in these two clips are similar when s i = s j and are dissimilar when s i = s j . In this sense, the relative speed labels are obtained through comparing s i and s j (i.e., clips c i and c j are labeled as a positive pair when s i = s j and otherwise negative). Such labels are more consistent with motion content in videos and reveal the precise motion distinction. For appearance-focused video instance discrimination task, we enforce the model to predict whether two clips c i and c l are sampled from the same video. The intuition is that clips sampled from the same video often share similar appearance contents, which can be used as an important clue for distinguishing videos. We also randomize the playback speed, i.e., s i can be equal or not equal to s l . In this way, models are encouraged to pay more attention on learning appearance features instead of finishing this task by learning playback speed information.</p><p>We use two individual projection heads g m (·; θ m ) and g a (·; θ a ) to map spatial-temporal features c i to m i and a i for two tasks, respectively. We train models on these two tasks jointly. The objective function is formulated as follows,</p><formula xml:id="formula_1">L(V; θ, θ a , θ m ) = L m (V; θ, θ m ) + λL a (V; θ, θ a ),<label>(1)</label></formula><p>where L m and L a denote the loss functions of each task, respectively and λ is a fixed hyper-parameter to control the relative importance of each term. During inference for downstream tasks, we forward a video clip through the spatial-temporal encoder f (·; θ) and obtain x i as its spatialtemporal features. The schematic of our approach is shown in <ref type="figure">Figure 2</ref>. In the following, we will introduce more details about two pretext tasks in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RSP and A-VID tasks</head><p>Relative speed perception. This task aims to maximize the similarity of two clips with same playback speed and minimize the similarity of two clips with different playback speeds. Given a video, we sample 3 clips c i , c j and c k with playback speeds s i , s j and s k , respectively, where s i = s j = s k . We feed each clip into the spatial-temporal encoder f (·; θ) followed by a projection head g m (·; θ m ) to get their corresponding features m i , m j , m k . Dot product function d(·, ·) is used to measure the similarity between two clips. As the clips with the same playback speed share similar motion features, we expect their features can be closer compared with the clips with different playback</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training method of RSPNet</head><p>Require: video set V = {vi} N i=1 , # negative pair for A-VID K. 1: Initialize parameters θ, θa, θm for f (·; θ), ga(·; θa), gm(·; θm), respectively 2: while no converge do <ref type="bibr">3:</ref> Randomly sample a video v + from V, extract clips ci, cj, c k from v + with speed si, sj, s k , where si = sj = s k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Sample K clips {cn} K n=1 from video set V \ {v + }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Extract features xi, xj, x k , and {xn} K n=1 from video clips ci, cj, c k , {cn} K n=1 using encoder f (·; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>// RSP task <ref type="bibr">7:</ref> Obtain features mi, mj, m k from xi, xj, x k using gm(·; θm) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Compute Lm using Equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>// A-VID task 10:</p><p>Obtain features ai, aj, {an} K n=1 from xi, xj, {xn} K n=1 using ga(·; θa) <ref type="bibr">11:</ref> Compute La and L using Equations <ref type="formula" target="#formula_4">(3)</ref> and <ref type="formula" target="#formula_1">(1)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update parameters θ, θa, θm via stochastic gradient descent. <ref type="bibr" target="#b12">13</ref>: end while speeds. We achieve this object by using a triplet loss <ref type="bibr" target="#b36">[37]</ref> as follows,</p><formula xml:id="formula_2">L m (V; θ, θ m ) = max(0, γ − (p + − p − )),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">p + = d(m i , m j ), p − = d(m i , m k )</formula><p>and γ &gt; 0 is a certain margin. We desire that the similarity of a positive pair is larger than a negative pair by a margin γ.</p><p>Appearance-focused video instance discrimination. To explicitly encourage models to learn appearance features, we propose an A-VID task to further regularize the learning process. Motivated by the fact that different clips from the same video are always with similar spatial information, we extent the contrastive learning in the image domain <ref type="bibr" target="#b43">[44]</ref> to the video domain. Specifically, we sample two clips c i and c j from the same randomly selected video v + and K clips {c n } K n=1 from K videos in subset V \ v + . After that, we feed each clip into the spatial-temporal encoder f (·; θ) followed by a projection head g a (·; θ a ) and get their corresponding features. The encoder f (·; θ) share weights with the encoder in RSP task while the weights of projection head g a (·; θ a ) is independent of g m (·; θ m ). We consider (c i , c j ) as positive pair and (c i , c n ) as negative pair. We further apply the InfoNCE loss <ref type="bibr" target="#b22">[23]</ref> as the training loss:</p><formula xml:id="formula_4">L a (V; θ, θ a ) = −log q + q + + K n=1 q − n ,<label>(3)</label></formula><p>where q + = exp(d(a i , a j )/τ ), q − n = exp(d(a i , a n )/τ ), and τ is a temperature hyper-parameter <ref type="bibr" target="#b43">[44]</ref> which affects the concentration level of distribution. Optimizing Equation (3) will pull closer the positive pairs while push away the negative pairs. An underlying question is that how to sample these video clips. A naive solution is to sample all clips at the same playback speed. In this sense, clips c i and c j will share similar motion features while the motion features in c i and c n are dissmilar. This may provide clues for models to find out whether any two clips are from the same video or not. To encourage models to pay more attention on learning appearance features, we propose a speed augmentation strategy. Concretely, we randomize the playback speed of each clip, i.e., s i , s j , and s n are randomly selected from possible playback speeds, such that the motion features cannot provide effective clues for this task. In this way models have to focus on learning other informative features, including background and object appearance for discriminating video instance. The training method is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We pre-train models on the training set of Kinetics-400 dataset <ref type="bibr" target="#b2">[3]</ref>, which consists of around 240K training videos with 400 human action classes. Each video lasts about 10 seconds. To reduce training costs in ablation studies, we build a lightweight dataset, namely Kinetics-100, by selecting 100 classes with the least disk size of videos from Kinetics-400. UCF101 <ref type="bibr" target="#b37">[38]</ref> dataset consists of 13,320 videos from 101 realistic action categories on YouTube. HMDB51 <ref type="bibr" target="#b28">[29]</ref> dataset consists of 6,849 clips from 51 cation classes. Compared with UCF101 and HMDB51, Something-Something-V2 (Something-V2) dataset <ref type="bibr" target="#b17">[18]</ref> contains 220,847 videos with 174 classes and focuses more on modeling temporal relationships <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Pre-training details. We instantiate the projection head as a fully connected layer with 128 output dimension. After pre-training, we drop the projection heads and use the features before them for downstream tasks. Unless otherwise stated, we sample 16 consecutive frames with 112 × 112 spatial size for each clip following Kim et al. <ref type="bibr" target="#b27">[28]</ref>. Clips are augmented by using random cropping with resiz-ing, random color jitter and random Gaussian blur <ref type="bibr" target="#b5">[6]</ref>. We use SGD as the optimizer with a mini-batch size of 64. We train the model for 200 epochs by default. The learning rate policy is linear cosine decay starting from 0.1. Following He et al. <ref type="bibr" target="#b22">[23]</ref>, we set τ = 0.07, K = 16384, γ = 0.15 and λ = 1 for Equations (1), <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref>. All videos are with 25 fps. The possible playback speed s for clips in this paper is set to 1x (i.e., sampling frames consecutively) and 2x (i.e., sampling interval is set 2 frames).</p><p>Fine-tuning details. We fine-tune our RSPNet on UCF101, HMDB51, and Something-V2 with labeled videos for action recognition. We train for 30, 70, 50 epochs on these datasets, respectively, with a learning rate of 0.01. Following Xu et al. <ref type="bibr" target="#b45">[46]</ref>, we initialize the models with the weights from the pre-trained RSPNet except for the newly appended fully-connected layer with randomly initialized weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation studies</head><p>Effectiveness of two pretext tasks. In this paper, we propose two tasks, namely RSP and A-VID, to learn video representation. To verify the effectiveness of each task, we pretrain models using either RSP or A-VID on three backbone networks.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, compared with training from scratch, using RSP or A-VID task for pre-training significantly improves the action recognition performance on UCF101 and HMDB51 datasets, which demonstrates models learn useful clues for action recognition through pre-training on our designed pretext task. The improvement brought by A-VID task is relatively larger than pair-wise speed discrimination. The underlying reason is that UCF101 and HMDB51 datasets focus more on modeling appearance information compared with temporal relationship <ref type="bibr" target="#b30">[31]</ref>. The models pretrained on A-VID are more sensitive to object appearance and background scene while models pre-trained on RSP are more sensitive to the movement of objects. When we pretrained models on both task jointly, we achieve the best results on all three models. Compared with w/o pre-training setting, we achieve relative improvement of 11.5%, 17.9%, <ref type="table">Table 2</ref>. Comparison with other unsupervised methods on UCF101 and HMDB51 datasets. We show the backbone architecture and the pre-training dataset of each method. *We pre-train the model for 1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Architecture Pre-train Dataset UCF101 HMDB51 Shuffle&amp;Learn <ref type="bibr" target="#b34">[35]</ref> CaffeNet <ref type="formula" target="#formula_1">UCF101</ref>  Does relative speed perception help? As discussed in Section 1, we train models to perceive relative speed of two clips to resolve the imprecise speed label issue. Here, we implement a variant of our method by replacing RSP with directly predicting speed of each clip (i.e., 1x or 2x speed). We formulated it as a classification problem and use a crossentropy loss to optimize it following Wang et al. <ref type="bibr" target="#b42">[43]</ref>. We denote this task as speed prediction (SP). <ref type="table" target="#tab_1">Table 1</ref> shows that exploiting relative speed as labels consistently improve the performance on three backbone networks and on two datasets compared with directly using playback speed of each clip (SP + A-VID v.s RSP + A-VID). These results demonstrate that relative speed labels are more consistent with the motion content and help models to learn more discriminative video features.</p><p>Does speed augmentation help? Instead of naively extend instance discrimination task from image domain to video domain, we propose to randomize the speed of each clip. To verify its effectiveness, we implement a variant by dropping speed augmentation. We denote it as VID as it is not appearance-focused. <ref type="table" target="#tab_1">Table 1</ref> shows that the speed augmentation strategy significantly improve the performance (RSP + VID v.s RSP + A-VID). The reason is that the speed augmentation strategy make the VID task become speedagnostic. In this way, models are encouraged to pay more attention on learning appearance features. Together with the motion features learnt from RSP task, models can extract more discriminative representation for appearance and motion, which are both important for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on action recognition task</head><p>Performance on UCF101 and HMDB51. We compare our method with the state-of-the-art self-supervised learning methods in <ref type="table">Table 2</ref>. We report top-1 accuracy on UCF101 and HMDB51 datasets together with the backbone and pre-training dataset. As the prior works use different backbone networks for experiments, we report results using the same settings as theirs for fair comparisons. Our RSPNet achieves the best results on all backbone networks over two datasets. Specifically, with C3D, our method outperforms MAS <ref type="bibr" target="#b41">[42]</ref>  A-VID loss RSP loss Accuracy on UCF101 <ref type="figure">Figure 3</ref>. Pre-training losses of two pretext tasks and Top-1 accuracy of UCF101 after fine-tuning. We pre-train S3D-G model on K-400 for 1000 epochs and report the results every 200 epochs.</p><p>6.1% and 7.3% absolute improvement on two datasets, respectively. With R(2+1)D, our RSPNet improves accuracy from 77.1% to 81.1% on UCF101 and from 36.6% to 44.6% on HMDB51. For S3D-G, we follow SpeedNet <ref type="bibr" target="#b0">[1]</ref> to use video frames with 224 × 224 as input for pre-training and fine-tuning. Under the same settings, our RSPNet increase the accuracy from 81.1% to 89.9% on UCF101 and from 48.8% to 59.6% on HMDB51. When we train longer (i.e., 1000 epochs), we can further improve the tkeop-1 accuracy to 93.7% and 64.7% for two datasets, respectively. In <ref type="figure">Figure 3</ref>, we show the curve of pre-training losses and the performance on UCF101 for S3D-G model using different checkpoints. As the losses decrease, the performance for downstream task increases consistently. This demonstrates the effectiveness of the proposed RSP and A-VID tasks.The model does learn semantic representation to solve them instead of learning trivial solution. Remarkably, without the need of any annotation for pre-training, our RSPNet outperforms the ImageNet supervised pre-trained variant (93.7% v.s 86.6%) and achieve close performance to the Kinetics supervised pre-trained model (96.8%).</p><p>Performance on Something-V2. We compare our RSP-Net with supervised learning methods on Something-V2, a challenging dataset in which temporal information is essential <ref type="bibr" target="#b30">[31]</ref>. Following the settings in Lin et al. <ref type="bibr" target="#b30">[31]</ref>, we train models for 50 epochs and set the initial learning rate to 0.01 (decays by 0.1 at epoch <ref type="bibr">20 and 40)</ref>. For the supervised pre-trained models, the ResNet-18 and S3D-G are pre-trained on K-400 dataset, and C3D is pre-trained on Sport-1M dataset <ref type="bibr" target="#b26">[27]</ref>. Both K-400 and Sport-1M are large-scale datasets with manually annotated action labels, and thus the supervised pre-trained models are strong baselines for our unsupervised pre-trained RSPNet.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, despite not using manual annotation, RSP-Net consistently increases the accuracy compared with the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on video retrieval task</head><p>Given a query video, we use the nearest neighbor search to retrieve relevant videos based on the cosine similarity of their features. Specifically, following previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b45">46]</ref>, we evenly sample 10 clips for each video and take the output of the last convolutional layer in spatial-temporal encoder as clip-level features. Then, we perform spatial maxpooling on each clip and average-pooling over 10 clips to obtain a video-level feature vector. We use the video in testing set to retrieve the videos in training set. We evaluate our method on the split 1 of UCF101 dataset and apply the topk accuracies (k=1, 5, 10, 20, 50) as evaluation metrics. Our RSPNet is pre-trained on K-400 dataset.</p><p>From <ref type="table" target="#tab_5">Table 4</ref>, our method outperforms state-of-thearts by a large margin under different values of k. For example, our method achieves much better performance than Pace <ref type="bibr" target="#b42">[43]</ref> under all values of k using the same C3D backbone. With ResNet-18 as backbone network, we can achieve better retrieval performance. These imply that the proposed pretext tasks help us to learn more discriminative features for video retrieval tasks.</p><p>We further provide some retrieval results in <ref type="figure" target="#fig_1">Figure 4</ref> as a qualitative study. For the two query clips, we successfully retrieve highly relevant videos with very similar appearance and motion. This implies that our method is able to learn both meaningful appearance and motion features for videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">RoI visualization</head><p>From Section 3.1, we formulate two pretext tasks as metric learning, which seeks to maximize the similarity of the positive pair. To better understand the clues learnt for the two pretext tasks, we visualize the region of interest (RoI) that contributes most to the similarity score using the classactivation map (CAM) technique <ref type="bibr" target="#b51">[52]</ref>. We will describe the technical details of visualization followed by the results analysis.</p><p>In our RSPNet, we calculate the similarity s between video clip features x i and x j using cosine distance, i.e., s =  R 128×C and W j ∈ R 128×C are the parameters for the projection head g m (or g a ).</p><formula xml:id="formula_5">(W j x j ) (W i x i ) = ((W j x j ) W i )x i , where W i ∈</formula><p>The features x i is average pooled from the last convolutional features F i ∈ R C×H×W ×T of the spatial-temporal encoder, where C is the number of channels and H, W, T are spatial-temporal sizes. In analogy with CAM <ref type="bibr" target="#b51">[52]</ref>, the similarity activation maps M s ∈ R H×W ×T of clip c i for similarity score s can be defined as</p><formula xml:id="formula_6">M s = ((W j x j ) W i )F i .<label>(4)</label></formula><p>Such similarity activation maps indicate the salient regions of clip c i that are used by models to figure out whether the two clips are positive pair. We can also obtain activation maps of clip c j in a similar manner. More details can refer to Zhou et al. <ref type="bibr" target="#b51">[52]</ref>. Although both RSP and A-VID pretext tasks are based on the same features F i , we use two independent projection heads g m and g a to map F i to different 128-D embedding spaces, as shown in <ref type="figure">Figure 2</ref>. Thus, the parameters of linear layers, i.e., W i and W j , for two pretext tasks are different. Consequently, the activation maps can be different and models can focus on learning different clues for completing each specific pretext task.</p><p>In <ref type="figure">Figure 5</ref>, we show the heatmaps in three positive clip pairs. We use the middle frame to represent a clip to visualize the heatmap. For the RSP task, the heatmaps tend to cover the whole region of actions, which provides rich information for perceiving the relative speed. For the A-VID task, models tend to focus on small but discriminative regions (e.g., the striped clothes and the eyes of a baby in two Positive pair for A-VID Positive pair for RSP <ref type="figure">Figure 5</ref>. Visualization of RoI learnt for RSP and A-VID. Our model focuses on the regions containing rich motion and appearance information for two pretext tasks, respectively. We outline the area where the heatmap is higher than a threshold with a rectangle.</p><p>pair samples, respectively) to identify two clips in the same video. One interesting finding is that the models are able to adaptively localize the same object even though they appear in different locations of a frame. This may provide a new perspective for person re-identification and we leave it for futher work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed an unsupervised video representation learning framework named RSPNet. We train models to perceive relative playback speed for learning motion features by using relative speed labels to resolve the imprecise speed label issue. Also, we extend instance discrimination task to video domain and propose a speed augmentation strategy to make models focus on learning appearance features. Extensive experiments show that the features learnt by RSPNet perform better on action recognition and video retrieval downstream tasks. Visualization of RoI implies that RSPNet can focus on discriminative area for two tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative examples of video retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different pre-training settings on UCF101 and HMDB51 datasets. All models are pre-trained on the Kinetics-100 dataset except for the w/o pre-training setting. SP denotes speed prediction for each individual clip. VID denotes video instance discrimination without speed augmentation strategy.</figDesc><table><row><cell>Pre-training settings</cell><cell cols="3">UCF101 TSM-18 ResNet-18 C3D</cell><cell cols="3">HMDB51 TSM-18 ResNet-18 C3D</cell></row><row><cell>w/o pre-training</cell><cell>49.7</cell><cell>42.3</cell><cell>59.0</cell><cell>17.5</cell><cell>19.0</cell><cell>24.9</cell></row><row><cell>w/ RSP only</cell><cell>54.5</cell><cell>49.7</cell><cell>67.2</cell><cell>26.5</cell><cell>25.9</cell><cell>29.4</cell></row><row><cell>w/ A-VID only</cell><cell>60.8</cell><cell>57.2</cell><cell>68.1</cell><cell>30.2</cell><cell>31.1</cell><cell>35.1</cell></row><row><cell>SP + A-VID</cell><cell>59.8</cell><cell>57.8</cell><cell>70.9</cell><cell>29.7</cell><cell>30.7</cell><cell>35.1</cell></row><row><cell>RSP + VID</cell><cell>57.5</cell><cell>54.2</cell><cell>70.8</cell><cell>30.1</cell><cell>29.9</cell><cell>34.5</cell></row><row><cell>RSP + A-VID (Ours)</cell><cell>61.2</cell><cell>60.2</cell><cell>71.5</cell><cell>32.2</cell><cell>32.6</cell><cell>36.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>by a large margin (76.7% v.s 61.2% on UCF101 and 44.6% v.s 33.4% on HMDB51).</figDesc><table><row><cell>loss</cell><cell>2 4 6 8 10</cell><cell>77.6</cell><cell>88.4</cell><cell>89.3</cell><cell>90.1</cell><cell>92.4</cell><cell>93.7</cell><cell>75 80 85 90 95 accuracy (%)</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>200</cell><cell cols="2">400 epoch 600</cell><cell>800</cell><cell>1000</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>With ResNet-18, our method outperforms DPC [20] by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on Something-V2.</figDesc><table><row><cell></cell><cell cols="3">ResNet-18 C3D S3D-G</cell></row><row><cell>w/o pre-training</cell><cell>42.1</cell><cell>45.8</cell><cell>51.2</cell></row><row><cell>Fully supervised</cell><cell>43.7</cell><cell>47.0</cell><cell>56.8</cell></row><row><cell>Unsupervised (Ours)</cell><cell>44.0</cell><cell>47.8</cell><cell>55.0</cell></row><row><cell cols="4">random initialized models on three backbone architectures.</cell></row><row><cell cols="4">Surprisingly, RSPNet even outperforms the supervised pre-</cell></row><row><cell cols="4">trained model on ResNet-18 and C3D, increasing from</cell></row><row><cell cols="4">43.7% to 44.0% and from 47.0% to 47.8%, respectively. It</cell></row><row><cell cols="4">shows the benefits of the discriminative features learnt from</cell></row><row><cell cols="2">the proposed two pretext tasks.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Video retrieval results on UCF101, measured by top-k retrieval accuracy (%).</figDesc><table><row><cell></cell><cell>Method</cell><cell>Architecture</cell><cell cols="5">Top-k k = 1 k = 5 k = 10 k = 20 k = 50</cell></row><row><cell></cell><cell>OPN [30]</cell><cell>OPN</cell><cell>19.9</cell><cell>28.7</cell><cell>34.0</cell><cell>40.6</cell><cell>51.6</cell></row><row><cell></cell><cell>Buchler et al. [2]</cell><cell>CaffeNet</cell><cell>25.7</cell><cell>36.2</cell><cell>42.2</cell><cell>49.2</cell><cell>59.5</cell></row><row><cell></cell><cell>ClipOrder [46]</cell><cell>R3D</cell><cell>14.1</cell><cell>30.3</cell><cell>40.0</cell><cell>51.1</cell><cell>66.5</cell></row><row><cell></cell><cell>SpeedNet [1]</cell><cell>S3D-G</cell><cell>13.0</cell><cell>28.1</cell><cell>37.5</cell><cell>49.5</cell><cell>65.0</cell></row><row><cell></cell><cell>VCP [33]</cell><cell>R(2+1)D</cell><cell>19.9</cell><cell>33.7</cell><cell>42.0</cell><cell>50.5</cell><cell>64.4</cell></row><row><cell></cell><cell>Pace [43]</cell><cell>C3D</cell><cell>31.9</cell><cell>49.7</cell><cell>59.2</cell><cell>68.9</cell><cell>80.2</cell></row><row><cell></cell><cell>RSPNet (Ours)</cell><cell>C3D ResNet-18</cell><cell>36.0 41.1</cell><cell>56.7 59.4</cell><cell>66.5 68.4</cell><cell>76.3 77.8</cell><cell>87.7 88.7</cell></row><row><cell>Query</cell><cell cols="2">Retrieval results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Positive pair</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for RSP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Positive pair</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for A-VID</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Büchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multim</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2723" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating visually aligned sound from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8292" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709, 2020. 5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning using variable playback speed prediction. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyung Jin Chang, and Wonjun Hwang</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why can&apos;t I dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Oops! predicting unintentional action in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Foley Music : Learning to Generate Music from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised moving vehicle tracking with stereo sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7052" to="7061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Location-aware graph convolutional networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11021" to="11028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno>abs/1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TSM: temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7834" to="7843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video cloze procedure for self-supervised spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lanckriet. Learning content similarity for music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Barrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">R G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2207" to="2218" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning video representations using contrastive bidirectional transformer. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<title level="m">Contrastive multiview coding. arXiv, abs</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Selfsupervised video representation learning by pace prediction. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Person reidentification with metric learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="791" to="805" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Breaking Winner-Takes-All : Iterative-Winners-Out Networks for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dense regression network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10284" to="10293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
