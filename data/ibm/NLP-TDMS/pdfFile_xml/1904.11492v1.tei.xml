<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>caoyue10@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a queryindependent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at https://github.com/xvjiarui/GCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Capturing long-range dependency, which aims to extract the global understanding of a visual scene, is proven to benefit a wide range of recognition tasks, such as image/video classification, object detection and segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14]</ref>. In convolution neural networks, as the convolution layer builds pixel relationship in a local neighborhood, the long-range dependencies are mainly modeled by deeply stacking convolution layers. However, directly repeating convolution layers is computationally inefficient and hard to optimize <ref type="bibr" target="#b30">[31]</ref>. This would lead to ineffective modeling of long-range dependency, due in part to difficulties in delivering messages between distant positions.</p><p>To address this issue, the non-local network <ref type="bibr" target="#b30">[31]</ref> is proposed to model the long-range dependencies using one layer, via self-attention mechanism <ref type="bibr" target="#b27">[28]</ref>. For each query position, the non-local network first computes the pairwise relations between the query position and all positions to form an attention map, and then aggregates the features of all positions by weighted sum with the weights defined by the attention map. The aggregated features are finally added to the features of each query position to form the output.</p><p>The query-specific attention weights in the non-local network generally imply the importance of the corresponding positions to the query position. While visualizing the queryspecific importance weights would help the understanding in depth, such analysis was largely missing in the original paper. We bridge this regret, as in <ref type="figure" target="#fig_0">Figure 1</ref>, but surprisingly observe that the attention maps for different query positions are almost the same, indicating only query-independent dependency is learnt. This observation is further verified by statistical analysis in <ref type="table">Table 1</ref> that the distance between the attention maps of different query positions is very small.</p><p>Based on this observation, we simplify the non-local block by explicitly using a query-independent attention map for all query positions. Then we add the same aggregated features using this attention map to the features of all query positions for form the output. This simplified block has significantly smaller computation cost than the original non-local block, but is observed with almost no decrease in accuracy on several important visual recognition tasks. Furthermore, we find this simplified block shares similar structure with the popular Squeeze-Excitation (SE) Network <ref type="bibr" target="#b13">[14]</ref>. They both strengthen the original features by the same features aggregated from all positions but differentiate each other by choices on the aggregation strategy, transformation and strengthening functions. By abstracting these functions, we reach a three-step general framework which unifies both the simplified NL block and the SE block: (a) a context modeling module which aggregates the features of all positions together to form a global context feature; (b) a feature transform module to capture the channel-wise interdependencies; and (c) a fusion module to merge the global context feature into features of all positions.</p><p>The simplified NL block and SE block are two instantiations of this general framework, but with different implementations of the three steps. By comparison study on each step, we find both the simplified non-local block and the SE block are sub-optimal, that each block has a part of the steps advancing over the other. By a combination of the optimal implementation at each step, we reach a new instantiation of the general framework, called global context (GC) block. The new block shares the same implementation with the simplified NL block on the context modeling (using global attention pooling) and fusion (using addition) steps, while shares the same transform step (using two-layer bottleneck) with SE block. The GC block is shown to perform better than both the simplified non-local block and SE block on multiple visual recognition tasks.</p><p>Like SE block, the proposed GC block is also lightweight which allows it to be applied to all residual blocks in the ResNet architecture, in contrast to the original non-local block which is usually applied after one or a few layers due to its heavy computation. The GC block strengthened network is named global context network (GCNet). On COCO object detection/segmentation, the GCNet outperforms NL-Net and SENet by 1.9% and 1.7% on AP box , and 1.5% and 1.5% on AP mask , respectively, with just a 0.07% relative increase in FLOPs. In addition, GCNet yields significant performance gains over three general visual recognition tasks: object detection/segmentation on COCO (2.7%↑ on AP bbox , and 2.4%↑ on AP mask over Mask R-CNN with FPN and ResNet-50 as backbone <ref type="bibr" target="#b8">[9]</ref>), image classification on Ima-geNet (0.8%↑ on top-1 accuracy over ResNet-50 <ref type="bibr" target="#b9">[10]</ref>), and action recognition on Kinetics (1.1%↑ on top-1 accuracy over the ResNet-50 Slow-only baseline <ref type="bibr" target="#b5">[6]</ref>), with less than a 0.26% increase in computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep architectures. As convolution networks have recently achieved great success in large-scale visual recognition tasks, a number of attempts have been made to improve the original architecture in a bid to achieve better accuracy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6]</ref>. An important direction of network design is to improve the functional formulations of basic components to elevate the power of deep networks. ResNeXt <ref type="bibr" target="#b33">[34]</ref> and Xception <ref type="bibr" target="#b2">[3]</ref> adopt group convolution to increase cardinality. Deformable ConvNets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref> design deformable convolution to enhance geometric modeling ability. Squeeze-Excitation Networks <ref type="bibr" target="#b13">[14]</ref> adopt channel-wise rescaling to explicitly model channel dependencies.</p><p>Our global context network is a new backbone architecture, with novel GC blocks to enable more effective global context modeling, offering superior performances on a wide range of vision tasks, such as object detection, instance segmentation, image classification and action recognition.</p><p>Long-range dependency modeling. The recent approaches for long-range dependency modeling can be categorized into two classes. The first is to adopt self-attention mechanism to model the pairwise relations. The second is to model the query-independent global context.</p><p>Self-attention mechanisms have recently been successfully applied in various tasks, such as machine translation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>, graph embedding <ref type="bibr" target="#b28">[29]</ref>, generative modeling <ref type="bibr" target="#b38">[39]</ref>, and visual recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="bibr" target="#b27">[28]</ref> is one of the first attempts to apply a self-attention mechanism to model long-range dependencies in machine translation. <ref type="bibr" target="#b11">[12]</ref> extends self-attention mechanisms to model the relations between objects in object detection. NLNet <ref type="bibr" target="#b30">[31]</ref> adopts selfattention mechanisms to model the pixel-level pairwise relations. CCNet <ref type="bibr" target="#b15">[16]</ref> accelerates NLNet via stacking two criss-cross blocks, and is applied to semantic segmentation. However, NLNet actually learns query-independent attention maps for each query position, which is a waste of computation cost to model pixel-level pairwise relations.</p><p>To model the global context features, SENet <ref type="bibr" target="#b13">[14]</ref>, GENet <ref type="bibr" target="#b12">[13]</ref>, and PSANet <ref type="bibr" target="#b40">[41]</ref> perform rescaling to different channels to recalibrate the channel dependency with global context. CBAM <ref type="bibr" target="#b31">[32]</ref> recalibrates the importance of different spatial positions and channels both via rescaling. However, all these methods adopt rescaling for feature fusion which is not effective enough for global context modeling.</p><p>The proposed GCNet can effectively model the global context via addition fusion as NLNet <ref type="bibr" target="#b30">[31]</ref> (which is heavyweight and hard to be integrated to multiple layers), with the lightweight property as SENet <ref type="bibr" target="#b13">[14]</ref> (which adopts scaling and is not effective enough for global context modeling). Hence, via more effective global context modeling, GCNet can achieve better performance than both NLNet and SENet on major benchmarks for various recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis on Non-local Networks</head><p>In this section, we first review the design of the nonlocal block <ref type="bibr" target="#b30">[31]</ref>. To give an intuitive understanding, we visualize the attention maps across different query positions generated by a widely-used instantiation of the non-local block. To statistically analyze its behavior, we average the distances (cosine distance and Jensen-Shannon divergence) between the attention maps of all query positions.  <ref type="figure">Figure 3</ref>: Architecture of non-local block (Embedded Gaussian) and its simplified version. The feature maps are shown by their dimensions, e.g. CxHxW. ⊗ is matrix multiplication, and ⊕ is broadcast element-wise addition. For two matrices with different dimensions, broadcast operations first broadcast features in each dimension to match the dimensions of the two matrices.</p><formula xml:id="formula_0">(b) Simplified NL block (Eqn 2) C x H x W + C x 1 x 1 C x H x W conv (1x1) × C x HW 1 x H x W conv (1x1) Softmax HW x 1 x 1 (a) NL block C x H x W + conv (1x1) × conv (1x1) C x HW conv (1x1) Softmax × C x HW HW x C conv (1x1) C x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting the Non-local Block</head><p>The basic non-local block <ref type="bibr" target="#b30">[31]</ref> aims at strengthening the features of the query position via aggregating information from other positions. We denote x={x i } Np i=1 as the feature map of one input instance (e.g., an image or video), where N p is the number of positions in the feature map (e.g., N p =H·W for image, N p =H·W·T for video). x and z denote the input and output of the non-local block, respectively, which have the same dimensions. The non-local block can then be expressed as</p><formula xml:id="formula_1">z i = x i + W z Np j=1 f (x i , x j ) C (x) (W v · x j ),<label>(1)</label></formula><p>where i is the index of query positions, and j enumerates all possible positions. f (x i , x j ) denotes the relationship between position i and j, and has a normalization factor C (x). W z and W v denote linear transform matrices (e.g., 1x1 convolution). For simplification, we denote</p><formula xml:id="formula_2">ω ij = f (xi,xj ) C(x)</formula><p>as normalized pairwise relationship between position i and j.</p><p>To meet various needs in practical applications, four instantiations of the non-local block with different ω ij are designed, namely Gaussian, Embedded Gaussian, Dot product, and Concat: (a) Gaussian denotes that f in ω ij is the Gaussian function, defined as ω ij = exp( xi,xj ) m exp( xi,xm ) ; (b) Embedded Gaussian is a simple extension of Gaussian, which computes similarity in an embedding space, defined as</p><formula xml:id="formula_3">ω ij = exp( Wqxi,W k xj ) m exp( Wqxi,W k xm ) ; (c) For Dot prod- uct, f in ω ij is defined as a dot-product similarity, formu- lated as ω ij = Wqxi,W k xj Np ; (d) Concat is defined literally, as ω ij = ReLU(Wq[xi,xj ]) Np</formula><p>. The most widely-used instantiation, Embedded Gaussian, is illustrated in <ref type="figure">Figure 3</ref>   <ref type="table">Table 1</ref>: Statistical analysis on four instantiations of nonlocal blocks. 'input' denotes the input of non-local block (x i ), 'output' denotes the output of the non-local block (z i − x i ), 'att' denotes the attention map of query positions (ω i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis</head><p>Visualization To intuitively understand the behavior of the non-local block, we first visualize the attention maps for different query positions. As different instantiations achieve comparable performance <ref type="bibr" target="#b30">[31]</ref>, here we only visualize the most widely-used version, Embedded Gaussian, which has the same formulation as the block proposed in <ref type="bibr" target="#b27">[28]</ref>. Since attention maps in videos are hard to visualize and understand, we only show visualizations on the object detection/segmentation task, which takes images as input. Following the standard setting of non-local networks for object detection <ref type="bibr" target="#b30">[31]</ref>, we conduct experiments on Mask R-CNN with FPN and Res50, and only add one non-local block right before the last residual block of res 4 .</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we randomly select six images from the COCO dataset, and visualize three different query positions (red points) and their query-specific attention maps (heatmaps) for each image. We surprisingly find that for different query positions, their attention maps are almost the same. To verify this observation statistically, we analyze the distances between the global contexts of different query positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>Denote v i as the feature vector for position i. The average distance measure is defined as</p><formula xml:id="formula_4">avg dist = 1 N 2 p Np i=1 Np j=1 dist (v i , v j ), where dist(·, ·)</formula><p>is the distance function between two vectors.</p><p>Cosine distance is a widely-used distance measure,</p><formula xml:id="formula_5">defined as dist(v i , v j )=(1 − cos(v i , v j ))/2.</formula><p>Here we compute the cosine distance between three kinds of vectors, the non-local block inputs (v i =x i , 'input' in <ref type="table">Table  1</ref>), the non-local block outputs before fusion (v i =z i -x i , 'output' in <ref type="table">Table 1</ref>), and the attention maps of query positions (v i =ω i , 'att' in <ref type="table">Table 1</ref>). The Jensen-Shannon divergence (JSD) is adopted to measure the statistical distance between two probability distributions, as</p><formula xml:id="formula_6">dist (v i , v j )= 1 2 Np k=1 v ik log 2v ik v ik +v jk + v jk log 2v jk v ik +v jk .</formula><p>As the summation over each attention map ω i is 1 (in Gaus-sian and E-Gaussian), we can regard each ω i as a discrete probability distribution. Hence we compute JSD between the attention maps (v i =ω i ) for Gaussian and E-Gaussian.</p><p>Results for two distance measures on two standard tasks are shown in <ref type="table">Table 1</ref>. First, large values of cosine distance in the 'input' column show that the input features for the non-local block can be discriminated across different positions. But the values of cosine distance in 'output' are quite small, indicating that global context features modeled by the non-local block are almost the same for different query positions. Both distance measures on attention maps ('att') are also very small for all instantiations, which again verifies the observation from visualization. In other words, although a non-local block intends to compute the global context specific to each query position, the global context after training is actually independent of query position. Hence, there is no need to compute query-specific global context for each query position, allowing us to simplify the non-local block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simplifying the Non-local Block</head><p>As different instantiations achieve comparable performance on both COCO and Kinetics, as shown in <ref type="table">Table 1</ref>, here we adopt the most widely-used version, Embedded Gaussian, as the basic non-local block. Based on the observation that the attention maps for different query positions are almost the same, we simplify the non-local block by computing a global (query-independent) attention map and sharing this global attention map for all query positions. Following the results in <ref type="bibr" target="#b11">[12]</ref> that variants with and without W z achieve comparable performance, we omit W z in the simplified version. Our simplified non-local block is defined as</p><formula xml:id="formula_7">z i = x i + Np j=1 exp (W k x j ) Np m=1 exp (W k x m ) (W v · x j ), (2)</formula><p>where W k and W v denote linear transformation matrices. This simplified non-local block is illustrated in <ref type="figure">Figure 3</ref></p><formula xml:id="formula_8">(b).</formula><p>To further reduce the computational cost of this simplified block, we apply the distributive law to move W v outside of the attention pooling, as</p><formula xml:id="formula_9">z i = x i + W v Np j=1 exp (W k x j ) Np m=1 exp (W k x m ) x j . (3)</formula><p>This version of the simplified non-local block is illustrated in <ref type="figure">Figure 4</ref> Different from the traditional non-local block, the second term in Eqn 3 is independent to the query position i, which means this term is shared across all query positions i. We thus directly model global context as a weighted average of  <ref type="figure">Figure 4</ref>: Architecture of the main blocks. The feature maps are shown as feature dimensions, e.g. CxHxW denotes a feature map with channel number C, height H and width W. ⊗ denotes matrix multiplication, ⊕ denotes broadcast elementwise addition, and denotes broadcast element-wise multiplication.</p><formula xml:id="formula_10">W v W v W k W k P j ↵ j x j P j ↵ j x j W v1 W v1 W v2 W v2 W k W k</formula><p>the features at all positions, and aggregate (add) the global context features to the features at each query position. In experiments, we directly replace the non-local (NL) block with our simplified non-local (SNL) block, and evaluate accuracy and computation cost on three tasks, object detection on COCO, ImageNet classification, and action recognition, shown in <ref type="table">Table 2</ref>(a), 4(a) and 5. As we expect, the SNL block achieves comparable performance to the NL block with significantly lower FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Global Context Modeling Framework</head><p>As shown in <ref type="figure">Figure 4(b)</ref>, the simplified non-local block can be abstracted into three procedures: (a) global attention pooling, which adopts a 1x1 convolution W k and softmax function to obtain the attention weights, and then performs the attention pooling to obtain the global context features; (b) feature transform via a 1x1 convolution W v ; (c) feature aggregation, which employs addition to aggregate the global context features to the features of each position.</p><p>We regard this abstraction as a global context modeling framework, illustrated in <ref type="figure">Figure 4</ref>(a) and defined as</p><formula xml:id="formula_11">z i = F x i , δ Np j=1 α j x j ,<label>(4)</label></formula><p>where (a) j α j x j denotes the context modeling module which groups the features of all positions together via weighted averaging with weight α j to obtain the global context features (global attention pooling in the simplified NL (SNL) block); (b) δ(·) denotes the feature transform to capture channel-wise dependencies (1x1 conv in the SNL block); and (c) F (·, ·) denotes the fusion function to aggregate the global context features to the features of each position (broadcast element-wise addition in the SNL block). Interestingly, the squeeze-excitation (SE) block proposed in <ref type="bibr" target="#b13">[14]</ref> is also an instantiation of our proposed framework. Illustrated in <ref type="figure">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Global Context Block</head><p>Here we propose a new instantiation of the global context modeling framework, named the global context (GC) block, which has the benefits of both the simplified nonlocal (SNL) block with effective modeling on long-range dependency, and the squeeze-excitation (SE) block with lightweight computation.</p><p>In the simplified non-local block, shown in <ref type="figure">Figure 4</ref>(b), the transform module has the largest number of parameters, including from one 1x1 convolution with C·C parameters. When we add this SNL block to higher layers, e.g. res 5 , the number of parameters of this 1x1 convolution, C·C=2048·2048, dominates the number of parameters of this block. To obtain the lightweight property of the SE block, this 1x1 convolution is replaced by a bottleneck transform module, which significantly reduces the number of parameters from C·C to 2·C·C/r, where r is the bottleneck ratio and C/r denotes the hidden representation dimension of the bottleneck. With default reduction ratio set to r=16, the number of params for transform module can be reduced to 1/8 of the original SNL block. More results on different values of bottleneck ratio r are shown in <ref type="table">Table 2</ref>(e).</p><p>As the two-layer bottleneck transform increases the dif-ficulty of optimization, we add layer normalization inside the bottleneck transform (before ReLU) to ease optimization, as well as to act as a regularizer that can benefit generalization. As shown in <ref type="table">Table 2</ref>(d), layer normalization can significantly enhance object detection and instance segmentation on COCO. The detailed architecture of the global context (GC) block is illustrated in <ref type="figure">Figure 4(d)</ref>, formulated as</p><formula xml:id="formula_12">zi = xi + Wv2ReLU LN Wv1 Np j=1 e W k x j Np m=1 e W k xm xj ,<label>(5)</label></formula><p>where α j = e W k x j m e W k xm is the weight for global attention pooling, and δ(·) = W v2 ReLU(LN(W v1 (·))) denotes the bottleneck transform. Specifically, our GC block consists of: (a) global attention pooling for context modeling; (b) bottleneck transform to capture channel-wise dependencies; and (c) broadcast element-wise addition for feature fusion.</p><p>Since the GC block is lightweight, it can be applied in multiple layers to better capture the long-range dependency with only a slight increase in computation cost. Taking ResNet-50 for ImageNet classification as an example, GC-ResNet-50 denotes adding the GC block to all layers (c3+c4+c5) in ResNet-50 with a bottleneck ratio of 16. GC-ResNet-50 increases ResNet-50 computation from ∼3.86 GFLOPs to ∼3.87 GFLOPs, corresponding to a 0.26% relative increase. Also, GC-ResNet-50 introduces ∼2.52M additional parameters beyond the ∼25.56M parameters required by ResNet-50, corresponding to a ∼9.86% increase.</p><p>Global context can benefit a wide range of visual recognition tasks, and the flexibility of the GC block allows it to be plugged into network architectures used in various computer vision problems. In this paper, we apply our GC block to three general vision tasks -image recognition, object detection/segmentation and action recognition -and observe significant improvements in all three.</p><p>Relationship to non-local block. As the non-local block actually learns query-independent global context, the global attention pooling of our global context block models the same global context as the NL block but with significantly lower computation cost. As the GC block adopts the bottleneck transform to reduce redundancy in the global context features, the numbers of parameters and FLOPs are further reduced. The FLOPs and number of parameters of the GC block are significantly lower than that of NL block, allowing our GC block to be applied to multiple layers with just a slight increase in computation, while better capturing long-range dependency and aiding network training.</p><p>Relationship to squeeze-excitation block. The main difference between the SE block and our GC block is the fusion module, which reflects the different goals of the two blocks. The SE block adopts rescaling to recalibrate the importance of channels but inadequately models long-range dependency. Our GC block follows the NL block by utilizing addition to aggregate global context to all positions for capturing long-range dependency. The second difference is the layer normalization in the bottleneck transform. As our GC block adopts addition for fusion, layer normalization can ease optimization of the two-layer architecture for the bottleneck transform, which can lead to better performance. Third, global average pooling in the SE block is a special case of global attention pooling in the GC block. Results in <ref type="table">Table 2</ref>(f) and 4(b) show the superiority of our GCNet compared to SENet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate the proposed method, we carry out experiments on three basic tasks, object detection/segmentation on COCO <ref type="bibr" target="#b20">[21]</ref>, image classification on ImageNet <ref type="bibr" target="#b4">[5]</ref>, and action recognition on Kinetics <ref type="bibr" target="#b16">[17]</ref>. Experimental results demonstrate that the proposed GCNet generally outperforms both non-local networks (with lower FLOPs) and squeeze-excitation networks (with comparable FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object Detection/Segmentation on COCO</head><p>We investigate our model on object detection and instance segmentation on COCO 2017 <ref type="bibr" target="#b20">[21]</ref>, whose train set is comprised of 118k images, validation set of 5k images, and test-dev set of 20k images. We follow the standard setting <ref type="bibr" target="#b8">[9]</ref> of evaluating object detection and instance segmentation via the standard mean average-precision scores at different boxes and the mask IoUs, respectively.</p><p>Setup. Our experiments are implemented with PyTorch <ref type="bibr" target="#b22">[23]</ref>. Unless otherwise noted, our GC block of ratio r=16 is applied to stage c3, c4, c5 of ResNet/ResNeXt.</p><p>Training. We use the standard configuration of Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> with FPN and ResNet/ResNeXt as the backbone architecture. The input images are resized such that their shorter side is of 800 pixels <ref type="bibr" target="#b19">[20]</ref>. We trained on 8 GPUs with 2 images per GPU (effective mini batch size of 16). The backbones of all models are pretrained on Ima-geNet classification <ref type="bibr" target="#b4">[5]</ref>, then all layers except for c1 and c2 are jointly finetuned with detection and segmentation heads. Unlike stage-wise training with respect to RPN in <ref type="bibr" target="#b8">[9]</ref>, endto-end training like in <ref type="bibr" target="#b24">[25]</ref> is adopted for our implementation, yielding better results. Different from the conventional finetuning setting <ref type="bibr" target="#b8">[9]</ref>, we use Synchronized Batch-Norm to replace frozen BatchNorm. All models are trained for 12 epochs using Synchronized SGD with a weight decay of 0.0001 and momentum of 0.9, which roughly corresponds to the 1x schedule in the Mask R-CNN benchmark <ref type="bibr" target="#b21">[22]</ref>. The learning rate is initialized to 0.02, and decays by a factor of 10 at the 9th and 11th epochs. The choice of hyper-parameters also follows the latest release of the Mask R-CNN benchmark <ref type="bibr" target="#b21">[22]</ref>.  <ref type="table">Table 2</ref>: Ablation study based on Mask R-CNN, using ResNet-50 as backbone with FPN, for object detection and instance segmentation on COCO 2017 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Study</head><p>The ablation study is done on COCO 2017 validation set. The standard COCO metrics including AP, AP 50 , AP 75 for both bounding boxes and segmentation masks are reported. Block design. Following <ref type="bibr" target="#b30">[31]</ref>, we insert 1 non-local block (NL), 1 simplified non-local block (SNL), or 1 global context block (GC) right before the last residual block of c4. Table 2(a) shows that both SNL and GC achieve performance comparable to NL with fewer parameters and less computation, indicating redundancy in computation and parameters in the original non-local design. Furthermore, adding the GC block in all residual blocks yields higher performance (1.1%↑ on AP bbox and 0.9%↑ on AP mask ) with a slight increase of FLOPs and #params.</p><p>Positions. The NL block is inserted after the residual block (afterAdd), while the SE block is integrated after the last 1x1 conv inside the residual block (after1x1). In <ref type="table">Table  2</ref>(b), we investigate both cases with GC block and they yield similar results. Hence we adopt after1x1 as the default.</p><p>Stages. <ref type="table">Table 2</ref>(c) shows the results of integrating the GC block at different stages. All stages benefit from global context modeling in the GC block (0.7%-1.7%↑ on AP bbox and AP mask ). Inserting to c4 and c5 both achieves better performance than to c3, demonstrating that better semantic features can benefit more from the global context modeling. With slight increase in FLOPs, inserting the GC block to all layers (c3+c4+c5) yields even higher performance than inserting to only a single layer.</p><p>Bottleneck design. The effects of each component in the bottleneck transform are shown in <ref type="table">Table 2</ref>(d). w/o ratio denotes the simplified NLNet using one 1x1 conv as the transform, which has much more parameters compared to the baseline. Even though r16 and r16+ReLU have much fewer parameters than the w/o ratio variant, two layers are found to be harder to optimize and lead to worse performance than a single layer. So LayerNorm (LN) is exploited to ease optimization, leading to performance similar to w/o ratio but with much fewer #params.</p><p>Bottleneck ratio. The bottleneck design is intended to reduce redundancy in parameters and provide a tradeoff between performance and #params. In <ref type="table">Table 2</ref>(e), we alter the ratio r of bottleneck. As the ratio r decreases (from 32 to 4) with increasing number of parameters and FLOPs, the performance improves consistently (0.8%↑ on AP bbox and 0.5%↑ on AP mask ), indicating that our bottleneck strikes a good balance of performance and parameters. It is worth noting that even with a ratio of r=32, the network still outperforms the baseline by large margins.</p><p>Pooling and fusion. The different choices on pooling and fusion are ablated in <ref type="table">Table 2</ref>(f). First, it shows that addition is more effective than scaling in the fusion stage. It is surprising that attention pooling only achieves slightly better results than vanilla average pooling. This indicates that how global context is aggregated to query positions (choice of fusion module) is more important than how features from all positions are grouped together (choice in context modeling module). It is worth noting that, our GCNet (att+add) significantly outperforms SENet, because of effective modeling of long-range dependency with attention pooling for context modeling, and addition for feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experiments on Stronger Backbones</head><p>We evaluate our GCNet on stronger backbones, by replacing ResNet-50 with ResNet-101 and ResNeXt-101 <ref type="bibr" target="#b33">[34]</ref>, adding Deformable convolution to multiple layers (c3+c4+c5) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref> and adopting the Cascade strategy <ref type="bibr" target="#b0">[1]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Classification on ImageNet</head><p>ImageNet <ref type="bibr" target="#b4">[5]</ref> is a benchmark dataset for image classification, containing 1.28M training images and 50K validation images from 1000 classes. We follow the standard setting in <ref type="bibr" target="#b9">[10]</ref> to train deep networks on the training set and report the single-crop top-1 and the top-5 errors on the validation set. Our preprocessing and augmentation strategy follows the baseline proposed in <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b13">[14]</ref>. To speed up the experiments, all the reported results are trained via two stages. We first train standard ResNet-50 for 120 epochs on 8 GPUs with 64 images per GPU (effective batch size of 512) with 5 epochs of linear warmup. Second, we insert newly-designed blocks into the model trained in the first stage and finetune for other 40 epochs with a 0.02 initial learning rate. The baseline also follows this two-stage training but without adding new blocks in second stage. Cosine    learning rate decay is used for both training and fine-tuning. Block Design. As done for block design on COCO, results on different blocks are reported in <ref type="table" target="#tab_6">Table 4</ref>(a). GC block performs slightly better than NL and SNL blocks with fewer parameters and less computation, which indicates the versatility and generalization ability of our design. By inserting GC blocks in all residual blocks (c3+c4+c5), the performance is further boosted (by 0.82%↑ on top-1 accuracy compared to baseline) with marginal computational overhead (0.26% relative increase on FLOPs).</p><p>Pooling and fusion. The functionality of different pooling and fusion methods is also investigated on image classification. Comparing <ref type="table" target="#tab_6">Table 4(b) with Table 2</ref>(f), it is seen that attention pooling has greater effect in image classification, which could be one of missing ingredients in <ref type="bibr" target="#b13">[14]</ref>. Also, attention pooling with addition (GCNet) outperforms vanilla average pooling with scale (SENet) by 0.44% on top-1 accuracy with almost the same #params and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action Recognition on Kinetics</head><p>For human action recognition, we adopt the widely-used Kinetics <ref type="bibr" target="#b16">[17]</ref> dataset, which has ∼240k training videos and 20k validation videos in 400 human action categories. All models are trained on the training set and tested on the validation set. Following <ref type="bibr" target="#b30">[31]</ref>, we report top-1 and top-5 recognition accuracy. We adopt the slow-only baseline in <ref type="bibr" target="#b5">[6]</ref>, the best single model to date that can utilize weights inflated <ref type="bibr" target="#b1">[2]</ref> from the ImageNet pretrained model. This inflated 3D strategy <ref type="bibr" target="#b30">[31]</ref> greatly speeds up convergence compared to training from scratch. All the experiment settings explicitly follow <ref type="bibr" target="#b5">[6]</ref>; the slow-only baseline is trained with 8 frames (8 × 8) as input, and multi(30)-clip validation is adopted.</p><p>The ablation study results are reported in <ref type="table" target="#tab_7">Table 5</ref>. For Kinetics experiments, the ratio of GC blocks is set to 4. First, when replacing the NL block with the simplified NL block and GC block, the performance can be regarded as on par (0.19%↓ and 0.11%↓ in top-1 accuracy, 0.15%↑ and 0.14%↑ in top-5 accuracy). As in COCO and ImageNet, adding more GC blocks further improves results and outperforms NL blocks with much less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The pioneering work for long-range dependency modeling, non-local networks, intends to model query-specific global context, but only models query-independent context. Based on this, we simplify non-local networks and abstract this simplified version to a global context modeling framework. Then we propose a novel instantiation of this framework, the GC block, which is lightweight and can effectively model long-range dependency. Our GCNet is constructed via applying GC blocks to multiple layers, which generally outperforms simplified NLNet and SENet on major benchmarks for various recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of attention maps (heatmaps) for different query positions (red points) in a non-local block on COCO object detection. The three attention maps are all almost the same. More examples are in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of attention maps (heatmaps) for different query positions (red points) in a non-local block on COCO object detection. In the same image, the attention maps of different query points are almost the same. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a).The non-local block can be regarded as a global context modeling block, which aggregates query-specific global context features (weighted averaged from all positions via a query-specific attention map) to each query position. As attention maps are computed for each query position, the time and space complexity of the non-local block are both quadratic to the number of positions N p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b). The FLOPs of the 1x1 conv W v is reduced from O(HWC 2 ) to O(C 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(c), it consists of: (a) global average pooling for global context modeling (set α j = 1 Np in Eqn. 4), named the squeeze operation in SE block; (b) a bottleneck transform module (let δ(·) in Eqn. 4 be one 1x1 convolution, one ReLU, one 1x1 convolution and a sigmoid function, sequentially), to compute the importance for each channel, named the excitation operation in SE block; and (c) a rescaling function for fusion (let F (·, ·) in Eqn. 4 be element-wise multiplication), to recalibrate the channelwise features. Different from the non-local block, this SE block is quite lightweight, allowing it to be applied to all layers with only a slight increase in computation cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>)</head><label></label><figDesc>Block design AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs baseline 37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G +1 NL 38.0 59.8 41.0 34.7 56.7 36.6 46.5M 288.7G +1 SNL 38.1 60.0 41.6 35.0 56.9 37.0 45.4M 279.4G +1 GC 38.1 60.0 41.2 34.9 56.5 37.2 44.5M 279.4G +all GC 39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.</figDesc><table><row><cell></cell><cell>6G</cell></row><row><cell></cell><cell>(b) Positions</cell></row><row><cell></cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs</cell></row><row><cell>baseline</cell><cell>37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G</cell></row><row><cell>afterAdd</cell><cell>39.4 61.9 42.5 35.8 58.6 38.1 46.9M 279.6G</cell></row><row><cell>after1x1</cell><cell>39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.6G</cell></row><row><cell></cell><cell>(c) Stages</cell></row><row><cell></cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs</cell></row><row><cell>baseline</cell><cell>37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G</cell></row><row><cell>c3</cell><cell>37.9 59.6 41.1 34.5 56.3 36.8 44.5M 279.5G</cell></row><row><cell>c4</cell><cell>38.9 60.9 42.2 35.5 57.6 37.7 45.2M 279.5G</cell></row><row><cell>c5</cell><cell>38.7 61.1 41.7 35.2 57.4 37.4 45.9M 279.4G</cell></row><row><cell>c3+c4+c5</cell><cell>39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.6G</cell></row><row><cell></cell><cell>(d) Bottleneck design</cell></row><row><cell></cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs</cell></row><row><cell>baseline</cell><cell>37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G</cell></row><row><cell>w/o ratio</cell><cell>39.4 61.8 42.8 35.9 58.6 38.1 64.4M 279.6G</cell></row><row><cell cols="2">r16 (ratio 16) 38.8 61.0 42.3 35.3 57.6 37.5 46.9M 279.6G</cell></row><row><cell>r16+ReLU</cell><cell>38.8 61.0 42.0 35.4 57.5 37.6 46.9M 279.6G</cell></row><row><cell cols="2">r16+LN+ReLU 39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.6G</cell></row><row><cell></cell><cell>(e) Bottleneck ratio</cell></row><row><cell></cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs</cell></row><row><cell>baseline</cell><cell>37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G</cell></row><row><cell>ratio 4</cell><cell>39.9 62.2 42.9 36.2 58.7 38.3 54.4M 279.6G</cell></row><row><cell>ratio 8</cell><cell>39.5 62.1 42.5 35.9 58.1 38.1 49.4M 279.6G</cell></row><row><cell>ratio 16</cell><cell>39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.6G</cell></row><row><cell>ratio 32</cell><cell>39.1 61.6 42.4 35.7 58.1 37.8 45.7M 279.5G</cell></row><row><cell></cell><cell>(f) Pooling and fusion</cell></row><row><cell></cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 #param FLOPs</cell></row><row><cell>baseline</cell><cell>37.2 59.0 40.1 33.8 55.4 35.9 44.4M 279.4G</cell></row><row><cell cols="2">avg+scale (SE) 38.2 60.2 41.2 34.7 56.7 37.1 46.9M 279.5G</cell></row><row><cell>avg+add</cell><cell>39.1 61.4 42.3 35.6 57.9 37.9 46.9M 279.5G</cell></row><row><cell>att+scale</cell><cell>38.3 60.4 41.5 34.8 57.0 36.8 46.9M 279.6G</cell></row><row><cell>att+add</cell><cell>39.4 61.6 42.4 35.7 58.4 37.6 46.9M 279.6G</cell></row></table><note>(a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>59.0 40.1 33.8 55.4 35.9 279.4G +GC r16 39.4 61.6 42.4 35.7 58.4 37.6 279.6G +GC r4 39.9 62.2 42.9 36.2 58.7 38.3 279.6G R101 baseline 39.8 61.3 42.9 36.0 57.9 38.3 354.0G +GC r16 41.1 63.6 45.0 37.4 60.1 39.6 354.3G +GC r4 41.7 63.7 45.5 37.6 60.5 39.8 354.3G</figDesc><table><row><cell></cell><cell></cell><cell>(a) test on validation set</cell></row><row><cell cols="2">backbone</cell><cell>AP bbox AP bbox 50 AP bbox 75 AP mask AP mask 50 AP mask 75 FLOPS</cell></row><row><cell cols="3">R50 baseline 37.2 X101 baseline 41.2 63.0 45.1 37.3 59.7 39.9 357.9G +GC r16 42.4 64.6 46.5 38.0 60.9 40.5 358.2G</cell></row><row><cell></cell><cell cols="2">+GC r4 42.9 65.2 47.0 38.5 61.8 40.9 358.2G</cell></row><row><cell>X101 +Cascade</cell><cell cols="2">baseline 44.7 63.0 48.5 38.3 59.9 41.3 536.9G +GC r16 45.9 64.8 50.0 39.3 61.8 42.1 537.2G +GC r4 46.5 65.4 50.7 39.7 62.5 42.7 537.3G</cell></row><row><cell>X101+DCN +Cascade</cell><cell cols="2">baseline 47.1 66.1 51.3 40.4 63.1 43.7 547.5G +GC r16 47.9 66.9 52.2 40.9 63.7 44.1 547.8G +GC r4 47.9 66.9 51.9 40.8 64.0 44.0 547.8G</cell></row><row><cell></cell><cell></cell><cell>(b) test on test-dev set</cell></row><row><cell>X101 +Cascade</cell><cell cols="2">baseline 45.0 63.7 49.1 38.7 60.8 41.8 536.9G +GC r16 46.5 65.7 50.7 40.0 62.9 43.1 537.2G +GC r4 46.6 65.9 50.7 40.1 62.9 43.3 537.3G</cell></row><row><cell>X101+DCN +Cascade</cell><cell cols="2">baseline 47.7 66.7 52.0 41.0 63.9 44.3 547.5G +GC r16 48.3 67.5 52.7 41.5 64.6 45.0 547.8G +GC r4 48.4 67.6 52.7 41.5 64.6 45.0 547.8G</cell></row><row><cell></cell><cell></cell><cell>The results of our GCNet with GC blocks integrated in all</cell></row><row><cell></cell><cell></cell><cell>layers (c3+c4+c5) with bottleneck ratios of 4 and 16 are re-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of GCNet (ratio 4 and 16) with stronger backbones on COCO 2017 validation and test-dev sets. ported.Table 3(a) presents detailed results on the validation set. It is worth noting that even when adopting stronger backbones, the gain of GCNet compared to the baseline is still significant, which demonstrates that our GC block with global context modeling is complementary to the capacity of current models. For the strongest backbone, with deformable convolution and cascade RCNN in ResNeXt-101, our GC block can still boost performance by 0.8%↑ on AP bbox and 0.5%↑ on AP mask . To further evaluate our proposed method, the results on the test-dev set are also reported, shown inTable 3(b). On test-dev, strong baselines are also boosted by large margins by adding GC blocks, which is consistent with the results on validation set. These results demonstrate the robustness of our proposed method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of GCNet with ResNet-50 on image classification on ImageNet validation set.</figDesc><table><row><cell cols="5">method Top-1 Acc Top-5 Acc #params(M) FLOPs(G)</cell></row><row><cell>baseline</cell><cell>74.94</cell><cell>91.90</cell><cell>32.45</cell><cell>39.29</cell></row><row><cell>+5 NL</cell><cell>75.95</cell><cell>92.29</cell><cell>39.81</cell><cell>59.60</cell></row><row><cell>+5 SNL</cell><cell>75.76</cell><cell>92.44</cell><cell>36.13</cell><cell>39.32</cell></row><row><cell>+5 GC</cell><cell>75.85</cell><cell>92.25</cell><cell>34.30</cell><cell>39.31</cell></row><row><cell>+all GC</cell><cell>76.00</cell><cell>92.34</cell><cell>42.45</cell><cell>39.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of GCNet and NLNet based on Slow-only baseline using R50 as backbone on Kinetics validation set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02344</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gather-excite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9423" to="9433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Detnet: A backbone network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06215</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<title level="m">Bag of tricks for image classification with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speedaccuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
