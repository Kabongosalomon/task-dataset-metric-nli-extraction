<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
						</author>
						<title level="a" type="main">Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach for unsupervised domain adaptation-with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift-from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels directly. Instead, we present a sampling-based implicit alignment approach, where the sample selection procedure is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed implicit alignment approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning aims to extract statistical patterns from data by learning to approximate the conditional density p(y|x). However, the generalization of the approximation is often sensitive to some dataset-specific factors. Dataset shift <ref type="bibr" target="#b34">(Quionero-Candela et al., 2009</ref>) frequently arises from real-world applications and can manifest in many different ways, such as sample selection bias <ref type="bibr" target="#b16">(Heckman, 1979;</ref><ref type="bibr" target="#b44">Torralba et al., 2011)</ref>, class distribution shift <ref type="bibr">(Webb &amp; Ting,</ref> Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). <ref type="bibr">2005</ref>), and covariate shift <ref type="bibr" target="#b40">(Shimodaira, 2000)</ref>. Unsupervised Domain Adaptation (UDA) aims to address domain shift with access to labeled data in the source domain and unlabeled data in the target domain <ref type="bibr" target="#b29">(Pan &amp; Yang, 2009</ref>). The fundamental algorithmic issue is to infer domain-invariant representations.</p><p>While considerable progress has been made in UDA <ref type="bibr" target="#b12">(Ganin et al., 2016)</ref>, they tend to focus on marginal distribution matching in the feature space, and less emphasis is made on discovering label distributions. In real-world applications, it is very common to have class imbalance within each domain and class distribution shift between different domains, necessitating the incorporation of label space distribution into adaptation. Explicit class-conditioned domain alignment <ref type="bibr">(Xie et al., 2018;</ref><ref type="bibr" target="#b30">Pan et al., 2019;</ref><ref type="bibr" target="#b19">Liang et al., 2019a;</ref><ref type="bibr" target="#b10">Deng et al., 2019)</ref> has emerged as a key approach to promoting class-conditioned invariance by aligning prototypical representations of each class. While explicit alignment has the advantage of directly minimizing class-conditioned misalignment, it presents critical vulnerabilities to error accumulation <ref type="bibr" target="#b5">(Chen et al., 2019a)</ref> and ill-calibrated probabilities <ref type="bibr" target="#b14">(Guo et al., 2017)</ref> due to its dependence on explicit supervision from pseudo-labels provided by model predictions.</p><p>We propose Implicit Class-Conditioned Domain Alignment that removes the need for explicit pseudo-label based optimization. Instead, we use the pseudo-labels implicitly to sample class-conditioned data in a way that maximally aligns the joint distribution between features and labels. The primary advantage of the sampling-based implicit domain alignment is the ability to address within-domain class imbalance and between-domain class distribution shift, in addition to many other benefits such as applications in cost-sensitive learning. The proposed method is simple, effective, and is supported by theoretical analysis on the empirical estimations of domain divergence measures. It also overcomes limitations of explicit alignment by allowing the domain adaptation algorithm to discover class-conditioned domain-invariance in an unsupervised way without explicit supervision from pseudo-labels.</p><p>The contributions of this paper are as follows: (i) We propose implicit class-conditioned domain alignment to address the challenge of within-domain class imbalance and between-domain class distribution shift, which overcomes the limitation of error accumulation in explicit domain align-arXiv:2006.04996v1 <ref type="bibr">[cs.</ref>LG] 9 Jun 2020 ment; (ii) We provide theoretical analysis on the empirical domain divergence and reveal the existence of a shortcut function that interferes with domain-invariant learning, which is addressed by the proposed approach; (iii) We show that the proposed approach is orthogonal to the choice of domain adaptation algorithms and offers consistent improvements to two adversarial domain adaptation algorithms; (iv) We report state-of-the-art UDA performance under extreme withindomain class imbalance and between-domain class distribution shift, and competitive results on standard UDA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We follow the notations by <ref type="bibr" target="#b0">(Ben-David et al., 2010)</ref> and define a domain as an ordered pair consisting of a distribution D on the input space X , and a labeling function f : X → Y that maps X to the label space Y. The source and target domains are denoted by D S ,f S and D T ,f T , respectively.</p><p>In unsupervised domain adaptation, the model is trained on labeled data from the source domain, together with unlabeled data from the target domain. The goal is to obtain a model h ∈ H which learns domain-invariant representations while simultaneously minimizing the classification error on D S . Adversarial training is the prevailing approach for domain adaptation <ref type="bibr" target="#b12">(Ganin et al., 2016)</ref>. It formulates a minimax problem where the maximizer maximizes the estimation of the domain divergence between the empirical samples, and the minimizer minimizes the sum of the source error and the domain divergence estimation obtained from the maximizer.</p><p>While matching the marginal distribution is a good step towards domain-invariant learning, it is still susceptible to the problem of conditional distribution mismatching. Prototype-based class-conditioned domain alignment <ref type="bibr" target="#b27">(Luo et al., 2017;</ref><ref type="bibr">Xie et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019a;</ref><ref type="bibr" target="#b30">Pan et al., 2019;</ref><ref type="bibr" target="#b19">Liang et al., 2019a;</ref> is designed to address this problem. We refer to this group of methods as explicit class-conditioned domain alignment. The explicit alignment is achieved by incorporating an auxiliary loss that minimizes the Euclidean distance of the class-conditioned prototypical representations c j between the source and target domains. The class-conditioned prototype c j is the average representation for all examples in a domain with class label j.</p><p>The main limitation of explicit class-conditioned domain alignment is in its reliance on explicit optimization of model parameters based on pseudo-labels. This learning procedure is vulnerable to error accumulation <ref type="bibr" target="#b5">(Chen et al., 2019a)</ref> as mistakes in the pseudo-label predictions can gradually accumulate leading to poor local minima in EM-style training. Furthermore, the pseudo-labels are likely to suffer from ill-calibrated probabilities <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>, especially for deep learning methods, which exacerbate the critical problem of error accumulation with misleadingly confident mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We begin with theoretical motivations of implicit alignment by decomposing the empirical domain divergence measure into class-aligned and class-misaligned divergence, and show that misaligned divergence is detrimental to domain adaptation. We then present the proposed implicit domain alignment framework that addresses class-misalignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Theoretical Motivations</head><p>The H∆H divergence between two domains is defined as</p><formula xml:id="formula_0">d H∆H (D S ,D T ) = 2 sup h,h ∈H |E D T [h = h ]−E D S [h = h ]|,<label>(1)</label></formula><p>where H denotes some hypothesis space, and h = h is the abbreviation for h(x) = h (x). <ref type="bibr" target="#b0">(Ben-David et al., 2010)</ref> theorized that the target domain error T (h) is bounded by the error of the source domain S (h) and the empirical domain divergenced</p><formula xml:id="formula_1">H∆H (U S , U T ) where U S , U T are unlabeled empirical samples drawn from D S , D T .</formula><p>In deep learning, minibatch-based optimization limits the amount of data available at each training step. This necessitates the analysis of the empirical estimations of d H∆H at the minibatch level, so as to shed light on the learning dynamics.</p><formula xml:id="formula_2">Definition 3.1. Let B S , B T be minibatches from U S and U T , respectively, where B S ⊆ U S , B T ⊆ U T , and |B S | = |B T |. The empirical estimation of d H∆H (B S , B T ) over the minibatches B S , B T is defined aŝ d H∆H (B S ,B T ) = sup h,h ∈H B T [h = h ]− B S [h = h ] . (2)</formula><p>Theorem 3.2 (The decomposition ofd H∆H (B S , B T )). Let H be a hypothesis space and Y be the label space of the classification task where B S , B T are minibatches drawn from U S , U T , respectively, and Y S , Y T are the label set of B S , B T . We define three disjoint sets on the label space: the shared labels Y C := Y S ∩ Y T , and the domain-specific labels</p><formula xml:id="formula_3">Y S := Y S −Y C , and Y T := Y T −Y C .</formula><p>We also define the following disjoint sets on the input space</p><formula xml:id="formula_4">where B C S := {x ∈ B S | y ∈ Y C }, B C S := {x ∈ B S | y / ∈ Y C }, B C T := {x ∈ B T | y ∈ Y C }, B C T := {x ∈ B T | y / ∈ Y C }.</formula><p>The empiricald H∆H (B S ,B T ) divergence can be decomposed into class aligned divergence and class-misaligned divergence:</p><formula xml:id="formula_5">d H∆H (B S ,B T ) = sup h,h ∈H ξ C (h,h )+ξ C (h,h ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">ξ C (h,h ) = B C T 1[h = h ]− B C S 1[h = h ],<label>(4)</label></formula><formula xml:id="formula_7">ξ C (h,h ) = B C T 1[h = h ]− B C S 1[h = h ].<label>(5)</label></formula><p>Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation The proof is provided in supplementary materials. </p><formula xml:id="formula_8">( , )<label>( , ) ( , )</label></formula><formula xml:id="formula_9">- misaligned divergence ξ C (h,h ) with sample x ∈ B C S ∪B C T , there exists a domain discriminator shortcut function f d (x) = 1 f c (x) ∈ Y S 0 f c (x) ∈ Y T ,<label>(6)</label></formula><p>such that the domain label can be solely determined by the domain-specific class labels. This shortcut interferes with adversarial domain adaptation because the model could bypass the optimization for domain-invariant representations, but rather optimize for a shortcut function that is independent of the covariate contributing to the domain difference. <ref type="figure">Figure 1</ref> illustrates a toy example where the source and target domains are aligned for class 4 but misaligned between classes 3 and 6 as a result of random sampling in the minibatch construction. The domain discriminator aims to predict domain labels based on their domain information, i.e., red and blue. However, due to the class shortcut for the misaligned samples (3 and 6), the domain discriminator could infer domain labels based on class information directly (digits 3 and 6), without the need to learn domain-specific information. This problem of class-misalignment is especially pronounced under extreme within-domain class imbalance and between-domain class distribution shift, where a simple random sample is more likely to fail in providing good coverage of the label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implicit Class-Conditioned Domain Alignment</head><p>Having identified the domain discriminator shortcut in class misaligned empirical samples, we now propose framework that aligns the two domains from a sampling perspective.  <ref type="figure" target="#fig_1">Figure 2</ref> depicts the proposed implicit class-conditioned domain alignment framework. We aim to align p S (x) and p T (x) at the input and label space jointly with the factorization p(x,y) = p(x|y)p(y) while ensuring that the sampled classes are aligned between the two domains. The alignment distribution p(y) is pre-specified, e.g., uniform distribution, to ensure samples are aligned in the shared label space in spite of different empirical label distributions of the two domains. This implicit alignment procedure minimizes the class-misaligned divergence ξ C (h,h ), providing a more reliable empirical estimation of domain divergence. For the unlabeled target domain, we use the model predictions to sample classconditioned data from p T (x|ŷ) to approximate p T (x|y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">CLASS-ALIGNED SAMPLING STRATEGY</head><p>Algorithm 1 presents the proposed sampling procedure that selects class-aligned examples for minibatch training. It is a type of stratified sampling where the dataset is partitioned into mutually exclusive subgroups to reflect the label information in a class-aligned manner.</p><p>First, we predict pseudo-labels of the target domain using the classifier f c (·;θ) parameterized by θ. The pseudo-labels will be later used in class-conditioned sampling. Second, we sample a set Y from the label space Y where p(y) defines the probability with which we pick the classes to align so as to ensure the empirical samples of the source and target domains share the same Y . This in turn minimizes the class-misaligned divergence ξ C (h,h ). Third, for each class y i ∈ Y , we sample class-conditioned examples for the source Algorithm 1 The proposed implicit alignment training</p><formula xml:id="formula_10">1: Input: dataset S = {(x i ,y i )} N i=1 , T = {x i } M i=1 , 2: label space Y, label alignment distribution p(y),</formula><p>3:</p><p>classifier f c (·;θ) 4: while not converged do 5:</p><formula xml:id="formula_11"># predict pseudo-labels for T 6:T ← {(x i ,f c (x i ;θ))} M i=1 where x i ∈ T 7: # sample N unique classes in the label space 8: Y ← draw N samples in Y from p(y) 9: # sample K examples conditioned on each y i ∈ Y 10: for y i in Y do 11: (X S ,Y S ) ←draw K samples in S from p S (x|y i )</formula><p>12:</p><formula xml:id="formula_12">X T ←draw K samples inT from p T (x|y i ) 13:</formula><p>end for 14:</p><p># domain adaptation training on this minibatch 15:</p><p>train minibatch (X S ,Y S ,X T ) 16: end while and target domains, respectively, and store them in (X S ,Y S ) and X T . This is equivalent to performing a table lookup to select a subset B i where all examples belong to class y i , followed by random sampling in B i . We use pseudo-labels to sample the target domain due to the lack of ground-truth labels. Once we obtained the class-aligned minibatch, we use it to train unsupervised domain adaptation algorithm and repeat this process until the model converges.</p><p>This algorithm addresses class imbalance within each domain as well as class distribution shift between different domains by specifying the sampling strategy p(y) in the label space. We use uniform sampling p(y) for all experiments in this paper, and leave more advanced specifications and their applications to cost-sensitive domain adaptation as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">INTEGRATING IMPLICIT ALIGNMENT INTO CLASSIFIER-BASED DOMAIN DISCREPANCY MEASURE</head><p>Section 3.2.1 describes the implicit alignment algorithm from a sampling perspective, where we sample minibatches in a way that maximizes class alignment implicitly. This sampling strategy is independent of the choice of domain divergence measures. In this section, we show how to integrate the sampling approach into Margin Disparity Discrepancy (MDD) (Zhang et al., 2019b)-a state-of-the-art classifier-based domain discrepancy measure-to further facilitate implicit alignment. MDD is defined as</p><formula xml:id="formula_13">d f,F (S,T ) = sup f ∈F disp D T (f ,f )−disp D S (f ,f ) ,<label>(7)</label></formula><p>where f and f are two independent scoring functions that predict class probabilities, and disp(f , f ) is a disparity measure between the scores provided by the classifiers f and f . The domain divergence is to estimate the discrepancy between the disparity measures of the two domains.</p><p>Following notations in Theorem A.2, we define the empirical MDD on class-misaligned samples aŝ</p><formula xml:id="formula_14">d f,F (B C S ,B C T ) = sup f ∈F B C T disp(f ,f )− B C S disp(f ,f ) . (8) Because B C S and B C T are disjoint in the label space, there exists a shortcut solution disp(f (x),f (x)) = 0 f c (x) ∈ Y S 1 f c (x) ∈ Y T ,<label>(9)</label></formula><p>which maximizes the divergence estimation of Eq. (8).</p><p>Although class-aligned sampling can mitigate this problem, it is difficult to fully eliminate the impact of misalignement due to imperfect pseudo-labels. To further eliminate the detrimental impact of class-misalignment, we introduce a masking scheme on the scoring functions f and f defined aŝ</p><formula xml:id="formula_15">d f,F (B S ,B T ) = sup f ∈F B T disp(f ω,f ω)− B S disp(f ω,f ω) ,</formula><p>(10) where f ω denotes element-wise multiplication between the output of f and ω. The alignment mask ω is a binary vector that denotes whether the i-th class is present in the sampled classes Y (i.e., the classes that we intend to align in the current minibatch). By doing so, we simultaneously align the source and target domains (i) in the input space and (ii) in the functional approximations of the domain divergence by masking the scoring functions f and f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. We evaluate on Office-31, Office-Home and VisDA2017. Office-31 <ref type="bibr" target="#b36">(Saenko et al., 2010)</ref> has three domains (Amazon, DSLR and Webcam) with 31 classes. We use three versions of Office-Home <ref type="bibr" target="#b48">(Venkateswara et al., 2017)</ref> that contains four domains (Art, Clip Art, Prduct, and Realworld) with 65 classes: (i) "standard": the standard Office-Home dataset. (ii) "balanced" <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>: a subset of the standard dataset where each class has the same number of examples. (iii) "RS-UT": Reversely-unbalanced Source (RS) and Unbalanced-Target (UT) distribution <ref type="bibr" target="#b43">(Tan et al., 2019)</ref> where both domains are imbalanced, but the majority class in the source domain is the minority class in the target domain. VisDA2017 (synthetic→real) <ref type="bibr" target="#b32">(Peng et al., 2017)</ref> is a largescale dataset with 12 classes and more than 200k images.</p><p>Model architecture. We use ResNet-50 <ref type="bibr" target="#b15">(He et al., 2016)</ref> pre-trained from ImageNet <ref type="bibr" target="#b35">(Russakovsky et al., 2015)</ref> as the backbone, and use hyper-parameters from (Zhang et al., 2019b) for MDD-based domain discrepancy measure. The batch size is 31 for Office-31 and 50 for Office-Home.</p><p>Baselines. Our main explicit alignment baselines are COAL <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>, PACET <ref type="bibr" target="#b20">(Liang et al., 2019b)</ref> and MCS <ref type="bibr" target="#b19">(Liang et al., 2019a)</ref>, state-of-the-art explicit alignment methods based on domain discriminator discrepancy. As our domain discrepancy measure is MDD, we re-implement various MDD-based explicit alignment for fair comparison.</p><p>Computational efficiency. We only update pseudo-labels periodically, i.e., every 20 steps, instead of at every training step. We show in the supplementary materials that our method does not require more frequent pseudo-label updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluating Extreme Class Distribution Shift</head><p>We use Office-Home (RS-UT), described in <ref type="figure">Figure 3</ref> (a), to evaluate the performance of different methods under extreme within-domain class imbalance and between-domain class distribution shift where the majority classes in the source domain are minority classes in the target domain. <ref type="table" target="#tab_1">Table 1</ref> presents the per-class average accuracy on Office-Home (RS-UT). Our main baseline is the explicit alignment method "covariate and label shift co-alignment" (COAL) designed to address data imbalance and class distribution shift. Our proposed implicit domain alignment works the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">THE IMPACT OF CLASS DISTRIBUTION SHIFT</head><p>Many baseline methods suffer from class distribution shift, and their performances degrade to "Source Only" training as they do not take into account within-domain class imbalance and between-domain class distribution shift. For MDD-based methods, after we apply balanced sampling for the source domain, the per-class average accuracy improved from 55.44% to 58.50%, which indicates balanced sampling is helpful for class distribution shift, despite only in the source domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">THE EFFECTIVENESS OF IMPLICIT ALIGNMENT</head><p>The effectiveness of implicit alignment is demonstrated through the comparison between "MDD+Implicit Alignment" and "MDD (source-balanced sampler)". Both methods use the same sampling procedure for the source. The only difference is that implicit alignment aligns the two domains by sampling aligned classes in the target domain, whereas "source-balanced sampler" only takes random samples from the target domain. <ref type="table" target="#tab_1">Table 1</ref> shows that implicit alignment performs better than "source-balanced sampler" because it is better-aligned, which confirms the effectiveness of implicit alignment. Besides, the proposed method also outperforms MDD-based explicit alignment, which validates the effectiveness of implicit alignment over the explicit alignment. <ref type="figure">Figure 3</ref> (b) compares the baseline, implicit and explicit alignments on Office-Home (balanced) and Office-Home (RS-UT). We observe that implicit alignment performs the best on both datasets. More importantly, implicit alignment is more robust to class distribution shift which greatly out-performs other methods under RS-UT distribution shift and has a smaller performance drop from the balanced version of Office-Home.   <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref> summarize the results for the standard Office-31 and Office-Home datasets which have a small degree of class imbalance. Our method outperforms the baselines in 3 out of 6 domain pairs for Office-31, and 10 out of 12 domain pairs for Office-Home (standard). The proposed implicit alignment exhibits larger performance gains on the Office-Home dataset because the dataset is more difficult for domain adaptation, and it has 65 classes compared with the 31 classes in Office-31. We also report state-of-the-art results for VisDA in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating Standard Domain Adaptation Datasets</head><p>Similar to findings in Section 4.2, we observe sourcebalanced sampling is helpful when comparing "MDD (source-balanced sampler)" with the MDD standard baseline, even without extreme class distribution shift.</p><p>The proposed method outperforms the state-of-the-art explicit alignment methods-PACET and MCS-across all domain pairs. We find it ineffective to incorporate prototype-based explicit alignment into MDD. This is method acc. (%)</p><p>JAN <ref type="bibr" target="#b25">(Long et al., 2017)</ref> 61.6 GTA <ref type="bibr" target="#b39">(Sankaranarayanan et al., 2018)</ref> 69.5 MCD <ref type="bibr" target="#b38">(Saito et al., 2018)</ref> 69.8 CDAN  70.0 MDD <ref type="bibr">(Zhang et al., 2019b)</ref> 74.6 MDD+Explicit Alignment 67.1 MDD+Implicit Alignment 75.8 <ref type="table">Table 4</ref>. VisDA2017 target accuracy  in contrast with domain-discriminator-based adversarial learning, where explicit alignment is shown to improve domain adaptation. This is because the classifier-based discrepancy MDD contains more abundant information than domain-discriminator-based discrepancy, owing to the availability of predictive probabilities provided by the classifiers. The rich information in domain discrepancy removes the need for prototype-based distances.   <ref type="table">Table 5</ref>. The impact of different implicit alignment options, i.e., masking in the MDD estimation and sampling class-aligned minibatches, on Office-Home (RS-UT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">IMPACT OF CLASS DIVERSITY AND ALIGNMENT</head><p>We analyze the impact of class diversity and alignment by designing experiments along three dimensions: the number of unique labels in each minibatch, whether the classes are aligned, and whether we use pseudo-labels or ground-truth labels when sampling the target domain.</p><p>Setup. "Baseline (random)" randomly samples examples of both domains. "Baseline (S-sampled, T-random)" uses N -way sampler for the source domain, and randomly samples the target domain. "Aligned (pseudo-labels)" is the proposed implicit alignment approach. "Aligned (Oracle)" is the oracle form of implicit alignment where the target domain uses ground-truth labels for sampling.</p><p>The impact of class diversity. Minibatch-based class diversity determines the sampling distribution of the label space, and a greater diversity corresponds to a more stable measure of this sampling distribution. <ref type="figure">Figure 4</ref> suggests a positive correlation between the model performance and class diversity: domain adaptation methods do not work well when the class diversity is very low-i.e., only sample 5 classes per batch among the 65 classes-and the alignment-based methods outperform the baseline as we increase class diversity.</p><p>The impact of alignment. We confirm the importance of the proposed implicit alignment algorithm from two perspectives. First, "Aligned (oracle)" consistently performs the best, which suggests perfect alignment can provide substantial benefits to unsupervised domain adaptation. Second, the comparison between "Aligned (pseudo-labels)" and "Baseline (S-sampled, T-random)" validates the effectiveness of pseudo-label based implicit alignment, although the pseudo-labels are approximations of the oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">ROBUSTNESS TO PSEUDO-LABEL ERRORS</head><p>We investigate whether implicit alignment is indeed more robust to pseudo-label errors when compared with explicit alignment. <ref type="figure">Figure 5</ref> illustrates the relationship between pseudo-label accuracy at training step t and the corresponding subsequent target accuracy at step t + 1000, i.e., after 1000 domain adaptation training steps. This process resembles a Markov chain that allows us to analyze the impact of pseudo-label accuracy on the learning dynamics.</p><p>It is evident that the drawbacks of explicit alignment are more severe when the pseudo-labels are less accurate, e.g., 10∼40%, where implicit alignment has more considerable performance improvements than explicit alignment. This suggests that implicit alignment is more robust to erroneous pseudo-label predictions because it does not require explicit supervision from the pseudo-labels. Implicit and explicit methods eventually converge at 76% and 74%, respectively.</p><p>Although many recent techniques attempt to address pseudo-label bias in explicit alignment, they depend on the assumption that probabilities of model predictions are wellcalibrated during training. They fail to address ill-calibrated probabilities <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>, where the model tends to make confident mistakes on the target domain. Moreover, given that models do not initially perform well when training begins, for a random classifier, implicit alignment selects random samples that is equivalent to training without sampling. In contrast, explicit alignment optimizes model parameters from these random labels explicitly. <ref type="table">Table 5</ref> presents the ablation study on Office-Home (RS-UT) that aims to assess the impact of different implicit alignment options: alignment in the domain divergence estimations in Section 3.2.1 (i.e., masking in MDD) and alignment in the input space in Section 3.2.1 (i.e., sampling class-conditioned examples). We observe that both alignment techniques are essential for domain adaptation because alignment should be enforced consistently across all aspects of adaptation. We report similar findings, in the supplementary material, on Office-Home (standard).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">ABLATION STUDY ONN MDD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Domain Target Domain</head><p>balanced imbalanced balanced imbalanced <ref type="figure">Figure 6</ref>. Interactions between within-domain class imbalance and between-domain class distribution shift. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">GENERALIZATION: IMPLICIT ALIGNMENT ALSO IMPROVES DANN</head><p>We design additional experiments to further demonstrate the effectiveness of the proposed approach on a different domain adaptation algorithm-DANN-on two synthetic domains with different degrees of class imbalance: "mild" (lighttailed class imbalance from a triangular-like distribution) and "extreme" (heavy-tailed class imbalance from a Pareto distribution). We synthetically manipulate the class distributions of SVHN and MNIST to simulate various interactions between within-domain class imbalance and betweendomain class distribution shift. As illustrated in <ref type="figure">Fig 6,</ref> we simulate three types of distribution shift when p S (y) = p T (y) (i) source-balanced, target-imbalanced; (ii) sourceimbalanced, target-balanced; (iii) both-imbalanced. <ref type="table" target="#tab_5">Table 6</ref>, 7 and 8 present the results for the abovementioned scenarios and all experiments are repeated five times. The proposed implicit alignment approach significantly improves the performance of DANN regardless of the degree of imbalance or the type of distribution shift. Besides, implicit alignment offers greater improvements over DANN when the degree of imbalance is more severe, i.e., comparing "mild" with "extreme". Implicit alignment overcomes this limitation of DANN and greatly improves the performance of the challenging task between SVHN and MNIST. We conclude that the proposed approach is independent of the choice of domain adaptation algorithms and helps both MDD and DANN.</p><p>Note that the aim of this subsection is to show that implicit alignment could help improve DANN on the digits dataset. More work is needed to compare with the current state-ofthe-art methods <ref type="bibr" target="#b18">(Kumar et al., 2018;</ref><ref type="bibr" target="#b41">Shu et al., 2018)</ref> on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>We review related work on unsupervised domain adaptation and discuss their relations with our proposed method.</p><p>Instance-based importance-weighting <ref type="bibr" target="#b4">(Chawla et al., 2002;</ref><ref type="bibr" target="#b17">Kouw &amp; Loog, 2019)</ref> aims to minimize the target error directly from the source domain data, weighted at the example level or class level. Unlike our approach, importance-weighting only uses the source data to train the classifier without learning domain invariant representations.</p><p>Feature-based distribution adaptation is the prevailing approach to domain adaptation that aims to minimize the distribution discrepancy between the source and target domains. The domain difference can be measured in various ways, such as Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b2">(Borgwardt et al., 2006)</ref>, which is further minimized to achieve domain invariance. The minimization of such discrepancy can be carried out by directly minimizing the distance <ref type="bibr" target="#b46">(Tzeng et al., 2014)</ref> or with the help of adversarial learning <ref type="bibr" target="#b12">(Ganin et al., 2016)</ref>.</p><p>Classifier-based distribution adaptation is a strong competitor to feature-based adaptation. It aims to minimize the discrepancy between two classifiers so that the learned representations respect the decision boundary of the classification task <ref type="bibr" target="#b38">(Saito et al., 2018;</ref><ref type="bibr">Zhang et al., 2019b)</ref>. We show that the proposed approach is beneficial to both classifier-based discrepancy MDD (Zhang et al., 2019b) and feature-based discrepancy DANN <ref type="bibr" target="#b12">(Ganin et al., 2016)</ref>.</p><p>Feature-classifier joint distribution adaptation aims to align the joint distribution between features and their corresponding predictions <ref type="bibr" target="#b23">(Long et al., 2013;</ref><ref type="bibr" target="#b45">Tsai et al., 2018)</ref>. The joint distribution can be represented in a multilinear map between features and classifier predictions , or the Cartesian product between the domain space and class space <ref type="bibr" target="#b9">(Cicek &amp; Soatto, 2019)</ref>. In our work, we implicitly align the joint distribution with the factorization p(x, y) = p(x|y)p(y) from a sampling perspective where p(y) is the pre-specified alignment distribution in the label space, and p(x|y) represents class-conditioned sampling.</p><p>Explicit class-conditioned domain alignment, or class prototype alignment, introduces a loss function that minimizes the distances of class-level prototypes between the source and target domains <ref type="bibr" target="#b42">(Snell et al., 2017;</ref><ref type="bibr" target="#b33">Pinheiro, 2018;</ref><ref type="bibr" target="#b30">Pan et al., 2019;</ref><ref type="bibr" target="#b10">Deng et al., 2019)</ref>. It is prone to error accumulation due to its reliance on explicit optimization of model parameters from the pseudo-labels. A variety of recent methods have been proposed to mitigate these limitations by estimating batch-level statistics <ref type="bibr">(Xie et al., 2018)</ref> and introducing an easy-to-hard curriculum that favors confident predictions <ref type="bibr" target="#b5">(Chen et al., 2019a)</ref>. Nevertheless, these algorithms suffer from ill-calibrated probabilities in the form of confident mistakes, and more work is needed to improve model calibration so as to better utilize explicit alignment.</p><p>Self-training <ref type="bibr" target="#b28">(Nigam &amp; Ghani, 2000)</ref> is a special form of co-training <ref type="bibr" target="#b1">(Blum &amp; Mitchell, 1998)</ref> where the model iteratively uses its predictions, i.e., pseudo-labels, as explicit supervision to re-train itself. The use of pseudo-labels has become an emerging trend in domain adaptation, because they provide estimations of the target domain label distribution that can be exploited by training algorithms. Apart from class prototype based methods <ref type="bibr" target="#b6">(Chen et al., 2011;</ref><ref type="bibr" target="#b37">Saito et al., 2017;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr" target="#b10">Deng et al., 2019)</ref> for explicit alignment, <ref type="bibr" target="#b50">(Wen et al., 2019)</ref> proposed the use of uncertainty estimates of the target domain predictions as second-order statistics to promote feature-label joint adaptation. For semantic segmentation tasks, <ref type="bibr">(Zou et al., 2018)</ref> proposed to iteratively generate pseudo-labels in the target domain and re-train the model on these labels; (Zhang et al., 2019a) proposed to use pseudo-labels to encourage examples to cluster together if they belong to the same class; <ref type="bibr" target="#b7">(Chen et al., 2019b)</ref> applied entropy minimization <ref type="bibr" target="#b13">(Grandvalet &amp; Bengio, 2005)</ref> on the pseudo-labels to encourage class overlap between domains. A main bottleneck for this approach is the bias in pseudo-label predictions. Directly optimizing these labels is prone to "entropy over-minimization" <ref type="bibr">(Zou et al., 2019)</ref> and negative transfer <ref type="bibr" target="#b21">(Lifshitz &amp; Wolf, 2020)</ref> where the model overfits to mistakes in the pseudo-labels. Moreover, the pseudo-labels are likely to suffer from ill-calibrated probabilities <ref type="bibr" target="#b14">(Guo et al., 2017)</ref>, especially for deep learning methods. The resulting misleadingly confident mistakes exacerbate the critical problem of error accumulation in pseudo-label bias. In contrast, our proposed method removes the need for direct supervision from pseudo-labels, and as a result is more robust to bias in how these labels are produced.</p><p>Reinforced sample selection <ref type="bibr" target="#b11">(Dong &amp; Xing, 2018)</ref> is proposed for one-shot domain adaptation where a model actively selects labeled examples to train the domain adaptation model. In comparison, the advantage of our approach is in its simplicity that no reinforcement learning is required to obtain the sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We introduce an approach for unsupervised domain adaptation-with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift-from a class-conditioned domain alignment perspective. We show theoretically that the proposed implicit alignment provides a more reliable measure of empirical domain divergence which facilitates adversarial domaininvariant representation learning, that would otherwise be hampered by the class-misaligned domain divergence. We show that our proposed approach leads to superior UDA performance under extreme within-domain class imbalance and between-domain class distribution shift, as well as competitive results on standard UDA tasks. We emphasize that the proposed method is robust to pseudo-label bias, simple to implement, has a unified training objective, and does not require additional parameter tuning. We also show that the proposed approach is orthogonal to the choice of domain adaptation algorithms and offers consistent improvements to featurebased and classifier-based domain adaptation algorithms.</p><p>Future work includes extensions to cost-sensitive learning for domain adaptation, and other setups where the label space between the source and target domains are not identical, as well as other domain adaptation setups . It is also important to analyze the probability calibration of different domain adaptation models and develop well-calibrated methods for more effective use of pseudo-labels.</p><p>Xie, S., Zheng, Z., <ref type="bibr">Chen, L., and Chen, C</ref> </p><formula xml:id="formula_16">d H∆H (B S ,B T ) = 1 m b sup h,h ∈H B T [h = h ]− B S [h = h ] .<label>(11)</label></formula><p>For simplicity, we drop the multiple 1 m b in the following analysis as it does not affect the result of optimization. Theorem A.2 (The decomposition ofd H∆H (B S , B T )). Let H be a hypothesis space and Y be the label space of the classification task where B S , B T are minibatches drawn from U S , U T , respectively, and Y S , Y T are the label set of B S , B T . We define three disjoint sets on the label space: the shared labels Y C := Y S ∩ Y T , and the domain-specific labels</p><formula xml:id="formula_17">Y S := Y S −Y C , and Y T := Y T −Y C .</formula><p>We also define the following disjoint sets on the input space</p><formula xml:id="formula_18">where B C S := {x ∈ B S | y ∈ Y C }, B C S := {x ∈ B S | y / ∈ Y C }, B C T := {x ∈ B T | y ∈ Y C }, B C T := {x ∈ B T | y / ∈ Y C }.</formula><p>The empiricald H∆H (B S ,B T ) divergence can be decomposed into class aligned divergence and class-misaligned divergence:</p><formula xml:id="formula_19">d H∆H (B S ,B T ) = sup h,h ∈H ξ C (h,h )+ξ C (h,h ) , (12) where ξ C (h,h ) = B C T 1[h = h ]− B C S 1[h = h ],<label>(13)</label></formula><formula xml:id="formula_20">ξ C (h,h ) = B C T 1[h = h ]− B C S 1[h = h ].<label>(14)</label></formula><p>Proof. By definition, we havê</p><formula xml:id="formula_21">d H∆H (B S ,B T ) = sup h,h ∈H B T 1[h = h ]− B S 1[h = h ]<label>(15)</label></formula><p>We rewrite the summation over all the samples B into the sum of disjoint subsets B C and B C .</p><formula xml:id="formula_22">B T 1[h = h ]− B S 1[h = h ] (16) =   B C T 1[h = h ]− B C S 1[h = h ]   (17) +    B C T 1[h = h ]− B C S 1[h = h ]    (18) =ξ C (h,h )+ξ C (h,h ).<label>(19)</label></formula><p>This completes the proof.  <ref type="table" target="#tab_8">Table 9</ref> presents additional evaluation on Office-Home (standard). We re-implement MDD using identical batch sizes (50) and random seeds for fair comparison. The results show that our proposed method has consistent improvements across all evaluation measures, and the improvements are not a result of batch sizes or random seeds.  <ref type="table" target="#tab_1">Table 10</ref> presents the ablation study on Office-Home (standard) that aims to assess the impact of different implicit alignment options: alignment in the domain divergence estimations (i.e., masking in MDD) and alignment in the input space (i.e., sampling class-conditioned examples). We observe that both alignment techniques are essential for domain adaptation because alignment should be enforced consistently across all aspects of the domain adaptation training. This is consistent to findings in the main paper.  <ref type="figure" target="#fig_4">Figure 7</ref> shows the learning curve of the target domain accuracy for different methods. The proposed implicit alignment converges better than other methods. Self-training requires estimating the target domain labels, which could be time-consuming depending on the size of the dataset. To improve the computational efficiency of our algorithm, we only update pseudo-labels periodically, i.e., every 20 steps, instead of at every training step. We show in <ref type="table" target="#tab_1">Table 11</ref> that different pseudo-label update frequencies exhibit similar performance on the target domain. Notably, implicit alignment outperforms the baseline method in spite of only updating the pseudo-labels every 500 training steps. This validates the robustness of implicit alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Additional Evaluation Measures on Office-Home</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Additional Ablation on Alignment Options</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Learning Curve</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Computational Efficiency</head><p>For the experiments described in Section B.3, training the baseline methods take 31 hours (wall clock time), whereas implicit alignment takes 34 hours under the same training condition when the pseudo-labels are updated every 20 steps. The 10% computational overhead is rather restricted. Moreover, from an engineering perspective, partially updating and caching the pseudo-labels could further improve the computational efficiency, and we leave them as future work.  <ref type="table" target="#tab_1">Table 12</ref> presents the impact of batch size on the target domain accuracy. We find that implicit alignment consistently improves the model performance over the MDD baseline across different batch sizes, and both methods work better with larger batch sizes.  <ref type="figure" target="#fig_5">Figure 8</ref> shows the empirical class diversity comparing implicit alignment with the baseline. In both experiments, the batch size is identical with the total number of classes (i.e., 31). For the baseline method, random sampling only obtains about 19 unique classes per-batch, which is much smaller than the batch size, in spite of the batch sizes being the same with the total number of classes. This is because random sampling can be viewed as sampling with replacement in the label space, whereas the implicit alignment can be viewed as sampling without replacement in the label space, which naturally increases the empirical class diversity. The expected class diversity of the baseline is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Impact of Batch Size</head><formula xml:id="formula_23">E[|Y |] = n 1− n−1 n k ,<label>(20)</label></formula><p>where n is the number of unique classes and k is the size of the minibatch. The expected class diversity is 19.78 if n = 31 and k = 31, which is consistent with the empirical class diversity shown in <ref type="figure" target="#fig_5">Figure 8</ref>.</p><p>For the implicit alignment method shown in <ref type="figure" target="#fig_5">Figure 8</ref>, although it has low class diversity at training step 0 due to the random pseudo-labels, it has a sharp increase in class diversity for the first few hundred training steps, and eventually being able to sample 28 classes from the total of 31 classes. This confirms that implicit alignment is effective in improving empirical class diversity beyond random sampling.</p><p>C. Datasets  <ref type="figure" target="#fig_7">Figure 10</ref> shows the frequencies of different classes for Cl→Rw on the Office-Home (RS-UT) dataset <ref type="bibr" target="#b43">(Tan et al., 2019)</ref>. In this dataset, the minority classes in the source domain are majority classes in the target domain, which creates extreme within-domain class imbalance and between-domain distribution shift. Code. We use PyTorch 1.2 as the training environment, and we observe that the adaptation performance on PyTorch 1.4 is slightly better PyTorch 1.2. The differences between PyTorch versions do not change the findings and the conclusions of this paper. Our code and training instructions are provided in https://github.com/xiangdal/ implicit_alignment.</p><p>Model architecture. We use ResNet-50 <ref type="bibr" target="#b15">(He et al., 2016)</ref> pre-trained from ImageNet <ref type="bibr" target="#b35">(Russakovsky et al., 2015)</ref> as the backbone, and use hyper-parameters from <ref type="bibr">(Zhang et al., 2019b)</ref> for MDD-based domain discrepancy measure. The backbone is followed by a 1-1ayer bottleneck. The classifier f and auxiliary classifier f are both 2-layer networks.</p><p>Optimization. We use the SGD optimizer with learning rate 0.001, nesterov momentum 0.9, and weight decay 0.0005. We empirically find that SGD converges better than Adam for adversarial optimization. We use a gradient reversal layer for minimax optimization, and we use a training scheduler <ref type="bibr" target="#b12">(Ganin et al., 2016)</ref> for gradient reversal layer defined as</p><formula xml:id="formula_24">λ p = 0.2 1+exp(− i 1000 ) −0.1,<label>(21)</label></formula><p>where i denotes the step number. We used the same scheduler from <ref type="bibr">(Zhang et al., 2019b)</ref> for all experiments and have not tried hyperparameter search for λ p . The batch size is 31 for Office-31 and 50 for Office-Home.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Remark 3.3 (The domain discriminator shortcut). Let the ordered triple (x,y c ,y d ) denote data sample x, and its associating class label y c and domain label y d , respectively, where x ∈ B, y c ∈ Y and y d ∈ {0,1}. Let f c be a classifier that maps x to a class label y c . Let f d be a domain discriminator that maps x to a binary domain label y d . For the empirical class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed framework. (a) We aim to align the source domain pS(x), colored by classes, with unlabeled target domain pT (x). (b) For pS(x), we sample x ∼ pS(x|y)p(y) based on the alignment distribution p(y). For pT (x), we sample a class aligned minibatch x ∼ pT (x|ŷ)p(y) using identical p(y), with the help of pseudo-labelsŷT . (c) The adversarial training aims to acquire domain-invariant representations z from the feature extractor parameterized by φ. (d) The classifier predicts class labels from z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3. (a) Source and target class distribution of Office-Home (RS-UT). (b) Accuracy comparison between Office-Home (RS-UT) and Office-Home (balanced) for Rw→Pr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>The impact of class diversity and alignment on domain adaptation for Ar→Cl, Office-Home (standard). The impact of pseudo-label errors on implicit and explicit alignment, Ar→Cl, Office-Home (standard).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Learning curve of the target domain accuracy for Pr→Rw, Office-Home (RS-UT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Empirical class diversity while training A→W (Office-31) with batch size 31.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 Figure 9 .</head><label>99</label><figDesc>shows the frequencies of different classes for Cl→Rw on the Office-Home (standard) dataset. This dataset is under natrual class imbalance where examples of different classes are not evenly distributed. Class frequency of Cl→Rw, Office-Home (standard)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Class distribution of of Cl→Rw, Office-Home (RS-UT) Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation D. Model Architecture and Training Details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Per-class average accuracy on Office-Home dataset with RS-UT label shifts (ResNet-50). Rw Pr Rw Cl Pr Rw Pr Cl Cl Rw Cl Pr Avg Source: Data of these baseline methods are cited from<ref type="bibr" target="#b43">(Tan et al., 2019)</ref>.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source Only  †</cell><cell>69.77</cell><cell>38.35</cell><cell>67.31</cell><cell>35.84</cell><cell>53.31</cell><cell>52.27 52.81</cell></row><row><cell>BSP (Chen et al., 2019c)  †</cell><cell>72.80</cell><cell>23.82</cell><cell>66.19</cell><cell>20.05</cell><cell>32.59</cell><cell>30.36 40.97</cell></row><row><cell>PADA (Cao et al., 2018)  †</cell><cell>60.77</cell><cell>32.28</cell><cell>57.09</cell><cell>26.76</cell><cell>40.71</cell><cell>38.34 42.66</cell></row><row><cell>BBSE (Lipton et al., 2018)  †</cell><cell>61.10</cell><cell>33.27</cell><cell>62.66</cell><cell>31.15</cell><cell>39.70</cell><cell>38.08 44.33</cell></row><row><cell>MCD (Saito et al., 2018)  †</cell><cell>66.03</cell><cell>33.17</cell><cell>62.95</cell><cell>29.99</cell><cell>44.47</cell><cell>39.01 45.94</cell></row><row><cell>DAN (Long et al., 2015)  †</cell><cell>69.35</cell><cell>40.84</cell><cell>66.93</cell><cell>34.66</cell><cell>53.55</cell><cell>52.09 52.90</cell></row><row><cell>F-DANN (Wu et al., 2019)  †</cell><cell>68.56</cell><cell>40.57</cell><cell>67.32</cell><cell>37.33</cell><cell>55.84</cell><cell>53.67 53.88</cell></row><row><cell>JAN (Long et al., 2017)  †</cell><cell>67.20</cell><cell>43.60</cell><cell>68.87</cell><cell>39.21</cell><cell>57.98</cell><cell>48.57 54.24</cell></row><row><cell>DANN (Ganin et al., 2016)  †</cell><cell>71.62</cell><cell>46.51</cell><cell>68.40</cell><cell>38.07</cell><cell>58.83</cell><cell>58.05 56.91</cell></row><row><cell>MDD (random sampler)</cell><cell>71.21</cell><cell>44.78</cell><cell>69.31</cell><cell>42.56</cell><cell>52.10</cell><cell>52.70 55.44</cell></row><row><cell>MDD (source-balanced sampler)</cell><cell>76.06</cell><cell>47.38</cell><cell>71.56</cell><cell>40.03</cell><cell>57.46</cell><cell>58.54 58.50</cell></row><row><cell>COAL (Tan et al., 2019)  †, ‡</cell><cell>73.65</cell><cell>42.58</cell><cell>73.26</cell><cell>40.61</cell><cell>59.22</cell><cell>57.33 58.40</cell></row><row><cell>MDD+Explicit Alignment (basic)  ‡</cell><cell>69.52</cell><cell>44.70</cell><cell>69.59</cell><cell>40.27</cell><cell>53.02</cell><cell>53.39 55.08</cell></row><row><cell>MDD+Explicit Alignment (moving avg.)  ‡</cell><cell>71.37</cell><cell>45.26</cell><cell>69.69</cell><cell>40.28</cell><cell>52.92</cell><cell>52.69 55.37</cell></row><row><cell>MDD+Explicit Alignment (curriculum)  ‡</cell><cell>70.02</cell><cell>45.48</cell><cell>69.71</cell><cell>40.86</cell><cell>53.26</cell><cell>52.99 55.39</cell></row><row><cell>MDD+Implicit Alignment</cell><cell>76.08</cell><cell>50.04</cell><cell>74.21</cell><cell>45.38</cell><cell>61.15</cell><cell>63.15 61.67</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ Methods using explicit class-conditioned domain alignment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Accuracy (%) on Office-31 (standard) for unsupervised domain adaptation. We repeated each experiment 5 times with different random seeds and report the average and the standard error of the accuracy.4±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3 76.1 DAN (Long et al., 2015) 80.5±0.4 97.1±0.2 99.6±0.1 78.6±0.2 63.6±0.3 62.8±0.2 80.4 DANN (Ganin et al., 2016) 82.0±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5 82.2 ADDA (Tzeng et al., 2017) 86.2±0.5 96.2±0.3 98.4±0.3 77.8±0.3 69.5±0.4 68.9±0.5 82.9 JAN (Long et al., 2017) 85.4±0.3 97.4±0.2 99.8±0.2 84.7±0.3 68.6±0.3 70.0±0.4 84.3 MADA (Pei et al., 2018) 90.0 ± 0.1 97.4±0.1 99.6±0.1 87.8±0.2 70.3±0.3 66.4±0.3 85.2 GTA (Sankaranarayanan et al., 2018) 89.5±0.5 97.9±0.3 99.8±0.4 87.7±0.5 72.8±0.3 71.4±0.4 86.5 MCD (Saito et al., 2018) 88.6±0.2 98.5±0.1 100.0±.0 92.2±0.2 69.5±0.1 69.7±0.3 86.5 CDAN (Long et al., 2018) 94.1±0.1 98.6±0.1 100.0±.0 92.9±0.2 71.0±0.3 69.3±0.3 87.7 MDD (Zhang et al., 2019b) 94.5±0.3 98.4±0.1 100.0±.0 93.5±0.2 74.6±0.3 72.2±0.1 88.9 PACET (Liang et al., 2019b) ‡</figDesc><table><row><cell>Method</cell><cell>A W</cell><cell>D W</cell><cell>W D</cell><cell>A D</cell><cell>D A</cell><cell>W A</cell><cell>Avg</cell></row><row><cell>Source only</cell><cell>68.90.8</cell><cell>97.6</cell><cell>99.8</cell><cell>90.8</cell><cell>73.5</cell><cell>73.6</cell><cell>87.4</cell></row><row><cell>CAT (Deng et al., 2019)  ‡</cell><cell cols="7">94.4±0.1 98.0±0.2 100.0±0.0 90.8±1.8 72.2±0.2 70.2±0.1 87.6</cell></row><row><cell>MDD (source-balanced sampler)</cell><cell cols="7">90.4±0.4 98.7±0.1 99.9±0.1 90.4±0.2 75.0±0.5 73.7±0.9 88.0</cell></row><row><cell>MDD+Explicit Alignment  ‡</cell><cell cols="2">92.3±0.1 98.2±0.1</cell><cell>99.8±.0</cell><cell cols="4">92.3±0.3 74.6±0.2 72.9±0.7 88.4</cell></row><row><cell>MDD+Implicit Alignment</cell><cell cols="2">90.3±0.2 98.7±0.1</cell><cell>99.8±.0</cell><cell cols="4">92.1±0.5 75.3±0.2 74.9±0.3 88.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ Methods using explicit class-conditioned domain alignment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Source only</cell><cell>34.9 50.0</cell><cell>58.0</cell><cell>37.4 41.9</cell><cell>46.2</cell><cell>38.5 31.2 60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9 46.1</cell></row><row><cell>DAN (Long et al., 2015)</cell><cell>43.6 57.0</cell><cell>67.9</cell><cell>45.8 56.5</cell><cell>60.4</cell><cell>44.0 43.6 67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3 56.3</cell></row><row><cell>DANN (Ganin et al., 2016)</cell><cell>45.6 59.3</cell><cell>70.1</cell><cell>47.0 58.5</cell><cell>60.9</cell><cell>46.1 43.7 68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8 57.6</cell></row><row><cell>JAN (Long et al., 2017)</cell><cell>45.9 61.2</cell><cell>68.9</cell><cell>50.4 59.7</cell><cell>61.0</cell><cell>45.8 43.4 70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8 58.3</cell></row><row><cell>CDAN (Long et al., 2018)</cell><cell>50.7 70.6</cell><cell>76.0</cell><cell>57.6 70.0</cell><cell>70.0</cell><cell>57.4 50.9 77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6 65.8</cell></row><row><cell>BSP (Chen et al., 2019c)</cell><cell>52.0 68.6</cell><cell>76.1</cell><cell>58.0 70.3</cell><cell>70.2</cell><cell>58.6 50.2 77.6</cell><cell>72.2</cell><cell>59.3</cell><cell>81.9 66.3</cell></row><row><cell>MDD (Zhang et al., 2019b)</cell><cell>54.9 73.7</cell><cell>77.8</cell><cell>60.0 71.4</cell><cell>71.8</cell><cell>61.2 53.6 78.1</cell><cell>72.5</cell><cell>60.2</cell><cell>82.3 68.1</cell></row><row><cell>MCS (Liang et al., 2019a)  ‡</cell><cell>55.9 73.8</cell><cell>79.0</cell><cell>57.5 69.9</cell><cell>71.3</cell><cell>58.4 50.3 78.2</cell><cell>65.9</cell><cell>53.2</cell><cell>82.2 66.3</cell></row><row><cell>MDD+Explicit Alignment  ‡</cell><cell>54.3 74.6</cell><cell>77.6</cell><cell>60.7 71.9</cell><cell>71.4</cell><cell>62.1 52.4 76.9</cell><cell>71.1</cell><cell>57.6</cell><cell>81.3 67.7</cell></row><row><cell cols="2">MDD (source-balanced sampler) 55.3 75.0</cell><cell>79.1</cell><cell>62.3 70.1</cell><cell>73.2</cell><cell>63.5 53.2 78.7</cell><cell>70.4</cell><cell>56.2</cell><cell>82.0 68.3</cell></row><row><cell>MDD+Implicit Alignment</cell><cell>56.2 77.9</cell><cell>79.2</cell><cell>64.4 73.1</cell><cell>74.4</cell><cell>64.2 54.2 79.9</cell><cell>71.2</cell><cell>58.1</cell><cell>83.1 69.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Accuracy (%) on Office-Home (standard) for unsupervised domain adaptation (ResNet-50). Method Ar Cl Ar Pr Ar Rw Cl Ar Cl Pr Cl Rw Pr Ar Pr Cl Pr Rw Rw Ar Rw Cl Rw Pr Avg‡ Methods using explicit class-conditioned domain alignment.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Per-class average accuracy (%) with mismatched prior where the source domain is balanced while the target domain is imbalanced.</figDesc><table><row><cell></cell><cell cols="2">SVHN→MNIST</cell><cell cols="2">MNIST→SVHN</cell></row><row><cell>method</cell><cell>mild</cell><cell>extreme</cell><cell>mild</cell><cell>extreme</cell></row><row><cell cols="5">source only 67.4±7.3 66.3±3.3 32.5±2.9 28.2±2.3</cell></row><row><cell>DANN</cell><cell cols="4">78.2±2.8 59.1±0.8 20.9±6.0 20.5±3.1</cell></row><row><cell cols="5">DANN+implicit 88.6±0.7 82.2±2.1 32.4±2.1 28.9±3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Per-class average accuracy (%) with mismatched prior where the source domain is imbalanced while the target domain is balanced.</figDesc><table><row><cell></cell><cell cols="2">SVHN→MNIST</cell><cell cols="2">MNIST→SVHN</cell></row><row><cell>method</cell><cell>mild</cell><cell>extreme</cell><cell>mild</cell><cell>extreme</cell></row><row><cell cols="5">source only 65.2±2.1 53.3±1.3 31.6±3.3 32.8±0.9</cell></row><row><cell>DANN</cell><cell cols="4">82.0±0.7 52.3±2.3 23.4±3.6 25.9±0.5</cell></row><row><cell cols="5">DANN+implicit 91.0±1.9 87.1±2.6 34.9±0.5 31.1±2.9</cell></row><row><cell cols="5">Table 8. Per-class average accuracy (%) with mismatched prior</cell></row><row><cell cols="3">where both domains are imbalanced.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SVHN→MNIST</cell><cell cols="2">MNIST→SVHN</cell></row><row><cell>method</cell><cell>mild</cell><cell>extreme</cell><cell>mild</cell><cell>extreme</cell></row><row><cell cols="5">source only 60.9±5.2 51.2±5.9 30.6±1.3 27.1±1.7</cell></row><row><cell>DANN</cell><cell cols="4">67.6±0.8 40.5±5.5 23.4±1.6 18.8±2.9</cell></row><row><cell cols="5">DANN+implicit 88.6±0.6 70.5±3.6 36.3±2.5 27.9±2.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Let B S , B T be minibatches from U S and U T , respectively, where B S ⊆ U S , B T ⊆ U T , and m b = |B S | = |B T |. The empirical estimation of d H∆H (B S ,B T ) over the minibatches B S , B T is defined aŝ</figDesc><table><row><cell>. Learning semantic</cell><cell>A. Theory</cell></row><row><cell>representations for unsupervised domain adaptation. In International Conference on Machine Learning, pp.</cell><cell>Definition A.1.</cell></row><row><cell>5419-5428, 2018.</cell><cell></cell></row><row><cell>Zhang, Q., Zhang, J., Liu, W., and Tao, D. Category</cell><cell></cell></row><row><cell>anchor-guided unsupervised domain adaptation for se-</cell><cell></cell></row><row><cell>mantic segmentation. In Advances in Neural Information</cell><cell></cell></row><row><cell>Processing Systems, pp. 433-443, 2019a.</cell><cell></cell></row><row><cell>Zhang, W., Ouyang, W., Li, W., and Xu, D. Collaborative and</cell><cell></cell></row><row><cell>adversarial network for unsupervised domain adaptation.</cell><cell></cell></row><row><cell>In Proceedings of the IEEE Conference on Computer</cell><cell></cell></row><row><cell>Vision and Pattern Recognition, pp. 3801-3809, 2018.</cell><cell></cell></row><row><cell>Zhang, Y., Liu, T., Long, M., and Jordan, M. Bridging theory</cell><cell></cell></row><row><cell>and algorithm for domain adaptation. In Chaudhuri, K. and</cell><cell></cell></row><row><cell>Salakhutdinov, R. (eds.), Proceedings of the 36th Interna-</cell><cell></cell></row><row><cell>tional Conference on Machine Learning, volume 97 of Pro-</cell><cell></cell></row><row><cell>ceedings of Machine Learning Research, pp. 7404-7413,</cell><cell></cell></row><row><cell>Long Beach, California, USA, 09-15 Jun 2019b. PMLR.</cell><cell></cell></row><row><cell>Zou, Y., Yu, Z., Vijaya Kumar, B., and Wang, J. Unsu-</cell><cell></cell></row><row><cell>pervised domain adaptation for semantic segmentation</cell><cell></cell></row><row><cell>via class-balanced self-training. In Proceedings of the</cell><cell></cell></row><row><cell>European Conference on Computer Vision (ECCV), pp.</cell><cell></cell></row><row><cell>289-305, 2018.</cell><cell></cell></row><row><cell>Zou, Y., Yu, Z., Liu, X., Kumar, B. V., and Wang, J. Confi-</cell><cell></cell></row><row><cell>dence regularized self-training. In The IEEE International</cell><cell></cell></row><row><cell>Conference on Computer Vision (ICCV), October 2019.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Evaluation on Office-Home (%) with ResNet-50.</figDesc><table><row><cell></cell><cell>Ar Cl</cell><cell>Pr Rw</cell></row><row><cell></cell><cell>MDD ours</cell><cell>MDD ours</cell></row><row><cell>accuracy</cell><cell>54.91 56.17</cell><cell>77.46 79.94</cell></row><row><cell>macro F1 score</cell><cell>53.66 55.29</cell><cell>75.86 78.42</cell></row><row><cell>weighted F1 score</cell><cell>53.97 55.81</cell><cell>77.24 79.79</cell></row><row><cell>macro precision</cell><cell>57.02 57.72</cell><cell>78.21 79.56</cell></row><row><cell cols="2">weighted precision 58.85 60.30</cell><cell>79.60 80.97</cell></row><row><cell>macro recall</cell><cell>56.41 57.76</cell><cell>76.30 78.61</cell></row><row><cell>weighted recall</cell><cell>54.91 56.17</cell><cell>77.65 79.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>The impact of different implicit alignment options, i.e., masking the classifier-based domain discrepancy measure and sampling examples from the source and target domains, on Ar→Cl and Cl→Pr, Office-Home (standard).</figDesc><table><row><cell></cell><cell cols="2">Alignment options</cell><cell></cell></row><row><cell cols="4">Domains masking sampling Accuracy</cell></row><row><cell>Ar Cl</cell><cell>× √ × √</cell><cell>× × √ √</cell><cell>55.3 55.5 54.6 56.2</cell></row><row><cell>Cl Pr</cell><cell>× √ × √</cell><cell>× × √ √</cell><cell>71.4 70.1 70.5 73.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>The impact of pseudo-label update frequency on Ar→Cl, Office-Home (standard).</figDesc><table><row><cell>pseudo-labels</cell><cell></cell></row><row><cell cols="2">updated every N steps accuracy</cell></row><row><cell>5</cell><cell>56.0</cell></row><row><cell>10</cell><cell>56.7</cell></row><row><cell>20</cell><cell>56.2</cell></row><row><cell>50</cell><cell>55.2</cell></row><row><cell>100</cell><cell>56.3</cell></row><row><cell>500</cell><cell>55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Impact of batch size on target domain accuracy (%), Ar→Cl, Office-Home (standard). The MDD results are based on our re-implementation.</figDesc><table><row><cell cols="3">batch size baseline implicit</cell></row><row><cell>8</cell><cell>48.9</cell><cell>49.7</cell></row><row><cell>16</cell><cell>52.7</cell><cell>52.8</cell></row><row><cell>32</cell><cell>54.9</cell><cell>56.2</cell></row><row><cell>50</cell><cell>55.3</cell><cell>56.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Imagia, Canada 2 Dalhousie University, Canada 3 Mila, Université de Montréal, Canada 4 Polish Academy of Sciences, Poland. Correspondence to: Xiang Jiang &lt;xiang.jiang@dal.ca&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Code: https://github.com/xiangdal/implicit_alignment</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for providing thoughtful feedback. The authors also thank Lisa Di Jorio, Tanya Nair, Francis Dutil, Cecil Low-Kam, Nicolas Chapados, and the Imagia team for their support. Xiang Jiang acknowledges the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via regularized conditional alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cluster alignment with a teacher for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9944" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaption in one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="573" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the econometric society</title>
		<imprint>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A review of domain adaptation without target labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9345" to="9356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distant supervised centroid shift: A simple and efficient approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2975" to="2984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring uncertainty in pseudo-label guided unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106996</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A sample selection approach for universal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05071</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="165" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing the effectiveness and applicability of co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cikm</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2239" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generalized domain adaptation with covariate and label shift co-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10320</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the application of roc analysis to predict classification performance under varying class distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bayesian uncertainty matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09693</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Domain adaptation with asymmetrically-relaxed distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
