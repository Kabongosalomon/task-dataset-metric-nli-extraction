<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Strong Baseline for Vehicle Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">V</forename><surname>Huynh</surname></persName>
							<email>su.huynh@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
							<email>nam.nguyen@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
							<email>ngoc.nguyen@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinh</forename><forename type="middle">Tq</forename><surname>Nguyen</surname></persName>
							<email>vinh.nguyen@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Huynh</surname></persName>
							<email>chau.huynh@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuong</forename><surname>Nguyen</surname></persName>
							<email>chuong.nguyen@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cybercore</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">A Strong Baseline for Vehicle Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across different cameras, hence plays an important role in modern traffic management systems. The technical challenges require the algorithms must be robust in different views, resolution, occlusion and illumination conditions. In this paper, we first analyze the main factors hindering the Vehicle Re-ID performance. We then present our solutions, specifically targeting the dataset Track 2 of the 5th AI City Challenge, including (1) reducing the domain gap between real and synthetic data, (2) network modification by stacking multi heads with attention mechanism, (3) adaptive loss weight adjustment. Our method achieves 61.34% mAP on the private CityFlow testset without using external dataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on the Veri benchmark. The code is available at https://github.com/ cybercore-co-ltd/track2_aicity_2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vehicle Re-ID aims to re-target vehicle images across non-overlapping camera views given a query image. It has many practical applications, such as for analyzing and managing the traffic flows in Intelligent Transport System. Despite many progresses have been made in the recent years thanks to deep learning, vehicle Re-ID is still facing many challenges, such as severe variations from different view points, partial occlusion, image blurry or illumination changes. The state-of-the-art methods <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b4">4]</ref> typically use a deep neural network to extract the vehicle visual representation. Some methods proposed to enhance the feature representation by using multi-head architecture to extract multi-scale information, such as Zheng et al. <ref type="bibr" target="#b26">[25]</ref>. However, they only use simple pooling operators to extract feature vectors, which then be averaged in inference stage. Hence, the feature lacks the vehicle detailed characteristics, which is important to distinguish objects with similar appearance. In contrast, in Face ID application, Kim et al. <ref type="bibr" target="#b6">[6]</ref> propose to retain the detailed characteristics into several latent groups, which improve the Re-ID confidence. This motivates us to develop a mechanism to integrate the feature vectors into several common groups, which help filtering out the candidates during retrieval. To improve the model generalization, a typical practice is to train the model on large datasets, such as the Veri dataset <ref type="bibr" target="#b9">[9]</ref> and PKU-VehicleID dataset <ref type="bibr" target="#b7">[7]</ref>. Another approach to obtain large dataset without costly human labeling is to utilize synthetic data generated from 3D simulation environment, by which we can have fully control of the vehicle's appearance <ref type="bibr" target="#b22">[21]</ref>. In the Track 2 of the 5th AI City Challenge, two datasets are provided, namely the real-world and synthetic data, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. However, as seen in <ref type="figure" target="#fig_0">Figure 1</ref>, there is always a domain gap between two data sources, which leads to feature distribution shifting. To tackle this problem, Zheng et al. <ref type="bibr" target="#b26">[25]</ref> adopt the image translation technique (UNIT <ref type="bibr" target="#b8">[8]</ref>) to transform the synthetic data closer to the realistic one. However, the translated images are still in poor quality, and the domain gap is still significant.</p><p>In addition, designing effective loss functions to train the network is also very important. A majority of previous works <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b4">4]</ref> typically use a combination of Triplet Loss and Cross Entropy Loss. The loss function 's objective is intuitive: pulling samples with the same ID together, while pushing those with different IDs far apart. However, the Triplet Loss function only uses one positive and one negative pair per anchor, and the hard negative mining process must be tuned carefully. Moreover, the ratio between Triplet Loss and Cross Entropy Loss is heuristically set to 1:1. However, our experiments show that this ratio setting also has a strong impact to the performance, but surprisingly, to the best of our knowledge, it is often overlooked in the previous works.</p><p>From the aforementioned analysis, we present our solutions to adress the problem. Our main contributions are:</p><p>(1) We adopt MixStyle Transfer <ref type="bibr" target="#b30">[29]</ref> as a regularization method to reduce the gap between the real and synthetic data.</p><p>(2) Multi-head with attention mechanism are attached to the backbone to help the model learn more detailed features. The features are then automatically grouped into subfeatures, each help narrows down the search space of the target identity.</p><p>(3) We replace the commonly used Triplet Loss with the Supervised Contrastive Loss <ref type="bibr" target="#b5">[5]</ref> which help the network learning more effectively. Additionally, a novel adaptive loss weight between the Supervised Contrastive Loss and the Cross Entropy Loss is provided to improve the performance dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In order to enrich visual representation for deep learning based models, a large scale dataset is necessary. Liu et al. <ref type="bibr" target="#b9">[9]</ref> propose the VeRi dataset, which contains a large number of vehicles captured by non overlapping cameras with different perspectives, scales and illuminations in real world urban traffic. In addition, annotating dataset is very costly and time consuming. To solve this problem, many efforts have been made to improve the data generation techniques. For instance, Yao et al. <ref type="bibr" target="#b22">[21]</ref> introduce a largescale synthetic dataset simulated by a flexible 3D graphic engine with editable attributes such as vehicle orientation, light direction and camera height. Recently, the generative adversarial network (GAN) can be used to generate new data by transferring vehicle style <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">18]</ref> or changing the vehicle attributes <ref type="bibr" target="#b28">[27]</ref>. Moreover, Zhou et al. propose the MixStyle method <ref type="bibr" target="#b30">[29]</ref>, which attempts to create a domaingeneralized model by mixing the feature statistics to simulate new styles. However, simply adding synthetic data to train the model often yeilds inferior results, due to the domain gap and feature bias between the synthetic and the real world data.</p><p>Other methods focus on developing more effective loss functions to improve the network training efficiency. For instance, the Large Margin Cosine Loss <ref type="bibr" target="#b18">[17]</ref> aims to maximize the inter-class variance and minimize intra-class variance. The Triplet Loss <ref type="bibr" target="#b20">[19]</ref> aims to learn visual representation by optimizing the distances between a set of three hard samples. Sun et al. <ref type="bibr" target="#b12">[12]</ref> propose the Circle Loss, which adaptively adjusts weights for each similarity score. In addition to loss functions, sampling strategy also plays an important role in re-id training. The Hierarchical Triplet Loss <ref type="bibr" target="#b1">[2]</ref> uses a predefined hierarchical tree to formulate informative training samples, which help to overcome the limitation of random sampling when training triplet loss. The semi-hard triplet mining <ref type="bibr" target="#b11">[11]</ref> focuses on negative examples which have close distances to the anchor positive distances. However, sampling strategy is generally heuristic, depending on the loss function, and hard to tune.</p><p>Additionally, post-processing is also important to reduce the false-positive prediction. For example, re-ranking can improve the accuracy of the ranking list. Re-ranking approaches are widely used in person re-id <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b10">[10]</ref>, which typically rely on the consistency and nearest-neighbor relationship of gallery images based on initial re-id ranking. Recently, Zhong et al. <ref type="bibr" target="#b29">[28]</ref> propose the k-reciprocal encod-ing method , which considers the original distance and the Jaccard distance between two images. In this work, we also perform an ablation study to find the best practice in applying post-processing steps to the vehicle Re-ID problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In section 3.1, we introduce the algorithm to bridge the gap between synthetic data and real data. Then we show our baseline architecture which applies multi-head with attention mechanism in section 3.2. In section 3.3, the alternative Contrastive Loss and Adaptive Loss Weight are introduced. Finally, we present some bag of post-processing tricks in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Domain Generalization</head><p>To train a model that generalizes to unseen domains, we adopt the MixStyle method <ref type="bibr" target="#b30">[29]</ref>, which aims to simulate new styles by mixing the statistical features of two samples from different domains. Given the input batch X (i.e., real and synthetic samples in a same batch training) and a shuffle of X, namedX, MixStyle computes the mixed feature's statistics by</p><formula xml:id="formula_0">µ m = λµ(X) + (1 − λ)µ(X) (1) σ m = λσ(X) + (1 − λ)σ(X)<label>(2)</label></formula><p>where λ is the weights sampled from Beta distribution, λ ∼ Beta(α, α). Following <ref type="bibr" target="#b30">[29]</ref>, we set α = 0.1 throughout all our experiments. Rely on the mixed feature statistic, stylenormalized X is computed as</p><formula xml:id="formula_1">MixStyle(X) = σ m X − µ(X) σ(X) + µ m .<label>(3)</label></formula><p>By leveraging the feature-level statistics, MixStyle implicitly regularizes the network. This makes the model become more robust to the domain difference and enforce the network to learn the object semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>We adopt the method proposed in <ref type="bibr" target="#b31">[30]</ref> as the baseline, and augment it with our proposed network modification. The network architecture, the training and inference pipeline are illustrated in <ref type="figure">Figure.</ref>2.</p><p>Backbone. We use Instance Batch Normalization (IBN) network family <ref type="bibr" target="#b21">[20]</ref> as the backbone due to its advantages. Firstly, by utilizing the instance normalization, the feature extractor can learn robust encoded representations that invariant to appearance differences. Secondly, it can improve the performance of other advanced deep neural network architecture such as ResNet, ResNeXt, and SENet. Moreover, we attempt to append MixStyle layers into the network to improve the domain generalization. Specifically, as described in <ref type="bibr" target="#b30">[29]</ref>, convolution layers in early stages encode the style information, while later stages tend to capture the semantic content. Therefore, we add the MixStyle module after the Block 1 and 2 in the ResNeXt ibn a 101 backbone <ref type="bibr" target="#b21">[20]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Multi-head with Attention Mechanism. Distinguishing thousands of vehicles with multiple views is challenging. Instead of using only one head, using multi-head encourages the re-id model to learn more diverse features from different vehicle characteristics. Thus, we adopted the multiples heads architecture <ref type="bibr" target="#b6">[6]</ref> to further enhance the quality of the visual representation for vehicle re-id. <ref type="figure" target="#fig_2">Figure 3</ref> shows the architecture of the multiple head with attention mechanism. In particular, the 2048-dim feature obtained from the backbone is fed into multiple parallel fully connected (FC) layers. Following <ref type="bibr" target="#b6">[6]</ref>, each FC layer is considered as one head and expected to learn distinct features which take into account different vehicle characteristics. Additionally, the attention mechanism determines which head's features are more important to the final encoding feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">ID and Metric Losses</head><p>For training re-id model, a common approach is to use a combination of ID Loss and Metric Loss. In particular, Cross Entropy (CE) is used for ID Loss to classify samples in different classes, while Metric Loss is often the Contrastive Loss, such as Triplet Loss <ref type="bibr" target="#b20">[19]</ref> or Circle Loss <ref type="bibr" target="#b13">[13]</ref>, to optimize the feature distance between each class. ID Loss. In this work, we use the CE Loss for the ID Loss. Given an input image, the ID embedding vector is extracted from the fully connected layer attached to the multi-head module with the dimension equal to the number of vehicles N . Let y is the ground truth ID label and p i is the ID prediction logits of class i, we use the Label Smoothing technique <ref type="bibr" target="#b15">[15]</ref> to prevent the model from over-fitting and improve robustness, the Label Smoothing CE Loss is defined as:</p><formula xml:id="formula_2">L(ID) = N i=1 −q i log(p i ) q i = 0, y = i q i = 1, y = i (4) q i = 1 − N −1 N ε if i = y ε/N otherwise,<label>(5)</label></formula><p>where, ε is a soft-margin to reduce the model overconfidence and is set to 0.1 in our experiments.</p><p>Metric Loss. To improve the model performance on hard samples, we adopt the Supervised Contrastive Loss (SupCon) <ref type="bibr" target="#b5">[5]</ref>. Specifically, the SupCon can be seen as a generalized case of the Triplet and N-pair loss. Instead of using only one positive and one negative pair for each anchor, the SupCon considers many positive and negative pairs. Applying SupCon to the ReID problem provides several benefits (1) the gradient of SupCon loss function encourages learning from hard positives and hard negatives;</p><p>and <ref type="formula" target="#formula_0">(2)</ref> it is less sensitive to hyper-parameters. The SupCon is computed as:</p><formula xml:id="formula_3">L = i∈I −1 |P (i)| p∈P (i) log exp(z i z p /τ ) a∈A(i) exp(z i z a /τ ) ,<label>(6)</label></formula><p>where P (i) ≡ {p ∈ A(i) :ỹ p =ỹ i } is the set of indices of all positives in the multi viewed batch distinct from i, |P (i)| is its cardinality, τ ∈ R + is a scalar temperature parameter, z i is an anchor feature, z p is a positive feature and z a is a negative feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Constructing Adaptive Loss Weight</head><p>Problems in Training. Training the ReID model requires optimizing a combination of ID Loss and Metric Loss. Conventionally, the loss weights are set equally, i.e. 1:1 ratio. However, in practice, ID Loss is relatively much larger than Metric Loss, which causes the imbalance and affects the training performance. <ref type="table" target="#tab_0">Table 1</ref> shows the sensitiveness of performance towards loss weight. Unfortunately, manually tuning the loss weight is sub-optimal and time consuming. Hence, motivated by the Adaptive Loss Weight Adjustment <ref type="bibr" target="#b24">[23]</ref>, we propose the Momentum Adaptive Loss Weight (MALW) to increase training stability by automatically updating loss weights according to the statistical characteristics of loss values.  Momentum Adaptive Loss Weight. Algorithm 1 and <ref type="figure">Figure 4</ref> describe how the MALW updates the weights during training progress. Let λ ID and λ M etric be the loss weights for ID Loss and Metric Loss, respectively. Initially, the ratio between λ ID and λ M etric is set to 1:1. After K iterations training, the ID loss weight λ ID is updated based on the standard deviation of the recorded ID Loss L ID and Metric Loss L M etric with a momentum factor. The MALW method improves our model performance by balancing the training losses without adding any computation cost to the inference step, as seen in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Post-Processing</head><p>Re-rank using k-reciprocal encoding. We improve performance of the re-id model using a re-ranking method described in <ref type="bibr" target="#b29">[28]</ref>. This approach refines the initial ranking list using the information of original distance and Jaccard distance between two vehicle images.</p><p>Fused distance. To reduce the influence of vehicle orientations and camera viewpoints, we adopt the fusion technique proposed in <ref type="bibr" target="#b31">[30]</ref>. Specifically, the vehicle ID, orientation and camera distance matrices are fused to get the cost fusion matrix, which is then used to find the optimal results for query images, as:</p><formula xml:id="formula_4">D(x i , x j ) = D v (x i , x j ) − λ 1 D o (x i , x j ) − λ 2 D c (x i , x j ), (7) where D v (x i , x j ), D o (x i , x j ), D c (x i , x j )</formula><p>are ID distance, orientation distance and camera distance between two vehicle images (x i , x j ) , respectively.</p><p>Tracklet-Level Re-Ranking. Additionally, we apply another re-ranking method using the tracklet information of vehicles which is included in the CityFlow dataset. To be specific, a vehicle 's tracklet is created from the detection and tracking results in one camera. Replacing features of each image in a tracklet with averaging the features of a subset of consecutive frames can help us enhance the visual representation of the same vehicle <ref type="bibr" target="#b31">[30]</ref>.</p><p>Ensemble. We combined all 3 models using different backbones, including ResNet50 ibn a <ref type="bibr" target="#b21">[20]</ref>, ResNeXt101 ibn a <ref type="bibr" target="#b21">[20]</ref>, ResNet152 <ref type="bibr" target="#b3">[3]</ref>, by taking the averaged distance of each query image to gallery images. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our ensemble model significantly increases 2.8% mAP on the CityFlow test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Analysis</head><p>The dataset of Track 2 challenge is the new version of CityFlow called CityFlowV2-ReID. There are 440 IDs retrieved from 52,717 images for training and other 440 identities come from 31.238 images in the test set. Following the restriction of using external data, our team tackles the problem of data limitation by leveraging the synthetic dataset called VehicleX <ref type="bibr" target="#b22">[21]</ref>. There are totally 1362 unique identities and 192150 synthetic images in VehicleX dataset, which can be used for model training or transfer learning. We split the real training data into Split-Train and Split-Test to validate offline. In particular, Split-Train contains 44375 images of 360 IDs, while Split-Test includes 8342 images of 80 IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Strategy</head><p>We resize the images to (320x320) and apply several data augmentation methods, such as color jitters, random flip, brightness and contrast adjustment, random erase and random cropping. We use ADAM <ref type="bibr" target="#b11">[11]</ref> optimizer with the cosine annealing scheduler and set the learning rate to 3.5e-4. The batch size is set to 128, which compose of 16 identities, where each identity contains 8 images. For vehicle re-id model, we adopt three strong backbones: ResNet50 ibn a <ref type="bibr" target="#b21">[20]</ref>, ResNeXt101 ibn a <ref type="bibr" target="#b21">[20]</ref> and ResNet152 <ref type="bibr" target="#b3">[3]</ref> as our feature extractor. In addition, only ResNeXt101 ibn a is used to train both Camera Re-ID and Orientation Re-ID models. All models are pretrained on ImageNet. For each single model, we first frozen the backbone and train multi heads for 1 epoch. Then we train the whole architecture for extra 11 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we use Split-Train and Split-Test for ablation study. The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>, 3, and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>We use the standard backbone ResNeXt101 ibn a <ref type="bibr" target="#b21">[20]</ref> along with CE Loss and Triplet Loss as a baseline for our vehicle re-id model, after training the baseline using the CityFlow dataset, we achieved 75.5% mAP and 22.1% mAP on Split-Test and CityFlow dataset.</p><p>Synthetic Data with MixStyle: To evaluate the effectiveness of using synthetic data, we train the baseline using the combination of real and synthetic data, result in the increment of mAP to 80.2% and 32.5% on Split-Test and CityFlow, respectively. This indicates that proper usage of synthetic data to train the network is helpful. Moreover, by using translated synthetic data instead of the original one, the mAP increases to reach 81.5% and 35.3%. We further alleviate the domain gap between these two data sources by applying MixStyle, gaining 2.3% and 2.4% more mAP score on these datasets, as shown in <ref type="table" target="#tab_2">Table 2</ref>. This result opens up the possibility of using synthetic data for training deep re-id networks to reduce the cost of collecting realword data and human annotation.</p><p>Multi-Head with Attention Mechanism: After getting the data strategy for training, we enhance the network capability by applying Multi-head with Attention Mechanism and achieve 84.5% and 41.9% mAP on Split-Test and CityFlow, respectively, as shown in <ref type="table" target="#tab_3">Table 3</ref>. This demonstrates that the visual representation features obtained from multi-head are more robust compared to using only one single head.</p><p>Losses: The combination of CE Loss and Triplet Loss is widely used in re-id tasks. Here, replacing the Triplet Loss by the Supervised Contrastive Loss <ref type="bibr" target="#b5">[5]</ref> results in the increment of 1.2% mAP, from 84.5% to 85.7% on Split-Test set. Moreover, the MALW is applied to balance the loss functions, which solves the slow convergence problem and eliminate the need of loss weight setting. We set K = 500 and     <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Post-Processing: <ref type="table" target="#tab_4">Table 4</ref> illustrates the results of applying different post-processing methods. Firstly, the reranking algorithm <ref type="bibr" target="#b29">[28]</ref> is widely used and demonstrated its improvement, therefore, by default we apply it to all models. Secondly, the fused distance approach using the vehicle ID, Orientation and Camera distances <ref type="bibr" target="#b31">[30]</ref> increases mAP from from 49.5% to 53.7%, which indicates that the Orientation and Camera information is useful for the ReID performance. Thirdly, by applying track-ranking algorithm, we further gain 4.8% mAP. Finally, after ensembling our three best single models, we achieve 61.34% mAP on the CityFlow test set without using any external data or pseudo tricks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on VeRi776</head><p>To further demonstrate the generalization across datasets, we also test our proposed method on the Veri benchmark dataset. For a fair comparison, we only use single model, including backbone ResNeXt101 ibn a, multihead, CE and SupCon losses and MALW without applying pos-processing technique and synthetic data. We achieve</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Veri dataset mAP(%) Rank 1 (%) Strong Baseline <ref type="bibr" target="#b14">[14]</ref> 67.6 90.2 DMML <ref type="bibr" target="#b25">[24]</ref> 70.1 90.2 PAMTRI(ALL) <ref type="bibr" target="#b16">[16]</ref> 71.8 92.8 VOC ReID <ref type="bibr" target="#b31">[30]</ref> 82.8 97.6 Our 87.1 97.0 <ref type="table">Table 5</ref>: Comparison with the state-of-the art methods on the VeRi776 dataset.</p><p>the state-of-the-art performance with a large margin compared to previous works, as shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of results</head><p>We visualize the query images and ranking lists obtained by the baseline model and our final model. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the baseline model fails to retrieve an accurate ranking list, since identifying vehicle objects from these query images is truly challenging. For example, the vehicle in the first row is occluded. Vehicles in the second and third row have similar appearance to other vehicles in the dataset, while the samples in the last row has very low resolution. On the contrary, our model surpasses the baseline and can retrieve a high quality ranking list, as shown <ref type="figure" target="#fig_5">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a strong baseline for the vehicle re-identification problem. By making improvements on utilizing the usage of real and synthetic data, employing the multi-head with attention mechanism and optimizing a combination of training losses, we achieve 61.34% mAP on the CityFlow dataset. In the VeRi dataset, we achieve 87.1% mAP, outperform the previous works with a large margin. Our method is simple, and focuses on improving the training techniques more efficiently. Hence, it can be  generally applied to a variety of Re-ID problems. We also released the code to facilitate the reproduction, hoping that it can serve a new baseline for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Domain Generalization with MixStyle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The training pipeline. GEM: Generalized Mean Pooling, BN: Batch Normalization, FC: Fully Connected, CE: Cross Entropy, SupCon: Supervised Contrastive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multi-head with attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Algorithm 1 9 :</head><label>419</label><figDesc>Momentum Adaptive Loss Weight (MALW). Momentum Adaptive Loss Weight Input: -ID Loss weight λ ID -Metric Loss weight λ M etric -Update iteration count k -Momentum factor α 1: Begin: 2: Initialize loss weight λ ID , λ M etric to 1 : 1 3: Build two empty sets S ID , S M etric for recording losses 4: for i = 0 to max iter do 5: Obtain initial loss L ID and L M etric 6: Set L ID = λ ID L ID , L M etric = λ M etric L M etric 7: Add L ID to S ID and L M etric to S M etric 8: if i%k == 0 then ID std = std(S ID ), M etric std = std(S M etric ) 10: Empty sets S ID , S M etric 11: if ID std &gt; M etric std then 12: new λ ID = 1 − (ID std − M etric std )/ID std 13: Update weight: λ ID = α * λ ID + new λ ID 14: end for Output: (λ ID , λ M etric )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Result on the baseline model. Each row presents the query images and retrieved top 6 gallery images. Green and red boxes denote true positive and false positive sample, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Result on the final model. Each row presents the query images and retrieved top 6 gallery images. Green and red boxes denote true positive and false positive sample, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Loss weight 1:1</cell><cell cols="3">1:2 0.5:0.5 MALW</cell></row><row><cell>mAP(%)</cell><cell cols="2">73.3 75.2</cell><cell>76.8</cell><cell>78.4</cell></row></table><note>: The performance of Baseline from [30] under dif- ferent loss weights (Cross Entropy Loss weight:Triplet Loss weight) and MALW.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Different datasets on Real-split and CityFlow.</figDesc><table><row><cell>Method</cell><cell cols="2">Real-split</cell><cell></cell><cell>CityFlow</cell></row><row><cell></cell><cell cols="4">mAP(%) Rank 1(%) mAP(%) Rank 1(%)</cell></row><row><cell>Baseline + Multiple Head</cell><cell>84.5</cell><cell>89.0</cell><cell>41.9</cell><cell>65.6</cell></row><row><cell>Baseline + Multiple Head + SupCon</cell><cell>85.7</cell><cell>90.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline + Multiple Head + SupCon + MALW</cell><cell>88.1</cell><cell>92.5</cell><cell>49.5</cell><cell>58.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Different training methods on Real-split and CityFlow.</figDesc><table><row><cell>Method</cell><cell>Performance</cell></row><row><cell>Re-rank</cell><cell></cell></row><row><cell>Orientation &amp; Camera ID</cell><cell></cell></row><row><cell>Track-rank ReID</cell><cell></cell></row><row><cell>Ensemble</cell><cell></cell></row><row><cell>mAP(%)</cell><cell>49.5 53.7 58.5 61.3</cell></row><row><cell>Rank 1(%)</cell><cell>58.2 64.7 70.2 72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Different pos-process techniques on CityFlow. α = 0.9 and this helps improve our mAP to 88.1% and 49.5% on Split-Test and CityFlow, respectively. The results are summarized in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="272" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-domain learning and identity mining for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Groupface: Learning latent groups and constructing group-based representations for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Cheol</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="5620" to="5629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1703.00848</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale vehicle reidentification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1711.09349</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ratnesh Kumar, David C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Schölkopf, and J. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi Xingang Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simulating content consistent vehicle datasets with attribute descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12351</biblScope>
			<biblScope unit="page" from="775" to="791" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting the loss weight adjustment in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<idno>abs/2103.09488</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno>abs/1604.02531</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going beyond real data: A robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="598" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vehiclenet: Learning robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">VOC-ReID: Vehicle re-identification based on vehicle-orientationcamera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
