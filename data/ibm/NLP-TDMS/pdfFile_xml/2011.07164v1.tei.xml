<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-13">13 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">△</forename><surname>Kyra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>†▽</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Twitter Cortex</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><forename type="middle">Edunov △</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-13">13 Nov 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling. The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, naïve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlabeled data has been leveraged in many ways in natural language processing including back-translation <ref type="bibr" target="#b0">(Bojar and Tamchyna, 2011;</ref><ref type="bibr" target="#b27">Sennrich et al., 2016b;</ref>, selftraining <ref type="bibr" target="#b10">(He et al., 2020)</ref>, or language model pretraining which led to improvements in many natural language tasks <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest <ref type="bibr" target="#b25">(Raffel et al., 2020)</ref> with controlled studies showing a clear trend of diminishing returns as the amount of training data increases .</p><p>In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation liter- † Work done while at Facebook during a Facebook AI Residency. ature which had been the workhorse of text generation tasks for decades before the arrival of neural sequence to sequence models <ref type="bibr" target="#b1">(Brown et al., 1993;</ref><ref type="bibr" target="#b14">Koehn et al., 2003)</ref>. Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival <ref type="bibr" target="#b35">(Yu et al., 2017;</ref>, it has been an important part in the winning entries of several high resource language pairs at WMT 2019 , improving over strong ensembles that used 500M back-translated sentences. At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry <ref type="bibr" target="#b2">(Chen et al., 2019)</ref>.</p><p>Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes' rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model. This enables the effective use of strong language models trained on large amounts of unlabeled data. The role of the backward model, or the channel model, is to validate outputs preferred by the language model with respect to the input.</p><p>A straightforward way to use language models is to pair them with standard sequence to sequence models <ref type="bibr">(Gülçehre et al., 2015;</ref><ref type="bibr" target="#b30">Stahlberg et al., 2018)</ref>. However, this does not address explaining away effects under which modern neural sequence models still suffer <ref type="bibr" target="#b12">(Klein and Manning, 2001;</ref><ref type="bibr" target="#b16">Li et al., 2019)</ref>. As a consequence, models are susceptible to producing fluent outputs that are unrelated to the input <ref type="bibr" target="#b16">(Li et al., 2019)</ref>. The noisy channel approach explicitly addresses this via the channel model.</p><p>However, a major obstacle to efficient noisy channel modeling is that generating outputs is much slower than decoding from a standard sequence to sequence model. We address this issue by introducing several simple yet highly effective approximations which increase the speed of noisy channel modeling by an order of magnitude to make it practical. This includes smaller channel models as well as scoring only a subset of the channel model vocabulary. Experiments on WMT English-Romanian translation show that noisy channel modeling can outperform recent pretraining results. Moreover, we show that noisy channel modeling benefits much more from larger beam sizes than strong pre-training methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Noisy Channel Approach</head><p>We assume a sequence to sequence task that takes the input x to predict the output y. A standard sequence to sequence model directly estimates the probability p(y|x), referred to as a direct model. On the other hand, the noisy channel approach applies Bayes' rule to model p(y|x) = p(x|y)p(y)/p(x) where p(x|y) predicts the source x given the target y and is referred to as the channel model, p(y) is a language model over the target y, and p(x) is generally not modeled since it is constant for all y.  use Transformer models to parameterize the direct model, the channel model and the language model. Similar to <ref type="bibr" target="#b35">Yu et al. (2017)</ref>, they use the following linear combination of the channel model, the language model as well as the direct model for decoding:</p><formula xml:id="formula_0">1 t log p(y|x) + λ 1 s log p(x|y) + λ 2 s log p(y) (1)</formula><p>where t is the length of the output prefix y, s is the length of the input sequence, and λ 1 , λ 2 are hyperparameters. Exact noisy channel model scoring with neural networks during decoding is prohibitively expensive since it requires a separate forward computation with the channel model for every token in the target vocabulary. To side step this issue, <ref type="bibr" target="#b35">Yu et al. (2017)</ref> propose the following approximations to beam search with beam width k 1 : determine the k 2 highest scoring extensions of each beam according to the direct model, then score the resulting k 1 × k 2 partial candidates by the direct model, the channel model and the language model using the linear combination in Equation 1. Finally, this set is pruned to beam size k 1 . Despite this approximation, noisy channel decoding is still significantly slower than decoding with the direct model alone as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The reason for this is that the channel model repeatedly scores the entire input sequence at each time-step and this is done k 2 times for each beam. Specifically, both the direct model and the language model compute k 1 × V scores at each timestep in order to make a decoding decision for each target token during beam search, where V denotes the vocabulary size which we assume to be similar between the input and output. In contrast, the channel model computes k 1 ×k 2 ×S×V scores for each target token, where S is the maximum source sequence length. This adds substantial compute and memory overhead, to the extent that the batch size at decoding often needs to be substantially reduced. This leads to slower inference on GPUs since less computation can be parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fast Noisy Channel Modeling</head><p>Naïve online noisy channel modeling is significantly slower than standard direct models. In this section, we present approximations to make noisy channel modeling substantially faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reducing Channel Model Size</head><p>Prior work on neural noisy channel used channel models which were of the same size as the direct model <ref type="bibr" target="#b35">(Yu et al., 2017;</ref>.</p><p>The most recent work uses standard Transformer models <ref type="bibr" target="#b36">Yu et al., 2020)</ref>. In this study, we hypothesize that the primary role of the channel model is to avoid explaining away effects by the language model. This primarily entails assigning low scores to unrelated outputs, which may not require a very powerful model. In this case, we may be able to substantially decrease the size of the channel model at only a small loss in accuracy.</p><p>Recent work demonstrates that direct models with shallow decoders can give comparable accuracy, while being faster at inference time, compared to models with deep decoders <ref type="bibr" target="#b33">(Wu et al., 2019;</ref><ref type="bibr" target="#b7">Elbayad et al., 2020;</ref><ref type="bibr" target="#b11">Kasai et al., 2020;</ref><ref type="bibr" target="#b8">Fan et al., 2020)</ref>. This is particularly attractive for direct models for which the decoder network accounts for most of the wall time during inference but the dynamics for channel models are different: the channel model repeatedly scores the entire input sequence given progressively larger target prefixes. Unlike for direct models, there is no straightforward way to reuse the encoder output between time-steps, and we opt to recompute the entire encoder and decoder of the channel model at every target time-step. Since the input sequence is given, channel model computation can be batched over all tokens in the target prefix and the input sequence. This implies that we are free to adjust both the encoder and decoder depth.</p><p>We pursue two strategies to reduce model size: first, we progressively reduce the model dimension of the base Transformer architecture, by first halving the model dimension from 512 to 256, as well as the feed forward dimension from 2048 to 1024 for the half model. The smallest configuration uses a model dimension of just 32 and a feed forward dimension of 128 (denoted as 16th model). Second, we consider models with only a single encoder block and a single decoder block. These models have a postfix 1 1, e.g., 16th 1 1. <ref type="table">Table 1</ref> shows the various model sizes as well as accuracy on the development set, newstest2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reducing the Output Vocabulary</head><p>During online noisy channel decoding, we need to allocate memory for a large number of channel model output probabilites (k 1 × k 2 × S × V , as explained in § 2). This substantially reduces the maximum possible batch size in order to prevent running out of memory while decoding on GPUs. A small batch size prevents the full utilization of parallel computation on GPUs, particularly, when the channel model is relatively small: some of our channel models have an embedding dimension of just 32.</p><p>To address this issue, we make use of the fact that we know exactly which input tokens need to be scored (since the input sequence is given) instead of computing probabilities for the entire vocabulary. This is similar to vocabulary reduction techniques used for early neural sequence to sequence models, and it is particularly convenient since we know exactly which tokens are in the input sequence <ref type="bibr" target="#b19">(Mi et al., 2016;</ref><ref type="bibr" target="#b15">L'Hostis et al., 2016)</ref>.</p><p>Similar to prior work on vocabulary reduction, we found it useful to not just score the input words but also a subset of the most frequent words in the vocabulary. Specifically, for each batch, we enumerate all input word types, add the 500 most frequent types and then compute output probabilities for this subset with the channel model. The number of output probabilities calculated is typically at least one order of magnitude smaller than the full vocabulary, as shown in § 5.4.1.</p><p>This approach substantially reduces the memory footprint of small channel models and enables the use of much larger batch sizes which leads to faster inference as we will see in § 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reducing the Number of Candidates</head><p>We also study the effect of reducing the number of next token candidates k 2 scored for each beam at each step of beam search. This reduces the computation as well as memory overhead of channel model scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We consider two datasets for our experiments: For German-English (De-En), we train on WMT'19 training data. Following , we apply language identification filtering <ref type="bibr">(Lui and Baldwin, 2012)</ref> and remove sentences longer than 250 tokens as well as sentence pairs with a source/target length ratio exceeding 1.5. This results in 26.8M sentence pairs. We validate on newstest2016 and test on newstest2014, new-stest2015, newstest2017, and newstest2018. For all models, the source vocabulary is a 24K byte pair encoding (BPE; <ref type="bibr">Sennrich et al., 2016)</ref> learned on the source portion of the bitext. For the target side, we use the vocabulary of the language model ( §4.2) so that both models score the exact same units during beam search.</p><p>For Romanian-English (Ro-En), we train on WMT'16 training data, comprising 612K sentence pairs, validate on newsdev2016 and test on new-stest2016. We learn a joint BPE vocabulary of 18K types on the bitext training data which is used for both the source and target. Different to German-English, we learn a joint BPE vocabulary to enable sharing the source and target embeddings which we found to perform better for Romanian-English in early experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Models</head><p>For German-English, we train a sentence-level English Transformer language model with 16 layers and Transformer-Big architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b24">Radford et al., 2018)</ref>. The model is trained on de-duplicated English Newscrawl data from 2007-2018 comprising 186 million sentences or 4.5B words after normalization and tokenization. We use a BPE vocabulary of 24K types learned on this data. For Romanian-English translation, we train a similar English Transformer language model that uses the joint BPE vocabulary learned on the Romanian-English bitext. The latter enables the LM to score the exact same units as the sequence to sequence model during beam search.</p><p>We train a sentence-level Romanian Transformer language model with 16 layers and Transformer-Big architecture.</p><p>The model is trained on de-duplicated Romanian Common-Crawl data consisting of 623M sentences or 21.7B words after normalization and tokenization <ref type="bibr" target="#b3">(Conneau et al., 2019;</ref><ref type="bibr" target="#b32">Wenzek et al., 2020)</ref>.</p><p>The German-English bitext training data as well as the language model training data are preprocessed with the Moses tokenizer <ref type="bibr" target="#b13">(Koehn et al., 2007)</ref>. We normalize punctuation and remove nonprinting characters. Romanian-English data is preprocessed following <ref type="bibr" target="#b26">Sennrich et al. (2016a)</ref> by applying Moses tokenization and special normalization for Romanian text. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Translation Models</head><p>For De-En, we use the Transformer-Big architecture for the direct model. We do not share encoder and decoder embeddings since the source and target vocabularies are different. For channel models, operating from English to German, we consider different variants ( §3.1, <ref type="table">Table 1</ref>) to better understand the speed-accuracy trade-off of decreasing the capacity of channel models.</p><p>For Ro-En and En-Ro with bitext only, the direct and channel models use a Transformer-Base architecture. For Ro-En with backtranslation, the direct and channel models use a Transformer-Big architecture. We share the encoder and decoder embeddings since the source and target vocabularies are the same and because this improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Online Noisy Channel Decoding Setup</head><p>In order to set weights for the linear combination of model scores (Equation 1), we randomly sample a set of hyperparameters and evaluate each configuration on the development set . Hyperparameters are sampled within the interval [0, 2], For direct models (dir), we sample ten random weights for the length penalty. For direct models combined with language models (dir + lm), we evaluate 100 randomly sampled configurations for the length penalty and the language model weight (λ 2 ). For direct models combined with language models and channel models (dir + lm + ch), we evaluate 1000 configurations of the length penalty, the language model weight (λ 2 ) and the channel model weight (λ 1 ). We use 16-bit floating point precision  for decoding with the online noisy channel setup.</p><p>Accuracy is measured via sacreBLEU <ref type="bibr" target="#b23">(Post, 2018)</ref> for WMT German-English. We report the average BLEU of the newstest2014-2015 and newstest2017-2018 test sets, averaged over 3 random seeds for model weight initialization. Speed is measured by the generation time (averaged over 3 trials) in seconds on the German-English new-stest2016 test set on a 32GB Volta V100 GPU using 16-bit floating point precision . Unless otherwise specified, the beam size is 5, and the number of candidates for noisy channel model scoring per beam is k 2 = 10, unless otherwise specified. Generation times are based on a tuned batch size for each model configuration. We select the batch size within <ref type="bibr">(1,</ref><ref type="bibr">10,</ref><ref type="bibr">25,</ref><ref type="bibr">50,</ref><ref type="bibr">75,</ref><ref type="bibr">100,</ref><ref type="bibr">125,</ref><ref type="bibr">150,</ref><ref type="bibr">200,</ref><ref type="bibr">300</ref>) that fits in memory and results in the fastest generation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fast Noisy Channel Modeling</head><p>In the first experiment, we evaluate the speed and accuracy of fast noisy channel decoding ( § 3) and compare to the naïve version without approximations . As additional baselines, we consider a single direct model (dir), ensembling two direct models (2 dir) and three direct models (3 dir), as well as adding a language model to each (lm). As channel models, we consider a big Transformer, a base Transformer, as well as a variant with model dimension of only 32 which is 1/16th of the model dimension of a base Transformer with a single layer in the encoder and decoder each (16th 1 1), totaling just 2.7M parameters. For fast noisy channel decoding, we reduce the channel model output vocabulary ( §3.2) and set k 2 = 3; we ablate these choices in § 5.4. <ref type="table" target="#tab_2">Table 2</ref> shows that the approximations we introduce to make noisy channel decoding fast also achieve similar accuracy (40.5 BLEU) to the much slower noisy channel approach of , while being about six times faster at inference time.</p><p>Table 2 also shows that dir + lm + 16th 1 1 is 0.7 BLEU score better than 3 dir at a similar decoding speed. Thus, using a small channel model and a language model with online noisy channel decoding is a better strategy  than ensembling 3 direct models. Noisy channel decoding is also complementary to ensembling direct models: 3 dir + lm + base 1 1 improves by 0.7 BLEU compared to 3 dir + lm. <ref type="table" target="#tab_4">Table 3</ref> compares fast noisy channel decoding with different channel model sizes. Generally, smaller channel models are only slightly less accurate than larger models while being significantly faster than their larger counterparts. For example, 16th 1 1 is over eight times faster than big and achieves nearly the same accuracy.</p><p>This observation is in line with the hypothesis that the primary role of the channel model is to tie back the language model generations to the input. We exploit the fact that small channel models work well to make noisy channel decoding very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Noisy Channel Decoding with Larger Beam Sizes</head><p>So far we used a standard beam size of five to enable fast decoding. However, previous work found that noisy channel modeling benefits more from  larger beam sizes than other methods . Next, we evaluate whether our efficiency improvements still enable good performance with larger beam sizes. <ref type="figure">Figure 2</ref> shows that for beam size 5, most channel models perform comparably. Larger models are slightly better but overall they are in a similar ball park. As the beam size increases, larger channel models do achieve better accuracy. However, there is no difference between a single layer big model (big 1 1) and a six layer version (big). As observed in previous work , the direct model and the direct ensembles (dir, 2 dir, 3 dir) do not benefit from larger beam sizes.</p><p>Next, we compare fast noisy channel decoding and naïve noisy channel decoding at larger beam sizes. As shown in <ref type="figure">Figure 4</ref>, the naïve approach is much slower. Fast approximations to noisy channel decoding scale much better in terms of speed as the beam size increases. <ref type="figure">Figure 3</ref> compares the accuracy of fast noisy channel decoding at larger beam sizes with that of naïve noisy channel decoding. Using the big and big 1 1 channel models gives the best performance across all beam sizes for naïve noisy channel decoding. With fast noisy channel decoding, we see an average drop of 0.3 BLEU and 0.2 BLEU for big and big 1 1 respectively. On the other hand, for smaller channel models, the difference between naïve and fast noisy channel decoding is generally smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on WMT Romanian-English</head><p>Next, we evaluate noisy channel modeling on WMT Romanian-English translation (Ro-En and En-Ro) which is a low resource setup compared to WMT German-English. We also compare to a recently introduced pre-training approach, mBART. The mBART model is pre-trained to denoise input sentences in multiple languages, followed by finetuning on the bitext . Following their setup for En-Ro evaluation, we apply Moses tokenization and normalize diacritics for Romanian <ref type="bibr" target="#b26">(Sennrich et al., 2016a)</ref>, and use tokenized BLEU. For Ro-En, we use SacreBLEU <ref type="bibr" target="#b23">(Post, 2018)</ref>.   . We also show the total amount of monolingual data used by each method in billions of tokens.  <ref type="table">Table 5</ref>: Speed and accuracy on Romanian-English (Ro-En) with backtranslation. Fast noisy channel decoding using base 1 1 achieves similar accuracy to mBART02 while being faster (beam=5). BLEU is measured on newstest2016 and generation time is measured on newsdev2016.</p><p>We also study the performance of noisy channel decoding on Romanian-English with backtranslated data generated using unrestricted sampling . 2 As compared to mBART02 , the previous stateof-the-art result on Romanian-English with backtranslation, we achieve a 0.5 BLEU improvement. We use a similar number of total model parameters, but much less monolingual English data. Our English language model is trained on 4.5B tokens, while mBART02 uses 66B tokens of English and Romanian monolingual data. <ref type="bibr">2</ref> The monolingual English data used for backtranslation comes from http://data.statmt.org/rsennrich/wmt16_backtranslations/ <ref type="bibr" target="#b28">(Sennrich et al., 2016c)</ref>.</p><p>Finally, <ref type="table">Table 5</ref> shows that fast approximations and smaller channel models achieve similar performance but much higher speed compared to naïve noisy channel decoding on WMT Romanian-English with back-translation. Fast noisy channel decoding with base 1 1 achieves comparable accuracy as mBART02 at slightly faster generation time with beam size 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablations</head><p>In this section we focus on some of the design choices we made to speed up noisy channel decoding. We measure the impact on speed and accuracy when reducing the output vocabulary size of the channel model, and reducing the number of beam candidates scored by the channel model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Reducing the Output Vocabulary</head><p>In the next experiment, we compare the speed of using the full output vocabulary for the channel model to a reduced version. Specifically, we reduce the vocabulary by selecting all source tokens in the batch as well as the most frequent 500 tokens in the training data (see § 3.2). We tune each setup by selecting the fastest batch size based on a sweep over different batch sizes <ref type="bibr">(1,</ref><ref type="bibr">10,</ref><ref type="bibr">25,</ref><ref type="bibr">50,</ref><ref type="bibr">75,</ref><ref type="bibr">100,</ref><ref type="bibr">125,</ref><ref type="bibr">150,</ref><ref type="bibr">200,</ref><ref type="bibr">300)</ref>. <ref type="table" target="#tab_11">Table 6</ref> shows that generating channel model scores for a small subset of the source vocabulary results in a small accuracy of up to 0.3 BLEU, but often less, while substantially increasing speed by 40-65% for single layer channel models and by 20-55% for other channel models. base 1 1 with a small vocabulary is nearly ten times faster than the approach proposed in   racy.</p><p>The average vocabulary size used for scoring the channel model is around 1050, as compared to full source vocabulary size of 28,048. This leads to a large reduction in memory consumption and enables fitting larger batches into memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Reducing the Number of Candidates</head><p>For each beam in each step of beam search, we need to make a choice about how many candidates k 2 we re-score with noisy channel modeling.  re-scored k 2 = 10 candidates for each beam at each step. We sweep over different values of k 2 to understand the speed-accuracy trade-off associated with the choice of k 2 . <ref type="table">Table  7</ref> shows that smaller values for k 2 are as accurate and much faster for beam size 5.  <ref type="figure">Figure 4</ref>: With larger beam sizes, the speed of fast approximations for noisy channel decoding scales much better than that of naïve noisy channel decoding. Results are based on generation using the big 1 1 channel model with the fastest batch size for each setting with beam 5 on newstest2016 De-En.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a number of approximations which greatly speed up noisy channel modeling for neural sequence to sequence models. This includes using channel models which are a fraction of the size of commonly used sequence to sequence models, pruning most of the channel model output vocabulary, and reducing the number of beam candidates scored by the channel model.</p><p>Our approximations are simple, yet, highly effective and enable comparable inference speed to ensembles of direct models while delivering higher accuracy. Our experiments show that noisy channel modeling can outperform pre-training approaches by being able to better exploit wider beams. Moreover, this is achieved while using a smaller amount of monolingual data.   <ref type="table">Table 7</ref>: Smaller number of rescoring candidates k 2 per beam are as accurate and much faster than larger values of k 2 for fast noisy channel decoding using base 1 1 with beam 5. BLEU is averaged over news2014, news2015, news2017 and news2018 of WMT De-En and generation time is on news2016.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Speed of decoding with a direct model (dir), direct model with language model (dir + lm) and a naïve noisy channel approach without fast approximations or optimizations. The latter is very slow compared to the direct model. Results are based on generation with the fastest batch size for each setting with beam 5 on newstest2016 De-En (cf. §4.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fast noisy channel modeling is more accurate than ensembles at comparable speed and the two methods are additive. All results use beam size 5, batch sizes for each configuration are optimized and BLEU is averaged over news2014, news2015, news2017 and news2018 of WMT German to English.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Smaller channel models perform similarly for</cell></row><row><cell>the standard beam size of 5. We exploit this fact to</cell></row><row><cell>speed up noisy channel decoding.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows that noisy channel decoding with</cell></row><row><cell>a wide beam can outperform multilingual pre-</cell></row><row><cell>training (mBART) across the board. Large beams</cell></row><row><cell>are not helpful for generation with mBART. Com-</cell></row><row><cell>pared to the direct model, noisy channel decoding</cell></row><row><cell>improves by 2.7/3.1 BLEU on En-Ro and Ro-En</cell></row><row><cell>respectively, and increasing the beam size gives</cell></row><row><cell>gains of 4.5/4.5 BLEU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: BLEU of noisy channel decoding on the Romanian-English newstest2016 test set with bitext-only as</cell></row><row><cell>well as with backtranslation (BT) compared to mBART</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison of accuracy (BLEU) and speed of online noisy channel decoding with and without the small output vocabulary approximation for different channel model sizes. Note we use k 2 = 10 for this ablation. BLEU is averaged over news2014, news2015, news2017 and news2018 of WMT De-En and generation time is on news2016.</figDesc><table><row><cell cols="3">k 2 BLEU Time (s)</cell></row><row><cell>2</cell><cell>40.4</cell><cell>76</cell></row><row><cell>3</cell><cell>40.5</cell><cell>88</cell></row><row><cell>5</cell><cell>40.4</cell><cell>124</cell></row><row><cell>10</cell><cell>40.5</cell><cell>168</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rsennrich/wmt16-scripts/tree/mas</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving translation model by monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Facebook ai&apos;s wat19 myanmar-english translation task submission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Unsupervised cross-lingual representation learning at scale. arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pre-trained language model representations for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth-adaptive transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Ç Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation. arXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional structure versus conditional estimation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L&amp;apos;</forename><surname>Gurvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hostis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<idno>abs/1610.00072</idno>
		<title level="m">Vocabulary Selection Strategies for Neural Machine Translation. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Don&apos;t say that! making inconsistent dialogue unlikely with unlikelihood training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2012. langid. py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 system demonstrations</title>
		<meeting>the ACL 2012 system demonstrations</meeting>
		<imprint>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facebook fair&apos;s wmt19 news translation task submission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL System Demonstrations</title>
		<meeting>of NAACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting bleu scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple fusion: Return of the language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grave</forename><surname>Andédouard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple and effective noisy channel modeling for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The neural noisy channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Better document-level machine translation with bayes&apos; rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sartran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
