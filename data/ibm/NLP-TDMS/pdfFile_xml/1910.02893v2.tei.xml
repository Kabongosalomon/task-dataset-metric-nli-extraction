<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Iterative Edit Models for Local Sequence Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Awasthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasna</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabyasachi</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">IIT Bombay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Iterative Edit Models for Local Sequence Transduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modelling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pretrained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction. The code and pre-trained models for GEC are available at https://github. com/awasthiabhijeet/PIE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In local sequence transduction (LST) an input sequence x 1 , . . . , x n needs to be mapped to an output sequence y 1 , . . . , y m where the x and y sequences differ only in a few positions, m is close to n, and x i , y j come from the same vocabulary Σ. An important application of local sequence transduction that we focus on in this paper is Grammatical error correction (GEC). We contrast local transduction with more general sequence transduction tasks like translation and paraphrasing which might entail different input-output vocabulary and non-local alignments. The general se- * Correspondence to: awasthi@cse.iitb.ac.in quence transduction task is cast as sequence to sequence (seq2seq) learning and modeled popularly using an attentional encoder-decoder (ED) model. The ED model auto-regressively produces each token y t in the output sequence conditioned on all previous tokens y 1 , . . . , y t−1 . Owing to the remarkable success of this model in challenging tasks like translation, almost all state-of-the-art neural models for GEC use it <ref type="bibr" target="#b39">(Zhao et al., 2019;</ref><ref type="bibr" target="#b22">Lichtarge et al., 2019;</ref><ref type="bibr" target="#b10">Ge et al., 2018b;</ref><ref type="bibr" target="#b5">Chollampatt and Ng, 2018b;</ref>.</p><p>We take a fresh look at local sequence transduction tasks and present a new parallel-iterativeedit (PIE) architecture. Unlike the prevalent ED model that is constrained to sequentially generating the tokens in the output, the PIE model generates the output in parallel, thereby substantially reducing the latency of sequential decoding on long inputs. However, matching the accuracy of existing ED models without the luxury of conditional generation is highly challenging. Recently, parallel models have also been explored in tasks like translation <ref type="bibr" target="#b34">(Stern et al., 2018;</ref><ref type="bibr" target="#b12">Gu et al., 2018;</ref><ref type="bibr" target="#b17">Kaiser et al., 2018)</ref> and speech synthesis (van den <ref type="bibr" target="#b28">Oord et al., 2018)</ref>, but their accuracy is significantly lower than corresponding ED models. The PIE model incorporates the following four ideas to achieve comparable accuracy on tasks like GEC in spite of parallel decoding. 1. Output edits instead of tokens: First, instead of outputting tokens from a large vocabulary, we output edits such as copy, appends, deletes, replacements, and case-changes which generalize better across tokens and yield a much smaller vocabulary. Suppose in GEC we have an input sentence: fowler fed dog. Existing seq2seq learning approaches would need to output the four tokens Fowler, fed, the, dog from a word vocabulary whereas we arXiv:1910.02893v2 [cs.CL] 15 May 2020 would predict the edits {Capitalize token 1, Append(the) to token 2, Copy token 3}. 2. Sequence labeling instead of sequence generation: Second we perform in-place edits on the source tokens and formulate local sequence transduction as labeling the input tokens with edit commands, rather than solving the much harder whole sequence generation task involving a separate decoder and attention module. Since input and output lengths are different in general such formulation is non-trivial, particularly due to edits that insert words. We create special compounds edits that merge token inserts with preceding edits that yield higher accuracy than earlier methods of independently predicting inserts <ref type="bibr" target="#b29">(Ribeiro et al., 2018;</ref>. 3. Iterative refinement: Third, we increase the inference capacity of the parallel model by iteratively inputting the model's own output for further refinement. This handles dependencies implicitly, in a way reminiscent of Iterative Conditional Modes (ICM) fitting in graphical model inference <ref type="bibr" target="#b19">(Koller and Friedman, 2009</ref>). <ref type="bibr" target="#b22">Lichtarge et al. (2019)</ref> and <ref type="bibr" target="#b10">Ge et al. (2018b)</ref> also refine iteratively but with ED models. 4. Factorize pre-trained bidirectional LMs: Finally, we adapt recent pre-trained bidirectional models like BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> by factorizing the logit layer over edit commands and their token argument. Existing GEC systems typically rely on conventional forward directional LM to pretrain their decoder, whereas we show how to use a bi-directional LM in the encoder, and that too to predict edits.</p><p>Novel contributions of our work are as follows:</p><p>• Recognizing GEC as a local sequence transduction (LST) problem, rather than machine translation. We then cast LST as a fast non-autoregressive, sequence labeling model as against existing auto-regressive encoderdecoder model.</p><p>• Our method of reducing LST to nonautoregressive sequence labeling has many novel elements: outputting edit operations instead of tokens, append operations instead of insertions in the edit space, and replacements along with custom transformations.</p><p>• We show how to effectively harness a pre-trained language model like BERT using our factorized logit architecture with edit-specific attention masks.</p><p>• The parallel inference in PIE is 5 to 15 times faster than a competitive ED based GEC model like <ref type="bibr" target="#b22">(Lichtarge et al., 2019)</ref> which performs sequential decoding using beam-search. PIE also attains close to state of the art performance on standard GEC datasets. On two other local transduction tasks, viz., OCR and spell corrections the PIE model is fast and accurate w.r.t. other existing models developed specifically for local sequence transduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Method</head><p>We assume a fully supervised training setup where we are given a parallel dataset of incorrect, correct sequence pairs: D = {(x i , y i ) : i = 1 . . . N }, and an optional large corpus of unmatched correct sequences L = {ỹ 1 , . . . ,ỹ U }. In GEC, this could be a corpus of grammatically correct sentences used to pre-train a language model.</p><p>Background: existing ED model Existing seq2seq ED models factorize Pr(y|x) to capture the full dependency between a y t and all previous y &lt;t = y 1 , . . . , y t−1 as m t=1 Pr(y t |y &lt;t , x). An encoder converts input tokens x 1 , . . . , x n to contextual states h 1 , . . . , h n and a decoder summarizes y &lt;t to a state s t . An attention distribution over contextual states computed from s t determines the relevant input context c t and the output token distribution is calculated as Pr(y t |y &lt;t , x) = Pr(y t |c t , s t ). Decoding is done sequentially using beam-search. When a correct sequence corpus L is available, the decoder is pre-trained on a next-token prediction loss and/or a trained LM is used to re-rank the beam-search outputs <ref type="bibr" target="#b39">(Zhao et al., 2019;</ref><ref type="bibr" target="#b4">Chollampatt and Ng, 2018a;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the PIE model</head><p>We move from generating tokens in the output sequence y using a separate decoder, to labelling the input sequence x 1 , . . . , x n with edits e 1 , . . . , e n . For this we need to design a function Seq2Edits that takes as input an (x, y) pair in D and outputs a sequence e of edits from an edit space E where e is of the same length as x in spite of x and y being of different lengths. In Section 2.2 we show how we design such a function.</p><p>Training: We invoke Seq2Edits on D and learn the parameters of a probabilistic model Pr(e|x, θ) to assign a distribution over the edit labels on tokens of the input sequence. In Section 2.3 we describe the PIE architecture in more detail. The correct corpus L , when available, is used to pre-train the encoder to predict an arbitrarily masked token y t in a sequence y in L, much like in BERT. Unlike in existing seq2seq systems where L is used to pre-train the decoder that only captures forward dependencies, in our pre-training the predicted token y t is dependent on both forward and backward contexts. This is particularly useful for GEC-type tasks where future context y t+1 . . . y m can be approximated by x t+1 , . . . , x n .</p><p>Inference: Given an input x, the trained model predicts the edit distribution for each input token independent of others, that is Pr(e|x, θ) = n t=1 Pr(e t |x, t, θ), and thus does not entail the latency of sequential token generation of the ED model. We output the most probable editsê = argmax e Pr(e|x, θ). Edits are designed so that we can easily get the edited sequenceŷ after applyinĝ e on x.ŷ is further refined by iteratively applying the model on the generated outputsŷ until we get a sequence identical to one of the previous sequences upto a maximum number of iterations I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Seq2Edits Function</head><p>Given a x = x 1 , . . . , x n and y = y 1 , . . . , y m where m may not be equal to n, our goal is to obtain a sequence of edit operations e = (e 1 , . . . , e n ) : e i ∈ E such that applying edit e i on the input token x i at each position i reconstructs the output sequence y. Invoking an off-the-shelf edit distance algorithm between x and y can give us a sequence of copy, delete, replace, and insert operations of arbitrary length. The main difficulty is converting the insert operations into in-place edits at each x i . Other parallel models <ref type="bibr" target="#b29">(Ribeiro et al., 2018;</ref> have used methods like predicting insertion slots in a pre-processing step, or predicting zero or more tokens in-between any two tokens in x. We will see in Section 3.1.4 that these options do not perform well. Hence we design an alternative edit space E that merges inserts with preceding edit operations creating compound append or replace operations. Further, we create a dictionary Σ a of common q-gram insertions or replacements observed in the training data. Our edit space (E) comprises of copy (C) x i , delete</p><p>Require: x = (x1, . . . , xn), y = (y1, . . . , ym) and T : List of Transformations diffs ← LEVENSHTEIN-DIST(x, y) with modified cost. diffs ← In diffs break substitutions, merge inserts into qgrams Σa ← M most common inserts in training data e ← EMPTYARRAY(n) </p><formula xml:id="formula_0">for t ← 1 to LENGTH(diffs) do if diffs[t] = (C, xi) or (D, xi) then ei ← diffs[t].op else if diffs[t] = (I, xi, w) then if ei = D then if (xi, w) match transformation T ∈ T then ei ← T else ei ← R(w) if w ∈ Σa else C else if ei = C then ei ← A(w) if w ∈ Σa else C return e</formula><formula xml:id="formula_1">(D) x i , append (A) a q-gram w ∈ Σ a after copy- ing x i , replace (R) x i with a q-gram w ∈ Σ a .</formula><p>For GEC, we additionally use transformations denoted as T 1 , . . . , T k which perform word-inflection (e.g. arrive to arrival). The space of all edits is thus:</p><formula xml:id="formula_2">E = {C, D, T 1 , . . . , T k } ∪ {A(w) : w ∈ Σ a } ∪ {R(w) : w ∈ Σ a } (1) Example 1 x [ Bolt can have run race ] y [ Bolt could have run the race ] diff (C,[) (C,Bolt) (D,can) (I,can,could) (C,have) (C,run) (I,run,the) (C,race) (C,]) e C C R(could) C A(the) C C ↑ ↑ ↑ ↑ ↑ ↑ ↑ [ Bolt can have run race ] Example 2 x [ He still won race ! ] y [ However , he still won ! ] diff (C,[) (I,[,However,) (D,He) (I,He,he) (C,still) (C,won) (D,-race) (C,!) (C,]) e A(However,) T case C C D C C ↑ ↑ ↑ ↑ ↑ ↑ ↑ [</formula><p>He still won race ! ] We present our algorithm for converting a sequence x and y into in-place edits on x using the above edit space in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="table" target="#tab_0">Table 1</ref> gives examples of converting (x, y) pairs to edit sequences. We first invoke the Levenshtein distance algorithm <ref type="bibr" target="#b21">(Levenshtein, 1966)</ref> to obtain diff between x and y with delete and insert cost as 1 as usual, but with a modified substitution cost to favor matching of related words. We detail this modified cost in the Appendix and show an example of how this modification leads to more sensible edits. The diff is post-processed to convert substitutions into deletes followed by inserts, and consecutive inserts are merged into a q-gram. We then create a dictionary Σ a of the M most frequent q-gram inserts in the training set. Thereafter, we scan the diff left to right: a copy at x i makes e i = C, a delete at x i makes e i = D, an insert w at x i and a e i = C flips the e i into a e i = A(w) if w is in Σ a , else it is dropped, an insert w at x i and a e i = D flips the e i into a e i = T(w) if a match found, else a replace R(w) if w is in Σ a , else it is dropped.</p><p>The above algorithm does not guarantee that when e is applied on x we will recover y for all sequences in the training data. This is because we limit Σ a to include only the M most frequently inserted q-grams. For local sequence transduction tasks, we expect a long chain of consecutive inserts to be rare, hence our experiments were performed with q = 2. For example, in NUCLE dataset <ref type="bibr" target="#b26">(Ng et al., 2014)</ref> which has roughly 57.1K sentences, less than 3% sentences have three or more consecutive inserts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Parallel Edit Prediction Model</head><p>We next describe our model for predicting edits e : e 1 , . . . , e n on an input sequence x : x 1 , . . . , x n . We use a bidirectional encoder to provide a contextual encoding of each x i . This can either be multiple layers of bidirectional RNNs, CNNs or deep bidirectional transformers. We adopt the deep bidirectional transformer architecture since it encodes the input in parallel. We pre-train the model using L much like in BERT pre-training recently proposed for language modeling <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. We first give an overview of BERT and then describe our model.</p><p>Background: BERT The input is token embedding x i and positional embedding p i for each token x i in the sequence x. Call these jointly as</p><formula xml:id="formula_3">h 0 i = [x i , p i ].</formula><p>Each layer produces a h i at each position i as a function of h −1 i and self-attention</p><formula xml:id="formula_4">over all h −1 j , j ∈ [1, n].</formula><p>The BERT model is pretrained by masking out a small fraction of the input tokens by a special MASK, and predicting the masked word from its bidirectional context captured as the last layer output: h 1 , . . . , h n . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">A Default use of BERT</head><p>Since we have cast our task as a sequence labeling task, a default output layer would be to compute Pr(e i |x) as a softmax over the space of edits E from each h i . If W e denotes the softmax parameter for edit e, we get:</p><formula xml:id="formula_5">Pr(e i = e|x) = softmax(W e h i )<label>(2)</label></formula><p>The softmax parameters, W e in Equation 2, have to be trained from scratch. We propose a method to exploit token embeddings of the pre-trained language model to warm-start the training of edits like appends and replaces which are associated with a token argument. Furthermore, for appends and replaces, we provide a new method of computing the hidden layer output via alternative input positional embeddings and self-attention. We do so without introducing any new parameters in the hidden layers of BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">An Edit-factorized BERT Architecture</head><p>We adapt a pre-trained BERT-like bidirectional language model to learn to predict edits as follows.</p><p>For suitably capturing the contexts for replace edits, for each position i we create an additional input comprising of r 0 i = [M, p i ] where M is the embedding for MASK token in the LM. Likewise for a potential insert between i and i + 1 we create an</p><formula xml:id="formula_6">input a 0 i = [M, p i +p i+1 2 ]</formula><p>where the second component is the average of the position embedding of the ith and i + 1th position. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, at each layer we compute self-attention for the ith replace unit r i over h j for all j = i and itself. Likewise, for the append unit a i the self-attention is over all h j for all js and itself. At the last layer we have h 1 , . . . , h n , r 1 , . . . , r n , a 1 , . . . , a n . Using these we compute logits factorized over edits and their token argument. For an edit e ∈ E at position i, let w denote the argument of the edit (if any). As mentioned earlier, w can be a q-gram for append and replace edits. Embedding of w, represented by φ(w) is obtained by summing up individual output embeddings of tokens in w. Additionally, in the outer layer we allocate edit-specific parameters θ corresponding to each distinct command in E. Using these, various edit logits are computed as follows:</p><p>Pr(e i |x) = softmax(logit(e i |x)) where logit(e i |x) =</p><p>(3)</p><formula xml:id="formula_7">               θ C h i + φ(x i ) h i + 0 if e i = C θ A(w) h i + φ(x i ) h i + φ(w) a i if e i = A(w) θ R(w) h i + 0 + (φ(w) − φ(x i )) r i if e i = R(w) θ D h i + 0 + 0 if e i = D θ T k h i + φ(x i ) h i + 0 if e i = T k</formula><p>The first term in the RHS of above equations captures edit specific score. The second term captures the score for copying the current word x i to the output. The third term models the influence of a new incoming token in the output obtained by a replace or append edit. For replace edits, score of the replaced word is subtracted from score of the incoming word. For transformation we add the copy score because they typically modify only the word forms, hence we do not expect meaning of the transformed word to change significantly.</p><p>The above equation provides insights on why predicting independent edits is easier than predicting independent tokens. Consider the append edit (A(w)). Instead of independently predicting x i at i and w at i+1, we jointly predict the tokens in these two slots and contrast it with not inserting any new w after x i in a single softmax. We will show empirically (Sec 3.1.4) that such selective joint prediction is key to obtaining high accuracy in spite of parallel decoding.</p><p>Finally, loss for a training example (e, x) is obtained by summing up the cross-entropy associated with predicting edit e i at each token x i .</p><p>L(e, x) = −Σ i log(Pr(e i |x)) (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compare our parallel iterative edit (PIE) model with state-of-the-art GEC models that are all based on attentional encoder-decoder architectures. In Section 3.2 we show that the PIE model is also effective on two other local sequence transduction tasks: spell and OCR corrections. Hyperparameters for all the experiments are provided in Table 12, 13 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Grammatical error correction (GEC)</head><p>We use Lang-8 <ref type="bibr" target="#b23">(Mizumoto et al., 2011)</ref>  <ref type="bibr" target="#b40">(Zhu et al., 2015)</ref> (800M words) to predict 15% randomly masked words using its deep bidirectional context. Next, we perform 2 epochs of training on a synthetically perturbed version of the One-Billion-word corpus <ref type="bibr" target="#b3">(Chelba et al., 2013)</ref>. We refer to this as synthetic training. Details of how we create the synthetic corpus appear in Section A.3 of the appendix. Finally we fine-tune on the real GEC training corpus for 2 epochs. We use a batch size of 64 and learning rate 2e-5. The edit space consists of copy, delete, 1000 appends, 1000 replaces and 29 transformations and their inverse. Arguments of Append and Replace operations mostly comprise punctuations, articles, pronouns, prepositions, conjunctions and verbs. Transformations perform inflections like add suffix s, d, es, ing, ed or replace suffix s to ing, d to s, etc. These transformations were chosen out of common replaces in the training data such that many replace edits map to only a few transformation edits, in order to help the model better generalize replaces across different words. In Section 3.1.4 we see that transformations increase the model's recall. The complete list of transformations appears in   These results show that our parallel prediction model is competitive without incurring the overheads of beam-search and slow decoding of sequential models. GLEU + score, that rewards fluency, is somewhat lower for our model on the JF-LEG test set because of parallel predictions. We do not finetune our model on the JFLEG dev set. We expect these to improve with re-ranking using a LM. All subsequent ablations and timing measurements are reported for non-ensemble models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Overall Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Running Time Comparison</head><p>Parallel decoding enables PIE models to be considerably faster than ED models. In <ref type="figure" target="#fig_2">Figure 3</ref> we compare wall-clock decoding time of PIE with 24 encoder layers (PIE-LARGE, F 0.5 = 59.7), PIE with 12 encoder layers (PIE-BASE, F 0.5 = 56.6) and competitive ED architecture by <ref type="bibr" target="#b22">Lichtarge et al. (2019)</ref> with 6 encoder and 6 decoder layers (T2T, F 0.5 = 56.8 ) on CoNLL-14 test set. All decoding experiments were run and measured on a n1-standard-2 2 VM instance with a single TPU shard (v-2.8). We observe that even PIE-LARGE is between a factor of 5 to 15 faster than an equivalent transformer-based ED model (T2T) with beam-size 4. The running time of PIE-LARGE increases sub-linearly with sentence length whereas the ED model's decoding time increases linearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Impact of Iterative Refinement</head><p>We next evaluate the impact of iterative refinements on accuracy in <ref type="table" target="#tab_7">Table 4</ref>. Out of 1312 sentences in the test set, only 832 sentences changed in the first round which were then fed to the second round where only 156 sentences changed, etc. The average number of refinement rounds per example was 2.7. In contrast, a sequential model on this dataset would require 23.2 steps corresponding to the average number of tokens in a sentence. The F 0.5 score increases from 57.9 to 59.5 at the end of the second iteration. <ref type="table" target="#tab_8">Table 5</ref> presents some sentences corrected by PIE. We see that PIE makes multiple parallel edits in a round if needed. Also, we see how refinement over successive iterations captures output space dependency. For example, in the second sentence interact gets converted to interacted followed by insertion of have in the next round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Ablation study on the PIE Architecture</head><p>In this section we perform ablation studies to understand the importance of individual features of the PIE model. Synthetic Training We evaluate the impact of training on the artifically generated GEC corpus in row 2 of <ref type="table" target="#tab_10">Table 6</ref>. We find that without it the F 0.5 score is 3.4 points lower. Factorized Logits We evaluate the gains due to   our edit-factorized BERT model (Section 2.3.2) over the default BERT model (Section 2.3.1). In <ref type="table" target="#tab_10">Table 6</ref> (row 3) we show that compared to the factorized model (row 2) we get a 1.2 point drop in F 0.5 score in absence of factorization.</p><p>Inserts as Appends on the preceding word was another important design choice. The alternative of predicting insert independently at each gap with a null token added to Σ a performs 2.7 F 0.5 points poorly <ref type="table" target="#tab_10">(Table 6</ref> row 4 vs row 2).</p><p>Transformation edits are significant as we observe a 6.3 drop in recall without them (row 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Language Model</head><p>We evaluate the benefit of starting from BERT's pre-trained LM by reporting accuracy from an un-initialized network (row 6). We observe a 20 points drop in F 0.5 establishing the importance of LMs in GEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Network Size</head><p>We train the BERT-Base model with one-third fewer parameters than BERT-LARGE. From Table 6 (row 7 vs 1) we see once again that size matters in deep learning! Since ancient times , human interact with others face by face . PIE1 Since ancient times , humans interacted with others face to face . PIE2 Since ancient times , humans have interacted with others face to face . x</p><p>However , there are two sides of stories always . PIE1 However , there are always two sides to stories always . PIE2 However , there are always two sides to the stories . PIE3 However , there are always two sides to the story . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">More Sequence Transduction Tasks</head><p>We demonstrate the effectiveness of PIE model on two additional local sequence transduction tasks recently used in <ref type="bibr" target="#b29">(Ribeiro et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spell Correction</head><p>We use the twitter spell correction dataset (Aramaki, 2010) which consists of 39172 pairs of original and corrected words obtained from twitter. We use the same train-dev-valid split as <ref type="bibr" target="#b29">(Ribeiro et al., 2018)</ref> (31172/4000/4000). We tokenize on characters, and our vocabulary Σ and Σ a comprises the 26 lower cased letters of English.</p><p>Correcting OCR errors We use the Finnish OCR data set 3 by <ref type="bibr" target="#b32">(Silfverberg et al., 2016)</ref> comprising words extracted from Early Modern   Finnish corpus of OCR processed newspaper text. We use the same train-dev-test splits as provided by <ref type="bibr" target="#b32">(Silfverberg et al., 2016)</ref>. We tokenize on characters in the word. For a particular split, our vocabulary Σ and Σ a comprises of all the characters seen in the training data of the split.</p><p>Architecture For all the tasks in this section, PIE is a 4 layer self-attention transformer with 200 hidden units, 400 intermediate units and 4 attention heads. No L pre-initialization is done. Also, number of iterations of refinements is set to 1.</p><p>Results <ref type="table" target="#tab_11">Table 7</ref> presents whole-word 0/1 accuracy for these tasks on PIE and the following methods: <ref type="bibr" target="#b29">Ribeiro et al. (2018)</ref>'s local transduction model (described in Section 4), and LSTM based ED models with hard monotonic attention <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017)</ref> and soft-attention <ref type="bibr" target="#b2">(Bahdanau et al., 2015)</ref> as reported in <ref type="bibr" target="#b29">(Ribeiro et al., 2018)</ref>. In addition, for a fair decoding time comparison, we also train a transformer-based ED model referred as Soft-T2T with 2 encoder, 2 decoder layers for spell-correction and 2 encoder, 1 PIE Soft-T2T Spell 80.43 36.62 OCR 65.44 43.02 decoder layer for OCR correction. We observe that PIE's accuracy is comparable with ED models in both the tasks. <ref type="table" target="#tab_12">Table 8</ref> compares decoding speed of PIE with Soft-T2T in words/second. Since more than 90% of words have fewer than 9 tokens and the token vocabulary Σ is small, decoding speed-ups of PIE over ED model on these tasks is modest compared to GEC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Grammar Error Correction (GEC) is an extensively researched area in NLP. See <ref type="bibr" target="#b27">Ng et al. (2013)</ref> and <ref type="bibr" target="#b26">Ng et al. (2014)</ref> for past shared tasks on GEC, and this 4 website for current progress.</p><p>Approaches attempted so far include rules <ref type="bibr">(Felice et al., 2014), classifiers (Rozovskaya and</ref><ref type="bibr" target="#b30">Roth, 2016)</ref>, statistical machine translation (SMT) <ref type="bibr" target="#b14">(Junczys-Dowmunt and Grundkiewicz, 2016)</ref>, neural ED models <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018a;</ref><ref type="bibr" target="#b9">Ge et al., 2018a)</ref>, and hybrids . All recent neural approaches are sequential ED models that predict either word sequences <ref type="bibr" target="#b39">(Zhao et al., 2019;</ref><ref type="bibr" target="#b22">Lichtarge et al., 2019)</ref> or character sequences <ref type="bibr" target="#b35">(Xie et al., 2016)</ref> using either multi-layer RNNs <ref type="bibr" target="#b13">(Ji et al., 2017;</ref> or CNNs <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018a;</ref><ref type="bibr" target="#b9">Ge et al., 2018a)</ref> or Transformers <ref type="bibr" target="#b22">Lichtarge et al., 2019)</ref>. Our sequence labeling formulation is similar to <ref type="bibr" target="#b37">(Yannakoudakis et al., 2017)</ref> and <ref type="bibr" target="#b16">(Kaili et al., 2018)</ref> but the former uses it to only detect errors and the latter only corrects five error-types using separate classifiers. Edits have been exploited in earlier GEC systems too but very unlike our method of re-architecting the core model to label input sequence with edits. <ref type="bibr" target="#b31">Schmaltz et al. (2017)</ref> interleave edit tags in target tokens but use seq2seq learning to predict the output sequence. <ref type="bibr" target="#b4">Chollampatt and Ng (2018a)</ref> use edits as features for rescoring seq2seq predictions.  use an edit-weighted MLE objective to emphasise corrective edits during seq2seq learning. <ref type="bibr" target="#b33">Stahlberg et al. (2019)</ref> use finite state transducers, whose state transitions denote possible edits, built from an unlabeled corpus to constrain the output of a neural beam decoder to a small GEC-feasible space.</p><p>Parallel decoding in neural machine translation <ref type="bibr" target="#b17">Kaiser et al. (2018)</ref> achieve partial parallelism by first generating latent variables sequentially to model dependency. <ref type="bibr" target="#b34">Stern et al. (2018)</ref> use a parallel generate-and-test method with modest speed-up. <ref type="bibr" target="#b12">Gu et al. (2018)</ref> generate all tokens in parallel but initialize decoder states using latent fertility variables to determine number of replicas of an encoder state. We achieve the effect of fertility using delete and append edits.  generate target sequences iteratively but require the target sequence length to be predicted at start. In contrast our in-place edit model allows target sequence length to change with appends.</p><p>Local Sequence Transduction is handled in <ref type="bibr" target="#b29">Ribeiro et al. (2018)</ref> by first predicting insert slots in x using learned insertion patterns and then using a sequence labeling task to output tokens in x or a special token delete. Instead, we output edit operations including word transformations. Their pattern-based insert pre-slotting is unlikely to work for more challenging tasks like GEC. <ref type="bibr" target="#b18">Koide et al. (2018)</ref> design a special editinvariant neural network for being robust to small edit changes in input biological sequences. This is a different task than ours of edit prediction. <ref type="bibr" target="#b38">Yin et al. (2019)</ref> is about neural representation of edits specifically for structured objects like source code. This is again a different problem than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a parallel iterative edit (PIE) model for local sequence transduction with a focus on the GEC task. Compared to the popular encoderdecoder models that perform sequential decoding, parallel decoding in the PIE model yields a factor of 5 to 15 reduction in decoding time. The PIE model employs a number of ideas to match the accuracy of sequential models in spite of parallel decoding: it predicts in-place edits using a carefully designed edit space, iteratively refines its own predictions, and effectively reuses state-ofthe-art pre-trained bidrectional language models. In the future we plan to apply the PIE model to more ambitious transduction tasks like translation.</p><p>We keep delete and insert cost as 1 as usual, but for substitutions, we use 1 + d, where d is the absolute difference between the number of characters of replaced and substituted word. We set to 0.001. <ref type="table" target="#tab_13">Table 9</ref> shows two minimum edit diffs if the substitution penalty has no such offset. In Diff-1, {.} substitutes {,} and Then substitutes then, followed by insertion of {,}. In Diff-2, {.} is inserted after sat followed by Then substituting {,} , followed by {,} substituting then . In absence of offset in substitution penalty, both the diffs have edit distance of 3. In presence of offset, Diff-1 has an edit-distance of 3, while Diff-2 has an edit-distance of 3.006, this allows Diff-1 to be preferred over Diff-2. As we observe, offset helps in selection of well aligned minimum edit diffs among multiple minimum edit diffs.   A.3 Artificial Error Generation <ref type="figure" target="#fig_5">Figure 4</ref> shows the algorithm used to introduce artificial errors in clean dataset. Given a sentence, first the number of errors in that sentence is determined by sampling from a multinoulli (over {0 . . . 4}). Similarly, an error is chosen independently from another multinoulli (over  {AppendError, V erbError, ReplaceError, DeleteError}). The distribution of the number of errors in a sentence and probability of each kind of error was obtained based on the available parallel corpus. For append, replace and delete errors, a position is randomly chosen for the error occurrence. For append error the word in that position is dropped. For delete error a spurious word from a commonly deleted words dictionary is added to that position. For replace error, both the actions are done. For a verb error, a verb is chosen at random from the sentence and is replaced by a random verb form of the same word. Commonly deleted words are also obtained from the parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Wall-clock Decoding Times</head><p>Average sentence length (words)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Seq2Edits function used to convert a sequence pair x, y into in-place edits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A Parallel Edit Architecture based on a 2layer bidirectional transformer. Input sequence length is 3. Arrows indicate attention mask for computation of h l i , r l i , a l i at position i for layer l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparing average decoding time in milliseconds (log scale) of PIE Vs transformer based ED model with two beam widths. Numbers in legend denote M2 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>x</head><label></label><figDesc>I started invoving into Facebook one years ago . PIE1 I started involving in Facebook one year ago . PIE2 I started involved in Facebook one year ago . PIE3 I started getting involved in Facebook one year ago . x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>x</head><label></label><figDesc>[ He sat , then he ran ] y [ He sat . Then , he ran ] op-1 [ C C S(, , .) S(then , Then) I(,) , C C ] Diff-1 [ He sat -, +. -then +Then +, he ran ] op-2 [ C C I(.) S(, , Then) S(then , ,) , C C ] Diff-2 [ He sat +. -, +Then -then +, he ran ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Algorithm to introduce errors in clean dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Two example sentence pairs converted into respective edit sequences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>of the</cell></row></table><note>+ (Napoles et al., 2016) scores on JFLEG corpus (Napoles et al., 2017) to evaluate fluency.1 https://github.com/google-research/ bert</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of non-ensemble model numbers for various GEC models. Best non-ensemble results are reported for each model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>compares ensemble model results of PIE</cell></row><row><cell>and other state of the art models, which all hap-</cell></row><row><cell>pen to be seq2seq ED models and also use en-</cell></row><row><cell>semble decoding. For PIE, we simply average</cell></row><row><cell>the probability distribution over edits from 5 in-</cell></row><row><cell>dependent ensembles. In Table-2 we compare</cell></row><row><cell>non-ensemble numbers of PIE with the best avail-</cell></row><row><cell>able non-ensemble numbers of competing meth-</cell></row></table><note>ods. On CoNLL-14 test-set our results are very close to the highest reported by Zhao et al. (2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of recent GEC models trained using publicly available corpus. All the methods here except ours perform sequential decoding. Precision and Recall are represented by P and R respectively.</figDesc><table><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>R3</cell><cell>R4</cell></row><row><cell cols="3">#edited 832 156</cell><cell>20</cell><cell>5</cell></row><row><cell>P</cell><cell cols="4">64.5 65.9 66.1 66.1</cell></row><row><cell>R</cell><cell cols="4">41.0 42.9 43.0 43.0</cell></row><row><cell>F 0.5</cell><cell cols="4">57.9 59.5 59.7 59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Statistics on successive rounds of iterative re- finement. First row denotes number changed sentences (out of 1312) with each round on CONLL-14 (test).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Parallel and Iterative edits done by PIE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the PIE model.</figDesc><table><row><cell>Methods</cell><cell>Spell OCR</cell></row><row><cell>Seq2Seq (Soft, lstm)</cell><cell>46.3 79.9</cell></row><row><cell>Seq2Seq (Hard,lstm)</cell><cell>52.2 58.4</cell></row><row><cell>Seq2Seq (Soft-T2T)</cell><cell>67.6 84.5</cell></row><row><cell cols="2">Ribeiro2018 (best model) 54.1 81.8</cell></row><row><cell>PIE</cell><cell>67.0 87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Comparing PIE with Seq2Seq models (top-</cell></row><row><cell>part) and Ribeiro's parallel model on two other local</cell></row><row><cell>sequence transduction tasks.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Wall-clock decoding speed in words/second of PIE and a comparable Seq2Seq T2T model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Two diffs having same edit distance in the absence of offset in substitution penalty A.2 Suffix transformations</figDesc><table><row><cell>Transformation</cell><cell cols="2">Example</cell></row><row><cell>ADDSUFFIX(s)</cell><cell>play</cell><cell cols="2">plays</cell></row><row><cell>ADDSUFFIX(d)</cell><cell cols="3">argue argued</cell></row><row><cell>ADDSUFFIX(es)</cell><cell cols="3">express expresses</cell></row><row><cell>ADDSUFFIX(ing)</cell><cell>play</cell><cell cols="2">playing</cell></row><row><cell>ADDSUFFIX(ed)</cell><cell>play</cell><cell cols="2">played</cell></row><row><cell>ADDSUFFIX(ly)</cell><cell cols="3">nice nicely</cell></row><row><cell>ADDSUFFIX(er)</cell><cell>play</cell><cell cols="2">player</cell></row><row><cell>ADDSUFFIX(al)</cell><cell cols="3">renew renewal</cell></row><row><cell>ADDSUFFIX(n)</cell><cell cols="3">rise risen</cell></row><row><cell>ADDSUFFIX(y)</cell><cell cols="3">health healthy</cell></row><row><cell>ADDSUFFIX(ation)</cell><cell cols="3">inform information</cell></row><row><cell>CHANGE-e-TO-ing</cell><cell>use</cell><cell cols="2">using</cell></row><row><cell>CHANGE-d-TO-t</cell><cell cols="3">spend spent</cell></row><row><cell>CHANGE-d-TO-s</cell><cell cols="3">compared compares</cell></row><row><cell>CHANGE-s-TO-ing</cell><cell cols="3">claims claiming</cell></row><row><cell>CHANGE-n-TO-ing</cell><cell cols="3">deafen deafening</cell></row><row><cell>CHANGE-nce-TO-t</cell><cell cols="3">insistence insistent</cell></row><row><cell>CHANGE-s-TO-ed</cell><cell cols="3">visits visited</cell></row><row><cell>CHANGE-ing-TO-ed</cell><cell>using</cell><cell></cell><cell>used</cell></row><row><cell>CHANGE-ing-TO-ion</cell><cell cols="3">creating creation</cell></row><row><cell cols="3">CHANGE-ing-TO-ation adoring</cell><cell>adoration</cell></row><row><cell>CHANGE-t-TO-ce</cell><cell cols="3">reluctant reluctance</cell></row><row><cell>CHANGE-y-TO-ic</cell><cell cols="3">homeopathy homeopathic</cell></row><row><cell>CHANGE-t-TO-s</cell><cell cols="3">meant means</cell></row><row><cell>CHANGE-e-TO-al</cell><cell cols="3">arrive arrival</cell></row><row><cell>CHANGE-y-TO-ily</cell><cell cols="3">angry angrily</cell></row><row><cell>CHANGE-y-TO-ied</cell><cell>copy</cell><cell cols="2">copied</cell></row><row><cell>CHANGE-y-TO-ical</cell><cell cols="2">biology</cell><cell>biological</cell></row><row><cell>CHANGE-y-TO-ies</cell><cell cols="3">family families</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>29 suffix transformations and their corresponding inverse make total 58 suffix transformations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Input: U: dataset of clean sentences AppendError ← 0 V erbError ← 1 ReplaceError ← 2 DeleteError ← 3 for sentence in U do errorCount ← multinoulli(0.05, 0.07, 0.25, 0.35, 0.28) for i ∈ 1 . . . errorCount do errorT ype ← multinoulli(0.30, 0.25, 0.25, 0.20) introduce error of type errorT ype return</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cloud.google.com/compute/ docs/machine-types#standard_machine_ types</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://nlpprogress.com/english/ grammatical_error_correction.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research was partly sponsored by a Google India AI/ML Research Award and Google PhD Fellowship in Machine Learning. We gratefully acknowledge Google's TFRC program for providing us Cloud-TPUs. We thank Varun Patil for helping us improve the speed of pre-processing and synthetic-data generation pipelines.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Morphological inflection generation with hard monotonic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2004" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Typo corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Aramaki</surname></persName>
		</author>
		<ptr target="http://luululu.com/tweet/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multilayer convolutional encoder-decoder neural network for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural quality estimation of grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2528" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fluency boost learning and inference for neural grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1055" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reaching human-level performance in automatic grammatical error correction: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01270</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near human-level performance in grammatical error correction with hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="284" to="290" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approaching neural grammatical error correction as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="595" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A simple but effective classification model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2390" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural edit operations for biological sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Koide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuro</forename><surname>Kutsuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Corpora generation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Lichtarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05780</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning sns for automated japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02592</idno>
		<title level="m">Gleu without tuning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jfleg: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The conll-2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel WaveNet: Fast highfidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
	<note>and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local string transduction as sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joana</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (ACL-COLING)</title>
		<meeting>the 27th International Conference on Computational Linguistics (ACL-COLING)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting sequence models for sentence correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2807" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data-driven spelling correction using weighted finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miikka</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Kauppinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krister Lindén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics Proceedings of the SIGFSM Workshop on Statistical NLP and Weighted Automata. ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10625</idno>
		<title level="m">Neural grammatical error correction with finite state transducers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blockwise parallel decoding for deep autoregressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural language correction with character-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1603.09727</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural sequencelabelling models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to represent edits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00138</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wall clock decoding time in milliseconds for various GEC models</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
