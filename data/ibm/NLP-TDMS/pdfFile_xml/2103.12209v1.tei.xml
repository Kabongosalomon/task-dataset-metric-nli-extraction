<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhil</forename><surname>Gurram</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Faruk Tuna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyi</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onay</forename><surname>Urfalioglu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>López</surname></persName>
						</author>
						<title level="a" type="main">Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised monocular depth estimation</term>
					<term>on- board vision</term>
					<term>domain adaptation</term>
					<term>ADAS</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth information is essential for on-board perception in autonomous driving and driver assistance. Monocular depth estimation (MDE) is very appealing since it allows for appearance and depth being on direct pixelwise correspondence without further calibration. Best MDE models are based on Convolutional Neural Networks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground truth (GT). Usually, this GT is acquired at training time through a calibrated multi-modal suite of sensors. However, also using only a monocular system at training time is cheaper and more scalable. This is possible by relying on structure-from-motion (SfM) principles to generate self-supervision. Nevertheless, problems of camouflaged objects, visibility changes, static-camera intervals, textureless areas, and scale ambiguity, diminish the usefulness of such self-supervision. In this paper, we perform monocular depth estimation by virtual-world supervision (MonoDEVS) and real-world SfM selfsupervision. We compensate the SfM self-supervision limitations by leveraging virtual-world images with accurate semantic and depth supervision, and addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms previous MDE CNNs trained on monocular and even stereo sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision</head><p>Akhil <ref type="bibr">Gurram 1,</ref><ref type="bibr" target="#b1">2</ref> , Ahmet Faruk Tuna 2 , Fengyi Shen 2,3 , Onay Urfalioglu 2 , and Antonio M. López <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4</ref> Abstract-Depth information is essential for on-board perception in autonomous driving and driver assistance. Monocular depth estimation (MDE) is very appealing since it allows for appearance and depth being on direct pixelwise correspondence without further calibration. Best MDE models are based on Convolutional Neural Networks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground truth (GT). Usually, this GT is acquired at training time through a calibrated multi-modal suite of sensors. However, also using only a monocular system at training time is cheaper and more scalable. This is possible by relying on structure-from-motion (SfM) principles to generate self-supervision. Nevertheless, problems of camouflaged objects, visibility changes, static-camera intervals, textureless areas, and scale ambiguity, diminish the usefulness of such self-supervision. In this paper, we perform monocular depth estimation by virtual-world supervision (MonoDEVS) and real-world SfM selfsupervision. We compensate the SfM self-supervision limitations by leveraging virtual-world images with accurate semantic and depth supervision, and addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms previous MDE CNNs trained on monocular and even stereo sequences.</p><p>Index Terms-Self-supervised monocular depth estimation, onboard vision, domain adaptation, ADAS, autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Understanding the semantic content of images is enough to solve many vision-based applications. However, augmenting semantic information with depth is essential for many other applications too, as for on-board perception in autonomous driving and driver assistance. Due to cost and maintenance considerations, we wish to predict depth from the same single camera used to predict semantics, so having a direct pixelwise correspondence without further calibration. Therefore, in this paper, we focus on monocular depth estimation (MDE) onboard vehicles, thus, facing outdoor traffic environments. Recent advances on MDE rely on Convolutional Neural Networks (CNNs). Let Ψ be a CNN architecture for MDE with weights θ, which takes a single image x as input, and estimates its pixelwise depth map d as output, i.e., Ψ(θ; x) → d. The Ψ's can be trained in a supervised manner, i.e., finding the values of θ by assuming access to a set of images with pixelwise 1 Akhil and Antonio are with the Dpt. of Computer Science, Universitat Autònoma de Barcelona (UAB), Spain. <ref type="bibr" target="#b1">2</ref> Akhil, Ahmet, Fengyi, and Onay are with the Huawei European Research Center, 80992 München, Germany. <ref type="bibr" target="#b2">3</ref> Fengyi is with the Dpt. of Informatics, Technische Universität München (TUM), Germany. <ref type="bibr" target="#b3">4</ref> Antonio is also with the Computer Vision Center (CVC) at UAB, Spain. Corresponding author: akhil.gurram@huawei.com Antonio acknowledges the financial support received for this research from the Spanish TIN2017-88709-R (MINECO/AEI/FEDER, UE) project. Antonio acknowledges the financial support to his general research activities given by ICREA under the ICREA Academia Program. Antonio acknowledges the support of the Generalitat de Catalunya CERCA Program as well as its ACCIO agency to CVC's general activities.</p><p>depth ground truth (GT). Usually, such a GT is acquired at training time through a multi-modal suite of sensors, at least consisting of a camera calibrated with a LiDAR or some type of 3D laser scanner variant <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Alternatively, we can use self-supervision based on either a calibrated stereo rig <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, or a monocular system and structure-from-motion (SfM) principles <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, or on a combination of both <ref type="bibr" target="#b18">[19]</ref>. Combining stereo self-supervision and LiDAR supervision has been also analyzed <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. In any case, the cheaper and simpler the suite of sensors used at training time, the better in terms of scalability and general access to the technology; however, the more challenging training a Ψ. Currently, supervised methods tend to outperform self-supervised ones <ref type="bibr" target="#b22">[23]</ref>, thus, improving the latter is an open challenge worth to pursue. This paper focuses on the most challenging setting, namely, when at training time we only have a single on-board camera allowing for SfM based self-supervision. Using only such a self-supervision may give rise to depth estimation inaccuracies due to camouflage (objects moving as the camera may not be distinguished from background), visibility changes (occlusion changes, non-Lambertian surfaces), static-camera cases (i.e., stopped ego-vehicle), and textureless areas, as well as to scale ambiguity (depth could only be estimated up to an unknown scale factor). In fact, an interesting approach to compensate for these problems could be leveraging virtual-world images (RGB) with accurate pixelwise depth (D) supervision. Using virtual worlds <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, we can acquire as many RGBD virtualworld samples as needed. However, these virtual-world samples can only be useful provided we address the virtual-to-real domain gap <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>, which links MDE with visual domain adaptation (DA), a realm of research in itself <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>.</p><p>We propose to perform monocular depth estimation by virtual-world supervision (MonoDEVS) and real-world SfM self-supervision, estimating depth in absolute scale. By relying on standard benchmarks, we show that our MonoDEVSNet outperforms previous ones trained on monocular and even stereo sequences. We think our released code and models 1 will help researchers and practitioners to address applications requiring on-board depth estimation, also establishing a strong baseline to be challenged in the future.</p><p>In the following, Section II summarizes previous works related to ours. Section III details our proposal. Section IV describes the experimental setting and discusses the obtained results. Finally, Section V summarizes the presented work and conclusions, and draws the work we target for the near future.</p><p>II. RELATED WORK MDE was first addressed by combining hand-crafted features and shallow machine learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. However, nowadays, best performing models are based on CNNs <ref type="bibr" target="#b22">[23]</ref>. Therefore, we review the CNN-based approaches to MDE which are most related to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised MDE</head><p>Relying on depth GT, Eigen et al. <ref type="bibr" target="#b0">[1]</ref> developed a Ψ architecture for coarse-to-fine depth estimation with a scaleinvariant loss function. This pioneering work inspired new CNN-based architectures to MDE <ref type="bibr">[2-6, 8, 9]</ref>, which also assume depth GT supervision. MDE has been also tackle as a task on a multi-task learning framework, typically together with semantic segmentation as both tasks aim at producing pixelwise information and, eventually, may help each other to improve their predictions at object boundaries. For instance, this is the case of some Ψ's for indoor scenarios <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. These proposals assume that pixelwise depth and class GT are simultaneously available at training time. However, this is expensive, being scarcely available for outdoor scenarios. In order to address this problem, Gurram et al. <ref type="bibr" target="#b6">[7]</ref> proposed a training framework which does not require depth and class GT to be available for the same images. Guizilini et al. <ref type="bibr" target="#b44">[45]</ref> used an out-of-the-box CNN for semantic segmentation to train semantically-guided depth features while training Ψ.</p><p>The drawback of these supervised approaches is that the depth GT usually comes from expensive LiDARs, which must be calibrated and synchronized with the cameras; i.e., even if the objective is to use only cameras for the functionality under development. Moreover, LiDAR depth is sparse compared to available image resolutions. Besides, surfaces like vehicle glasses or dark vehicles may be problematic for LiDAR sensing. Consequently, depth self-supervision and alternative sources of supervision are receiving increasing interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self-supervised MDE</head><p>Using a calibrated stereo rig to provide self-supervision for MDE is a much cheaper alternative to camera-LiDAR suites. Garg et al. <ref type="bibr" target="#b11">[12]</ref> pioneered this approach by training Ψ with a warping loss involving pairs of stereo images. Godard et al. <ref type="bibr" target="#b12">[13]</ref> introduced epipolar geometry constraints with additional terms for smoothing and enforcing consistency between leftright image pairs. Chen et al. <ref type="bibr" target="#b45">[46]</ref> improved MDE results by enforcing semantic consistency between stereo pairs, via a joint training of Ψ and semantic segmentation. Pillai et al. <ref type="bibr" target="#b13">[14]</ref> implemented sub-pixel convolutional layers for depth superresolution, as well as a novel differentiable layer to improve depth prediction on image boundaries, a known limitation of stereo self-supervision. Other authors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> still complement stereo self-supervision with sparse LiDAR supervision.</p><p>SfM principles <ref type="bibr" target="#b46">[47]</ref> can be also followed to provide selfsupervision for MDE. In fact, in this setting we can assume a monocular on-board system at training time. Briefly, the underlying idea is that obtaining a frame, x t , from consecutive ones, x t±1 , can be decomposed into jointly estimating the scene depth for x t and the camera pose at time t relative to its pose at time t ± 1; i.e., the camera ego-motion. Thus, we can train a CNN to estimate (synthesize) x t from x t±1 , where, basically, the photo-metric error between x t andx t acts as training loss, beingx t the output of this CNN (i.e., the synthesized view). After the training process, part of the CNN can perform MDE up to a scale factor (relative depth).</p><p>Zhou et al. <ref type="bibr" target="#b14">[15]</ref> followed this idea, adding an explainability mask to compensate for violations of SfM assumptions (due to frame-to-frame changes on the visibility of frame's content, textureless surfaces, etc.). This mask is estimated by a CNN jointly trained with Ψ to output a pixelwise belief on the synthesized views. Later, Yin et al. <ref type="bibr" target="#b15">[16]</ref> proposed GeoNet, which aims at improving MDE by also predicting optical flow to explicitly consider the motion introduced by dynamic objects (e.g., vehicles, pedestrians), i.e. a motion that violates SfM assumptions. However, this was effective on predicting occlusions, but not in significantly improving MDE accuracy. Godard et al. <ref type="bibr" target="#b18">[19]</ref> followed the idea of having a mask to indicate stationary pixels, which should not be taken into account by the loss driving the training. Such pixels typically appear on vehicles moving at the same speed as the camera, or can even correspond to full frames in case the ego-vehicle stops and, thus, the camera becomes stationary for a while. Pixels of similar appearance in consecutive frames are considered as stationary. A simple definition which can work because, instead of using a training loss based on absolute photometric errors (i.e. on minimizing pairwise pixel differences), it is used the structure similarity index measurement (SSIM) <ref type="bibr" target="#b47">[48]</ref>. Moreover, within the so-called MonoDepth2 framework, Godard et al. <ref type="bibr" target="#b18">[19]</ref> combine SfM and stereo self-supervision to establish state-of-the-art results. Alternatively, Guizilini et al. <ref type="bibr" target="#b44">[45]</ref> addressed the presence of dynamic objects by a two-stage MDE training process. The first stage ignores the presence of such objects, returning a Ψ trained with a loss based on SSIM. Then, before running the second stage, the training sequences are processed to filter out frames that may contain erroneous depth estimations due to moving objects. Such frames are identified by applying Ψ, a RANSAC algorithm to estimate the ground plane from their estimated depth, and determining if there is a significant number of pixels that would be projected far below the ground plane. Finally, in the second stage, Ψ is retrained form scratch without the filtered frames.</p><p>Zhao et al. <ref type="bibr" target="#b16">[17]</ref> focused on avoiding scale inconsistencies among different frames as produced under SfM selfsupervision, specially when they are from sequences whose underlying depth range is too different. In this case, both depth and optical flow estimation CNNs are trained, but not a pose estimation one. Instead, the optical flow between two frames is used to find robust pixel correspondences between them, which are used to compute their relative camera pose, computing the fundamental matrix by the 8-point algorithm, and then performing triangulation between the corresponding pixels of these frames. Overall, a sparse depth pseudo-GT is estimated and used as supervision to train Ψ. However, even robustifying scale consistency among frames, this method still outputs relative depth rather than absolute one. To avoid this problem, Guizilini et al. <ref type="bibr" target="#b21">[22]</ref> used sparse LiDAR supervision with SfM self-supervision, relying on depth and pose estimation networks. More recently, Guizilini et al. <ref type="bibr" target="#b17">[18]</ref> relied on camera velocity (i.e., the available vehicle velocity) to solve scale ambiguity in a pure SfM self-supervision setting. In particular, a velocity supervision loss trains the pose estimation CNN to learn scale-aware camera translation which, in turn, enables scale-aware depth estimation.</p><p>Overall, this literature shows the relevance of achieving MDE via SfM self-supervision and strategies to account for violation of SfM assumptions, as well as to obtain absolute depth values. Among these strategies, complementing SfM self-supervision with supervision (depth GT) coming from additional sensors such as a LiDAR and/or a stereo rig seems to be the most robust approach to address all the problems at once. However, then, a single camera would not be enough at training time. In this paper, we also complement SfM selfsupervision with accurate depth supervision. However, instead of relying on additional sensors, we use virtual-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Virtual-world data for MDE</head><p>Training Ψ on virtual-world images to later perform on realworld ones, requires to address the virtual-to-real domain gap. Many approaches perform a virtual-to-real image-to-image translation coupled to the training of Ψ. This translation usually relies on generative adversarial networks (GANs) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, since to train them only unpaired and unlabeled sets of realand virtual-world images are required.</p><p>Zheng et al. <ref type="bibr" target="#b30">[31]</ref> proposed T 2 Net. In this case, a GAN and Ψ are jointly trained, where the GAN aims at performing virtual-to-real translation while acting as an auto-encoder for real-world images. The translated images are the input for Ψ since they have depth supervision. Additionally, a GAN operating on the encoder weights (features) of Ψ was incorporated during training to force similar depth feature distributions between translated and real-world images. However, this featurelevel GAN worsen MDE results in outdoor scenarios. Kundu et al. <ref type="bibr" target="#b31">[32]</ref> proposed AdaDepth, which trains a common feature space for real-and virtual-world images, i.e., a space where it is not possible to distinguish the domain of the input images. Then, depth estimation is trained from this feature space. To achieve this, adversarial losses are used at the feature space level as well as at the estimated depth level.</p><p>Cheng et al. <ref type="bibr" target="#b34">[35]</ref> proposed S 3 Net, which extends T 2 Net with SfM self-supervision. In this case, GAN training involves semantic and photo-metric consistency. Semantic consistency between the virtual-world images and their GAN-translated counterparts is required, which is measured via semantic segmentation (which involves also to jointly train a CNN for this task). Photo-metric consistency is required for consecutive GAN-translated images, which is measured via optical flow. Note that semantic segmentation and optical flow GT is available for virtual-world images. Ψ uses the GAN-translated images as input and is trained end-to-end with the GAN. Then, a further fine-tuning step of Ψ is performed using only the realworld sequence and SfM self-supervision, i.e., involving the training of a pose estimation CNN while fine-tuning. During this process, a masking mechanism inspired in <ref type="bibr" target="#b18">[19]</ref> is also used to compensate for SfM-adverse scenarios. Contrary to AdaDepth and T 2 Net, S 3 Net just outputs relative depth.</p><p>Zhao et al. <ref type="bibr" target="#b32">[33]</ref> proposed GASDA, which leverages realworld stereo and virtual-world data. In this case, the Cy-cleGAN idea <ref type="bibr" target="#b50">[51]</ref> is used to perform DA, which actually involves two GANs, one for virtual-to-real image translation and another for real-to-virtual. Two Ψ's are trained coupled to CycleGAN, one intended to process images with realworld appearance (actual real-wold images or GAN-translated from the virtual domain), the other to process images with synthetic appearance (actual virtual-world images or GANtranslated from the real domain). In fact, at testing time, the most accurate depth results are obtained by averaging the output of these two Ψ's, which also involves to translate the real-world images to the virtual domain by the corresponding GAN. Thanks to the stereo data, left-right depth and geometry consistency losses are also included during training aiming at obtaining a more accurate Ψ. PNVR et al. <ref type="bibr" target="#b33">[34]</ref> proposed SharinGAN for training a DA GAN coupled to a specific task. One of the selected tasks is MDE with stereo selfsupervision, as in <ref type="bibr" target="#b32">[33]</ref>. In this case, real-and virtual-world images are transformed to a new image domain where their appearance discrepancies are minimized to perform MDE from them, i.e. the GAN and the Ψ are jointly trained end-toend. SharinGAN outperformed GASDA. However, at testing time, before performing the MDE, the real-world images must be translated by the GAN to the new image domain. Both GASDA and SharinGAN produce absolute scale depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Relationship of MonoDEVSNet with previous literature</head><p>In term of operational training conditions, the most similar paper to ours is S 3 Net <ref type="bibr" target="#b34">[35]</ref>. However, contrary to S 3 Net, our MonoDEVSNet can estimate depth in absolute scale. On the other hand, methods based on pure SfM self-supervision such as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref> (only SfM setting), and <ref type="bibr" target="#b44">[45]</ref>, just report relative depth. In order to compare MonoDEVSNet with them, we have estimated relative depth too. We will see how we outperform these methods, proving the usefulness of leveraging depth supervision from virtual worlds. In fact, regarding relative depth, we also outperform S 3 Net. Methods leveraging virtual-world data such as GASDA <ref type="bibr" target="#b32">[33]</ref> and SharinGAN <ref type="bibr" target="#b33">[34]</ref>, rely on real-world stereo data at training time, while we only require monocular sequences. On the other hand, our training framework can be extended to accommodate stereo data if available, although it is not our current focus. S 3 Net, GASDA, SharinGAN, T 2 Net <ref type="bibr" target="#b30">[31]</ref>, and AdaDepth <ref type="bibr" target="#b31">[32]</ref>, leverage ideas from GAN-based DA to reduce the virtual-to-real domain gap, either in image space (S 3 Net, GASDA, SharinGAN, T 2 Net) or in feature space (AdaDepth). We have analyzed both, image and feature based DA, finding that the later outperforms the former. In particular, by using the Gradient-Reversal-Layer (GRL) DA strategy <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, up to the best of our knowledge, not previously applied to MDE. Currently, we outperform the SfM self-supervision framework in <ref type="bibr" target="#b17">[18]</ref> thanks to the virtualworld supervision and our GRL DA strategy. However, using vehicle velocity to obtain absolute depth as in <ref type="bibr" target="#b17">[18]</ref>, is a complementary strategy that could be also incorporated in our framework, although it is not the focus on this paper. We show the involved images, GT, weights, and losses. Red and blue lines are paths of real and virtual-world data, respectively. The discontinuous line is a common path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this section, we introduce MonoDEVSNet, which aims at leveraging virtual-world supervision to improve real-world SfM self-supervision. Since we train from both real-and virtual-world data jointly, we describe our supervision and self-supervision losses, the loss for addressing the virtual-toreal domain gap, and the strategy to obtain depth in absolute scale. Our proposal is visually summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training data</head><p>For training MonoDEVSNet, we assume two sources of data. On the one hand, we have image sequences acquired by a monocular system on-board a vehicle while driving in realworld traffic. We denote as x r t one of such frames acquired at time t. We denote these data as X r = {x r t } N r t=1 , where N r is the number of frames from the real-world sequences. These frames do not have associated GT. On the other hand, we have analogous sequences but acquired on a virtual world, i.e., onboard a vehicle immersed in a traffic simulation. We denote as x s t one of such virtual-world frames acquired at time t. We refer to these data as</p><formula xml:id="formula_0">X s = {x s t } N s t=1 ,</formula><p>where N s is the number of frames from the virtual-world sequences. The images in X s do have associated GT, since it can be automatically generated. In particular, as it is commonly available in today's simulators, we assume pixelwise depth and semantic class GT. We define Y s = {&lt; d s t , c s t &gt;} N s t=1 to be this GT; i.e., given x s t , d s t is its depth GT, and c s t its semantic class GT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MonoDEVSNet architecture: Ψ(θ; x)</head><p>MonoDEVSNet, i.e., our Ψ(θ; x), is composed of three main blocks: a encoding block of weights θ enc , a multi-scale pyramidal block, θ pyr , and a decoding block inspired in <ref type="bibr" target="#b18">[19]</ref>, θ dec . Therefore, the total set of weights is θ = {θ enc , θ pyr , θ dec }.</p><p>Here, θ enc acts as a backbone of features. Moreover, since we aim at evaluating several encoders, the role of the multi-scale pyramid block is to adapt the bottleneck of the chosen encoder to the decoder. At testing time Ψ(θ; x) will process any realworld image x acquired on-board the ego-vehicle, while at training time either x ∈ X r or x ∈ X s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Problem formulation</head><p>Training Ψ(θ; x) consists in finding the optimum weight values, θ * , by solving the problem:</p><formula xml:id="formula_1">θ * = min θ L(θ; X r , X s .Y s ) ,<label>(1)</label></formula><p>where L is a loss function, and X s .Y s indicates the use of the virtual-world frames with their GT. As we are going to detail, L relies on three different losses, namely, L sf , L sp and L DA . The loss L sf focuses on training θ based on SfM self-supervision, thus, only relying on real-world data sequences. The SfM selfsupervision is achieved with the support of a camera pose estimation task performed by a CNN, T, of weights ϑ sf . Thus, we have L sf (θ, ϑ sf ; X r ). The loss L sp focuses on training θ with virtual-world supervision, in particular, using depth and semantic GT from virtual-world sequences. Therefore, we have L sp (θ; X s .Y s ). Finally, L DA focuses on creating domaininvariant features θ enc as part of θ. In particular, we rely on a binary real/virtual domain-classifier CNN, D, of weights</p><formula xml:id="formula_2">{θ enc , ϑ DA }. Thus, we have L DA (θ enc , ϑ DA ; X r , X s ). D. SfM Self-supervised loss: L sf (θ, ϑ sf ; X r )</formula><p>Since we focus on improving MDE by the additional use of virtual-world data, for the SfM self-supervision we leverage from the state-of-the-art proposal in <ref type="bibr" target="#b18">[19]</ref>, which we briefly summarize here for the sake of completeness as:</p><formula xml:id="formula_3">L sf (θ, ϑ sf ; X r ) = N r −1 t=2 P r t (θ, ϑ sf ) + λS r t (θ) .<label>(2)</label></formula><p>As previously introduced in <ref type="bibr" target="#b12">[13]</ref>, the term λS r t (θ) is a constant weighted loss to force local smoothness on Ψ(θ; x r t ), taking into account the edges of x r t . The term P r t (θ, ϑ sf ) is the actual SfM-inspired loss. It involves the joint training of the depth estimation weights, θ, and the relative camera pose estimation weights, ϑ sf . <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the CNN, T, associated to these weights, which takes as input two consecutive frames, e.g., (x r t , x r t+1 ), and outputs the pose transform (rotation and translation),T r t→t+1 = T(ϑ sf ; x r t , x r t+1 ), between them. Then, as can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, a projection module takeŝ T r t→t+1 , x r t+1 , and the depth estimation Ψ(θ; x r t ), to generate the synthesized framex r t+1→t (θ, ϑ sf ) which, ideally, should match x r t . In fact, both frames adjacent to x r t are considered for robustness. Accordingly, the SfM-inspired component of L sf can be defined as:</p><formula xml:id="formula_4">P r t (θ, ϑ sf ) = cpe(x r t ,x r t±1→t (θ, ϑ sf ), x r t±1 ) ,<label>(3)</label></formula><p>where cpe() is a pixelwise conditioned photo-metric error and cpe() its average over the pixels. Obtaining cpe() starts by computing two pixelwise photo-metric error measurements,</p><formula xml:id="formula_5">pe(x r t−1 , x r t , x r t+1 ) and pe(x r t−1→t (θ, ϑ sf ), x r t ,x r t+1→t (θ, ϑ sf )),</formula><p>where pe(x r −1 , x r 0 , x r +1 ) = min(pe(x r 0 , x r −1 ), pe(x r 0 , x r +1 )), and pe(x r A , x r B ) is the pixelwise photo-metric error between x r A and x r B proposed in <ref type="bibr" target="#b12">[13]</ref>, i.e., based on local structural similarity (SSIM) and pixelwise photo-metric absolute differences between x r A and x r B . Thus, min() applies pixelwise. Then, a pixelwise binary mask, called auto-mask in <ref type="bibr" target="#b18">[19]</ref>, is computed as:</p><formula xml:id="formula_6">r t (x r t ,x r t±1→t (θ, ϑ sf ), x r t±1 ) = (4) pe(x r t−1→t (θ, ϑ sf ), x r t ,x r t+1→t (θ, ϑ sf )) &lt; pe(x r t−1 , x r t , x r t+1 ) I , where [] I denotes the Iverson bracket applied pixelwise. Fi- nally, cpe() is computed as: cpe(x r t ,x r t±1→t (θ, ϑ sf ), x r t±1 ) = r t (x r t ,x r t±1→t (θ, ϑ sf ), x r t±1 ) (5) pe(x r t−1→t (θ, ϑ sf ), x r t ,x r t+1→t (θ, ϑ sf )) , where</formula><p>stands for pixelwise multiplication. The auto-mask r t () conditions which pixels of pe() are considered during the gradient computation of L sf , i.e., r t () is computed in the forward pass of CNN training, but it is considered as a constant during back-propagation. As explained in <ref type="bibr" target="#b18">[19]</ref>, the aim of r t () is to remove, during training, the influence of pixels which remain the same between adjacent frames because they are assumed to often indicate SfM violations such as a static camera, objects moving as the camera, or low texture regions. Finally, we remark that the support of ϑ sf is needed at training time, but not at testing time.</p><formula xml:id="formula_7">E. Supervised loss: L sp (θ; X s .Y s )</formula><p>In this case, since we address an estimation problem and we have accurate GT, we base L sp on the L1 metric. On the other hand, MDE is specially interesting to determine how far is the ego-vehicle from vehicles, pedestrians, etc. Accordingly, since Y s includes semantic class GT, we use it to increase the relevance of accurately estimating the depth for such major traffic protagonists. Moreover, since virtualworld depth maps are based on the Z-buffer involved on image rendering, the range of depth values available as GT tend to be over-optimistic even for active sensors such as LiDAR. For instance, there can be depth values larger than 300 m in the Z-buffer. Since we do not aim at estimating depth beyond a reasonable threshold (in m), d max , to compute L sp we will also discard pixels p with d s t (p) ≥ d max . For each x s t , both the semantic class relevance and the out-of-range depth values, can be codified as real-valued weights running on [0, 1] and arranged on a mask, s t . Thus, s t depends on d s t , d max , and c s t . However, contrarily to r t (), we can compute s t offline, i.e., before starting the training process. Taking all these details into account, we define our supervised loss as:</p><formula xml:id="formula_8">L sp (θ; X s .Y s ) = N s t=1 s t (Ψ(θ; x s t ) − d s t ) 1 .<label>(6)</label></formula><p>F. Domain adaptation loss: L DA (θ enc , ϑ DA ; X r , X s )</p><p>As can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, we aim at learning depth features, θ enc , so that it cannot be distinguished whether they were generated from a real-world input frame (target domain) or a virtual-world one (source domain); in other words, learning a domain invariant θ enc . Taking into account that we do not have accurate depth GT in the target domain, while we do have it for the source domain, we need to apply an unsupervised DA technique to train θ enc . In addition, as part of θ, the training of θ enc must result on an accurate Ψ(θ; x). Achieving this accuracy and domain invariance are adversarial goals. Accordingly, we propose to use the Gradient-Reversal-Layer (GRL) idea introduced in <ref type="bibr" target="#b51">[52]</ref>, which, up to the best of our knowledge, has not been applied before for DA in the context of MDE. In this approach, the domain invariance of θ enc is measured by a binary target/source domain-classifier CNN, D, of weights {θ enc , ϑ DA }. In <ref type="bibr" target="#b51">[52]</ref>, a logistic loss is proposed to train the domain classifier. In our case, this is set as:</p><formula xml:id="formula_9">L DA (θ enc , ϑ DA ; X r , X s ) = (7) N r t=1 log(D(θ enc , ϑ DA ; x r t )) + N s t=1 log(1 − D(θ enc , ϑ DA ; x s t )) ,</formula><p>where we assume that D(θ enc , ϑ DA ; x) outputs 1 if x ∈ X r and 0 if x ∈ X s . The GRL has no parameters and connects θ enc with ϑ DA (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Its behavior is exactly as explained in <ref type="bibr" target="#b51">[52]</ref>. This means that during forward passes of training, it acts as an identity function, while, during back-propagation, it reverses the gradient vector passing through it. Both the GRL and ϑ DA are required at training time, but not at testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Overall training procedure</head><p>Algorithm 1 summarizes the steps to compute the needed gradient vectors for mini-batch optimization. In particular, we need the gradients related to MonoDEVSNet weights, θ = {θ enc , θ pyr , θ dec }, and the weights of the auxiliary tasks, i.e., ϑ sf for SfM self-supervision, and ϑ DA for DA. Regarding gradient computation, we do not need to distinguish θ pyr from θ dec , so we define θ pyde = {θ pyr , θ dec }. In Alg. 1, we introduce an equalizing factor between supervised and selfsupervised losses, ω sf ∈ R, which aims at avoiding one loss dominating over the other along the training. A priori, we could set a constant factor. However, in practice, we have found that having an adaptive value is more useful. Therefore, inspired by the GradNorm idea <ref type="bibr" target="#b53">[54]</ref>, we use the ratio between the supervised and self-supervised losses. Algorithm 1 also introduces the scaling factor ω DA ∈ R which, following <ref type="bibr" target="#b51">[52]</ref>, controls the trade-off between optimizing θ enc to obtain an accurate Ψ(θ; x) model versus being domain invariant. Finally, L DA (θ enc , ϑ DA ; ∅, X s B ) and L DA (θ enc , ϑ DA ; X r B , ∅) indicate whether this loss must be computed only using virtual-or real-world data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Absolute depth computation</head><p>The virtual-world supervised data trains Ψ(θ; x) on absolute depth values, while the real-world SfM self-supervised data trains Ψ(θ; x) on relative depth values. Thanks to the unsupervised DA, the depth features θ enc are trained to be domain invariant. However, according to our experiments, this is not sufficient for Ψ(θ; x) producing accurate absolute depth values at testing time. Fortunately, thanks to the use of virtual-world data, we can still compute a global scaling factor, ψ ∈ R, so that ψΨ(θ; x) is accurate in absolute depth terms. For that, we assume that the sequences in X s are acquired with a camera analogous to the one used to acquire the sequences in X r . Here analogous refers to using the same number of pixels, field of view, frame rate, and mounted on-board in similar heading directions. Note that simulators are flexible enough for setting these camera parameters as needed. Accordingly, we train a Ψ(θ; x) model using only data from X s and SfM self-supervision, i.e. as if we would not have supervision for X s . Then, we find the median depth value produced by this model on the virtual-world data,d s,m ∈ R. Finally, we set ψ = d s,m /d s,m , where d s,m ∈ R is the median depth value of the GT. This pre-processing step is performed once and the model discarded afterwards. Other works follow a similar idea <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref> to compute absolute depth but relying on LiDAR data as GT reference, while we only rely on virtualworld data.</p><formula xml:id="formula_10">Algorithm 1: Computing the gradients ∆ θ enc , ∆ θ pyde , ∆ ϑ sf , ∆ ϑ DA for a mini-batch X r B ⊂ X r , X s B .Y s B ⊂ X s .Y s . ∇ ξi F (ξ 1 , ξ 2 ) refers to back-propagation on F (ξ 1 , ξ 2 ) with respect to weights ξ i ⊂ ξ 1 ∪ ξ 2 . ∅ is the empty set. Forward Passes with {X s B , Y s B } sp (θ) ←L sp (θ; X s B .Y s B ) DA,s (θ enc , ϑ DA ) ←ω DA L DA (θ enc , ϑ DA ; ∅, X s B ) Back-propagation for Supervision &amp; DA ∆ s θ pyde ←∇ θ pyde sp (θ) ∆ s θ enc ←∇ θ enc ( sp (θ) − DA,s (θ enc , ϑ DA )) ∆ s ϑ DA ←∇ ϑ DA DA,s (θ enc , ϑ DA ) Forward Passes with X r B sf (θ, ϑ sf ) ←L sf (θ, ϑ sf ; X r B ) DA,r (θ enc , ϑ DA ) ←ω DA L DA (θ enc , ϑ DA ; X r B , ∅) Back-propagation for Self-supervision &amp; DA ∆ r θ pyde ←∇ θ pyde sf (θ, ϑ sf ) ∆ r θ enc ←∇ θ enc ( sf (θ, ϑ sf ) − DA,r (θ enc , ϑ DA )) ∆ r ϑ DA ←∇ ϑ DA DA,r (θ enc , ϑ DA )</formula><p>Setting the final gradient vectors</p><formula xml:id="formula_11">∆ ϑ sf ←∇ ϑ sf sf (θ, ϑ sf ) ∆ ϑ DA ←∆ s ϑ DA + ∆ r ϑ DA ω sf ← sp (θ)/ sf (θ, ϑ sf ) ∆ θ pyde ←∆ s θ pyde + ω sf ∆ r θ pyde ∆ θ enc ←∆ s θ enc + ω sf ∆ r θ enc</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we start by defining the datasets and evaluation metrics used in our experiments. After, we provide relevant implementation and training details of MonoDEVSNet. Finally, we present and discuss our quantitative and qualitative results, comparing them with those from previous literature as well as performing an ablative analysis focused on the main components of MonoDEVSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and evaluation metrics</head><p>We use publicly available datasets and metrics which are de facto standards in MDE research. In particular, we use KITTI Raw (KR) <ref type="bibr" target="#b54">[55]</ref> and Virtual KITTI (VK) <ref type="bibr" target="#b24">[25]</ref> as real-and virtual-world sequences, respectively. We follow Zhou et al. <ref type="bibr" target="#b14">[15]</ref> training-testing split. From the training split we select 12K monocular triplets, i.e., samples of the form</p><formula xml:id="formula_12">{x r t−1 , x r t , x r t+1 }.</formula><p>The testing split consists of 697 isolated images with LiDAR-based GT, actually introduced by Eigen et al. <ref type="bibr" target="#b0">[1]</ref>. In addition, for considering the semantic content of the images in the analysis of results, we also use KITTI Stereo 2015 (KS) <ref type="bibr" target="#b55">[56]</ref> for testing. This dataset consists of 200 isolated images with enhanced depth maps and semantic labels. VK is used only for training, we also use 12K monocular triplets (non-rotated camera subset) with associated depth GT. In this case, the triplets are used to calibrate the global scaling factor ψ (see Sect. III-H), while for actual training supervision only 12K isolated frames are used. As the depth GT of VK ranges up to ∼ 655m, to match the range of KR's LiDAR-based GT, we clip it to 80m (d max ). VK includes similar weather conditions as KR/KS, and adds situations with fog, overcast, and rain, as well as sunrise and sunset illumination.</p><p>Finally, as is common since <ref type="bibr" target="#b12">[13]</ref>, we use Make3D dataset <ref type="bibr" target="#b56">[57]</ref> for assessing generalization since it is based on photographs at urban and natural areas. Therefore, Make3D shows views and content pretty much different from those on-board a vehicle as KR, KS, and VK. The images come with depth GT acquired by a 3D scanner. There are 534 images with depth GT, organized in a standard split of 400 for training and 134 for testing. We use the latter, since we rely on Make3D only for testing our proposal.</p><p>In order to assess quantitative MDE results, we use the standard metrics introduced by Eigen et al. <ref type="bibr" target="#b0">[1]</ref>, i.e., the average absolute relative error (abs-rel), the average squared relative error (sq-rel), the root mean square error (rms), and the rms log error (rms-log). For these metrics, the lower the better. In addition, the accuracy (termed as δ) under a threshold τ ∈ {1.25, 1.25 2 , 1.25 3 } is also used as metric. In this case, the higher the better. The abs-rel error and the δ &lt; τ are percentage measurements, sq-rel and rms are reported in meters, and rms-log is similar (reported in meters) to rms but applied to logarithm depth values.</p><p>These metrics are applied to absolute depth values for MDE models trained with depth supervision coming from either LiDAR <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b21">22]</ref>, stereo <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b18">19]</ref>, real-world stereo and virtual-world depth <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, or stereo and LiDAR <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. However, MDE models trained on pure SfM self-supervision can only estimate depth in relative terms, i.e., up to scale.</p><p>Moreover, the scale factor varies from image to image, a problem known as scale inconsistency. In this case, before computing the above metrics, it is applied a per-image correction factor computed at testing time <ref type="bibr">[15-17, 19, 35, 45]</ref>. In particular, given a test image x with GT and estimated depth d(x) and d(x), respectively, the common practice consists of computing a scale ψ(x) ∈ R as the ratio median(d(x))/median(d(x)), and then compare ψ <ref type="figure">(x)d(x) with d(x)</ref>. On the other hand, SfM self-supervision with the help of additional information can train models able to produce absolute scale in testing time. For instance, <ref type="bibr" target="#b17">[18]</ref> uses the ego-vehicle speed and, in fact, virtual-world supervision can help too <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. The latter approach is the one followed in this paper, especially thanks to the procedure presented in Sect. III-H. Therefore, Ψ(θ; x) will be evaluated in relative scale terms, and ψΨ(θ; x) in absolute terms. Please, note that our ψ ∈ R scaling factor is constant for all the evaluated images and computed at training time. In the following, when presenting quantitative results, we will make clear if they are in relative or absolute terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>We start by selecting the actual CNN layers to implement Ψ(θ; x). Since we leverage the SfM self-supervision idea from <ref type="bibr" target="#b18">[19]</ref>, a straightforward implementation would be to use its ResNet-based architecture as it is. However, the High-Resolution Network (HRNet) architecture <ref type="bibr" target="#b57">[58]</ref>, exhibits better accuracy in visual tasks such as semantic segmentation and object detection, suggesting that it can be a better backbone than ResNet. Thus, we decided to start our experiments by comparing ResNet and HRNet backbones using the SfM selfsupervision framework provided in <ref type="bibr" target="#b18">[19]</ref>. In particular, we assess different ResNet/HRNet architectures for θ enc , while using the proposal in <ref type="bibr" target="#b18">[19]</ref> for θ dec . Then, when using ResNet we have θ pyr = ∅, while for HRNet θ pyr consists of pyramidal layers adapting the θ enc and θ dec CNN architectures under test. For these experiments, we rely on KR. TableI shows the accuracy (in relative scale terms) of the tested variants and their number of weights. We see how HRNet outperforms ResNet, being HRNet-W48 the best. Thus, for our following experiments, we will rely on HRNet-W48 although being the heaviest. We show the corresponding pyramidal architecture of θ pyr in <ref type="figure" target="#fig_1">Fig. 2</ref>. It is composed of five blocks (P i ), where each block is a pipeline of three consecutive layers consisting of convolution, batch normalization, and ReLU. As a deep learning framework, we use PyTorch 1.5v <ref type="bibr" target="#b58">[59]</ref>.</p><p>In order to train the camera pose estimation network, T(ϑ sf ; x r t , x r t±1 ), we follow <ref type="bibr" target="#b18">[19]</ref> but using ResNet-50 instead of ResNet-18 since the former is more accurate. Four convolutional layers are used to convert the ResNet-50 bottleneck features to the 6-DoF relative pose vector (3D translation and rotation). For training the classification block of D(θ enc , ϑ DA ; x), i.e., ϑ DA , we use a standard classification pipeline based on convolutions, ReLU and fully connected layers. Finally, we remark that these networks are not required at testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training details</head><p>The input images are processed (at training and testing time) at a resolution of 640 × 192 (W × H), where LANCZOS  <ref type="bibr" target="#b18">[19]</ref>. MW column stands for millions of θ enc weights to be learnt. The 1.25 n columns, n ∈ {1, 2, 3}, refer to the τ in the usual δ &lt; τ accuracy metrics. Here and in all the tables of Sect. IV, bold stands for best, and underline for second-best. interpolation is performed from the ∼ 1242 × 375 original resolution. As optimizer, we use ADAM with learning rate set as lr = 10 −4 , and the rest of its hyper-parameters remain with default values. The weights θ enc are initialized from available ImageNet pre-training, θ pyr , θ dec , and ϑ DA are randomly initialized with Kaiming weights, while the ResNet-50 part of ϑ sf is also initialized with ImageNet and the rest (convolutional layers to output the pose vector) following Kaiming. The mini-batch size is of 16 images, 50%/50% from real/virtual domains. To minimize over-fitting, we apply standard data augmentation such as horizontal flip, a 50% chance of random brightness, contrast, saturation, and hue jitter with ranges of ±0.2, ±0.2, ±0.2, and ±0.1, respectively. Remaining hyperparameters were set as λ = 0.001 in Eq. <ref type="formula" target="#formula_3">(2)</ref>, ω DA = 10 in Alg. 1, and in Eq. (6) our mask s t is set to have values of 1.0 for traffic participants (vehicles, pedestrians, etc.), 0.5 for static infrastructure (buildings, road, vegetation, etc.), and 0.0 for the sky and pixels with depth over d max (here 80m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and discussion</head><p>1) Relative depth assessment: We start by assessing MDE in relative terms. <ref type="table" target="#tab_0">Table II</ref> presents MonoDEVSNet results (Ours) and those from previous works based on SfM selfsupervision. From this table we can draw several observations. Regarding DA, MonoDEVSNet (VK v1) outperforms S 3 Net (VK v1) in all metrics. The new version of VK (VK v2) allows us to obtain even better results. MonoDEVSNet with virtual-world supervision outperforms the version with only SfM self-supervision (best result in <ref type="table" target="#tab_0">Table I</ref>) in all metrics, no matter the VK version we use. Overall, MonoDEVSNet outperforms most previous methods, being on pair with <ref type="bibr" target="#b44">[45]</ref>.</p><p>2) Absolute depth assessment: While assessing depth in relative terms is a reasonable option to compare methods purely based on SfM self-supervision, the most relevant evaluation is in terms of absolute depth. These are presented in <ref type="table" target="#tab_0">Table III</ref>. The first (top) block of this table shows results based on depth supervision from LiDAR, thus, a priori they can be thought of as upper-bounds for methods based on self-supervision. The second block shows methods that only use virtual-world supervision. The third and fourth (bottom) blocks show results based on stereo and SfM self-supervision, respectively. Methods in gray use DA supported by VK. We can draw several observations from this table. MonoDEVS-Net (Ours) is the best performing among those leveraging supervision from VK v1 and, consistently with the results on relative depth, by using VK v2 we improve MonoDEVSNet results. In fact, MonoDEVSNet based on VK v2 outperforms all self-supervised methods, including those using stereo rigs instead of monocular systems. We are not yet able to reach the performance of the best methods supervised with LiDAR data. However, it is clear that our proposal is able to successfully combine real-world SfM self-supervision and virtual-world supervision. Thus, we think it is worth to keep this line of research until reaching the LiDAR-based upper-bounds.</p><p>3) Ablative analysis of MonoDEVSNet: It is also worth to analyze the contribution of the main components of our proposal. In rows 1-6 of Table IV, we add one component at a time showing performance for absolute depth. The 1st row corresponds to using the real-world data with SfM selfsupervision and the virtual-world images with only depth supervision, i.e., without using neither semantic supervision ( s t ), nor gradient equalization (ω sf ), nor domain adaptation (ϑ DA ), nor mixed mini-batches (50/50), nor the global scaling factor (ψ). By comparing 1st and 2nd rows (i.e., w/o ψ and w/ ψ, resp.), we can see how relevant is obtaining a good global scaling factor to output absolute depth. In fact, adding ψ to the virtual-world depth supervision shows the higher improvement among all the components of our proposal. Then, using mixed mini-batches of real-and virtual-world data improves the performance over alternating mini-batches of only either realor virtual-world data. This can be seen by comparing 2nd and 3rd rows (i.e., w/o 50/50 and w/ 50/50, resp.). If we alternate the domains, the optimization of a mini-batch is dominated TABLE III: Absolute depth results up to 80m on the (KR) Eigen et al. <ref type="bibr" target="#b0">[1]</ref> testing split. We divide the results into four blocks. From top to bottom, the blocks refer to: methods based on LiDAR supervision, only virtual-world supervision, stereo self-supervision, SfM self-supervision. In these blocks, we remark best and second-best results per block. Methods in gray use DA supported by VK. We remark some additional comments: ( 1 ) in addition to LiDAR supervision, it also uses stereo self-supervision; ( 2 ) it uses stereo and SfM selfsupervision; ( 3 ) in this case, the MDE network is pre-trained on Cityscapes dataset <ref type="bibr" target="#b59">[60]</ref> and then fine-tuned on KITTI. by self-supervision (real-world data), and the optimization of the next mini-batch is dominated by supervision (virtualworld data). Thus, there is not an actual joint optimization of SfM self-supervised and supervised losses, which turns to be relevant. Yet, as can be seen in 4th row, when we add the DA component (ϑ DA ) we improve further the depth estimation results. As can bee seen in 5th row, adding the equalization (ω sf ) between gradients coming from supervision and selfsupervision also improves the depth estimation results. Finally, adding the virtual-world mask ( s t ) leads to the best performance in 6th row. Overall, this analysis shows how all the considered components are relevant in our proposal. We also remark that these components are needed only to train θ, but only ψ and θ are required at testing time. Additionally, we have assessed the effect of simplifying the SfM self-supervised loss that we leverage from <ref type="bibr" target="#b18">[19]</ref>, here summarized in Sect. III-D. In particular, we neither use the auto-mask ( r t ()), nor the multiscale depth loss, and we replaced the minimum re-projection loss by the usual average re-projection loss (i.e., we re-define pe(x r −1 , x r 0 , x r +1 ) in Sect. III-D). Results are shown in the 7th row. The metrics show worse values than in 6th row (All), but still outperforming or being on pair with PackNet-SfM and the stereo self-supervised methods of <ref type="table" target="#tab_0">Table III</ref>.</p><p>In addition to this ablative analysis, we did additional experiments changing the DA mechanism. In particular, instead of taking direct real-and virtual-world images as input to train Ψ(θ; x), a GAN-based CNN, G, processes these images refers to mini-batches of 50% real-world samples and 50% or virtual-world ones; not using 50/50 (rows 1-2) means that we alternate mini-batches of pure real-or virtual-world samples. Row 7 corresponds to a simplification of the SfM self-supervised loss. ϑ G (rows 8-9) refers to a GAN-based DA approach. LB (lower bound, row 10) indicates the use of only virtual-world data. UB (upper bound, row 12) indicates the use of KITTI LiDAR-based supervision instead of virtualworld data. Rows 11 and 13 show the difference of our best model (All) with respect to LB and UB, respectively. ↑ D means that All is D units better, while ↓ D means that it is D units worse. All/W18 (row 14) and All/W32 (row 15) refer to using the All configuration by relying on HRNet-W18 and HRNet-W32, respectively. to create a common image space in which (hopefully) it is not possible to distinguish the domain. In short, we train an overall CNN Ψ(θ; G(ϑ G ; x)), where x can come from either the real or the virtual domain, and ϑ G are the weights of G. These weights are jointly trained with all the rest (θ, ϑ sf , ϑ DA ) to optimize depth estimation and minimize the possibility of discriminating the original domain of a sample x G = G(ϑ G ; x). <ref type="table" target="#tab_0">Table IV</ref> shows results using such a GAN when removing ϑ DA (8th row) and when keeping it (9th row). As we can see, this approach does not improve performance. Moreover, it turns out in a more complex training and G(ϑ G ; x) would be required at testing time. Thus, we discarded it. We also assessed the improvement of our proposal with respect a lower-bound model (LB) trained on virtual-world images and their depth GT (X s .Y s ), but neither using realworld data (X r ), nor DA (ϑ sf ), nor the mask ( s t ). Results are shown in 10th row of <ref type="table" target="#tab_0">Table IV</ref>, and we explicitly show the improvement of our proposal over such LB in 11th row. Likewise, we have trained an upper-bound model (UB) replacing VK data by KR data with LiDAR-based supervision, so that DA is not required. Results are shown in 12th row, and the distance of our model to this UB is explicitly shown in 13th row. Comparing 11th and 13th rows we can see how we are clearly closer to the UB than to the LB.</p><p>Finally, we have done experiments using HRNet-W18 and HRNet-W32. The results are shown in 14th and 15th rows of <ref type="table" target="#tab_0">Table IV</ref>, respectively. Indeed, as it happens with the results on relative depth <ref type="table" target="#tab_0">(Table I)</ref>, HRNet-W48 outperforms these more lightweight versions of HRNet. However, by using HRNet-W18 and HRNet-W32 we still outperform or are on pair with the state-of-the-art self-supervised methods shown in <ref type="table" target="#tab_0">Table III</ref>, i.e., those based on stereo self-supervision and PackNet-SfM. 4) Qualitative results: <ref type="figure" target="#fig_2">Figure 3</ref> presents qualitatively results relying on the depth color map commonly used in the MDE literature. We show results for representative methods in <ref type="table" target="#tab_0">Table III</ref>, namely, DORN (LiDAR supervision), SharinGAN (stereo self-supervision and virtual-world supervision), PackNet-SfM (SfM self-supervision and ego-vehicle speed supervision), and MonoDEVSNet (Ours) using VK v1 and VK v2 (SfM self-supervision and virtual-world supervision). We also show the corresponding LiDAR-based GT. This GT shows that for LiDAR configurations such as the one used to acquire KITTI dataset, detecting some close vehicles may be problematic since only a few LiDAR points capture their presence. Despite being trained on LiDAR supervision, DORN provides more accurate depth information in these corner cases than the raw LiDAR, which is an example of the relevance of MDE in general. However, DORN shows worse results in these corner cases than the rest (SharinGAN/PackNet-SfM/Ours), even being more accurate in terms of MDE metrics, which focus on global assessment. SharinGAN has more difficulties than PackNet-SfM and our proposal for providing sharp borders in vertical objects/infra-structure (e.g., vehicles, pedestrians, traffic signs, trees). An interesting point to highlight is also the qualitative difference that we observe on our results depending on the use of VK version. In VK v1 data, vehicle windows appear as transparent to depth, like in many cases happens with LiDAR data, while in VK v2 they appear as solid. This is translated to the MDE results as we can observe comparing the two bottom rows of <ref type="figure" target="#fig_2">Fig. 3</ref>. Technically, we think the qualitative results of VK v2 make more sense since the windows are there at the given depth. However, what we would like to highlight is that we can select one option or another thanks to the use of virtual-world data. 5) Additional insights: In terms of qualitative results we think the best performing and most similar approaches are PackNet-SfM and MonoDEVSNet, both relying only on realworld monocular systems. Thus, we perform a deeper comparison of them. First, following the analysis introduced in the PackNet-SfM article <ref type="bibr" target="#b17">[18]</ref>, <ref type="figure" target="#fig_3">Fig. 4</ref> plots the abs-rel metric of both methods as a function of depth. Results are similar up to 20m, then our proposal clearly outperforms PackNet-SfM up to 70m, where both methods start to perform similarly and in the last part of the range, up to 80m, PackNet-SfM outperforms our proposal. How these differences translate to the abs-rel global metric depends on the number of pixels falling in each distance range, which we show as an histogram in the same plot. We see how for the KR testing set most of the pixels fall in the 5−20m depth range, where both methods perform more similarly. Second, we provide further comparative insights by using KS data since it has associated per-class semantic GT, which we are going to use for evaluation purposes. Note that,   <ref type="bibr" target="#b0">[1]</ref>. The height of each bar indicates the percentage of pixels whose depth GT falls within the depth range covered by the bar. We indicate five values as reference. We compare PackNet-SfM and MonoDEVSNet (Ours). although KS is a different data split than the one used in the experiments shown so far (KR), still is KITTI data; thus, we are not yet facing experiments about generalization. <ref type="figure" target="#fig_4">Figure  5</ref> compares qualitative results of PackNet-SfM vs. MonoDE-VSNet. We can see how PackNet-SfM misses some vehicles that our proposal does not. We believe that these vehicles  may be moving at a similar speed w.r.t the ego-vehicle, which may be problematic for pure SfM-based approaches and we hypothesize that virtual-world supervision can help to avoid this problem. <ref type="figure" target="#fig_5">Figure 6</ref> shows the corresponding abs-rel metric per-class, focusing on the most relevant classes for driving. Note how the main differences between PackNet-SfM and MonoDEVSNet are observed on vehicles, especially on cars. Additional qualitative results are added in <ref type="figure" target="#fig_6">Fig. 7</ref>, where we can see how original images from KR and KS can be rendered as a textured point cloud. In particular, the viewpoint of these renders can change with respect to the original images thanks to the absolute depth values obtained with MonoDEVSNet. 6) Generalization results: As done in the previous literature using VK to support MDE <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>, we assess generalization on Make3D dataset. As in this literature, we follow the standard data conditioning (cropping and resizing) for models trained on KR, as well as the standard protocol introduced in [13] to compute MDE evaluation metrics (e.g. only depth below 70m is considered). <ref type="table" target="#tab_5">Table V</ref> presents the quantitative results usually reported for Make3D, and ours. Note how, in generalization terms, our method also outperforms the rest. Moreover, <ref type="figure">Fig. 8</ref> shows how our proposal captures the depth structure even better than the depth GT, which is build from 55 × 305 depth maps acquired by a 3D scanner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>For on-board perception, we have addressed monocular depth estimation by virtual-world supervision (MonoDEVS) and real-world SfM-inspired self-supervision; the former compensating for the inherent limitations of the latter. This challenging setting allows to rely on a monocular system not only at testing time, but also at training time; a cheap and scalable approach. We have designed a CNN, MonoDEVSNet, which seamlessly trains on real-and virtual-world data, exploiting semantic and depth supervision from the virtual-world data, and addressing the virtual-to-real domain gap by a relatively simple approach which does not add computational complexity in testing time. We have performed a comprehensive set of experiments assessing quantitative results in terms of relative and absolute depth, generalization, and we show the relevance of the components involved on MonoDEVSNet training. Our proposal yields state-of-the-art results within the SfM-based setting, even outperforming stereo-based self-supervised approaches. Qualitative results also confirm that MonoDEVSNet properly captures the depth structure of the images. As a result, we show the usefulness of leveraging virtual-world supervision to ultimately reach the upper-bound performance of methods based on LiDAR supervision. Therefore, our next steps will focus on analyzing the detailed differences between LiDARbased supervision methods and MonoDEVSNet to find better ways to benefit from virtual-world supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Training framework for MonoDEVSNet, i.e., Ψ(θ; x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>θFig. 2 :</head><label>2</label><figDesc>enc Backb. MW abs-rel sq-rel rms rms-log 1.25 1.25 2 1.25 3 ResNet-18 11.6 0.115 0.882 4.701 0.190 0.879 0.961 0.982 ResNet-50 25.5 0.110 0.831 4.642 0.187 0.883 0.962 0.982 ResNet-101 44.5 0.110 0.809 4.712 0.187 0.878 0.960 0.982 ResNet-152 60.2 0.107 0.800 4.629 0.184 0.885 0.962 0.982 HRNet-W18 9.5 0.107 0.846 4.671 0.184 0.887 0.962 0.982 HRNet-W32 29.3 0.107 0.881 4.794 0.187 0.886 0.961 0.981 HRNet-W48 65.3 0.105 0.791 4.590 0.182 0.888 0.963 0.982 Pyramidal architecture of θ pyr .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on the (KR) Eigen et al. testing slit<ref type="bibr" target="#b0">[1]</ref>. From top to bottom: input images, their LiDAR-based depth GT (interpolated for visualization using<ref type="bibr" target="#b60">[61]</ref>), DORN, SharinGAN, PackNet-SfM, ours VK v1, and ours VK v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Abs-rel error as a function of depth, computed on (KR) Eigen et al. testing split</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results on KS data. From left to right: input images, PackNet-SfM, MonoDEVSNet (Ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Per-class abs-rel results of PackNet-SfM and MonoDE-VSNet (Ours) on KS data. This per-class score is the average of the abs-rel metric only considering the pixels of each class at a time. We also indicate the percentage of pixels of each class over all the pixels from the images in KS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Point cloud representation on KR Eigen test split [1] and KS data from left to right. From top to bottom: input images, MonoDEVSNet textured point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparing ResNet and HRNet as backbone for our Ψ(θ; x) model, training only on SfM self-supervision (relative scale) using the framework in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Relative depth results up to 80m on the (KR) Eigen et al. [1] testing split. These methods rely on SfM selfsupervision. In addition, methods in gray use DA supported by VK. ( 1 ) MonoDepth2 is based only on SfM self-supervision. Method abs-rel sq-rel rms rms-log 1.25 1.25 2 1.25 3 [15] (Zhou et al.)</figDesc><table><row><cell></cell><cell>0.183 1.595 6.709 0.270 0.734 0.902 0.959</cell></row><row><cell>[16] GeoNet</cell><cell>0.149 1.060 5.567 0.226 0.796 0.935 0.975</cell></row><row><cell>[19] MonoDepth2 1</cell><cell>0.115 0.903 4.863 0.193 0.877 0.959 0.981</cell></row><row><cell>[17] (Zhao et al.)</cell><cell>0.113 0.704 4.581 0.184 0.871 0.961 0.984</cell></row><row><cell cols="2">[45] (Guizilini et al.) 0.102 0.698 4.381 0.178 0.896 0.964 0.984</cell></row><row><cell cols="2">[35] S 3 Net (VK v1) 0.124 0.826 4.981 0.200 0.846 0.955 0.982</cell></row><row><cell>Ours (VK v1)</cell><cell>0.105 0.753 4.389 0.179 0.890 0.965 0.983</cell></row><row><cell>Ours (VK v2)</cell><cell>0.102 0.685 4.303 0.178 0.894 0.966 0.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Absolute depth ablative results of MonoDEVSNet (VK v2) up to 80m on the (KR) Eigen et al. [1] testing split. Rows 1-6 show the progressive use of the components of our proposal (each row adds a new component). 50/50</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>↑All vs. ↓LB ↑0.061 ↑0.559 ↑1.232 ↑0.063 ↑0.103 ↑0.046 ↑0.018 12. UB 0.088 0.583 3.978 0.164 0.906 0.970 0.986 13. ↑All vs. ↓UB ↓0.016 ↓0.138 ↓0.418 ↓0.021 ↓0.026 ↓0.008 ↓0.003</figDesc><table><row><cell>Configuration</cell><cell>abs-rel sq-rel rms rms-log 1.25 1.25 2 1.25 3</cell></row><row><cell cols="2">1. {X r , X s .Y s } 0.368 2.601 8.025 0.514 0.080 0.478 0.883</cell></row><row><cell>2. +ψ</cell><cell>0.140 0.876 4.915 0.217 0.828 0.950 0.980</cell></row><row><cell>3. +50/50</cell><cell>0.128 0.880 4.618 0.198 0.844 0.957 0.982</cell></row><row><cell>4. +ϑ DA</cell><cell>0.110 0.724 4.450 0.187 0.873 0.960 0.983</cell></row><row><cell>5. +ω sf</cell><cell>0.106 0.716 4.441 0.188 0.876 0.962 0.982</cell></row><row><cell cols="2">6. + s t (All) 7. Simplified L sf 0.105 0.736 4.471 0.190 0.875 0.960 0.981 0.104 0.721 4.396 0.185 0.880 0.962 0.983</cell></row><row><cell cols="2">8. All+ϑ G ; −ϑ DA 0.119 0.809 4.654 0.196 0.857 0.958 0.982</cell></row><row><cell>9. All+ϑ G</cell><cell>0.106 0.748 4.503 0.191 0.873 0.959 0.981</cell></row><row><cell>10. LB</cell><cell>0.165 1.280 5.628 0.248 0.777 0.916 0.965</cell></row><row><cell>11. 14. All/W18</cell><cell>0.109 0.773 4.524 0.190 0.871 0.960 0.982</cell></row><row><cell>15. All/W32</cell><cell>0.107 0.754 4.510 0.188 0.875 0.960 0.982</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Absolute depth results on Make3D testing set. All the shown methods use Make3D only for testing (generalization), except ( 1 ) which fine-tunes on Make3D training set too. Qualitative results of MonoDEVSNet on Make3D. From left to right: input images, depth GT, MonoDEVSNet.</figDesc><table><row><cell>Method</cell><cell>abs-rel</cell><cell>sq-rel</cell><cell>rms</cell></row><row><cell>[31] T 2 Net (VK v1)</cell><cell>0.508</cell><cell>6.589</cell><cell>8.935</cell></row><row><cell cols="2">[32] AdaDepth-S 1 (VK v1) 0.452</cell><cell>5.71</cell><cell>9.559</cell></row><row><cell>[33] GASDA (VK v1)</cell><cell>0.403</cell><cell>6.709</cell><cell>10.424</cell></row><row><cell>[34] SharinGAN (VK v1)</cell><cell>0.377</cell><cell>4.900</cell><cell>8.388</cell></row><row><cell>Ours (VK v1)</cell><cell>0.381</cell><cell>3.997</cell><cell>7.949</cell></row><row><cell>Ours (VK v2)</cell><cell>0.377</cell><cell>3.782</cell><cell>8.011</cell></row><row><cell>Fig. 8:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular depth estimation by learning from heterogeneous datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Urfalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Halfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bouzaraa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4676" to="4689" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth estimation using monocular and stereo cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SuperDepth: Self-supervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambruş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards better generalization: Joint depth-pose learning without PoseNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raventos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wearable depth camera: Monocular depth estimation via sparse optimization under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Accesss</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="41" to="337" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust semisupervised monocular depth estimation with reprojected distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On deep learning techniques to boost monocular depth estimation for autonomous navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Grassi</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">103701</biblScope>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10773</idno>
	</analytic>
	<monogr>
		<title level="j">Virtual KITTI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: a large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AirSim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Field and Service Robotics (FSR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">T2Net: Synthetic-to-realistic translation for solving single-image depth estimation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AdaDepth: Unsupervised content congruent adaptation for depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Krishna</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometry-aware symmetric domain adaptation for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SharinGAN: Combining synthetic and real data for unsupervised geometry estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pnvr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">S3Net: Semantic-aware self-supervised depth estimation with monocular videos and synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Saggu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Comprehensive Survey on Domain Adaptation for Visual Applications, ser. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth estimation from single image using defocus and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikakulapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and depth estimation with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analyzing modular CNN architectures for joint depth prediction and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semanticallyguided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representation (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semanticaware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A survey of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Özyeşil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="364" />
			<date type="published" when="2017-05-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pedestrian detection combining RGB and dense LiDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
