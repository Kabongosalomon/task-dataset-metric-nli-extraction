<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-X Learning for Fine-Grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China Agricultural University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
							<email>yangxitongbob@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Mo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China Agricultural University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-X Learning for Fine-Grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at https: //github.com/cswluo/CrossX.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization (FGVC) aims at classifying objects from very similar categories, e.g. subcategories of birds <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10]</ref>, dogs <ref type="bibr" target="#b13">[14]</ref> and cars <ref type="bibr" target="#b15">[16]</ref>. It has long been considered as a challenging task due to the large intra-class and small inter-class variation, as well as the deficiency of annotated data. Benefiting from the progress of deep neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>, the recognition performance of FGVC has improved steadily in recent years and the community has more recently focused on weakly-supervised FGVC that obviates the need of laborintensive part-based annotation. There are two main approaches to weakly-supervised FGVC, namely, exploiting relationships between fine-grained labels to regularize feature learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref> and localizing discriminative parts for part-specific feature extraction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. Compared to labelrelationship based methods, the localization-based methods have the advantages of extracting fine-grained features from local regions where subtle differences between subcategories usually exist.</p><p>Early work on localization-based methods typically adopts a multi-stage learning framework: part detectors are first obtained by training on DCNN features <ref type="bibr" target="#b35">[36]</ref> or exploiting the hidden representations in DCNNs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, and then used to extract part-specific features for fine-grained classification. More recent work merges these two stages into an end-to-end learning framework that utilizes the final objective to optimize both part localization and fine-grained classification at the same time <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref>. These methods localize semantic parts independently on each image while neglecting the relationships between the part-specific features from different images. <ref type="bibr" target="#b27">[28]</ref> explores the relationships between object parts by proposing a soft attentionbased model. The model first generates attention region features of each input image via multiple excitation modules and then guides the attention features to have semantic meaning by adopting a metric learning framework. However, the improvement from their model is limited as optimizing such a metric learning loss is challenging and involves a non-trivial sample selection procedure <ref type="bibr" target="#b32">[33]</ref>.</p><p>We propose Cross-X learning, a simple but effective approach that leverages the relationships between different images and between different network layers for robust finegrained recognition. Similar to <ref type="bibr" target="#b27">[28]</ref>, our approach first generates attention region features via multiple excitation modules, but it further involves two novel components: a crosscategory cross-semantic regularizer (C 3 S) and a cross-layer regularizer (CL). C 3 S is introduced to guide the attention features from different excitation modules to represent different semantic parts. Ideally, the attention features for the same semantic parts, although coming from different images with different class labels, should be more correlated than those for different semantic parts (see <ref type="figure">Fig. 2</ref>). Therefore, C 3 S regulates the feature learning by maximizing the correlation between attention features extracted by the same excitation module while decorrelating those extracted by different excitation modules. Compared to the metric learning loss, C 3 S can be naturally integrated into the model and easily optimized without any sampling procedure. Meanwhile, we exploit the relationships between different network layers for robust multi-scale feature learning. We first adapt FPN <ref type="bibr" target="#b19">[20]</ref> to generate merged features. The merged features enable our model to discover local discriminative structures with both fine spatial resolution and rich highlevel semantic information. To further improve the robustness of the multi-scale features, we introduce a cross-layer regularizer (CL) that matches the prediction distribution of the mid-level features to that of the high-level features by minimizing their KL-divergence. Experimental results on five benchmark datasets show that our approach outperforms or achieves comparable performance to the state-ofthe-art methods. Moreover, our approach is easy to train and is scalable to large-scale datasets as it does not involve multi-stage or multi-crop mechanisms. We make the following contributions:</p><p>• We propose a Cross-X learning approach for finegrained feature learning. Cross-X learning explores relationships between features from different images and different network layers to learn semantic part features.</p><p>• We address the issue of robust multi-scale feature learning through cross-layer regularization, which matches prediction distributions across layers, thus increasing the robustness of features in different layers.</p><p>The remainder of the paper is organized as follows: Section 2 briefly reviews related work to our approach. Our approach is studied and detailed in Section 3. The model ablation studies and experimental results are analyzed and presented in Section 4. We conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fine-grained categorization: Benefiting from the development of DCNNs, e.g. AlexNet <ref type="bibr" target="#b16">[17]</ref>, VGGNet <ref type="bibr" target="#b26">[27]</ref>, InceptionNet <ref type="bibr" target="#b28">[29]</ref>, ResNet <ref type="bibr" target="#b8">[9]</ref>, the study of FGVC has been gradually shifted from strongly-supervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref> to weakly-supervised <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref> in recent years. In the weakly-supervised configuration, to induce models to learn features from the mostly discriminative regions, creating structural relationship between labels through either intermediate concepts <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref> or shared attributes <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22]</ref>, often accompanied by data augmentation <ref type="bibr" target="#b2">[3]</ref>, has been proposed. Multi-task learning is typically used to make the learning feasible <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>. Another line of research localizes semantic parts first and then learns feature from the localized parts in a multi-stage learning framework <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>. Recently, this line of research combines part localization and feature learning in an end-to-end framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>. Exploring relationships between objects in different images for part feature learning has also been investigated but with limited performance <ref type="bibr" target="#b27">[28]</ref>, due to the non-trivial sample selection involved in optimizing the loss function. Our approach is a step towards improving the efficiency and effectiveness of robustly exploring relationships between different images. We explore correlations between objects from different images in regularization learning and learn robust multi-scale features.</p><p>Multi-scale features: Exploiting multi-scale features improves the performance of many visual tasks. Among them, a number of methods make predictions by combining results inferred from multiple individual layers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>, several other approaches first combine multiple layer features and then make a prediction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>. These approaches marry low-level features' spatial resolution with high-level features' semantic properties. More recent studies have constructed high-resolution multi-scale semantic features by building feature pyramids in DCNNs through lateral connections of bottom-up and top-down feature maps <ref type="bibr" target="#b19">[20]</ref>. Nonlinear and progressive connecting structures are studied in <ref type="bibr" target="#b37">[38]</ref> to enhance the exploitation of multi-scale features. Multi-scale features have also studied using multigranularity labels <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref>. These approaches learn multiscale features by training networks with different granularity of labels. Our work also involves the utilization of multiscale features but exploits the interactions between features at different scales by matching prediction distributions of different layer feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Cross-X learning involves two main components: 1) A cross-category cross-semantic regularizer (C 3 S) that learns semantic part features by leveraging the correlations between different images (Sec. 3.2). 2) A cross-layer regularizer (CL) that learns robust features by matching prediction distributions between different layers (Sec. 3.3). An overview of our approach is depicted in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>We begin by briefly reviewing the one-squeeze multiexcitation (OSME) block <ref type="bibr" target="#b27">[28]</ref> that learns multiple attention region features for each input image. Let U = [u 1 , · · · , u C ] ∈ R W ×H×C denote the output feature map of  <ref type="figure">Figure 1</ref>. Overview of our approach. Our network outputs multiple feature maps by employing the OSME block. Two OSME blocks, each with two excitations, are depicted in the last two stages to illustrate our approach. Feature maps from stage L − 1 (blue) and L (red) are combined to generate the merged feature maps (orange). Top-left corner is a zoomed in display of the merging process of the merged feature maps. Feature maps are then aggregated to obtain the corresponding pooling features through GAP or GMP. The pooled features from the same stage are mutually constrained by the C 3 S regularizer and are simultaneously concatenated to feed into a fully-connected layer to generate logits. The logits are constrained through the CL regularizer after conversion into class probabilities and are combined for classification. Best viewed in color. a residual block τ . In order to generate multiple attentionspecific feature maps, the OSME block extends the original residual block by performing one-squeeze and multipleexcitation operations.</p><p>Formally, OSME first performs global average pooling to squeeze U and produce a channel-wise descriptor z = [z 1 , · · · , z C ] ∈ R C . Then a gating mechanism is independently employed on z for each excitation module, p = 1, · · · , P , to output:</p><formula xml:id="formula_0">m p = σ(W p 2 δ(W p 1 z)) = [m p 1 , · · · , m p C ] ∈ R C ,<label>(1)</label></formula><p>where σ and δ refer to the Sigmoid and ReLU functions. Finally, the attention-specific features U p are generated by re-weighting the channels of the original feature maps U:</p><formula xml:id="formula_1">U p = [m p 1 u 1 , · · · , m p 2 u C ] ∈ R W ×H×C .<label>(2)</label></formula><p>Although OSME can generate attention-specific features, guiding these features to have semantic meanings is challenging. <ref type="bibr" target="#b27">[28]</ref> tackles this by optimizing a metric learning loss which pulls features from the same excitation closer and pushes features from different excitations away. However, optimizing such a loss still poses a challenge and involves a non-trivial sample selection procedure <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Category Cross-Semantic Regularizer</head><p>Instead of optimizing a metric loss as in <ref type="bibr" target="#b27">[28]</ref>, we propose to learn semantic features by exploring the correlations between feature maps from different images and different excitation modules. Ideally, we want the extracted features from the same excitation module to have the same semantic meaning, even though they come from different images with different class labels. And the extracted features from different excitation modules should have different semantic meanings, even though they come from the same image (see <ref type="figure">Fig. 2</ref> for an illustration). To achieve this goal, we introduce the cross-category cross-semantic regularizer (C 3 S) that maximizes the correlation of features from the same excitation module while minimizes the correlation of features from different excitation modules.</p><p>Formally, we first perform global average pooling (GAP) on U p to obtain the corresponding pooled features f p ∈ R C , followed by 2 normalization (f p ← f p / f p ). Then the correlations between all pairs of excitation modules p and p form a matrix S:</p><formula xml:id="formula_2">S p,p = 1 N 2 F p T F p ,<label>(3)</label></formula><p>where T is the transpose operator, N is the batch size and F p = [f p,1 , · · · , f p,N ] ∈ R C×N is a matrix storing the pooled features from excitation module p for all samples in the batch. The C 3 S regularization loss is then constructed from two parts: 1) maximizing the diagonal of S to maximize the correlation within the same excitation module and, 2) penalizing the norm of S to minimize the correlation between different excitation modules:</p><formula xml:id="formula_3">L C 3 S (S) = 1 2 S 2 F − 2 diag(S) 2 2 ,<label>(4)</label></formula><p>where · is the Frobenius norm, and the diag(·) operator extracts the main diagonal of a matrix into a vector. Compared to the triplet based metric learning loss, C 3 S loss can be naturally integrated into the OSME block and is easily optimized without any sampling procedure. <ref type="figure">Figure 2</ref>. An illustration of the C 3 S learning. Take the center image as an example, C 3 S encourages the excitation modules, U1 and U2, to be activated on different semantic parts by exploiting relationships between features from different images (orange dash box) and features from different excitation modules (blue shaded box). Best viewed in color.</p><formula xml:id="formula_4">… ... … ... … ... 1 N 2 … ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Layer Regularizer</head><p>Exploiting semantic features from different layers of CNNs has been shown to be beneficial to many vision tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2]</ref>. A simple extension of this idea to fine-grained recognition is to combine the prediction outputs of different layers for the final prediction. However, we observe in our experiments that this simple strategy usually leads to inferior performance (see Sec. 4.3). We hypothesize that the problem is due to two reasons: 1) midlevel features are more sensitive to input changes <ref type="bibr" target="#b4">[5]</ref> which makes them less robust for fine-grained recognition where the intra-class variation is large, 2) relationships between the predictions of features are not exploited. To alleviate these problems, we adapt the feature pyramid network (FPN) <ref type="bibr" target="#b19">[20]</ref> to integrate features from different layers and propose a novel cross-layer regularizer (CL) that learns robust features by matching the prediction distribution between different layers.</p><p>Formally, let</p><formula xml:id="formula_5">U L = {U L p } P p=1 , U L−1 = {U L−1 p } P p=1</formula><p>be the feature maps at stage L and L − 1 (here a stage refers to a group of layers that produce feature maps with the same size <ref type="bibr" target="#b8">[9]</ref>). We generate the merged feature maps U G p in a similar way to FPN <ref type="bibr" target="#b19">[20]</ref> but with two differences. First, the dimensionality reduction of U L p is performed before upsampling. Second, batch normalization (BN) <ref type="bibr" target="#b12">[13]</ref> is used after the anti-aliasing operation on the merged feature maps. The procedure can be summarized as:</p><formula xml:id="formula_6">U G p = BN K 2 * U L−1 p + Bilinear(K 1 * U L p ) ,<label>(5)</label></formula><p>where * is convolutional operation, Bilinear(·) denotes bilinear interpolation, K 1 , K 2 are 1 × 1 and 3 × 3 filters, re-spectively. U G integrates the property of fine spatial resolution in the mid-level layers and the rich high-level semantic in the top-level layers.</p><p>To further exploit the relationships between the predictions of features, we propose the CL regularizer that matches the prediction distribution between different layers. Let Pr L = σ(f (U L )) and Pr L−1 = σ(f (U L−1 )) be the prediction outputs of stage L and L − 1, where σ(·) is the softmax function and f (·) denotes the output layer. The CL regularizer encourages Pr L−1 to match Pr L by minimizing their KL-divergence:</p><formula xml:id="formula_7">L CL (Pr L , Pr L−1 ) = KL(Pr L || Pr L−1 ) = 1 N N n=1 K k=1 p L nk log p L nk p L−1 nk ,<label>(6)</label></formula><p>where K is the number of classes. A similar regularizer can be added to constrain the feature maps U L and U G as well.</p><p>The CL regularizer can be viewed as knowledge distillation <ref type="bibr" target="#b6">[7]</ref> that uses "soft targets" from U L with rich structure information to guide the feature learning of U L−1 and U G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>Given the feature maps U L , U L−1 and U G , our final prediction can be obtained by combining their prediction outputs:</p><formula xml:id="formula_8">Pr = σ f (U L ) + f (U L−1 ) + f (U G ) .<label>(7)</label></formula><p>Putting this all together, the full objective function of Cross-X learning is:</p><formula xml:id="formula_9">L = L data + γL C 3 S + λL CL ,<label>(8)</label></formula><formula xml:id="formula_10">L data = − 1 N N n=1 K k=1 c nk log p nk ,<label>(9)</label></formula><formula xml:id="formula_11">L C 3 S = γ 1 L C 3 S (S L ) + γ 2 L C 3 S (S L−1 ) + γ 3 L C 3 S (S G ),<label>(10)</label></formula><formula xml:id="formula_12">L CL = λ 1 L CL (Pr L , Pr L−1 ) + λ 2 L CL (Pr L , Pr G ),<label>(11)</label></formula><p>where L data is the classification loss, γ and λ are hyperparameters that balance the contribution of different costs. Our model can be trained end-to-end using stochastic gradient descent (SGD) and does not require other optimization tricks such as multiple crops <ref type="bibr" target="#b36">[37]</ref>, data augmentation <ref type="bibr" target="#b2">[3]</ref>, model ensemble <ref type="bibr" target="#b41">[42]</ref>, and separate initialization <ref type="bibr" target="#b31">[32]</ref>  <ref type="table">Table 1</ref>. The statistics of fine-grained datasets in this paper Caltech-UCSD Birds (CUB-Birds) <ref type="bibr" target="#b29">[30]</ref>, Stanford Cars <ref type="bibr" target="#b15">[16]</ref>, Stanford Dogs <ref type="bibr" target="#b13">[14]</ref> and FGVC-Aircraft <ref type="bibr" target="#b24">[25]</ref>. Note that NABirds is a recently released dataset with much larger scale and many more fine-grained categories. The detailed statistics such as category numbers and data splits are summarized in Tab. 1. We report top-1 accuracy in this study.</p><p>Baselines: We compare our approach with various stateof-the-art methods using weakly-supervised learning for fine-grained recognition. For fair comparison, we mainly compare to the results with ResNet-50 as their backbone network and include the best results of VGG based methods for completeness in the following, unless otherwise stated. In addition, an ablation study of Cross-X learning is analyzed based on the SENet backbone <ref type="bibr" target="#b10">[11]</ref>, since OSME is a direct extension of the SE block. Moreover, we also report results of our approach on the ResNet-50 backbone <ref type="bibr" target="#b8">[9]</ref>. All the baselines are listed as follows:</p><p>• FCAN [23]: fully convolutional attention network that adaptively selects multiple task-driven visual attentions by reinforcement learning.</p><p>• RA-CNN <ref type="bibr" target="#b5">[6]</ref>: recurrent attention convolutional neural network that localizes discriminative areas and extracts features from coarse to fine scale.</p><p>• DT-RAM <ref type="bibr" target="#b17">[18]</ref>: recurrent visual attention model that selects a sequence of regions through a dynamic continue/stop gating mechanism.</p><p>• MA-CNN <ref type="bibr" target="#b41">[42]</ref>: multi-attention convolutional neural network that generates multiple parts from spatiallycorrelated channels via multi-task learning.</p><p>• DFB-CNN <ref type="bibr" target="#b31">[32]</ref>: discriminative filter bank approach that learns a bank of convolutional filters that capture class-specific discriminative patches.</p><p>• NTS-Net <ref type="bibr" target="#b36">[37]</ref>: navigator-teacher-scrutinizer network finds consistent informative regions through multiagent cooperation.</p><p>• MAMC-CNN <ref type="bibr" target="#b27">[28]</ref>: multi-attention multi-class constraint approach that learns soft attention masks by regularizing features from different images.</p><p>• MaxEnt-CNN <ref type="bibr" target="#b3">[4]</ref>: maximum entropy approach provides a training routine to maximize the entropy of the output probability distributions for FGVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We develop our model in PyTorch, on top of the implementation of SENet/ResNet-50. Specifically, we place the OSME block after conv5_3 and conv4_6 in SENet/ResNet-50. The size of the output feature maps of the two blocks are 14 × 14 × 2048 and 28 × 28 × 1024, respectively. Therefore, the channel sizes of U L , U L−1 and U G are 4096, 2048 and 2048 when P = 2. We initialize most of our network using the weights pretrained on Im-ageNet and initialize the newly introduced layers (OSME blocks, FPN blocks) from scratch. No part or bounding box annotations are used during training.</p><p>Our network is trained using SGD on a single NVIDIA P6000 GPU with momentum 0.9 and a mini-batch size of 32. The initial learning rate is set to be 0.01 except for the experiments on Stanford Dogs where 0.001 is used. We train the network for 30 epochs and decay the learning rate by 0.1 every 15 epochs. For datasets that do not provide a validation set, we randomly take 10% out of the training samples from each category for validation. Input images are cropped to 448 × 448 and flipped horizontally with a probability of 0.5. We report our results on a single scale of 448 × 448 from a single model. More details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Effectiveness of C 3 S and CL: The effectiveness of our regularization is studied in <ref type="figure" target="#fig_1">Fig. 3</ref>. We find the performance of our base network (OSME, putting the OSME block after conv5_3 in SENet-50.) is lower than that of the SENet-50 on almost all datasets (SE vs. OSME), this indicates the training difficulties when employing the OSME block for multiple outputs. As we expected, C 3 S can effectively regularize the learning of our network to force excitations in the OSME block to be activated on different semantic parts, thus resulting in better features for classification (C3S vs. OSME). In addition, we find combining mid-level (stage L − 1) and high-level (stage L) features without a constraint between them results in a performance drop (C3S vs. C3S+GMP). However, CL can effectively increase the robustness of the mid-level feature and thus boosting the performance (C3S+GMP vs. C3S+GMP+CL).</p><p>Benefits from the merged feature maps: Employing the merged feature maps can bring systematic performance improvement on all datasets, whether CL is used or not (C3S+GxP vs. C3S+GxP+FP and C3S+GxP+CL vs. C3S+GxP+FP+CL in <ref type="figure" target="#fig_1">Fig. 3-4</ref>). This validates the effectiveness of our proposal that extra semantic features can be introduced to improve the FGVC performance and, the correctness of our operation that generates the merged feature maps. An interesting observation is that the performance of C3S+GxP+FP is systematically lower than that of C3S+GxP+FP+CL in <ref type="figure" target="#fig_1">Fig. 3-4</ref>; this signifies that increasing   <ref type="table">Table 2</ref>. Performance of our approach on five benchmark datasets with GAP and GMP alternatively employed on U L−1 p . The top group compares results from the approach with CL but without merged feature maps. The bottom group shows results from the approach with merged feature maps but without CL.</p><p>the robustness of the newly introduced merged feature maps is also necessary and it further demonstrates that CL has the capability to improve the robustness of mid-level features.</p><p>GMP vs. GAP: As indicated in <ref type="figure">Fig. 1</ref>, GAP and global max pooling (GMP) can be alternatively adopted to pool feature maps. However, we only switch the pooling method from GAP to GMP in U L−1 , since we initially thought the discriminative structure of FGVC is local and subtle, thus GMP should have advantage over GAP to capture the these structures, and provide a better feature representation. This is verified on almost all datasets (the top group of Tab. 2).</p><p>The results indicate CL can collaborate well with GMP to provide robust mid-level features. However, when the network is enhanced by the merged feature maps, which use GAP, but without CL, the results show different behaviour (the bottom group of Tab. 2). GAP+, where GAP is employed on U L−1 , achieves the best performance on Cars, Dogs, and Aircraft but fails to surpass the performance of GMP−, where GMP is employed on U L−1 , on Birds. This phenomenon indicates that GMP is necessary for ascertaining local and subtle structures in categories with fine-andrich texture. The difference caused by employing GMP or GAP on U L−1 can also be observed in <ref type="figure" target="#fig_2">Fig. 5 (b)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art</head><p>Results on NABirds: Most previous methods do not report results on this dataset because of the computational complexity of the multi-crop, multi-scale, and multi-stage optimization. Due to the simplicity of our approach, it scales well to big datasets. Tab. 3 compares results from methods that are all optimized on single-crop inputs. Our re-implementation of SENet/ResNet-50 is better than the more sophisticated posed-normalized PN-CNN <ref type="bibr" target="#b0">[1]</ref> and the maximum entropy regularized MaxEnt-ResNet-50. The MaxEnt-CNN improves its performance to 83.0% by employing the DenseNet-161 architecture <ref type="bibr" target="#b11">[12]</ref>. This shows the benefits brought by more advanced network architectures. However, our Cross-X learning can further outperform it by 3.2% with a relatively simple ResNet-50 backbone, which signifies the effectiveness of our approach.</p><p>Results on CUB-Birds: The classification results for CUB birds are presented in Tab. 4. Compared to previous methods, our approach achieves the state-of-the-art performance in a much easier experimental setting, in which only one feedforward operation on a single scale input is needed, without any specialized initialization. Notice that DFB- VGGNet-driven results in Tab. 5. Compared to MAMC-CNN <ref type="bibr" target="#b27">[28]</ref>, which learns multiple feature maps by embedding the OSME block in a metric learning framework, our Cross-X learning outperforms it by 1.6%. The improvement indicates the effectiveness of our proposal to learn semantic part features by exploring the correlations between excitation modules and to extract robust features by bridging the relationship between features in different layers. Results on Stanford Dogs: Classification results are presented in Tab. <ref type="bibr" target="#b5">6</ref>. Surprisingly, the performance of our reimplementation of SENet/ResNet-50 surpasses many previous methods. Even though they can improve their performance by employing more advanced architectures, e.g., MAMC-CNN <ref type="bibr" target="#b27">[28]</ref> with ResNet-101 (85.2%) and MaxEnt-CNN <ref type="bibr" target="#b3">[4]</ref> with DenseNet-161 (83.6%) as reported in their papers, still falling behind us. However, our Cross-X learning can beat ResNet-50 a bit and achieve the state-of-theart performance by combining with SENet-50 and ResNet-50, respectively. FCAN <ref type="bibr" target="#b22">[23]</ref> also achieves the best performance, but it is more complicated than our approach and needs multi-scale multi-crops for model training and testing. In contrast, Cross-X learning is simple and effective.</p><p>Results on FGVC-Aircraft: Tab. 7 reports the average class-prediction accuracy. Our approach obtains the best result among methods reporting results on this dataset, even compared to those based on more advanced network architectures. As the main difference of the categories in this dataset results from the changes of aircraft structures, this result implies that our Cross-X learning is applicable to classification problems with relatively large inter-class structural variation. Notice that the performance of Kernel-Pooling <ref type="bibr" target="#b2">[3]</ref>, MaxEnt <ref type="bibr" target="#b3">[4]</ref> and DFB-CNN <ref type="bibr" target="#b31">[32]</ref> methods drop to 83.9%, 85.7% and 91.7% respectively when supported by ResNet-50 instead of VGGNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>Fig <ref type="figure" target="#fig_2">. 5</ref> displays the resized activation maps <ref type="bibr" target="#b42">[43]</ref> of 6 images from 3 datasets (see the supplementary material for more displays). Activation maps from the same layer complement each other -they concentrate on different regions of the objects. In addition, we find the activations in corresponding columns of (b)∼(d) cover the same object parts at different scales. Compared to the activation maps (c) U L , the highly-activated area in (b) U L−1 and (d) U G have respectively a relatively small scale and a highlighted center. The activation maps of U G can further be seen as the enhanced activation maps of U L from that of U L−1 , e.g. head of birds, wings of planes. This is consistent with the design of the fine spatial-resolution and rich high-level semantic feature in FPN <ref type="bibr" target="#b19">[20]</ref>. The difference caused by employing GMP or GAP on U L−1 can also be observed in (b) where GMP leads to consistent activation in a single region (the first two rows) while GAP results in scattered activations in multiple regions (the last 4 rows). We further present the combined activation maps in (e) to demonstrate the refined final maps taken as input in our approach for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed Cross-X learning to learn robust finegrained feature by exploiting relationships between features from different images and different network layers. Our approach leverages the fact that features for the same semantic parts, although coming from different images with different class labels, should be more correlated than those for different semantic parts. Experiments evaluated on five benchmark datasets, ranging from 100 to 555 categories, validate the effectiveness of our approach. Ablation studies further demonstrate the role of every component of Cross-X.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Ablation performance on 5 benchmark datasets with GMP employed on U L−1 p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Superimposed display of activation maps (b) U L−1 p , (c) U L p and (d) U G p from CUB-Birds, Stanford Cars, and FGVC-Aircraft. The first column (a) shows original images and the last two columns (e) are combined activation maps from corresponding columns of U L−1 p , U L p and U G p . Each of (b)∼(e) shows activations of two excitation modules in the corresponding layers. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The legend only shows the added block/regularizer names with the default ResNet-50 backbone omitted, e.g. SE means SENet-50. Best viewed in color.</figDesc><table><row><cell></cell><cell cols="5">NABirds CUB-Birds Cars Dogs Aircraft</cell></row><row><cell>GMP-</cell><cell>81.7</cell><cell>85.2</cell><cell>93.0</cell><cell>83.0</cell><cell>91.1</cell></row><row><cell>GAP-</cell><cell>76.3</cell><cell>84.7</cell><cell>90.4</cell><cell>87.3</cell><cell>89.4</cell></row><row><cell>GMP+</cell><cell>80.9</cell><cell>84.2</cell><cell>91.9</cell><cell>86.7</cell><cell>90.7</cell></row><row><cell>GAP+</cell><cell>81.7</cell><cell>84.7</cell><cell cols="2">93.8 87.3</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>(seeSec. 4.5). Therefore, we report the final results on Cars, Dogs, and Aircraft with GAP employed on U L−1 while on Birds with GMP employed on U L−1 in Sec. 4.4</figDesc><table><row><cell></cell><cell>95 100</cell><cell>SE OSME</cell><cell cols="2">C3S C3S+GAP</cell><cell>C3S+GAP+CL C3S+GAP+FP</cell><cell>C3S+GAP+FP+CL</cell></row><row><cell>Accuracy (%)</cell><cell>80 85 90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell cols="3">NABirds CUB-Birds</cell><cell>Cars</cell><cell>Dogs</cell><cell>Aircrafts</cell></row><row><cell cols="6">Figure 4. Ablation performance on 5 benchmark datasets with</cell></row><row><cell cols="4">GAP employed on U L−1 p</cell><cell cols="2">. C3S, CL and FP represent C 3 S, CL</cell></row><row><cell cols="6">and merged feature maps, respectively. Best viewed in color.</cell></row><row><cell></cell><cell></cell><cell>Approach AlexNet-fc6 [10]</cell><cell></cell><cell cols="2">1-Stage Sep. Init. Accuracy √ × 35.0</cell></row><row><cell></cell><cell></cell><cell>PN-CNN [10]</cell><cell></cell><cell>×</cell><cell>×</cell><cell>74.0</cell></row><row><cell></cell><cell cols="2">MaxEnt-CNN [4] (ResNet-50) SENet-50 [11] ResNet-50 [9]</cell><cell></cell><cell>√ √ √</cell><cell>× × ×</cell><cell>69.2 82.1 82.2</cell></row><row><cell></cell><cell cols="3">MaxEnt-CNN [4] (DenseNet-161) Cross-X (SENet) Cross-X (ResNet)</cell><cell>√ √ √</cell><cell>× × ×</cell><cell>83.0 86.4 86.2</cell></row></table><note>. Performance on NABirds. The result of PN-CNN is im- plemented with part annotations based on AlexNet. 1-Stage means the network is trained end-to-end after initialization. Sep. Init. de- notes separate initialization.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 .Table 7</head><label>67</label><figDesc>Results on Stanford Cars: Tab. 5 shows the results on Stanford Cars. Our Cross-X learning also achieves the state-of-the-art performance on this dataset, even though DBF-CNN<ref type="bibr" target="#b31">[32]</ref> and NTS-Net<ref type="bibr" target="#b36">[37]</ref> employ separate layer initialization and multi-scale crops, respectively. Kernel-Pooling attains a better result when coupled with VGGNet compared to that of ResNet-50 (91.9%), thus we report the Performance on Stanford Dogs. The first group uses multi-crop operations while the others are not.</figDesc><table><row><cell cols="4">Approach FCAN [23] RA-CNN [6] DT-RAM [18] MA-CNN [42] NTS-Net [37] MaxEnt-CNN [4] SENet-50 [11] ResNet-50 [9] Kernel-Pooling [3] MAMC-CNN [28] DFB-CNN [32] Cross-X (SENet) Cross-X (ResNet) Table 4. Performance on CUB-Birds. RA-CNN and MA-CNN are 1-Stage Sep. Init. Accuracy √ × 84.3 × √ 85.3 × × 86.0 √ √ 86.5 √ × 87.5 √ × 80.4 √ × 83.0 √ × 84.5 √ √ 84.7 √ × 86.2 √ √ 87.4 √ × 87.5 √ × 87.7 based on VGGNet. Multi-crop operations are employed in the first group while not in others. Approach 1-Stage Sep. Init. Accuracy RA-CNN [6] × √ 92.5 MA-CNN [42] √ √ 92.8 FCAN [23] √ × 93.1</cell><cell>Approach RA-CNN [6] FCAN [23] MaxEnt-CNN [4] MAMC-CNN [28] SENet-50 [11] ResNet-50 [9] Cross-X (SENet) Cross-X (ResNet) Approach MA-CNN [42] NTS-Net [37] Kernel-Pooling [3] MaxEnt-CNN [4] ResNet-50 [9] SENet-50 [11] DFB-CNN [32] Cross-X (SENet) Cross-X (ResNet)</cell><cell>1-Stage Sep. Init. Accuracy × √ 87.3 √ × 88.9 √ × 73.6 √ × 84.8 √ × 87.1 √ × 88.1 √ × 88.2 √ × 88.9 1-Stage Sep. Init. Accuracy √ √ 89.9 √ × 91.4 √ √ 86.9 √ × 89.8 √ × 90.3 √ × 90.6 √ √ 92.0 √ × 92.7 √ × 92.6</cell></row><row><cell>DT-RAM [18] NTS-Net [37] SENet-50 [11] Kernel-Pooling [3] ResNet-50 [9] MAMC-CNN [28] DFB-CNN [32] MaxEnt-CNN [4] Cross-X (SENet) Cross-X (ResNet)</cell><cell>× √ √ √ √ √ √ √ √ √</cell><cell>× × × √ × × √ × × ×</cell><cell>93.1 93.9 91.6 92.4 92.9 93.0 93.8 93.9 94.5 94.6</cell></row><row><cell cols="4">Table 5. Performance on Stanford Cars. Kernel-Pooling, RA-</cell></row><row><cell cols="4">CNN, and MA-CNN are based on VGGNet. Multi-crop training</cell></row><row><cell cols="3">and testing are employed in the first group.</cell><cell></cell></row><row><cell cols="4">CNN [32] needs a separate layer initialization to prevent</cell></row><row><cell cols="4">the model learning from degeneration and NTS-Net [37]</cell></row><row><cell cols="4">conducts feature combinations from multiple crops. MA-</cell></row><row><cell cols="4">CNN [42] obtains comparable results based on VGGNet</cell></row><row><cell cols="4">with part localization pretraining and multi-crop inputs.</cell></row><row><cell cols="4">MaxEnt-CNN [4] can achieve 86.5% when implemented</cell></row><row><cell cols="4">with DenseNet-161, MAMC-CNN [28] improves to 86.5%</cell></row><row><cell cols="4">when using ResNet-101 and Kernel-Pooling reaches 86.2%</cell></row><row><cell cols="4">when combined with VGGNet as reported in their work;</cell></row><row><cell cols="2">however, still clearly lower than ours.</cell><cell></cell><cell></cell></row></table><note>. Performance on FGVC-Aircraft. The first group uses multi-crop operations. Kernel-Pooling and DFB-CNN are based on VGGNet. MaxEnt-CNN is implemented with DenseNet-161.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Experiments 4.1. Datasets and Baselines Datasets: We conduct experiments on five fine-grained visual categorization datasets, including NABirds[10], Datasets #category #training #testing NABirds[10] 555 23,929 24,633 CUB-Birds [30] 200 5, 994 5, 794 Stanford Cars [16] 196 8,144 8,041 Stanford Dogs [14] 120 12,000 8,580 FGVC-Aircraft [25] 100 6,667 3,333</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-X learning involves 6 hyper-parameters -P , γ 1 , γ 2 , γ 3 , λ 1 , λ 2 . Among them, P is the number of excitations employed in OSME; γ 1 , γ 2 and γ 3 are used to balance the effects of C 3 S for different layers (see Eq. (10)); λ 1 and λ 2 are adopted to adjust the effects of CL (see Eq. <ref type="formula">(11)</ref>). These hyper-parameters are determined by evaluating models on hold-out validation datasets. The hyper-parameters for various datasets are presented in Tab. 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training details</head><p>All experiments in ablation studies are implemented on the SENet backbone (Sec. 4.3). On all datasets, images are resized to 448 × 448 for training and testing. OSMEs with 2 excitations are used in all experiments on all datasets except that on Stanford Dogs where 3 excitations are employed.</p><p>To present the state-of-the-art performance (Sec. 4.4), images on CUB-Birds, NABirds, and VGG-Aircraft are first resized to 600 × 600, and then image patches of size 448 × 448 from random cropping and center cropping are used for training and testing, respectively. We did not observe any advantage of this trick on Stanford Cars and Stanford Dogs, thus default operations as that implemented in the ablation study are employed on these two datasets. The re-implementation of SENet-50 and ResNet-50 in Sec. 4.4 also obeys these operation rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization</head><p>We display additional activation maps in this section for images from birds <ref type="figure">(Fig. 6)</ref>, cars ( <ref type="figure">Fig. 7)</ref>, aircraft ( <ref type="figure">Fig. 8</ref>) and dogs <ref type="figure">(Fig. 9</ref>). The images shown here are consistent with the analysis presented in Section 4.5 of the paper.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ramesh Raskar, and Nikhil Naik. Maximum entropy fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding deep architectures using a recursive convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinton</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessie</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization (FGVC) at CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th IEEE Workshop on 3D Representation and Recognition at ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for finegrained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Localizing by describing: Attribute-guided attention localization for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fully convolutional attention localization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding label structures for fine-grained feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fine-grained image classification by exploring bipartite-graph labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
