<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidden Two-Stream Convolutional Networks for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Merced</orgName>
								<address>
									<postCode>95343</postCode>
									<settlement>Merced</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Merced</orgName>
								<address>
									<postCode>95343</postCode>
									<settlement>Merced</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidden Two-Stream Convolutional Networks for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action recognition · Optical flow · Unsupervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not endto-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of human action recognition has advanced rapidly over the past few years. We have moved from manually designed features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> to learned convolutional neural network (CNN) features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>; from encoding appearance information to encoding motion information <ref type="bibr" target="#b18">[19]</ref>; and from learning local features to learning global video features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. The performance has continued to soar higher as we incorporate more of the steps into an end-to-end learning framework. Nevertheless, current state-of-the-art CNN structures still have difficulty in capturing motion information directly from video frames. Instead, traditional local optical flow estimation methods are used to pre-compute motion information for the CNNs <ref type="bibr" target="#b18">[19]</ref>. This two-stage pipeline, first compute optical flow and then learn the mapping from optical flow to action labels, is sub-optimal for the following reasons:</p><p>-The pre-computation of optical flow is time consuming and storage demanding compared to the CNN step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion MotionNet</head><p>Temporal Stream CNN Spatial Stream CNN <ref type="figure">Fig. 1</ref>: Illustration of proposed hidden two-stream networks. MotionNet takes consecutive video frames as input and estimates motion. Then the temporal stream CNN learns to project the motion information to action labels. Late fusion is performed through the weighted averaging of the prediction scores of the temporal and spatial streams. Both streams are end-to-end trainable.</p><p>-Traditional optical flow estimation is completely independent of the final tasks like action recognition and is therefore potentially sub-optimal.</p><p>To solve the above problems, researchers have proposed various methods other than optical flow to capture motion information in videos. For example, new representations like motion vectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> and RGB image difference <ref type="bibr" target="#b24">[25]</ref> or architectures like recurrent neural networks (RNN) <ref type="bibr" target="#b15">[16]</ref> and 3D CNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. However, most of these are not as effective as optical flow for human action recognition <ref type="bibr" target="#b2">3</ref> . Therefore, in this paper, we aim to address the above mentioned problems in a more direct way. We adopt the end-to-end CNN approach to learn optical flow so that we can avoid costly computation and storage and obtain task-specific motion representations. However, we face many challenges to learn such a motion estimation model:</p><p>-We need to train the models without supervision. The ground truth flow required for supervised training is usually not available except for limited synthetic data <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. -We need to train our optical flow estimation models from scratch. The models (filters) learned for optical flow estimation tasks are very different from models (filters) learned for other vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. -We cannot simply use the traditional optical flow estimation loss functions.</p><p>We are concerned chiefly with how to learn an optimal motion representation for video action recognition.</p><p>To address these challenges, we first train a CNN with the goal of generating optical flow from a set of consecutive frames. Through a set of specially designed operators and unsupervised loss functions, our new training step can generate optical flow that is similar to that generated by one of the best traditional methods <ref type="bibr" target="#b31">[32]</ref>. As illustrated in the bottom of <ref type="figure">Figure 1</ref>, we call this network MotionNet. Given the MotionNet, we concatenate it with a temporal stream CNN that maps the estimated optical flow to the target action labels. We then fine-tune this stacked temporal stream CNN in an end-to-end manner with the goal of predicting action classes for the input frames. We call our new approach hidden two-stream networks as it implicitly generates motion information for action recognition. Our contributions include:</p><p>-Our method is both computationally and storage efficient. It is around 10x faster than its two-stage baseline, and we do not need to store the precomputed optical flow. -Our method outperforms previous real-time approaches on four challenging action recognition datasets by a large margin. -The proposed MotionNet is flexible in that it can be directly concatenated with other video action recognition frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> to improve their efficiency. -We demonstrate the generalizability of our end-to-end learned optical flow by showing promising results on four optical flow benchmarks without finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Significant advances in understanding human activities in video have been made over the past few years. Initially, traditional handcrafted features such as Improved Dense Trajectories (IDT) <ref type="bibr" target="#b22">[23]</ref> dominated the field of video analysis for several years. Despite their superior performance, IDT and its improvements are computationally formidable for real applications. CNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, which are often several orders of magnitude faster than IDTs, performed much worse than IDTs in the beginning. This inferior performance is mostly because CNNs have difficulty in capturing motion information among frames. Later on, two-stream CNNs <ref type="bibr" target="#b18">[19]</ref> addressed this problem by pre-computing the optical flow using traditional optical flow estimation methods <ref type="bibr" target="#b31">[32]</ref> and training a separate CNN to encode the pre-computed optical flow. This additional stream (a.k.a., the temporal stream) significantly improved the accuracy of CNNs and finally allowed them to outperform IDTs on several benchmarks. These accuracy improvements indicate the importance of temporal motion information for action recognition as well as the inability of existing CNNs to capture such information. However, compared to the CNN, the optical flow calculation is computationally expensive. It is the major speed bottleneck of the current two-stream approaches. As an alternative, Zhang et al. <ref type="bibr" target="#b32">[33]</ref> proposed to use motion vectors to replace the more precise optical flow. This simple improvement brought more than 20x speedup compared to the traditional two-stream approaches. However, this speed improvement came with an equally significant accuracy drop. The encoded motion vectors lack fine structures, and contain noisy and inaccurate motion patterns, leading to much worse accuracy compared to the more precise optical flow <ref type="bibr" target="#b31">[32]</ref>. These weaknesses are fundamental and can not be improved. Another more promising approach is to learn to predict optical flow using supervised CNNs, which is closer to our approach. Ng. et al. <ref type="bibr" target="#b14">[15]</ref> used optical flow calculated by traditional methods as supervision to train a network to predict optical flow. This method avoids the pre-computation of optical flow at inference time and greatly speeds up the process. However, the quality of the optical flow calculated by this approach is limited by the quality of the traditional flow estimation, which again limits its potential on action recognition. Ilg et al. <ref type="bibr" target="#b7">[8]</ref> use a network trained on synthetic data where ground truth flow exists. The ability of synthetic data to represent the complexity of real data is very limited. Ilg et al. <ref type="bibr" target="#b7">[8]</ref> actually show that there is a domain gap between real data and synthetic data. To address this gap, they simply grow the synthetic data to narrow the gap. The problem with this solution is that it may not work for other datasets and it is not feasible to do this for all datasets. Our work addresses the optical flow estimation problem in a much more fundamental and promising way. We predict optical flow on-the-fly using CNNs, thus addressing the computation and storage problems. And we perform unsupervised pre-training on real data, thus addressing the domain gap problem.</p><p>Besides the computational problem, traditional optical flow estimation is completely independent of the high-level final tasks like action recognition and is therefore potentially sub-optimal. However, our approach is end-to-end optimized. It is important to distinguish between these two ways of introducing motion information to the encoding CNNs. Although optical flow is currently being used to represent the motion information in the videos, we do not know whether it is an optimal representation. There might be an underlying motion representation that is better than optical flow. In fact, a recent work <ref type="bibr" target="#b29">[30]</ref> demonstrated that fixed flow estimation is not as good as task-oriented flow for general computer vision tasks. Hence, we believe that our end-to-end learning framework will help us extract better motion representations than traditional optical flow for action recognition. However, for notational convenience, we still refer our learned motion representation as optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hidden Two-Stream Networks</head><p>In this section, we describe our proposed hidden two-stream networks in detail. We first introduce our unsupervised network for optical flow estimation along with employed good practices in Section 3.1. We name it MotionNet. In Section 3.2, we stack the temporal stream network upon MotionNet to allow end-to-end training. Finally, we introduce the hidden two-stream CNNs in Section 3.3 which combines our stacked temporal stream with a spatial stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Optical Flow Learning</head><p>We treat optical flow estimation as an image reconstruction problem <ref type="bibr" target="#b30">[31]</ref>. Given a frame pair, we hope to generate the optical flow that allows us to reconstruct one frame from the other. Formally, taking a pair of adjacent frames I 1 and I 2 as input, our CNN generates a flow field V . Then using the predicted flow field V and I 2 , we get the reconstructed frame I 1 using backward warping, i.e.,</p><formula xml:id="formula_0">I 1 = T [I 2 , V ],</formula><p>where T is the inverse warping function. Our goal is to minimize the photometric error between I 1 and I 1 . The intuition is that if the estimated flow and the next frame can be used to reconstruct the current frame, then the network should have learned useful representations of the underlying motions. MotionNet Our MotionNet is a fully convolutional network, consisting of a contracting part and an expanding part. The contracting part is a stack of convolutional layers and the expanding part is a chain of combined convolutional and deconvolutional layers. The details of our network can be seen in the supplementary material. We describe the challenges and proposed good practices to learn better motion representation for action recognition below.</p><p>First, we design a network that focuses on small displacement motion. For real data such as YouTube videos, we often encounter the problem that foreground motion (human actions of interest) is small, but the background motion (camera motion) is dominant. Thus, we adopt 3 × 3 kernels throughout the network to detect local, small motions. Besides, we keep the high frequency image details for later stages. Our first two convolutional layers do not use striding. We use strided convolution instead of pooling for image downsampling because pooling is shown to be harmful for dense per-pixel prediction tasks.</p><p>Second, our MotionNet computes multiple losses at multiple scales. Due to the skip connections between the contracting and expanding parts, the intermediate losses can regularize each other and guide earlier layers to converge faster to the final objective. We explore three loss functions that help us to generate better optical flow. These loss functions are as follows.</p><p>-A standard pixelwise reconstruction error function, which is calculated as:</p><formula xml:id="formula_1">L pixel = 1 hw h i w j ρ(I 1 (i, j) − I 2 (i + V x i,j , j + V y i,j )).<label>(1)</label></formula><p>The V x and V y are the estimated optical flow in the horizontal and vertical directions. The inverse warping T is performed using a spatial transformer module <ref type="bibr" target="#b8">[9]</ref>. Here we use a robust convex error function, the generalized Charbonnier penalty ρ(x) = (x 2 + 2 ) α , to reduce the influence of outliers. h and w denote the height and width of images I 1 and I 2 .</p><p>-A smoothness loss that addresses the aperture problem that causes ambiguity in estimating motions in non-textured regions. It is calculated as:</p><formula xml:id="formula_2">L smooth = ρ(∇V x x ) + ρ(∇V x y ) + ρ(∇V y x ) + ρ(∇V y y ).<label>(2)</label></formula><p>∇V x x and ∇V x y are the gradients of the estimated flow field V x in each direction. Similarly, ∇V y x and ∇V y y are the gradients of V y . The generalized Charbonnier penalty ρ(x) is the same as in the pixelwise loss.</p><p>-A structural similarity (SSIM) loss function <ref type="bibr" target="#b25">[26]</ref> that helps us to learn the structure of the frames. SSIM is a perceptual quality measure. Given two K × K image patches I p1 and I p2 , it is calculated as</p><formula xml:id="formula_3">SSIM(I p1 , I p2 ) = (2µ p1 µ p2 + c 1 )(2σ p1p2 + c 2 ) (µ 2 p1 + µ 2 p2 + c 1 )(σ 2 p1 + σ 2 p2 + c 2 ) .<label>(3)</label></formula><p>Here, µ p1 and µ p2 are the mean of image patches I p1 and I p2 , σ p1 and σ p2 are the variance of image patches I p1 and I p2 , and σ p1p2 is the covariance of these two image patches. c 1 and c 2 are two constants to stabilize division by a small denominator. In our experiments, K is set to 8 and c 1 and c 2 are 0.0001 and 0.001, respectively. In order to compare the similarity between two images I 1 and I 1 , we adopt a sliding window approach to partition the images into local patches. The stride for the sliding window is set to 8 in both the horizontal and vertical directions. Hence, our SSIM loss function is defined as:</p><formula xml:id="formula_4">L ssim = 1 N N n (1 − SSIM(I 1n , I 1n )).<label>(4)</label></formula><p>where N is the number of patches we can extract from an image given the sliding stride of 8, n is the patch index. I 1n and I 1n are two corresponding patches from original image I 1 and the reconstructed image I 1 . Our experiments show that this simple strategy significantly improves the quality of our estimated flows. It forces our MotionNet to produce flow fields with clear motion boundaries.</p><p>Hence, the loss at each scale s is a weighted sum of the pixelwise reconstruction loss, the piecewise smoothness loss, and the region-based SSIM loss,</p><formula xml:id="formula_5">L s = λ 1 · L pixel + λ 2 · L smooth + λ 3 · L ssim<label>(5)</label></formula><p>where λ 1 , λ 2 , and λ 3 weight the relative importance of the different metrics during training. Since we have predictions at five scales (flow2 to flow6) due to five expansions in the decoder, the overall loss of MotionNet is a weighted sum of loss L s :</p><formula xml:id="formula_6">L all = 5 s=1 δ s L s<label>(6)</label></formula><p>where the δ s are set to balance the losses at each scale and are numerically of the same order. We describe how we determine the values of these weights in the supplementary materials. Third, unsupervised learning of optical flow introduces artifacts in homogeneous regions because the brightness assumption is violated. We insert additional convolutional layers between deconvolutional layers in the expanding part to yield smoother motion estimation. We also explored other techniques in the literature, like adding flow confidence and multiplying by the original color images <ref type="bibr" target="#b7">[8]</ref> during expanding. However, we did not observe any improvements.</p><p>In Section 5.1, we conduct an ablation study to demonstrate the contributions of each of these strategies. Though our network structure is similar to a concurrent work <ref type="bibr" target="#b7">[8]</ref>, MotionNet is fundamentally different from FlowNet2. First, we perform unsupervised learning while <ref type="bibr" target="#b7">[8]</ref> performs supervised learning for optical flow prediction. Unsupervised learning allows us to avoid the domain gap between synthetic data and real data. Unsupervised learning also allows us to train the model for target tasks like action recognition in an end-to-end fashion even if the datasets of target applications do not have ground truth optical flow. Second, our network architecture is carefully designed to balance efficiency and accuracy. For example, MotionNet only has one network, while FlowNet2 has 5 similar sub-networks. The model footprints of MotionNet and FlowNet2 <ref type="bibr" target="#b7">[8]</ref> are 170M and 654M, and the prediction speeds are 370fps and 25fps, respectively. We also present an architecture search in the supplementary materials to obtain deep insights in terms of the model trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projecting Motion Features to Actions</head><p>Given that MotionNet and the temporal stream are both CNNs, we would like to combine these two modules into one stage and perform end-to-end training. There are multiple ways to design such a combination to project motion features to action labels. Here, we explore two ways, stacking and branching.</p><p>Stacking is the most straightforward approach and just places MotionNet in front of the temporal stream, treating MotionNet as an off-the-shelf flow estimator. Branching is more elegant in terms of architecture design. It uses a single network for both motion feature extraction and action classification. The convolutional features are shared between the two tasks. Due to space limitations, we show in the supplementary materials that stacking is more effective than branching. It achieves better action recognition performance while remaining complementary to the spatial stream. From now on, we choose stacking to project the motion features to action labels.</p><p>For stacking, we first need to normalize the estimated flows before feeding them to the encoding CNN. More specifically, as suggested in <ref type="bibr" target="#b18">[19]</ref>, we first clip the motions that are larger than 20 pixels to 20 pixels. Then we normalize and quantize the clipped flows to have a range between 0 ∼ 255. We find such a normalization is important for good temporal stream performance and design a new normalization layer for it.</p><p>Second, we need to determine how to fine tune the network, including which loss to use during the fine tuning. We explored different settings. (a) Fixing MotionNet, which means that we do not use the action loss to fine-tune the optical flow estimator. (b) Both MotionNet and the temporal stream CNN are fine-tuned, but only the action categorical loss function is computed. No unsupervised objective (5) is involved. (c) Both MotionNet and the temporal stream CNN are fine-tuned, and all the loss functions are computed. Since motion is largely related to action, we hope to learn better motion estimators by this multi-task way of learning. As will be demonstrated later in Section 4.2, model (c) achieves the best action recognition performance.</p><p>Third, we need to capture relatively long-term motion dependencies. We accomplish this by inputting a stack of multiple consecutive flow fields. Simonyan and Zisserman <ref type="bibr" target="#b18">[19]</ref> found that a stack of 10 flow fields achieves a much higher accuracy than only using a single flow field. To make fair comparison, we also fix the length of our input to be 11 frames to allow us to generate 10 optical flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hidden Two-Stream Networks</head><p>We also show the results of combining our stacked temporal stream with a spatial stream. These results are important as they are strong indicators of whether our stacked temporal stream indeed learns complementary motion information or just appearance information.</p><p>Following the testing scheme of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>, we evenly sample 25 frames/clips for each video. For each frame/clip, we perform 10x data augmentation by cropping the 4 corners and 1 center, flipping them horizontally and averaging the prediction scores (before softmax operation) over all crops of the samples. In the end, we fuse the two streams' scores with a spatial to temporal stream ratio of 1:1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Datasets</head><p>We perform experiments on four widely used action recognition benchmarks, UCF101 <ref type="bibr" target="#b19">[20]</ref>, HMDB51 <ref type="bibr" target="#b11">[12]</ref>, THUMOS14 <ref type="bibr" target="#b4">[5]</ref> and ActivityNet <ref type="bibr" target="#b6">[7]</ref>. UCF101 is composed of realistic action videos from YouTube. It contains 13, 320 video clips distributed among 101 action classes. HMDB51 includes 6, 766 video clips of 51 actions extracted from a wide range of sources, such as online videos and movies. Both UCF101 and HMDB51 have a standard three-split evaluation protocol and we report the average recognition accuracies over the three splits. THUMOS14 and ActivityNet are large-scale video datasets for action recognition and detection, which contain long untrimmed videos. THUMOS14 has 101 action classes. It includes a training set, validation set, test set and background set. We don't use the background set in our experiments. We use 13,320 training and 1,010 validation videos for training and report the performance on 1,574 test videos. For ActivityNet, we use its 1.2 version which has 100 action classes. Following the standard evaluation split, 4,819 training and 2,383 validation videos are used for training and 2,480 videos for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In this section, we evaluate our proposed framework on the first split of UCF101. We report the accuracy as well as the processing speed of the inference step in frames per second. The results are shown in <ref type="table">Table 1</ref>. The implementation details are in the supplementary materials. Top section of <ref type="table">Table 1</ref>: Here we compare the performance of two-stage approaches. By two-stage, we mean optical flow is pre-computed, cached, and then <ref type="table">Table 1</ref>: Comparison of accuracy and efficiency. Top section: Two-stage temporal stream approaches. Middle Section: End-to-end temporal stream approaches. Bottom Section: Two-stream approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) fps TV-L1 <ref type="bibr" target="#b31">[32]</ref> 85.65 14.75 FlowNet <ref type="bibr" target="#b3">[4]</ref> 55.27 52.08 FlowNet2 <ref type="bibr" target="#b7">[8]</ref> 79.64 8.05 NextFlow <ref type="bibr" target="#b17">[18]</ref> 72.2 42.02 Enhanced Motion Vectors <ref type="bibr" target="#b32">[33]</ref> 79.3 390.7 MotionNet (2 frames) 84.09 48.54 ActionFlowNet (2 frames) <ref type="bibr" target="#b14">[15]</ref> 70.0 200.0 ActionFlowNet (16 frames) <ref type="bibr" target="#b14">[15]</ref> 83 fed to a CNN classifier to project flow to action labels. For fair comparison, our MotionNet here is pre-trained on UCF101, but not fine-tuned using the action classification loss. It only takes frame pairs as input and outputs one flow estimate. The results show that our MotionNet achieves a good balance between accuracy and speed in this setting. In terms of accuracy, our unsupervised MotionNet is competitive to TV-L1 while performing much better (4% ∼ 12% absolute improvement) than other methods of generating flows, including supervised training using synthetic data (FlowNet <ref type="bibr" target="#b3">[4]</ref> and FlowNet2 <ref type="bibr" target="#b7">[8]</ref>), and directly getting flows from compressed videos (Enhanced Motion Vectors <ref type="bibr" target="#b32">[33]</ref>). These improvements are very significant in datasets like UCF101. In terms of speed, we are also among the best of the CNN based methods and much faster than TV-L1, which is one of the fastest traditional methods. Middle section of <ref type="table">Table 1</ref>: Here we examine the performance of end-toend CNN based approaches. None of these approaches store intermediate flow information and thus run much faster than the two-stage approaches. If we compare the average running time of these approaches to the two-stage ones, we can see that the time spent on writing and reading intermediate results is almost 3x as much as the time spent on all other steps. Therefore, from an efficiency perspective, it is important to do end-to-end training and predict optical flow on-the-fly.</p><p>ActionFlowNet <ref type="bibr" target="#b14">[15]</ref> is what we denote as a branched temporal stream. It is a multi-task learning model to jointly estimate optical flow and recognize actions.</p><p>The convolutional features are shared which leads to faster speeds. However, even the 16 frames ActionFlowNet performs 1% worse than our stacked temporal stream. Besides, ActionFlowNet uses optical flow from traditional methods as labels to perform supervised training. This indicates that during the training phase, it still needs to cache flow estimates which is computation and storage demanding for large-scale video datasets. Also the algorithm will mimic the failure cases of the classical approaches.</p><p>If we compare the way we fine-tune our stacked temporal stream CNNs, we can see that model (c) where we include all the loss functions to do end-to-end training, is better than the other models including fixing MotionNet weights (model (a)) and only using the action classification loss function (model (b)). These results show that both end-to-end fine-tuning and fine-tuning with unsupervised loss functions are important for stacked temporal stream CNN training. Bottom section of <ref type="table">Table 1</ref>: Here we compare the performance of two-stream networks by fusing the prediction scores from the temporal stream CNN with the prediction scores from the spatial stream CNN. These comparisons are mainly used to show that stacked temporal stream CNNs indeed learn motion information that is complementary to what is learned in appearance streams.</p><p>The accuracy of the single stream spatial CNN is 80.97%. We observe from <ref type="table">Table 1</ref> that significant improvements are achieved by fusing a stacked temporal stream CNN with a spatial stream CNN to create a hidden two-stream CNN. These results show that our stacked temporal stream CNN is able to learn motion information directly from the frames and achieves much better accuracy than spatial stream CNN alone. This observation is true even in the case where we only use the action loss for fine-tuning the whole network (model (b)). This result is significant because it indicates that our unsupervised pre-training indeed finds a better path for CNNs to learn to recognize actions and this path will not be forgotten in the fine-tuning process. If we compare the hidden two-stream CNNs to the stacked temporal stream CNNs, we can see that the gap between model (c) and model (a)/(b) widens. The reason may be because, without the regularization of the unsupervised loss, the networks start to learn appearance information. Hence they become less complementary to the spatial CNNs.</p><p>Finally, we can see that our models achieve very similar accuracy to the original two-stream CNNs. Among the two representative works we show, Two-Stream CNNs <ref type="bibr" target="#b18">[19]</ref> is the earliest two-stream work and Very Deep Two-Stream CNNs <ref type="bibr" target="#b23">[24]</ref> is the one we improve upon. Therefore, Very Deep Two-Stream CNNs <ref type="bibr" target="#b23">[24]</ref> is the most comparable work. We can see that our approach is about 1% worse than Very Deep Two-Stream CNNs <ref type="bibr" target="#b23">[24]</ref> in terms of accuracy but about 10x faster in terms of speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies for MotionNet</head><p>Because of our specially designed loss functions and operators, our proposed Mo-tionNet can produce high quality motion estimates, which allows us to achieve promising action recognition accuracy. Here, we run an ablation study to understand the contributions of these components. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Small Disp indicates using a network that focuses on small displacements. CDC means adding an extra convolution between deconvolutions in the expanding part of MotionNet. MultiScale indicates computing losses at multiple scales. First, we examine the importance of using a network structure that focuses on small displacement motions. We keep the other aspects of the implementation the same, but use a larger kernel size and stride in the beginning of the network. The accuracy drops from 82.71% to 82.22%. This drop shows that using smaller kernels with a deeper network indeed helps to detect small motions.</p><p>Second, we examine the importance of adding the SSIM loss. Without SSIM, the action recognition accuracy drops to 81.58%. This more than 1% performance drop shows that it is important to focus on discovering the structure of frame pairs.</p><p>Third, we examine the effect of removing convolutions between the deconvolutions in the expanding part of MotionNet. This strategy is designed to smooth the motion estimation. As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, removing extra convolutions brings a significant performance drop from 82.71% to 81.25%.</p><p>Fourth, we examine the advantage of incorporating the smoothness objective. Without the smoothness loss, we obtain a much worse result of 80.14%. This result shows that our real-world data is very noisy. Adding smoothness regularization helps to generate smoother flow fields by suppressing noise. This suppression is important for the following temporal stream CNNs to learn better motion representations for action recognition.</p><p>Fifth, we examine the necessity of computing losses at multiple scales during deconvolution. Without the multi-scale scheme, the action recognition accuracy drops to 80.63%. The performance drop shows that it is important to regularize the output at each scale in order to produce the best flow estimation in the end. Otherwise, we found that the intermediate representations during deconvolution may drift to fit the action recognition task, and not predict optical flow.</p><p>Finally, we explore a model that does not employ any of these practices. As expected, the performance is the worst, which is 4.94% lower than our full MotionNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learned Optical Flow</head><p>In this section, we systematically investigate the effects of different motion estimation models for action recognition, as well as their flow estimation quality. We also show some visual examples to discover possible directions for future improvement. Here, we compare three optical flow models: TV-L1, MotionNet and FlowNet2. To quantitatively evaluate the quality of learned flow, we test the three models on four well received benchmarks, MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. For action recognition accuracy, we report their performance on UCF101 split1. The results can be seen in <ref type="table" target="#tab_2">Table 3</ref>. We use EPE (endpoint error) to evaluate MPI-Sintel, KITTI 2012 and Middlebury with lower being better. We use Fl (percentage of optical flow outliers) to evaulate KITTI 2015 with lower being better. We use classification accuracy to evaluate UCF101 with higher being better. For flow quality, FlowNet2 generally performs better, except on Middlebury because it mostly contains small displacements. Our MotionNet has similar performance to TV-L1 on Sintel and Middlebury, and outperforms TV-L1 on KITTI 2012 and KITTI 2015. The result is encouraging because the KITTI benchmark contains real data (not synthetic), which indicates that the flow estimation from our MotionNet is robust and generalizable. In addition, although FlowNet2 ranks higher on optical flow benchmarks, it performs the worst on action recognition tasks. This interesting observation means that lower EPE does not always lead to higher action recognition accuracy. This is because EPE is a very simple metric based on L2 distance, which does not consider motion boundary preservation or background motion removal. This is crucial, however, for recognizing complex human actions.</p><p>We also show some visual samples in <ref type="figure" target="#fig_0">Figure 2</ref> to help understand the effect of the quality of estimated flow fields for action recognition. The color scheme follows the standard flow field color coding in <ref type="bibr" target="#b7">[8]</ref>. In general, the estimated flow fields from all three models look reasonable. MotionNet has lots of background noise compared to TV-L1 due to its global learning. This maybe the reason why it performs worse than TV-L1 for action recognition. FlowNet2 has very crisp motion boundaries, fine structures and smoothness in homogeneous regions. It is indeed a good flow estimator in terms of both EPE and visual inspection. However, it achieves much worse results for action recognition, 3.5% lower than TV-L1 and 2.9% lower than our MotionNet. Thus, which motion representation is best for action recognition remains an open question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Comparison to State-of-the-Art Real-Time Approaches</head><p>In this section, we compare our proposed method to recent real-time state-of-theart approaches as shown in <ref type="table">Table 4</ref> 4 . Among all real-time methods, our hidden two-stream networks achieves the highest accuracy on the four benchmarks. We also show the flexibility of our MotionNet by concatenating it to temporal streams with different backbone CNN architectures, e.g., VGG16 <ref type="bibr" target="#b23">[24]</ref>, TSN <ref type="bibr" target="#b24">[25]</ref> and I3D <ref type="bibr" target="#b0">[1]</ref>. With deeper networks, we can achieve higher recognition accuracy and still be real-time. We are 6.1% better on UCF101, 14.2% better on HMDB51, 8.5% better on THUMOS14 and 7.8% better on ActivityNet than the previous state-of-the-art. This indicates that our stacked end-to-end learning framework can implicitly learn better motion representations than motion vectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref> and RGB differences <ref type="bibr" target="#b24">[25]</ref> with respect to the task of action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed a new framework called hidden two-stream networks to recognize human actions in video. It addresses the problem of capturing the temporal relationships among video frames which the current CNN architectures have difficulty with. Different from the current common practice of using traditional <ref type="table">Table 4</ref>: Comparison to state-of-the-art real-time approaches on four benchmarks with respect to mean classification accuracy. * indicates results from our implementation.</p><p>Method UCF101(%) HMDB51(%) THUMOS14(%) ActivityNet(%) MV + FV <ref type="bibr" target="#b9">[10]</ref> 78.5 46.7 − − EMV <ref type="bibr" target="#b32">[33]</ref> 80.2 − 41.6 − C3D (1 Net) <ref type="bibr" target="#b20">[21]</ref> 82.3 49.7 * 54.6 74.1 ActionFlowNet <ref type="bibr" target="#b14">[15]</ref> 83.9 56.4 51.3 * 68.8 * RGB + EMV <ref type="bibr" target="#b32">[33]</ref> 86.4 − 61.5 − 3DNet <ref type="bibr" target="#b1">[2]</ref> 90.2 − − − RGB Diff (TSN) <ref type="bibr" target="#b24">[25]</ref> 91.0 64. local optical flow estimation methods to pre-compute the motion information for CNNs, we use an unsupervised pre-training approach. Our MotionNet is computationally efficient and end-to-end trainable. It is flexible and can be directly applied in other frameworks for various video understanding applications. Experimental results on four challenging benchmarks demonstrate the effectiveness of our approach.</p><p>In the future, we would like to improve our hidden two-stream networks in the following directions. First, we would like to improve our optical flow prediction based on the observation that the smoothness loss has significant impact on the quality of the motion estimations for action recognition. Second, we would like to incorporate other best practices that improve the overall performance of the networks. For example, joint training of the two streams instead of a simple late fusion. Third, it would be interesting to see how addressing the false label assignment problem can help improve our overall performance. Finally, removing global camera motion and partial occlusion within the CNN framework would be helpful for both optical flow estimation and action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Visual comparisons of estimated flow field from TV-L1, MotionNet and FlowNet2. Left: ApplyEyeMakeup, BabyCrawling, BodyWeightSquats, Boxing-PunchingBag and CleanAndJerk. Right: Hammering, PlayingFlute, Pommel-Horse, WallPushups and YoYo. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of good practices employed in MotionNet.</figDesc><table><row><cell cols="7">Method Small Disp SSIM CDC Smoothness MultiScale Accuracy (%)</cell></row><row><cell>MotionNet</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>77.79</cell></row><row><cell>MotionNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>×</cell><cell>80.63</cell></row><row><cell>MotionNet</cell><cell></cell><cell></cell><cell></cell><cell>×</cell><cell></cell><cell>80.14</cell></row><row><cell>MotionNet</cell><cell></cell><cell></cell><cell>×</cell><cell></cell><cell></cell><cell>81.25</cell></row><row><cell>MotionNet</cell><cell></cell><cell>×</cell><cell></cell><cell></cell><cell></cell><cell>81.58</cell></row><row><cell>MotionNet</cell><cell>×</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.22</cell></row><row><cell>MotionNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of optical flow and action classification. For flow evaluation, lower error is better. For action recognition, higher accuracy is better.</figDesc><table><row><cell cols="5">Method Sintel KITTI2012 KITTI2015 Middlebury UCF101</cell></row><row><cell>FlowNet2 6.02</cell><cell>1.8</cell><cell>11.48</cell><cell>0.52</cell><cell>81.97</cell></row><row><cell>TV-L1 10.46</cell><cell>14.6</cell><cell>47.64</cell><cell>0.45</cell><cell>85.65</cell></row><row><cell>MotionNet 11.93</cell><cell>7.5</cell><cell>30.65</cell><cell>0.91</cell><cell>84.88</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Detailed comparisons can be found in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In general, the requirement for real-time processing is 25 fps. We also compare to other non real-time approaches in the supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We gratefully acknowledge the support of NVIDIA Corporation through the donation of the Titan Xp GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling Video Evolution for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazrba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Asynchronous Doubly Stochastic Sparse Kernel Learning. In: Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial Transformer Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Feature Extraction, Encoding and Classification for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Largescale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HMDB: A Large Video Database for Human Motion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Local Video Feature for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct Shape Regression Networks for End-to-End Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ActionFlowNet: Learning Motion Representation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Next-Flow: Hybrid Multi-Tasking with Next-Frame Prediction to Boost Optical-Flow Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03777</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<title level="m">Towards Good Practices for Very Deep Two-Stream ConvNets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image Quality Assessment: From Error Visibility to Structural Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00636</idno>
		<title level="m">Compressed Video Action Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<title level="m">Rethinking Spatiotemporal Feature Learning For Video Understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Texture Manifold for Ground Terrain Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09078</idno>
		<title level="m">Video Enhancement with Task-Oriented Flow</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Duality Based Approach for Realtime TV-L1 Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th DAGM conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time Action Recognition with Enhanced Motion Vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02295</idno>
		<title level="m">Guided Optical Flow Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards Universal Representation for Unseen Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DenseNet for Dense Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Optical Flow via Dilated Networks and Occlusion Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
