<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META-DATASET: A DATASET OF DATASETS FOR LEARNING TO LEARN FROM FEW EXAMPLES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and Vector Institute, † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Berkeley Correspondence</orgName>
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
						</author>
						<title level="a" type="main">META-DATASET: A DATASET OF DATASETS FOR LEARNING TO LEARN FROM FEW EXAMPLES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose META-DATASET: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on META-DATASET, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in META-DATASET. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.</p><p>arXiv:1903.03096v4 [cs.</p><p>LG] 8 Apr 2020</p><p>Published as a conference paper at ICLR 2020 META-DATASET aims to improve upon previous benchmarks in the above directions: it is significantly larger-scale and is comprised of multiple datasets of diverse data distributions; its task creation is informed by class structure for ImageNet and Omniglot; it introduces realistic class imbalance; and it varies the number of classes in each task and the size of the training set, thus testing the robustness of models across the spectrum from very-low-shot learning onwards.</p><p>The main contributions of this work are: 1) A more realistic, large-scale and diverse environment for training and testing few-shot learners. 2) Experimental evaluation of popular models, and a new set of baselines combining inference algorithms of meta-learners with non-episodic training. 3) Analyses of whether different models benefit from more data, heterogeneous training sources, pre-trained weights, and meta-training. 4) A novel meta-learner that performs strongly on META-DATASET.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Few-shot learning refers to learning new concepts from few examples, an ability that humans naturally possess, but machines still lack. Improving on this aspect would lead to more efficient algorithms that can flexibly expand their knowledge without requiring large labeled datasets. We focus on few-shot classification: classifying unseen examples into one of N new 'test' classes, given only a few reference examples of each. Recent progress in this direction has been made by considering a meta-problem: though we are not interested in learning about any training class in particular, we can exploit the training classes for the purpose of learning to learn new classes from few examples, thus acquiring a learning procedure that can be directly applied to new few-shot learning problems too. This intuition has inspired numerous models of increasing complexity (see Related Work for some examples). However, we believe that the commonly-used setup for measuring success in this direction is lacking. Specifically, two datasets have emerged as de facto benchmarks for few-shot learning: Omniglot <ref type="bibr" target="#b11">(Lake et al., 2015)</ref>, and mini-ImageNet <ref type="bibr" target="#b29">(Vinyals et al., 2016)</ref>, and we believe that both of them are approaching their limit in terms of allowing one to discriminate between the merits of different approaches. Omniglot is a dataset of 1623 handwritten characters from 50 different alphabets and contains 20 examples per class (character). Most recent methods obtain very high accuracy on Omniglot, rendering the comparisons between them mostly uninformative. mini-ImageNet is formed out of 100 ImageNet <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> classes (64/16/20 for train/validation/test) and contains 600 examples per class. Albeit harder than Omniglot, it has the same property that most recent methods trained on it present similar accuracy when controlling for model capacity. We advocate that a more challenging and realistic benchmark is required for further progress in this area.</p><p>More specifically, current benchmarks: 1) Consider homogeneous learning tasks. In contrast, real-life learning experiences are heterogeneous: they vary in terms of the number of classes and examples per class, and are unbalanced. 2) Measure only within-dataset generalization. However, we are eventually after models that can generalize to entirely new distributions (e.g., datasets). 3) Ignore the relationships between classes when forming episodes. Specifically, the coarse-grained classification of dogs and chairs may present different difficulties than the fine-grained classification of dog breeds, and current benchmarks do not establish a distinction between the two.</p><p>Meta-Learners for Few-shot Classification In the episodic setting, models are trained end-to-end for the purpose of learning to build classifiers from a few examples. We choose to experiment with Matching Networks <ref type="bibr" target="#b29">(Vinyals et al., 2016)</ref>, Relation Networks <ref type="bibr" target="#b28">(Sung et al., 2018)</ref>, Prototypical Networks <ref type="bibr" target="#b27">(Snell et al., 2017)</ref> and Model Agnostic Meta-Learning <ref type="bibr">(MAML, Finn et al., 2017</ref>) since they cover a diverse set of approaches to few-shot learning. We also introduce a novel meta-learner which is inspired by the last two models.</p><p>In each training episode, episodic models compute for each query example x * ∈ Q, the distribution for its label p(y * |x * , S) conditioned on the support set S and allow to train this differentiablyparameterized conditional distribution end-to-end via gradient descent. The different models are distinguished by the manner in which this conditioning on the support set is realized. In all cases, the performance on the query set drives the update of the meta-learner's weights, which include (and sometimes consist only of) the embedding weights. We briefly describe each method below.</p><p>Prototypical Networks Prototypical Networks construct a prototype for each class and then classify each query example as the class whose prototype is 'nearest' to it under Euclidean distance. More concretely, the probability that a query example x * belongs to class k is defined as:</p><formula xml:id="formula_0">p(y * = k|x * , S) = exp(−||g(x * ) − c k || 2 2 ) k ∈{1,...,N } exp(−||g(x * ) − c k || 2 2 )</formula><p>where c k is the 'prototype' for class k: the average of the embeddings of class k's support examples.</p><p>Matching Networks Matching Networks (in their simplest form) label each query example as a (cosine) distance-weighted linear combination of the support labels:</p><formula xml:id="formula_1">p(y * = k|x * , S) = |S| i=1 α(x * , x i )1 yi=k ,</formula><p>where 1 A is the indicator function and α(x * , x i ) is the cosine similarity between g(x * ) and g(x i ), softmax-normalized over all support examples x i , where 1 ≤ i ≤ |S|.</p><p>Relation Networks Relation Networks are comprised of an embedding function g as usual, and a 'relation module' parameterized by some additional neural network layers. They first embed each support and query using g and create a prototype p c for each class c by averaging its support embeddings. Each prototype p c is concatenated with each embedded query and fed through the relation module which outputs a number in [0, 1] representing the predicted probability that that query belongs to class c. The query loss is then defined as the mean square error of that prediction compared to the (binary) ground truth. Both g and the relation module are trained to minimize this loss.</p><p>MAML MAML uses a linear layer parametrized by W and b on top of the embedding function g(·; θ) and classifies a query example as</p><formula xml:id="formula_2">p(y * |x * , S) = softmax(b + W g(x * ; θ )),</formula><p>where the output layer parameters W and b and the embedding function parameters θ are obtained by performing a small number of within-episode training steps on the support set S, starting from initial parameter values (b, W, θ). The model is trained by backpropagating the query set loss through the within-episode gradient descent procedure and into (b, W, θ). This normally requires computing second-order gradients, which can be expensive to obtain (both in terms of time and memory). For this reason, an approximation is often used whereby gradients of the within-episode descent steps are ignored. This variant is referred to as first-order MAML (fo-MAML) and was used in our experiments. We did attempt to use the full-order version, but found it to be impractically expensive (e.g., it caused frequent out-of-memory problems).</p><p>Moreover, since in our setting the number of ways varies between episodes, b, W are set to zero and are not trained (i.e., b , W are the result of within-episode gradient descent initialized at 0), leaving only θ to be trained. In other words, MAML focuses on learning the within-episode initialization θ of the embedding network so that it can be rapidly adapted for a new task.</p><p>Introducing Proto-MAML We introduce a novel meta-learner that combines the complementary strengths of Prototypical Networks and MAML: the former's simple inductive bias that is evidently effective for very-few-shot learning, and the latter's flexible adaptation mechanism.</p><p>As explained by <ref type="bibr" target="#b27">Snell et al. (2017)</ref>, Prototypical Networks can be re-interpreted as a linear classifier applied to a learned representation g(x). The use of a squared Euclidean distance means that output logits are expressed as</p><formula xml:id="formula_3">−||g(x * ) − c k || 2 = −g(x * ) T g(x * ) + 2c T k g(x * ) − c T k c k = 2c T k g(x * ) − ||c k || 2 + constant</formula><p>where constant is a class-independent scalar which can be ignored, as it leaves output probabilities unchanged. The k-th unit of the equivalent linear layer therefore has weights W k,· = 2c k and biases b k = −||c k || 2 , which are both differentiable with respect to θ as they are a function of g(·; θ).</p><p>We refer to (fo-)Proto-MAML as the (fo-)MAML model where the task-specific linear layer of each episode is initialized from the Prototypical Network-equivalent weights and bias defined above and subsequently optimized as usual on the given support set. When computing the update for θ, we allow gradients to flow through the Prototypical Network-equivalent linear layer initialization. We show that this simple modification significantly helps the optimization of this model and outperforms vanilla fo-MAML by a large margin on META-DATASET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">META-DATASET: A NEW FEW-SHOT CLASSIFICATION BENCHMARK</head><p>META-DATASET aims to offer an environment for measuring progress in realistic few-shot classification tasks. Our approach is twofold: 1) changing the data and 2) changing the formulation of the task (i.e., how episodes are generated). The following sections describe these modifications in detail. The code is open source and publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">META-DATASET'S DATA</head><p>META-DATASET's data is much larger in size than any previous benchmark, and is comprised of multiple existing datasets. This invites research into how diverse sources of data can be exploited by a meta-learner, and allows us to evaluate a more challenging generalization problem, to new datasets altogether. Specifically, META-DATASET leverages data from the following 10 datasets: <ref type="bibr">ILSVRC-2012</ref><ref type="bibr">(ImageNet, Russakovsky et al., 2015</ref>, <ref type="bibr">Omniglot (Lake et al., 2015)</ref>, Aircraft <ref type="bibr" target="#b13">(Maji et al., 2013)</ref>, <ref type="bibr">CUB-200-2011</ref><ref type="bibr">(Birds, Wah et al., 2011</ref>, Describable Textures <ref type="bibr" target="#b3">(Cimpoi et al., 2014)</ref>, Quick Draw <ref type="bibr" target="#b9">(Jongejan et al., 2016)</ref>, Fungi <ref type="bibr" target="#b26">(Schroeder &amp; Cui, 2018)</ref>, VGG Flower <ref type="bibr" target="#b16">(Nilsback &amp; Zisserman, 2008)</ref>, Traffic Signs <ref type="bibr" target="#b8">(Houben et al., 2013)</ref> and MSCOCO <ref type="bibr" target="#b12">(Lin et al., 2014)</ref>. These datasets were chosen because they are free and easy to obtain, span a variety of visual concepts (natural and human-made) and vary in how fine-grained the class definition is. More information about each of these datasets is provided in the Appendix.</p><p>To ensure that episodes correspond to realistic classification problems, each episode generated in META-DATASET uses classes from a single dataset. Moreover, two of these datasets, Traffic Signs and MSCOCO, are fully reserved for evaluation, meaning that no classes from them participate in the training set. The remaining ones contribute some classes to each of the training, validation and test splits of classes, roughly with 70% / 15% / 15% proportions. Two of these datasets, ImageNet and Omniglot, possess a class hierarchy that we exploit in META-DATASET. For each dataset, the composition of splits is available online 2 .</p><p>ImageNet ImageNet is comprised of 82,115 'synsets', i.e., concepts of the WordNet ontology, and it provides 'is-a' relationships for its synsets, thus defining a DAG over them. META-DATASET uses the 1K synsets that were chosen for the ILSVRC 2012 classification challenge and defines a new class split for it and a novel procedure for sampling classes from it for episode creation, both informed by its class hierarchy.</p><p>Specifically, we construct a sub-graph of the overall DAG whose leaves are the 1K classes of ILSVRC-2012. We then 'cut' this sub-graph into three pieces, for the training, validation, and test splits, such that there is no overlap between the leaves of any of these pieces. For this, we selected the synsets 'carnivore' and 'device' as the roots of the validation and test sub-graphs, respectively. The leaves that are reachable from 'carnivore' and 'device' form the sets of the validation and test classes, respectively. All of the remaining leaves constitute the training classes. This method of splitting ensures that the training classes are semantically different from the test classes. We end up with 712 training, 158 validation and 130 test classes, roughly adhering to the standard 70 / 15 / 15 (%) splits.</p><p>Omniglot This dataset is one of the established benchmarks for few-shot classification as mentioned earlier. However, contrary to the common setup that flattens and ignores its two-level hierarchy of alphabets and characters, we allow it to influence the episode class selection in META-DATASET, yielding finer-grained tasks. We also use the original splits proposed in Lake et al. <ref type="formula">(2015)</ref>: (all characters of) the 'background' and 'evaluation' alphabets are used for training and testing, respectively. However, we reserve the 5 smallest alphabets from the 'background' set for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EPISODE SAMPLING</head><p>In this section we outline META-DATASET's algorithm for sampling episodes, featuring hierarchicallyaware procedures for sampling classes of ImageNet and Omniglot, and an algorithm that yields realistically imbalanced episodes of variable shots and ways. The steps for sampling an episode for a given split are:</p><p>Step 0) uniformly sample a dataset D, Step 1) sample a set of classes C from the classes of D assigned to the requested split, and</p><p>Step 2) sample support and query examples from C.</p><p>Step 1: Sampling the episode's class set This procedure differs depending on which dataset is chosen. For datasets without a known class organization, we sample the 'way' uniformly from the range [5, MAX-CLASSES], where MAX-CLASSES is either 50 or as many as there are available. Then we sample 'way' many classes uniformly at random from the requested class split of the given dataset. ImageNet and Omniglot use class-structure-aware procedures outlined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet class sampling</head><p>We adopt a hierarchy-aware sampling procedure: First, we sample an internal (non-leaf) node uniformly from the DAG of the given split. The chosen set of classes is then the set of leaves spanned by that node (or a random subset of it, if more than 50). We prevent nodes that are too close to the root to be selected as the internal node, as explained in more detail in the Appendix. This procedure enables the creation of tasks of varying degrees of fine-grainedness: the larger the height of the internal node, the more coarse-grained the resulting episode.</p><p>Omniglot class sampling We sample classes from Omniglot by first sampling an alphabet uniformly at random from the chosen split of alphabets (train, validation or test). Then, the 'way' of the episode is sampled uniformly at random using the same restrictions as for the rest of the datasets, but taking care not to sample a larger number than the number of characters that belong to the chosen alphabet. Finally, the prescribed number of characters of that alphabet are randomly sampled. This ensures that each episode presents a within-alphabet fine-grained classification.</p><p>Step 2: Sampling the episode's examples Having already selected a set of classes, the choice of the examples from them that will populate an episode can be broken down into three steps. We provide a high-level description here and elaborate in the Appendix with the accompanying formulas.</p><p>Step 2a: Compute the query set size The query set is class-balanced, reflecting the fact that we care equally to perform well on all classes of an episode. The number of query images per class is set to a number such that all chosen classes have enough images to contribute that number and still remain with roughly half on their images to possibly add to the support set (in a later step). This number is capped to 10 images per class.</p><p>Step 2b: Compute the support set size We allow each chosen class to contribute to the support set at most 100 of its remaining examples (i.e., excluding the ones added to the query set). We multiply this remaining number by a scalar sampled uniformly from the interval (0, 1] to enable the potential generation of 'few-shot' episodes even when multiple images are available, as we are also interested in studying that end of the spectrum. We do enforce, however, that each chosen class has a budget for at least one image in the support set, and we cap the total support set size to 500 examples.</p><p>Step 2c: Compute the shot of each class We now discuss how to distribute the total support set size chosen above across the participating classes. The un-normalized proportion of the support set that will be occupied by a given chosen class is a noisy version of the total number of images of that class in the dataset. This design choice is made in the hopes of obtaining realistic class ratios, under the hypothesis that the dataset class statistics are a reasonable approximation of the real-world statistics of appearances of the corresponding classes. We ensure that each class has at least one image in the support set and distribute the rest according to the above rule.</p><p>After these steps, we complete the episode creation process by choosing the prescribed number of examples of each chosen class uniformly at random to populate the support and query sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>In this work we evaluate four meta-learners on META-DATASET that we believe capture a good diversity of well-established models. Evaluating other few-shot classifiers on META-DATASET is beyond the scope of this paper, but we discuss some additional related models below.</p><p>Similarly to MAML, some train a meta-learner for quick adaptation to new tasks <ref type="bibr" target="#b19">(Ravi &amp; Larochelle, 2017;</ref><ref type="bibr" target="#b15">Munkhdalai &amp; Yu, 2017;</ref><ref type="bibr" target="#b22">Rusu et al., 2019;</ref><ref type="bibr" target="#b35">Yoon et al., 2018)</ref>. Others relate to Prototypical Networks by learning a representation on which differentiable training can be performed on some form of classifier <ref type="bibr" target="#b1">(Bertinetto et al., 2019;</ref><ref type="bibr" target="#b6">Gidaris &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b17">Oreshkin et al., 2018)</ref>. Others relate to Matching Networks in that they perform comparisons between pairs of support and query examples, using either a graph neural network <ref type="bibr" target="#b25">(Satorras &amp; Estrach, 2018)</ref> or an attention mechanism <ref type="bibr" target="#b14">(Mishra et al., 2018)</ref>. Finally, some make use of memory-augmented recurrent networks <ref type="bibr" target="#b24">(Santoro et al., 2016)</ref>, some learn to perform data augmentation <ref type="bibr" target="#b7">(Hariharan &amp; Girshick, 2017;</ref><ref type="bibr" target="#b34">Wang et al., 2018)</ref> in a low-shot learning setting, and some learn to predict the parameters of a large-shot classifier from the parameters learned in a few-shot setting <ref type="bibr" target="#b32">(Wang &amp; Hebert, 2016;</ref><ref type="bibr" target="#b33">Wang et al., 2017)</ref>. Of relevance to Proto-MAML is MAML++ <ref type="bibr" target="#b0">(Antoniou et al., 2019)</ref>, which consists of a collection of adjustments to MAML, such as multiple meta-trained inner loop learning rates and derivative-order annealing. Proto-MAML instead modifies the output weight initialization scheme and could be combined with those adjustments.</p><p>Finally, META-DATASET relates to other recent image classification benchmarks. The CVPR 2017 Visual Domain Decathlon Challenge trains a model on 10 different datasets, many of which are included in our benchmark, and measures its ability to generalize to held-out examples for those same datasets but does not measure generalization to new classes (or datasets). <ref type="bibr" target="#b7">Hariharan &amp; Girshick (2017)</ref> propose a benchmark where a model is given abundant data from certain base ImageNet classes and is tested on few-shot learning novel ImageNet classes in a way that doesn't compromise its knowledge of the base classes. <ref type="bibr" target="#b34">Wang et al. (2018)</ref> build upon that benchmark and propose a new evaluation protocol for it. <ref type="bibr" target="#b2">Chen et al. (2019)</ref> investigate fine-grained few-shot classification using the CUB dataset <ref type="bibr">(Wah et al., 2011</ref>, also featured in our benchmark) and crossdomain transfer between mini-ImageNet and CUB. Larger-scale few-shot classification benchmarks were also proposed using CIFAR-100 <ref type="bibr" target="#b10">(Krizhevsky et al., 2009;</ref><ref type="bibr" target="#b1">Bertinetto et al., 2019;</ref><ref type="bibr" target="#b17">Oreshkin et al., 2018)</ref>, tiered-ImageNet <ref type="bibr" target="#b20">(Ren et al., 2018)</ref>, and ImageNet-21k <ref type="bibr" target="#b4">(Dhillon et al., 2019)</ref>. Compared to these, META-DATASET contains the largest set of diverse datasets in the context of few-shot learning and is additionally accompanied by an algorithm for creating learning scenarios from that data that we advocate are more realistic than the previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Training procedure META-DATASET does not prescribe a procedure for learning from the training data. In these experiments, keeping with the spirit of matching training and testing conditions, we trained the meta-learners via training episodes sampled using the same algorithm as we used for META-DATASET's evaluation episodes, described above. The choice of the dataset from which to sample the next episode was random uniform. The non-episodic baselines are trained to solve the large classification problem that results from 'concatenating' the training classes of all datasets.</p><p>Validation Another design choice was to perform validation on (the validation split of) ImageNet only, ignoring the validation sets of the other datasets. The rationale behind this choice is that the performance on ImageNet has been known to be a good proxy for the performance on different datasets. We used this validation performance to select our hyperparameters, including backbone architectures, image resolutions and model-specific ones. We describe these further in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>We gave each meta-learner the opportunity to initialize its embedding function from the embedding weights to which the k-NN Baseline model trained on ImageNet converged to. We treated the choice of starting from scratch or starting from this initialization as a hyperparameter. For a fair comparison with the baselines, we allowed the non-episodic models to start from this initialization too. This is especially important for the baselines in the case of training on all datasets since it offers the opportunity to start from ImageNet-pretrained weights.</p><p>Main results <ref type="table" target="#tab_0">Table 1</ref> displays the accuracy of each model on the test set of each dataset, after they were trained on ImageNet-only or all datasets. Traffic Signs and MSCOCO are not used for training in either case, as they are reserved for evaluation. We propose to use the average (over the datasets) rank of each method as our metric for comparison, where smaller is better. A method receives rank 1 if it has the highest accuracy, rank 2 if it has the second highest, and so on. If two models share the best accuracy, they both get rank 1.5, and so on. We find that fo-Proto-MAML is the top-performer according to this metric, Prototypical Networks also perform strongly, and the Finetune Baseline notably presents a worthy opponent 3 . We include more detailed versions of these tables displaying confidence intervals and per-dataset ranks in the Appendix. sources. This is reasonable for datasets whose images are significantly different from ImageNet's: we indeed expect to gain a large benefit from training on some images from (the training classes of) these datasets. Interestingly though, on the remainder of the test sources, we don't observe a gain from all-dataset training. This result invites research into methods for exploiting heterogeneous data for generalization to unseen classes of diverse sources. Our experiments show that learning 'naively' across the training datasets (e.g., by picking the next dataset to use uniformly at random) does not automatically lead to that desired benefit in most cases. Ways and shots analysis We further study the accuracy as a function of 'ways' <ref type="figure" target="#fig_0">(Figure 2a</ref>) and the class precision as a function of 'shots' <ref type="figure" target="#fig_0">(Figure 2b</ref>). As expected, we found that the difficulty increases as the way increases, and performance degrades. More examples per class, on the other hand, indeed make it easier to correctly classify that class. Interestingly, though, not all models benefit at the same rate from more data: Prototypical Networks and fo-Proto-MAML outshine other models in very-low-shot settings but saturate faster, whereas the Finetune baseline, Matching Networks, and fo-MAML improve at a higher rate when the shot increases. We draw the same conclusions when performing this analysis on all datasets, and include those plots in the Appendix. As discussed in the Appendix, we recommend including this analysis when reporting results on Meta-Dataset, aside from the main table. The rationale is that we're not only interested in performing well on average, but also in performing well under different specifications of test tasks.</p><p>Effect of pre-training In <ref type="figure">Figures 3a and 3b</ref>, we quantify how beneficial it is to initialize the embedding network of meta-learners using the weights of the k-NN baseline pre-trained on ImageNet, as opposed to starting their episodic training from scratch. We find this procedure to often be beneficial, both for ImageNet-only training and for training on all datasets. It seems that this ImageNet-influenced initialization drives the meta-learner towards a solution which yields increased performance on natural image test datasets, especially ILSVRC, Birds, Fungi, Flowers and MSCOCO.  <ref type="figure">Figure 3</ref>: The effects of pre-training and meta-training (w/ 95% confidence intervals). (ImageNet) or (All datasets) is the training source.</p><p>Perhaps unsusprisingly, though, it underperforms on significantly different datasets such as Omniglot and Quick Draw. These findings show that, aside from the choice of the training data source(s) (e.g., ImageNet only or all datasets, as discussed above), the choice of the initialization scheme can also influence to an important degree the final solution and consequently the aptness of applying the resulting meta-learner to different data sources at test time. Finally, an interesting observation is that MAML seems to benefit the most from the pre-trained initialization, which may speak to the difficulty of optimization associated with that model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of meta-training</head><p>We propose to disentangle the inference algorithm of each meta-learner from the fact that it is meta-learned, to assess the benefit of meta-learning on META-DATASET.</p><p>To this end, we propose a new set of baselines: 'Prototypical Networks Inference', 'Matching Networks Inference', and 'fo-Proto-MAML Inference', that are trained non-episodically but evaluated episodically (for validation and testing) using the inference algorithm of the respective meta-learner. This is possible for these meta-learners as they don't have any additional parameters aside from the embedding function that explicitly need to be learned episodically (as opposed to the relation module of Relation Networks, for example). We compare each Inference-only method to its corresponding meta-learner in <ref type="figure">Figures 3c and 3d</ref>. We find that these baselines are strong: when training on ImageNet only, we can usually observe a small benefit from meta-learning the embedding weights but this benefit often disappears when training on all datasets, in which case meta-learning sometimes actually hurts. We find this result very interesting and we believe it emphasizes the need for research on how to meta-learn across multiple diverse sources, an important challenge that META-DATASET puts forth.</p><p>Fine-grainedness analysis We use ILVRC-2012 to investigate the hypothesis that finer-grained tasks are harder than coarse-grained ones. Our findings suggest that while the test sub-graph is not rich enough to exhibit any trend, the performance on the train sub-graph does seem to agree with this hypothesis. We include the experimental setup and results for this analysis in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have introduced a new large-scale, diverse, and realistic environment for few-shot classification. We believe that our exploration of various models on META-DATASET has uncovered interesting directions for future work pertaining to meta-learning across heterogeneous data: it remains unclear what is the best strategy for creating training episodes, the most appropriate validation creation and the most appropriate initialization. Current models don't always improve when trained on multiple sources and meta-learning is not always beneficial across datasets. Current models are also not robust to the amount of data in test episodes, each excelling in a different part of the spectrum. We believe that addressing these shortcomings consitutes an important research goal moving forward. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Chelsea Finn for fruitful discussions and advice on tuning fo-MAML and ensuring the correctness of implementation, as well as Zack Nado and Dan Moldovan for the initial dataset code that was adapted, and Cristina Vasconcelos for spotting an issue in the ranking of models. Finally, we'd like to thank John Bronskill for suggesting that we experiment with a larger inner-loop learning rate for MAML which indeed significantly improved our fo-MAML results on META-DATASET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX .1 RECOMMENDATION FOR REPORTING RESULTS ON META-DATASET</head><p>We recommend that future work on META-DATASET reports two sets of results:</p><p>1. The main tables storing the average (over 600 test episodes) accuracy of each method on each dataset, after it has been trained on ImageNet only and on All datasets, where the evaluation metric is the average rank. This corresponds to <ref type="table" target="#tab_0">Table 1</ref> in our case (or the more complete version in <ref type="table" target="#tab_4">Table 2</ref> in the Appendix). 2. The plots that measure robustness in variations of shots and ways. In our case these are <ref type="figure" target="#fig_0">Figures 2b and 2a</ref> in the main text for ImageNet-only training, and <ref type="figure" target="#fig_3">Figures 5b and 5a</ref> in the Appendix for the case of training on all datasets.</p><p>We propose to use both of these aspects to evaluate performance on META-DATASET: it is not only desirable to perform well on average, but also to perform well under different specifications of test tasks, as it is not realistic in general to assume that we will know in advance what setup (number of ways and shots) will be encountered at test time. Our final source code will include scripts for generating these plots and for automatically computing ranks given a table to help standardize the procedure for reporting results.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DETAILS OF META-DATASET'S SAMPLING ALGORITHM</head><p>We now provide a complete description of certain steps that were explained on a higher level in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STEP 1: SAMPLING THE EPISODE'S CLASS SET</head><p>ImageNet class sampling The procedure we use for sampling classes for an ImageNet episode is the following. First, we sample a node uniformly at random from the set of 'eligible' nodes of the DAG structure corresponding to the specified split (train, validation or test). An internal node is 'eligible' for this selection if it spans at least 5 leaves, but no more than 392 leaves. The number 392 was chosen because it is the smallest number so that, collectively, all eligible internal nodes span all leaves in the DAG. Once an eligible node is selected, some of the leaves that it spans will constitute the classes of the episode. Specifically, if the number of those leaves is no greater than 50, we use all of them. Otherwise, we randomly choose 50 of them.</p><p>This procedure enables the creation of tasks of varying degrees of fine-grainedness. For instance, if the sampled internal node has a small height, the leaf classes that it spans will represent semanticallyrelated concepts, thus posing a fine-grained classification task. As the height of the sampled node increases, we 'zoom out' to consider a broader scope from which we sample classes and the resulting episodes are more coarse-grained. where C is the set of selected classes and Im(c) denotes the set of images belonging to class c. The min over classes ensures that each class has at least q images to add to the query set, thus allowing it to be class-balanced. The 0.5 multiplier ensures that enough images of each class will be available to add to the support set, and the minimum with 10 prevents the query set from being too large. b) Computing the support set size We compute the total support set size as:</p><formula xml:id="formula_4">|S| = min 500, c∈C β min{100, |Im(c)| − q}</formula><p>where β is a scalar sampled uniformly from interval (0, 1]. Intuitively, each class on average contributes either all its remaining examples (after placing q of them in the query set) if there are less than 100 or 100 otherwise, to avoid having too large support sets. The multiplication with β enables the potential generation of smaller support sets even when multiple images are available, since we are also interested in examining the very-low-shot end of the spectrum. The 'ceiling' operation ensures that each selected class will have at least one image in the support set. Finally, we cap the total support set size to 500.</p><p>c) Computing the shot of each class We are now ready to compute the 'shot' of each class. Specifically, the proportion of the support set that will be devoted to class c is computed as:</p><formula xml:id="formula_5">R c = exp(α c )|Im(c)| c ∈C exp(α c )|Im(c )|</formula><p>where α c is sampled uniformly from the interval [log(0.5), log <ref type="formula">(2)</ref>). Intuitively, the un-normalized proportion of the support set that will be occupied by class c is a noisy version of the total number of images of that class in the dataset Im(c). This design choice is made in the hopes of obtaining realistic class ratios, under the hypothesis that the dataset class statistics are a reasonable approximation of the real-world statistics of appearances of the corresponding classes. The shot of a class c is then set to:</p><formula xml:id="formula_6">k c = min { R c * (|S| − |C|) + 1, |Im(c)| − q}</formula><p>which ensures that at least one example is selected for each class, with additional examples selected proportionally to R c , if enough are available.</p><p>.3 DATASETS META-DATASET is formed of data originating from 10 different image datasets. A complete list of the datasets we use is the following. ILSVRC-2012 (ImageNet, <ref type="bibr" target="#b21">Russakovsky et al., 2015)</ref> A dataset of natural images from 1000 categories <ref type="figure" target="#fig_2">(Figure 4a</ref>). We removed some images that were duplicates of images in another dataset in META-DATASET (43 images that were also part of Birds) or other standard datasets of interest (92 from Caltech-101 and 286 from Caltech-256). The complete list of duplicates is part of the source code release.</p><p>Omniglot <ref type="bibr" target="#b11">(Lake et al., 2015)</ref> A dataset of images of 1623 handwritten characters from 50 different alphabets, with 20 examples per class <ref type="figure" target="#fig_2">(Figure 4b</ref>). While recently <ref type="bibr" target="#b29">Vinyals et al. (2016)</ref> proposed a new split for this dataset, we instead make use of the original intended split Lake et al. <ref type="formula">(2015)</ref> which is more challenging since the split is on the level of alphabets (30 training alphabets and 20 evaluation alphabets), not characters from those alphabets, therefore posing a more challenging generalization problem. Out of the 30 training alphabets, we hold out the 5 smallest ones (i.e., with the least number of character classes) to form our validation set, and use the remaining 25 for training.</p><p>Aircraft <ref type="bibr" target="#b13">(Maji et al., 2013)</ref> A dataset of images of aircrafts spanning 102 model variants, with 100 images per class <ref type="figure" target="#fig_2">(Figure 4c</ref>). The images are cropped according to the providing bounding boxes, in order not to include other aircrafts, or the copyright text at the bottom of images. <ref type="bibr" target="#b30">-200-2011</ref><ref type="bibr">(Birds, Wah et al., 2011</ref> A dataset for fine-grained classification of 200 different bird species <ref type="figure" target="#fig_2">(Figure 4d</ref>). We did not use the provided bounding boxes to crop the images, instead the full images are used, which provides a harder challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB</head><p>Describable Textures (DTD, <ref type="bibr" target="#b3">Cimpoi et al., 2014)</ref> A texture database, consisting of 5640 images, organized according to a list of 47 terms (categories) inspired from human perception <ref type="figure" target="#fig_2">(Figure 4e</ref>).</p><p>Quick Draw <ref type="bibr" target="#b9">(Jongejan et al., 2016)</ref> A dataset of 50 million black-and-white drawings across 345 categories, contributed by players of the game Quick, Draw! <ref type="figure" target="#fig_2">(Figure 4f</ref>).</p><p>Fungi <ref type="bibr" target="#b26">(Schroeder &amp; Cui, 2018)</ref> A large dataset of approximately 100K images of nearly 1,500 wild mushrooms species <ref type="figure" target="#fig_2">(Figure 4g</ref>).</p><p>VGG Flower <ref type="bibr" target="#b16">(Nilsback &amp; Zisserman, 2008)</ref> A dataset of natural images of 102 flower categories.</p><p>The flowers chosen to be ones commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images <ref type="figure" target="#fig_2">(Figure 4h</ref>).</p><p>Traffic Signs <ref type="bibr" target="#b8">(Houben et al., 2013)</ref> A dataset of 50,000 images of German road signs in 43 classes <ref type="figure" target="#fig_2">(Figure 4i)</ref>. <ref type="bibr">MSCOCO Lin et al. (2014)</ref> A dataset of images collected from Flickr with 1.5 million object instances belonging to 80 classes labelled and localized using bounding boxes. We choose the train2017 split and create images crops from original images using each object instance's groundtruth bounding box <ref type="figure" target="#fig_2">(Figure 4j</ref>).</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HYPERPARAMETERS</head><p>We used three architectures: a commonly-used four-layer convolutional network, an 18-layer residual network and a wide residual network. While some of the baseline models performed best with the latter, we noticed that the meta-learners preferred the resnet-18 backbone and rarely the four-layerconvnet. For Relation Networks only, we also allow the option to use another architecture, aside from the aforementioned three, inspired by the four-layer-convnet used in the Relation Networks paper <ref type="bibr" target="#b28">(Sung et al., 2018)</ref>. The main difference is that they used the usual max-pooling operation only in the first two layers, omitting it in the last two, yielding activations of larger spatial dimensions. In our case, we found that these increased spatial dimensions did not fit in memory, so as a compromise we used max-pooling on the first 3 out of the 4 layer of the convnet.</p><p>For fo-MAML and fo-Proto-MAML, we tuned the inner-loop learning rate, the number of inner loop steps, and the number of additional such steps to be performed in evaluation (i.e., validation or test) episodes.</p><p>For the baselines, we tuned whether the cosine classifier of Baseline++ will be used, as opposed to a standard forward pass through a linear classification layer. Also, since <ref type="bibr" target="#b2">Chen et al. (2019)</ref> added weight normalization <ref type="bibr" target="#b23">(Salimans &amp; Kingma, 2016)</ref> to their implementation of the cosine classifier layer, we also implemented this and created a hyperparameter choice for whether or not it is enabled. This hyperparameter is independent from the one that decides if the cosine classifier is used. Both are applicable to the k-NN Basline (for its all-way training classification task) and to the Finetune Baseline (both for its all-way training classification and for its within-episode classification at validation and test times). For the Finetune Baseline, we tuned a binary hyperparameter deciding if gradient descent or ADAM is used for the within-task optimization. We also tuned the decision of whether all embedding layers are finetuned or, alternatively, the embedding is held fixed and only the final classifier on top of it is optimized. Finally, we tuned the number of finetuning steps that will be carried out.</p><p>We also tried two different image resolutions: the commonly-used 84x84 and 126x126. Finally, we tuned the learning rate schedule and weight decay and we used ADAM to train all of our models. All other details, dataset splits and the complete set of best hyperparameters discovered for each model are included in the source code.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPLETE MAIN RESULTS AND RANK COMPUTATION</head><p>Rank computation We rank models by decreasing order of accuracy and handle ties by assigning tied models the average of their ranks. A tie between two models occurs when a 95% confidence interval statistical test on the difference between their mean accuracies is inconclusive in rejecting the null hypothesis that this difference is 0. Our recommendation is that this test is ran to determine if ties occur. As mentioned earlier, our source code will include this computation.</p><p>Complete main tables For completeness, <ref type="table" target="#tab_4">Table 2</ref> presents a more detailed version of <ref type="table" target="#tab_0">Table 1</ref> that also displays confidence intervals and per-dataset ranks computed using the above procedure.</p><p>.6 ANALYSIS OF PERFORMANCE ACROSS SHOTS AND WAYS For completeness, in <ref type="figure" target="#fig_3">Figure 5</ref> we show the results of the analysis of the robustness to different ways and shots for the variants of the models that were trained on all datasets. We observe the same trends as discussed in our Experiments section for the variants of the models that were trained on ImageNet.  <ref type="figure">Figure 6</ref> shows side-to-side the performance of each model trained on ILSVRC only vs. all datasets. The difference between the performance of the all-dataset trained models versus the ImageNet-only trained ones is also visualized in <ref type="figure">Figure 1</ref> in the main paper.</p><p>As discussed in the main paper, we notice that we do not always observe a clear generalization advantage in training from a wider collection of image datasets. While some of the datasets that were added to the meta-training phase did see an improvement across all models, in particular for Omniglot and Quick Draw, this was not true across the board. In fact, in certain cases the performance is slightly worse. We believe that more successfully leveraging diverse sources of data is an interesting open research problem.  <ref type="formula">(6)</ref> 76.08±0.76 <ref type="formula">(7)</ref> 88.72±0.67</p><p>(1)</p><p>Traffic Signs 40.11±1.10 <ref type="formula">(6)</ref> 66.74±1.23</p><p>(1) 55.57±1.08 <ref type="formula">(2)</ref> 46.48±1.00 <ref type="formula">(4)</ref> 42.91±1.31 <ref type="formula">(5)</ref> 37.48±0.93 <ref type="formula">(7)</ref> 52.42±1.08 <ref type="formula">(3)</ref> MSCOCO 29.55±0.96 <ref type="formula">(5)</ref> 35.17±1.08 <ref type="formula">(3)</ref> 28.79±0.96 <ref type="formula">(5)</ref> 39.87±1.06 <ref type="formula">(2)</ref> 29.37±1.08 <ref type="formula">(5)</ref> 27.41±0.89 <ref type="formula">(7)</ref> 41.74±1.13</p><p>(1)</p><p>Avg. rank  For each meta-learner, we selected the best model (based on validation on ImageNet's validation split) out of the ones that used the pre-trained initialization, and the best out of the ones that trained from scratch. We then ran the evaluation of each on (the test split of) all datasets in order to quantify how beneficial this pre-trained initialization is. We performed this experiment twice: for the models that are trained on ImageNet only and for the models that are trained on (the training splits of) all datasets.</p><p>The results of this investigation were reported in the main paper in <ref type="figure">Figure 3a</ref> and <ref type="figure">Figure 3b</ref>, for ImageNet-only training and all dataset training, respectively. We show the same results in <ref type="figure" target="#fig_5">Figure 7</ref>, printed larger to facilitate viewing of error bars. For easier comparison, we also plot the difference in performance of the models that were pre-trained over the ones that weren't, in <ref type="figure">Figures 8a and 8b</ref>. These figures make it easier to spot that while using the pre-trained solution usually helps for datasets that are visually not too different from ImageNet, it may hurt for datasets that are significantly different from it, such as Omniglot, Quickdraw (and surprisingly Aircraft). Note that these three datasets are the same three that we found benefit from training on All datasets instead of ImageNet-only. It appears that using the pre-trained solution biases the final solution to specialize on ImageNet-like datasets.</p><p>.9 EFFECT OF META-LEARNING VERSUS INFERENCE-ONLY <ref type="figure" target="#fig_6">Figure 9</ref> shows the same plots as in <ref type="figure">Figures 3c and 3d</ref> but printed larger to facilitate viewing of error bars. Furthermore, as we have done for visualizing the observed gain of pre-training, we also present in <ref type="figure">Figures 10a and 10b</ref> the gain observed from meta-learning as opposed to training the corresponding inference-only baseline, as explained in the Experiments section of the main paper. This visulization makes it clear that while meta-training usually helps on ImageNet (or doesn't hurt too much), it sometimes hurts when it is performed on all datasets, emphasizing the need for further research into best practices of meta-learning across heterogeneous sources.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">FINEGRAINEDNESS ANALYSIS</head><p>We investigate the hypothesis that finer-grained tasks are more challenging than coarse-grained ones by creating binary ImageNet episodes with the two classes chosen uniformly at random from the DAG's set of leaves. We then define the degree of coarse-grainedness of a task as the height of the lowest common ancestor of the two chosen leaves, where the height is defined as the length of the longest path from the lowest common ancestor to one of the selected leaves. Larger heights then correspond to coarser-grained tasks. We present these results in <ref type="figure">Figure 11</ref>. We do not detect a significant trend when performing this analysis on the test DAG. The results on the training DAG,      <ref type="figure">Figure 11</ref>: Analysis of performance as a function of the degree of fine-grainedness. Larger heights correspond to coarser-grained tasks. The bands display 95% confidence intervals. The camera-ready version includes updated results for MAML and Proto-MAML following an external suggestion to experiment with larger values for the inner-loop learning rate α of MAML. We found that re-doing our hyperparameter search with a revised range that includes larger α values significantly improved fo-MAML's performance on META-DATASET. For consistency, we applied the same change to fo-Proto-MAML and re-ran those experiments too.</p><p>We found that the value of this α that performs best for fo-MAML both for training on ImageNet only and training on all datasets is approximately 0.1, which is an order of magnitude larger than our previous best value. Interestingly, fo-Proto-MAML does not choose such a large α value, with best α being 0.0054 when training on ImageNet only and 0.02 when training on all datasets. Plausibly this difference can be attributed to the better initialization of Proto-MAML which requires a less aggressive optimization for the adaptation to each new task. This hypothesis is also supported by the fact that fo-Proto-MAML chooses to take fewer adaptation steps than fo-MAML does. The complete set of best discovered hyperparameters is available in our public code.</p><p>To emphasize the importance of properly tuning this hyperparameter, <ref type="table" target="#tab_9">Table 3</ref> displays the previous best and the new best fo-MAML results side-by-side, showcasing the large performance gap when using the appropriate value for α.</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">THE CHOICE OF A META-VALIDATION PROCEDURE FOR META-DATASET</head><p>The design choice we made, as discussed in the main paper, is to use (the meta-validation set of) ImageNet only for model selection in all of our experiments. In the absence of previous results on the topic, this is a reasonable strategy since ImageNet has been known to consitute a useful proxy for performance on other datasets. However, it is likely that this is not the optimal choice: there might be certain hyperparameters for a given model that work best for held-out ImageNet episodes, but not for held-out episodes of other datasets. An alternative meta-validation scheme would be to use the average (across datasets) validation accuracy as an indicator for early stopping and model selection.</p><p>We did not choose this method due to concerns about the reliability of this average performance. Notably, taking a simple average would over-emphasize larger datasets, or might over-emphasize datasets with natural images (as opposed to Omniglot and Quickdraw). Nevertheless, whether this strategy is beneficial is an interesting empirical question.</p><p>.13 ADDITIONAL PER-DATASET ANALYSIS OF SHOTS AND WAYS</p><p>In our previous analysis of performance across different shots and ways <ref type="figure" target="#fig_0">(Figures 2b, 2a, and 5)</ref>, the performance is averaged over all evaluation datasets. In this section we further break down those plots by presenting the results separately for each dataset. <ref type="figure" target="#fig_0">Figures 12 and 13</ref> show the analysis of performance as a function of ways and shots (respectively) for each evaluation dataset, for models that were trained on ImageNet only. For completeness, <ref type="figure" target="#fig_2">Figures 14 and 15</ref> show the same for the models trained on (the training splits of) all datasets.  <ref type="figure" target="#fig_3">Figure 15</ref>: The performance across different shots, with 95% confidence intervals, shown separately for each evaluation dataset. All models had been trained on (the training splits of) all datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The effect of different ways and shots on test performance (w/ 95% confidence intervals) when training on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>STEP 2 :</head><label>2</label><figDesc>SAMPLING THE EPISODE'S EXAMPLES a) Computing the query set size The query set is class-balanced, reflecting the fact that we care equally to perform well on all classes of an episode. The number of query images per class is computed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Training examples taken from the various datasets forming META-DATASET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Analysis of performance as a function of the episode's way, shots for models whose training source is (the training data of) all datasets. The bands display 95% confidence intervals. .7 EFFECT OF TRAINING ON ALL DATASETS OVER TRAINING ON ILSVRC-2012 ONLY For more clearly observing whether training on all datasets leads to improved generalization over training on ImageNet only,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Comparing pre-training to starting from scratch. Same plots as Figure 3a and Figure 3b, only larger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Comparing the meta-trained variant of meta-learners against their inference-only counterpart. Same plots asFigure 3candFigure 3d, only larger. The gain from meta-training on All datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :Figure 13 :Figure 14 :</head><label>121314</label><figDesc>The performance across different ways, with 95% confidence intervals, shown separately for each evaluation dataset. All models had been trained on ImageNet-only. The performance across different shots, with 95% confidence intervals, shown separately for each evaluation dataset. All models had been trained on ImageNet-only. The performance across different ways, with 95% confidence intervals, shown separately for each evaluation dataset. All models had been trained on (the training splits of) all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Few-shot classification results on META-DATASET using models trained on ILSVRC-2012 only (top) and trained on all datasets (bottom).</figDesc><table><row><cell>Test Source</cell><cell>k-NN</cell><cell>Finetune</cell><cell>MatchingNet</cell><cell>ProtoNet</cell><cell>fo-MAML</cell><cell>RelationNet</cell><cell>fo-Proto-MAML</cell></row><row><cell>ILSVRC</cell><cell>41.03</cell><cell>45.78</cell><cell>45.00</cell><cell>50.50</cell><cell>45.51</cell><cell>34.69</cell><cell>49.53</cell></row><row><cell>Omniglot</cell><cell>37.07</cell><cell>60.85</cell><cell>52.27</cell><cell>59.98</cell><cell>55.55</cell><cell>45.35</cell><cell>63.37</cell></row><row><cell>Aircraft</cell><cell>46.81</cell><cell>68.69</cell><cell>48.97</cell><cell>53.10</cell><cell>56.24</cell><cell>40.73</cell><cell>55.95</cell></row><row><cell>Birds</cell><cell>50.13</cell><cell>57.31</cell><cell>62.21</cell><cell>68.79</cell><cell>63.61</cell><cell>49.51</cell><cell>68.66</cell></row><row><cell>Textures</cell><cell>66.36</cell><cell>69.05</cell><cell>64.15</cell><cell>66.56</cell><cell>68.04</cell><cell>52.97</cell><cell>66.49</cell></row><row><cell>Quick Draw</cell><cell>32.06</cell><cell>42.60</cell><cell>42.87</cell><cell>48.96</cell><cell>43.96</cell><cell>43.30</cell><cell>51.52</cell></row><row><cell>Fungi</cell><cell>36.16</cell><cell>38.20</cell><cell>33.97</cell><cell>39.71</cell><cell>32.10</cell><cell>30.55</cell><cell>39.96</cell></row><row><cell>VGG Flower</cell><cell>83.10</cell><cell>85.51</cell><cell>80.13</cell><cell>85.27</cell><cell>81.74</cell><cell>68.76</cell><cell>87.15</cell></row><row><cell>Traffic Signs</cell><cell>44.59</cell><cell>66.79</cell><cell>47.80</cell><cell>47.12</cell><cell>50.93</cell><cell>33.67</cell><cell>48.83</cell></row><row><cell>MSCOCO</cell><cell>30.38</cell><cell>34.86</cell><cell>34.99</cell><cell>41.00</cell><cell>35.30</cell><cell>29.15</cell><cell>43.74</cell></row><row><cell>Avg. rank</cell><cell>5.7</cell><cell>2.9</cell><cell>4.65</cell><cell>2.65</cell><cell>3.7</cell><cell>6.55</cell><cell>1.85</cell></row><row><cell>Test Source</cell><cell>k-NN</cell><cell>Finetune</cell><cell>MatchingNet</cell><cell>ProtoNet</cell><cell>fo-MAML</cell><cell>RelationNet</cell><cell>fo-Proto-MAML</cell></row><row><cell>ILSVRC</cell><cell>38.55</cell><cell>43.08</cell><cell>36.08</cell><cell>44.50</cell><cell>37.83</cell><cell>30.89</cell><cell>46.52</cell></row><row><cell>Omniglot</cell><cell>74.60</cell><cell>71.11</cell><cell>78.25</cell><cell>79.56</cell><cell>83.92</cell><cell>86.57</cell><cell>82.69</cell></row><row><cell>Aircraft</cell><cell>64.98</cell><cell>72.03</cell><cell>69.17</cell><cell>71.14</cell><cell>76.41</cell><cell>69.71</cell><cell>75.23</cell></row><row><cell>Birds</cell><cell>66.35</cell><cell>59.82</cell><cell>56.40</cell><cell>67.01</cell><cell>62.43</cell><cell>54.14</cell><cell>69.88</cell></row><row><cell>Textures</cell><cell>63.58</cell><cell>69.14</cell><cell>61.80</cell><cell>65.18</cell><cell>64.16</cell><cell>56.56</cell><cell>68.25</cell></row><row><cell>Quick Draw</cell><cell>44.88</cell><cell>47.05</cell><cell>60.81</cell><cell>64.88</cell><cell>59.73</cell><cell>61.75</cell><cell>66.84</cell></row><row><cell>Fungi</cell><cell>37.12</cell><cell>38.16</cell><cell>33.70</cell><cell>40.26</cell><cell>33.54</cell><cell>32.56</cell><cell>41.99</cell></row><row><cell>VGG Flower</cell><cell>83.47</cell><cell>85.28</cell><cell>81.90</cell><cell>86.85</cell><cell>79.94</cell><cell>76.08</cell><cell>88.72</cell></row><row><cell>Traffic Signs</cell><cell>40.11</cell><cell>66.74</cell><cell>55.57</cell><cell>46.48</cell><cell>42.91</cell><cell>37.48</cell><cell>52.42</cell></row><row><cell>MSCOCO</cell><cell>29.55</cell><cell>35.17</cell><cell>28.79</cell><cell>39.87</cell><cell>29.37</cell><cell>27.41</cell><cell>41.74</cell></row><row><cell>Avg. rank</cell><cell>5.05</cell><cell>3.6</cell><cell>4.95</cell><cell>2.85</cell><cell>4.25</cell><cell>5.8</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Effect of training on all datasets instead of ImageNet only It's interesting to examine whether training on (the training splits of) all datasets leads to improved generalizaton compared to training on (the training split of) ImageNet only. Specifically, while we might expect that training on more data helps improve generalization, it is an empirical question whether that still holds for heterogeneous data. We can examine this by comparing the performance of each model between the top and bottom sets of results ofTable 1, corresponding to the two training sources (ImageNet only and all datasets, respectively). For convenience,Figure 1visualizes this difference in a barplot. Notably, for Omniglot, Quick Draw and Aircraft we observe a substantial increase across the board from training on all The performance difference on test datasets, when training on all datasets instead of ILSVRC only. A positive value indicates an improvement from all-dataset training.</figDesc><table><row><cell>20 10 0 10 20 30 40 50 ILSVRC 20 10 0 10 20 30 40 50 Quick Draw</cell><cell>Omniglot Fungi</cell><cell>Aircraft VGG Flower Traffic Sign CU Birds</cell><cell>Textures MSCOCO</cell><cell>Model k-NN baseline Finetune baseline MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Hugo, and Kevin came up with the benchmark idea and requirements. Eleni developed the core of the project, and worked on the experiment design and management with Tyler and Kevin, as well as experiment analysis. Carles, Ross, Kelvin, Pascal, Vincent, and Tyler helped extend the benchmark by adding datasets. Eleni, Vincent, and Utku contributed the Prototypical Networks, Matching Networks, and Relation Networks implementations, respectively. Tyler implemented baselines, MAML (with Kevin) and Proto-MAML models, and updated the backbones to support them. Writing was mostly led by Eleni, with contributions by Hugo, Vincent, and Kevin and help from Tyler and Pascal for visualizations. Pascal and Pierre-Antoine worked on code organization, efficiency, and open-sourcing,</figDesc><table><row><cell>AUTHOR CONTRIBUTIONS</cell></row><row><cell>Eleni,</cell></row></table><note>Pascal and Vincent optimized the efficiency of the data input pipeline. Pierre-Antoine supervised the code development process and reviewed most of the changes, Hugo and Kevin supervised the overall direction of the research.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Few-shot classification results on META-DATASET.</figDesc><table><row><cell>Test Source</cell></row></table><note>(a) Models trained on ILSVRC-2012 only.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 6: Accuracy on the test datasets, when training on ILSVRC only or All datasets (same results as shown in the main tables). The bars display 95% confidence intervals.</figDesc><table><row><cell>0 10 20 30 40 50 60 70 80 90 ILSVRC 0 10 20 30 40 90 Quick Draw 80 70 60 50</cell><cell>Omniglot Fungi</cell><cell>Aircraft VGG Flower Traffic Sign CU Birds</cell><cell>Textures MSCOCO</cell><cell>Model k-NN baseline Finetune baseline MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML Train Source ILSVRC-2012 All datasets</cell></row><row><cell cols="4">.8 EFFECT OF PRE-TRAINING VERSUS TRAINING FROM SCRATCH</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The pre-trained weights that we consider are the ones that the k-NN baseline converged to when it was trained on ImageNet. Positive values indicate that this pre-training was beneficial.</figDesc><table><row><cell>15 10 5 0 5 10 15 20 25 30 ILSVRC 15 10 5 0 5 10 15 20 25 30 Quick Draw (a) The gain from pre-training (ImageNet). Omniglot Aircraft CU Birds Textures Model MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML Fungi VGG Flower Traffic Sign MSCOCO Figure 8: The performance difference of initializing the embedding weights from a pre-trained 20 10 0 10 20 30 40 ILSVRC Omniglot Aircraft CU Birds Textures Model MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML 20 10 0 10 20 30 40 Quick Draw Fungi VGG Flower Traffic Sign MSCOCO (b) The gain from pre-training (All datasets). solution, before episodically training on ImageNet or all datasets, over using a random initialization 30 40 50 60 70 80 Omniglot Aircraft CU Birds Textures 90 ILSVRC Model MatchingNet ProtoNet fo-Proto-MAML 20 10 0 10 20 30 40 50 60 70 80 90 Quick Draw Fungi VGG Flower Traffic Sign MSCOCO Meta-training Inference-only (a) ImageNet. 0 10 20 30 40 50 60 70 80 90 ILSVRC Omniglot Aircraft CU Birds Textures Model MatchingNet ProtoNet fo-Proto-MAML 0 10 20 30 40 50 60 70 80 90 Quick Draw Fungi VGG Flower Traffic Sign MSCOCO Meta-training Inference-only of those weights. 0 (b) All datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 10: The performance difference of meta-learning over the corresponding inference-only baseline of each meta-learner. Positive values indicate that meta-learning was beneficial.</figDesc><table><row><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.60 0.65 0.70 0.75 0.80 0.90 0.85 Accuracy</cell><cell>1</cell><cell>2</cell><cell>3 Height of Lowest Common Ancestor 4</cell><cell>5 Model k-NN baseline Finetune baseline MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML</cell><cell>6</cell><cell>0.9 0.5 0.6 0.7 0.8 Accuracy</cell><cell>0</cell><cell>2</cell><cell>4 Height of Lowest Common Ancestor 6 8</cell><cell>10 Model k-NN baseline Finetune baseline 12 MatchingNet ProtoNet fo-MAML RelationNet fo-Proto-MAML</cell></row><row><cell cols="6">(a) Fine-grainedness Analysis (on ImageNet's test</cell><cell cols="5">(b) Fine-grainedness Analysis (on ImageNet's train</cell></row><row><cell cols="2">graph)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">graph graph)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Improvement of fo-MAML when using a larger inner learning rate α.(a) Models trained on ILSVRC-2012 only. seem to indicate that our hypothesis holds to some extent. We conjecture that this may be due to the richer structure of the training DAG, but we encourage further investigation..11 THE IMPORTANCE OF MAML'S INNER-LOOP LEARNING RATE HYPERPARAMETER.</figDesc><table><row><cell>Test Source</cell><cell cols="2">Method: Accuracy (%) ± confidence (%) fo-MAML α = 0.01 (old) fo-MAML α ≈ 0.1</cell></row><row><cell>ILSVRC</cell><cell>36.09±1.01</cell><cell>45.51±1.11</cell></row><row><cell>Omniglot</cell><cell>38.67±1.39</cell><cell>55.55±1.54</cell></row><row><cell>Aircraft</cell><cell>34.50±0.90</cell><cell>56.24±1.11</cell></row><row><cell>Birds</cell><cell>49.10±1.18</cell><cell>63.61±1.06</cell></row><row><cell>Textures</cell><cell>56.50±0.80</cell><cell>68.04±0.81</cell></row><row><cell>Quick Draw</cell><cell>27.24±1.24</cell><cell>43.96±1.29</cell></row><row><cell>Fungi</cell><cell>23.50±1.00</cell><cell>32.10±1.10</cell></row><row><cell cols="2">VGG Flower 66.42±0.96</cell><cell>81.74±0.83</cell></row><row><cell cols="2">Traffic Signs 33.23±1.34</cell><cell>50.93±1.51</cell></row><row><cell>MSCOCO</cell><cell>27.52±1.11</cell><cell>35.30±1.23</cell></row><row><cell></cell><cell cols="2">(b) Models trained on all datasets.</cell></row><row><cell>Test Source</cell><cell cols="2">Method: Accuracy (%) ± confidence (%) fo-MAML α = 0.01 (old) fo-MAML α ≈ 0.1</cell></row><row><cell>ILSVRC</cell><cell>32.36±1.02</cell><cell>37.83±1.01</cell></row><row><cell>Omniglot</cell><cell>71.91±1.20</cell><cell>83.92±0.95</cell></row><row><cell>Aircraft</cell><cell>52.76±0.90</cell><cell>76.41±0.69</cell></row><row><cell>Birds</cell><cell>47.24±1.14</cell><cell>62.43±1.08</cell></row><row><cell>Textures</cell><cell>56.66±0.74</cell><cell>64.16±0.83</cell></row><row><cell>Quick Draw</cell><cell>50.50±1.19</cell><cell>59.73±1.10</cell></row><row><cell>Fungi</cell><cell>21.02±0.99</cell><cell>33.54±1.11</cell></row><row><cell cols="2">VGG Flower 70.93±0.99</cell><cell>79.94±0.84</cell></row><row><cell cols="2">Traffic Signs 34.18±1.26</cell><cell>42.91±1.31</cell></row><row><cell>MSCOCO</cell><cell>24.05±1.10</cell><cell>29.37±1.08</cell></row><row><cell>though, do</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/google-research/meta-dataset 2 github.com/google-research/meta-dataset/tree/master/meta_dataset/dataset_conversion/splits</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We improved MAML's performance significantly in the time between the acceptance decision and publication, thanks to a suggestion to use a more aggressive inner-loop learning rate. In the Appendix, we present the older MAML results side by side with the new ones, and discuss the importance of this hyperparameter for MAML.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Jongmin Kim, and Nick Fox-Gieg. The Quick, Draw! -A.I. experiment. quickdraw.withgoogle.com</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Kawashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft. arXiv, abs/1306</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">FGVCx fungi classification challenge 2018. github.com/ visipedia/fgvcx_fungi_comp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigit</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
