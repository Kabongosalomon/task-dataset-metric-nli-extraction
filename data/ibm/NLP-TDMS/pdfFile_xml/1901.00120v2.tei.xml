<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Systems Engineering Discipline</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Monash University Malaysia</orgName>
								<address>
									<settlement>Bandar Sunway, Selangor</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Bioinformatics Institute</orgName>
								<address>
									<addrLine>30 Biopolis Street</addrLine>
									<postCode>138671</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Image and Pervasive Access Lab (IPAL)</orgName>
								<orgName type="institution">CNRS UMI 2955</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Singapore Eye Research Institute</orgName>
								<address>
									<addrLine>20 College Road</addrLine>
									<postCode>169856</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Oklahoma</orgName>
								<address>
									<postCode>73019</postCode>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2019.Doi</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lung cancer is the second most common cancer in men and women <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In 2018 alone, there have been approximately 234,030 new cases and 154,050 deaths from lung cancer <ref type="bibr" target="#b1">[2]</ref> altogether, which makes it by far the most common cause of cancer death among both men and women. Every year, more people die of lung cancer than of colon, breast, and prostate cancers combined <ref type="bibr" target="#b1">[2]</ref>. Early diagnosis of lung cancer is extremely important for early treatment and cure of the disease. Early-stage lung cancer is typified by a small nodule, which can be detected as a round, spherical structure in computed tomography (CT) scans <ref type="bibr" target="#b2">[3]</ref>. Doctors typically extract multiple features/characteristics of nodules from CT scans, such as size, morphology, contours, interval growth between CT examinations, multiplicity, location, and calcifications <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These characteristics help to classify a nodule as benign (non-cancerous) or malignant (cancerous), e.g., malignant nodules frequently have more irregular boundaries/margins as compared to benign nodules, which generally have more smooth boundaries <ref type="bibr" target="#b4">[5]</ref>.</p><p>In recent years, Convolution Neural Networks (CNNs) have been gaining widespread popularity in general applications <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and have also been applied in recent studies <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> to classify/detect lung nodules. However, one of the main issues facing Computer-Aided Diagnosis (CAD) schemes for lung nodule detection and classification is the wide variation of nodule sizes. In our preliminary analysis, we observed that most nodules that are malignant tend to have a bigger nodule size/diameter than benign nodules <ref type="bibr" target="#b4">[5]</ref>. However, the CAD schemes proposed in all the previous studies <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> did not thoroughly examine the relationship between nodule size and malignancy in their proposed methodologies/approaches to solve the nodule classification problem.</p><p>To perform our preliminary analysis and throughout this study, we used 1,018 CT scans from the public Lung Image Database Consortium and Image Database Resource Initiative (LIDC-LDRI) dataset, which were collated and released by the National Institutes of Health (NIH) <ref type="bibr" target="#b10">[11]</ref>. The nodule diameters in the LIDC-IDRI dataset range between 3 to 30 millimeters (mm). <ref type="figure" target="#fig_0">Figure 1</ref> displays the nodule diameter distribution of all the nodules in the LIDC-LDRI dataset. From <ref type="figure" target="#fig_0">Figure 1</ref>, we observe that the malignant nodules in the dataset have diameters that typically exceed 12 mm. On the other hand, the benign nodules have diameters that are generally less than 5 mm. These two distributions describe the nodules that are "easy" to classify based on their nodule size/diameter. Namely, by applying a simple thresholding operator (i.e., nodule is malignant if size &gt; 12 mm, benign if size &lt; 5 mm), one can likely obtain a good/reasonable nodule classification score. However, the nodule sizes between 5 to 12 mm including the intersection of the two malignant and benign distributions in <ref type="figure" target="#fig_0">Figure 1</ref> (at 8 mm) represent the nodules that are "difficult" to classify in this dataset. There is no straightforward/easy way to classify these nodules, and a novel methodology is required to more accurately classify the nodules in this size range, which has not been thoroughly examined in the previous studies. Problems of this nature are generally known as multi-scale problems in the field of computer vision. Over the last few years, researchers have proposed various scale-invariant models to resolve this issue <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Most of these methods transform an image into multiple sizes, and these images are then forwarded to a classification network. The transformation step can be applied at the beginning/start, middle stage, or at the end of the network. More recently, Shen et al. <ref type="bibr" target="#b14">[15]</ref> applied a Multi Crop Convolutional Neural Network (MC-CNN) strategy to capture multi-scale features for the lung nodule classification task. Using the MC-CNN method, max-pooling, cropping, or both max-pooling and cropping steps are applied to the input features of the networks. The output of these operations are feature maps with different spatial sizes. However, the issue with this and other scale-invariant based methods is that the max-pooling operation throws away many features, and only retains/selects the maximum result. Consequently, the resulting output resolution of the features is considerably reduced.</p><p>In this paper, we present a novel and completely different strategy/approach to address the nodule size and classification problem. In our new approach, in contrast to the previous methods, instead of reducing the resolution of the features, we increased the local receptive fields of the convolutional filter to cover a wider image area without increasing the number of parameters. Our new approach is based on the Dilated Convolution or Atrous Convolution Neural Network <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The Dilated Convolutional Neural Network has been applied successfully as a replacement for pooling in semantic image segmentation <ref type="bibr" target="#b17">[18]</ref>, generic image classification <ref type="bibr" target="#b18">[19]</ref>, sound wave synthesis <ref type="bibr" target="#b19">[20]</ref>, and machine translation <ref type="bibr" target="#b20">[21]</ref>. To the best of our knowledge, this is the first time that this network is combined with a gating mechanism on a medical imaging based classification problem. Similar to <ref type="bibr" target="#b14">[15]</ref>, we use multiple filters to capture multiple-scale features. However, in contrast to <ref type="bibr" target="#b14">[15]</ref>, we do not reduce the feature resolution and/or discard any features.</p><p>Moreover, we also designed a novel context-aware sublayer to guide the features through the network layers, as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. The context-aware sub-layer generates signals that are responsible for closing or opening the gate that is located in front of each dilation neural network. This gives the network the ability to choose the right dilation for each nodule, depending on the nodule size as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Thus, our Gated-Dilated (GD) network is based on the principle of using multiple dilated convolutional neural networks to capture the scale-invariant features, as well as a new gating mechanism to guide these features in the network. We will describe the methodology and inner workings/mechanisms of our new GD network in detail in the Methods section (i.e., Section III). Overall, our main contributions in this study can be summarized as follows: 1) We propose a new multiple dilated CNN to capture scale-invariant features that more accurately classify the lung nodules as benign or malignant. 2) We propose a novel context-aware sub-layer to guide the features between the dilated convolutions. 3) We performed a comprehensive validation of the GD network on the public and comprehensive LIDC-IDRI lung dataset of 1,018 CT scans and achieved state-ofthe-art results.</p><p>The rest of the paper is organized as follows: The Related Work section that describes the background of the paper is presented in Section II. The Methods section is presented in Section III, which describes the proposed GD network in detail and also describes the experimental setup and methodology. The Results are described in Section IV.</p><p>Finally, the Discussion and the Conclusions of the study are provided in Sections V and VI, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>One of the few articles that analyzed other types of deep learning networks instead of CNN is Sun et al. <ref type="bibr" target="#b22">[22]</ref>. In ref. <ref type="bibr" target="#b22">[22]</ref>, the authors compared three different deep learning methods, namely CNN, Deep Belief Networks (DBNs), and Stacked Denoising Auto-Encoder (SDAE). The authors extracted 134,668 samples from the LIDC dataset. They obtained so many samples by enlarging their dataset through data augmentation procedures, namely rotation of the extracted regions of interest (ROIs) to four different directions. Their results showed that CNN outperformed DBN and SDAE, in terms of the accuracy and area under the receiver operating characteristic (ROC) curve (AUC) measurements.</p><p>Deep residual neural networks (e.g., Resnet) have been analyzed on generic datasets (e.g., CIFAR-10, CIFAR-100 and ImageNet) to tackle the vanishing-gradient problem in recent studies in the literature <ref type="bibr" target="#b6">[7]</ref>. By introducing "skip connections" in the network architecture, this enabled the deep neural network to have up to 1,000 layers. Nibali et al. <ref type="bibr" target="#b7">[8]</ref> applied three Resnet-18 network topologies/structures to the nodule classification task, whereby each of the three networks were applied to different views/planes of the nodule, namely the axial, coronal, and sagittal views. The outputs of the three different networks were then passed to a fully-connected multi-layer perceptron network. Before training their network with the LIDC-IDRI dataset, they first trained it with the generic and non-medical CIFAR-10 dataset, to analyze the effect of transfer learning on the accuracy of the malignancy classification.</p><p>Hussein et al. <ref type="bibr" target="#b23">[23]</ref> proposed using a three-dimensional (3D) CNN for lung nodule risk stratification, to utilize volumetric information in CT scans. Similar to ref. <ref type="bibr" target="#b7">[8]</ref>, the authors examined a transfer learning strategy to train their CNN. However, unlike ref. <ref type="bibr" target="#b7">[8]</ref> their CNN could not be trained on 2D images, such as CIFAR-10 or Imagenet due to its 3D topology/structure. Thus, the authors trained their 3D CNN on a sports dataset comprising of 1 million videos <ref type="bibr" target="#b24">[24]</ref>.</p><p>Inspired by evolutionary intelligence based methods, Silva et al <ref type="bibr" target="#b25">[25]</ref> examined an evolutionary CNN to classify lung nodules. Namely, the authors combined Particle Swarm Optimization (PSO) <ref type="bibr" target="#b26">[26]</ref>, genetic algorithms <ref type="bibr" target="#b27">[27]</ref> and CNNs to solve the nodule classification problem. First, the authors combined PSO and Otsu's algorithm <ref type="bibr" target="#b28">[28]</ref> to segment the nodules. The authors subsequently used a CNN to classify the nodules as malignant or benign. Designing a CNN requires knowledge of hyperparameters, such as the number of kernel filters and the number of neurons. Usually, these hyperparameters are fine-tuned by hand or by applying automatic algorithms to select the best hyperparameters, e.g., Grid-Search or Random Sampling <ref type="bibr" target="#b29">[29]</ref>. In contrast to the previous methods, the authors examined genetic algorithms to select the best hyperparameters of a CNN.</p><p>In ref. <ref type="bibr" target="#b30">[30]</ref>, Liu et al. proposed a multi-view CNN applied to a lung nodule detection problem. The authors utilized three scales and four views, resulting in 12 different images altogether. Each image was used for training a CNN separately, following which, the authors combined all the models and re-trained the CNN again. During the preprocessing stages, the authors applied linear interpolation, normalized spherical sampling using an icosahedron positioned at the nodule centers, and nodule radius approximation by thresholding. Using their proposed approach, the authors achieved a high classification rate of 92.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III . METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In this section, we describe our new Dilated Convolution and</head><p>Gated-Dilated sub-network method in detail. In Section III.A, we first formulate and explain the dilated convolution network and our new Gated-Dilated Layer. We then explain the overall network architecture in Section III.B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. THE GATED-DILATED LAYER</head><p>The dilated convolution operation is a variation of the standard/regular convolution operation <ref type="bibr" target="#b17">[18]</ref>. Namely, if we have input features, X (e.g., image pixels) and a filter, K we can write the convolution operation as follows:</p><formula xml:id="formula_0">( * )( , ) = ∑ ∑ ( − * , − * ) ( , )<label>(1)</label></formula><p>where r is the dilation rate. The advantage of using dilated convolutions over regular convolutions is that the dilated convolution covers a wide range of input features. For example, if the filter size is 3x3 with a dilation rate of two, the receptive field covers an area of 5x5 without increasing the number of parameters and without reducing the resolution of the features. Thus, dilated convolutions can be used instead of max-pooling, but without reducing the output resolution or losing any information. In each GD layer, we used two dilated convolutions 1 and 2 with dilation rate 1 and 2, respectively and kernel size of 3x3. The 1 filter works as a conventional convolution filter which is good for capturing small patterns, whereas the second filter 2 covers a wider area of features and is therefore suitable for capturing bigger patterns. Both kernels are applied with stride = 1, and zeros are padded to the input before applying the convolution to maintain the same spatial size as the input.</p><p>Another unique and novel contribution of our approach is that we present a novel Context-Aware sub-network to guide the features between the multiple dilated convolutions. Unlike other methods in the literature and previous studies, our Gated-Dilated (GD) network has a Context-Aware sub-network that analyzes the context of other input features and generates attention signals to guide the features between the multiple dilated convolutions. The architecture of the Context-Aware sub-network is depicted in the magnified image in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The context-aware sub-network consists of a single 3x3 convolution filter with a Rectified Linear Unit (ReLU) activation function, followed by a global average pooling layer. The function of the pooling layer is just to reduce the number of parameters and features before we feed them to the next stage, and does not reduce the resolution of the input features, X. The kernel size of the global average pooling equals the spatial size of the input feature which is 32. Thus, the output of the global pooling is just a scalar value. Finally, the scalar value is passed through a single neuron with a sigmoid activation function. The output of the sigmoid activation function, α represents the attention signal, which will be used to guide the features between the convolutions. As α is conditioned on input, X, we can express the function/operation of the Context-Aware sub-network as:</p><formula xml:id="formula_1">( | )<label>(2)</label></formula><p>We use the gate function to control the flow of features through the dilation filter. The attention gate function is simply an element-wise multiplication between input and the scalar signal. The gate function uses a soft gate instead of a hard gate; the advantage of using a soft gate is that is continuous (i.e., not a binary/hard threshold), and can be any value between 0 and 1. In our network architecture (see magnified image of <ref type="figure" target="#fig_1">Figure 2</ref>), we have two attention gates corresponding to two different dilation rates, applicable to smaller and bigger nodules, respectively. Thus, if the value of is close to 1, the attention (or weight) will be on the convolution with the first dilation rate, 1 rather than the second dilation rate 2 , as follows:</p><formula xml:id="formula_2">= (3) = ( − )<label>(4)</label></formula><p>The outputs of the attention gates, 1 and 2 are then passed to two dilated convolution filters, 1 and 2 , respectively. In our method, we implemented these two filters to have a 3x3 kernel size and a ReLU activation function. Thus, the dilated convolutions are performed as:</p><formula xml:id="formula_3">( , ) = ( * )( , )<label>(5)</label></formula><formula xml:id="formula_4">( , ) = ( * )( , )<label>(6)</label></formula><p>The output of these two filters, 1 and 2 are used to capture the variation in the nodule sizes, whereby 1 acts as a regular convolution that is suitable for smaller nodules, whereas 2 is suitable for bigger nodules. Finally, the results of 1 and 2 are channel-wise concatenated and passed to the next layer. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the entire network architecture or top-level diagram of our new GD network. We applied five consecutive GD layers with 32, 32, 64, 64, and 64 channels, respectively. For each GD layer, the number of channels is equally divided/separated between the two dilated convolutional layers. For example, in the first GD layer, the number of channels is 32. This means that the convolutional layer with dilation rate of 2 has 16 channels, and the convolutional layer with dilation rate of 1 also has 16 channels. Therefore, after concatenating the outputs of the two layers, the total number of channels sums up to 32. It is common in the deep convolutional neural network to increase the channels as you go deeper <ref type="bibr" target="#b6">[7]</ref>, as the high-level features are more detailed than the generic low-level features; thus, the deeper layers require more filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OVERALL NETWORK ARCHITECTURE</head><p>To prevent the network from "overfitting" on the training dataset, multiple dropouts <ref type="bibr" target="#b31">[31]</ref> were implemented in our network architecture. Networks that are less likely to overfit the training dataset are capable of better generalization on the independent (i.e., unseen) testing dataset. Thus, in our implementation/framework, we placed the first and second dropout layers after the second and fourth GD layer, respectively with a dropout rate (or probability) of 0.25. Another dropout layer was placed after the fifth GD layer with a dropout rate of 0.5. We increased the dropout rate (from 0.25 to 0.5) as we proceed deeper into the network as deeper layers have more parameters than shallower layers, which makes them more prone to overfit on the training dataset.</p><p>After the five GD layers, we used global max-pooling to summarize the feature space. The global max-pooling operation computes the maximum value of all features in the feature maps. That is, for each channel, we obtain a single value which is the maximum value of the channel. For example, the last GD layer has 64 channels; thus, after applying global max-pooling, we obtain 64 features altogether. We used max-pooling to reduce the number of trainable parameters in the fully-connected network layer, which comes after the max-pooling layer. This is helpful as the size of the dataset is small (see Sections III.C and III.G), which may lead to overfitting if the number of parameters is huge. These features are then passed/transmitted to the last layer, which consists of a single neuron with a sigmoid activation function. The output of the sigmoid activation function is the probability of the nodule in question being malignant. Namely, if the output of the sigmoid exceeds 0.5, this means that the network predicts that the nodule in question has a higher probability of being malignant. Otherwise, it is more likely to be benign. As we have only one output and a two-class (malignant/benign) classification problem, we used the binary cross-entropy loss function. We also optimized the network parameters using the Adam optimizer <ref type="bibr" target="#b32">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DATASET DESCRIPTION</head><p>The dataset used in this work is the LIDC-IDRI dataset <ref type="bibr" target="#b10">[11]</ref> released by the National Cancer Institute, NIH. The LIDC-IDRI dataset is the largest and most comprehensive public lung nodule dataset. It consists of 1,018 CT scans collated from 1,010 patients altogether. The large size of the dataset and its public availability makes the LIDC-IDRI dataset suitable for developing and comparing/validating different deep learning based methods and it is a dataset that is frequently studied in the literature <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref>. As the images were collated from four different institutions using different CT scanners, there is a wide variation of image parameters within this dataset. For example, the image resolution, namely the pixel spacing in mm of different CT scanners are different; the slice thickness of the CT scans also range from 0.45 to 5.0 mm. Using a diverse dataset like LIDC-IDRI to develop algorithms has the advantage that the algorithms developed can be robust to unseen/generalized data as they have been trained on a diverse dataset.</p><p>The malignancy suspiciousness of each nodule in the LIDC-IDRI dataset was rated by four experienced radiologists. First, the radiologists annotated all nodules in each CT scan, whereby the nodule boundaries were provided in individual XML files. Only nodules with diameters between 3 and 30 mm were annotated. Similar to previous studies, we observed that there are wide variations/variabilities in the nodule annotations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b33">[33]</ref>. Namely, although the scans were annotated by four experienced radiologists altogether, only some of the nodules in the dataset were annotated by the majority of the radiologists (i.e., by at least three out of four of the radiologists). Thus, similar to our previous work in ref <ref type="bibr" target="#b2">[3]</ref>, we used a gold standard of the majority or at least three out of four radiologists to define a nodule. To group the annotations that have the same nodules, we used a nodule size report <ref type="bibr" target="#b34">[34]</ref> similar to ref. <ref type="bibr" target="#b7">[8]</ref>.</p><p>The radiologists also rated the malignancy suspiciousness of the nodules from 1 to 5, indicating an increasing degree of malignancy suspiciousness (namely, 1 represents a benign nodule; 5 is highly malignant). We combined the radiologists' ratings by taking the median of the malignancy levels: ratings less than three were considered as benign, whereas ratings above 3 were considered as malignant. Similar to previous similar studies <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref>, we also excluded nodules that had ratings of exactly 3 as these nodules have an indeterminate malignancy status. We also excluded nodules with ambiguous IDs from our dataset. In this way, we obtained 848 nodules altogether of which 442 are benign and 406 are malignant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. PREPROCESSING</head><p>First, to segment the nodules from their surrounding regions, we used the nodule annotations of the four radiologists. Second, to avoid partial volume effects caused by the different CT scanning protocols across different vendors, we used trilinear interpolation to normalize the CT scan volumes, resulting in isotropic resolution in all three (x, y, and z) dimensions. We then extracted a 32 by 32 millimeter square region about the center of each nodule, which we provided to the input of the GD network (see input image of <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. DATA AUGMENTATION</head><p>Deep learning based methods are generally data hungry and require training with large datasets to generalize well on unseen testing datasets. Thus, we trained our GD network with augmented datasets to improve its generalization capabilities given the limited number of training samples. From each nodule, we extracted three 2D views from the axial, coronal, and sagittal planes. We applied four rotation angles (i.e., 0º, 90º, 180º, and 270º) to each 2D view. We also applied Gaussian blurring with scale, σ = 1 on the axial, coronal, and sagittal views of the 0º image, and a recent article showed that training the CNN on Gaussian-blurred images improved the overall diagnosis of thorax diseases in 2D chest x-rays <ref type="bibr" target="#b35">[35]</ref>. Thus, we obtained 15 different images altogether of each nodule through the data augmentation procedure. We used all 15 images to train the networks. During testing, we obtained the output of the three views separately and averaged their results to obtain the final output.</p><p>We also normalized all the images in our dataset to have zero mean and unit variance using the standard score (or zscore). The mean and the standard deviation of the training dataset was computed, and normalization with the same mean and standard deviation was applied to the images in the testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. BASELINE METHODS/COMPARISONS</head><p>Besides the GD network, we designed and implemented three other networks to form the baseline methods or comparisons for our experiments and compared them with our method's performance. The first baseline method is a conventional CNN model that has the same number of layers as our model and the same number of channels in each layer. That is, we implemented a network architecture similar to <ref type="figure" target="#fig_1">Figure 2</ref>; however, instead of using the GD layer in the magnified image in <ref type="figure" target="#fig_1">Figure 2</ref>, we used a conventional CNN instead. The purpose/objective of using this method as a baseline comparison is so that we can analyze the mechanism of the GD layer and examine whether the GD layer can successfully differentiate between malignant/benign nodules (using attention gates to guide the dilated convolutions), compared to the conventional CNN constructed with the same parameters.</p><p>Besides conventional CNN, we designed another two ablation studies to analyze the contributions of the dilated convolution and the Context-Aware Sub-network, respectively. The first study is called GD-No-Dilation, which is similar to our proposed method, except both 1 and 2 have the same dilation rate of one. This study will help us understand whether having different dilation rates (of 1 and 2 in the original network architecture) will have any benefits/effects on the obtained results or not. In the second ablation experiment, we designed a model called GD-No-Gate, which is similar to our proposed method except that there is no gating or Context-Aware Sub-network in the network architecture. This ablation study will shed light on whether the Context-Aware Sub-network is useful in guiding the features through the network layers.</p><p>Additionally, we implemented a state-of-the-art lung nodule classification model called Multi-Crop Convolutional Neural Network (Multi-Crop) <ref type="bibr" target="#b14">[15]</ref>. We also implemented other state-of-the-art generic image classifiers models, namely Resnet-50 <ref type="bibr" target="#b6">[7]</ref> and Densenet-161 <ref type="bibr" target="#b36">[36]</ref> pre-trained with the Imagenet dataset <ref type="bibr" target="#b37">[37]</ref> to improve their performance by transfer learning. As Resnet and Densenet work with "Red Green Blue" or RGB images, we duplicated the nodule grayscale image to all three RGB channels. We also resized the nodule images to 224x224 to meet the input requirement dimensions of the Resnet and Densenet networks. Regarding the network topology of Resnet and Densenet, we only modified the last layer by replacing it with a single neuron to meet the requirement of the two-class (malignant/benign) nodule classification problem. We tested two methodologies of transfer learning: In the first one, we fine-tuned all the parameters of the network using the LIDC-IDRI dataset, and we refer to these networks throughout the paper as Resnet-Full and Densenet-Full, respectively. In the second methodology, we only fine-tuned the last layer, whereas the other layers were trained using the Imagenet dataset, and we refer to these networks throughout the paper as Resnet and Densenet. By fixing all the layers except the last one during training, we reduce the risk of overfitting the data. Moreover, the gradients only pass through the last layer, hence the gradients will not explode or vanish.</p><p>For all baseline methods except Multi-Crop, we used the same loss function and optimizer as the GD network and duplicated all other hyperparameters of the GD network (also see Section III.G on Experimental Design and Evaluation). This was performed to maintain all the parameter settings across the nine methods as much as possible, to ensure a fair and comparable comparison of our GD network with these state-of-the-art baseline methods. For Multi-Crop, we applied the same settings as ref. <ref type="bibr" target="#b14">[15]</ref> including maintaining the input size at 64x64x64 as described in ref. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. EXPERIMENTAL DESIGN AND EVALUATION</head><p>Our GD network implementation was based on Pytorch <ref type="bibr" target="#b38">[38]</ref>. We ran our experiments using a NVIDIA Titan X Pascal GPU. The GD network was trained with a batch size of 256 for 50 epochs. We initially set the learning rate to 1.0×10-3 and decreased it to 1.0×10-4 after the 20th epoch, similar to what was performed for training Resnet in refs. <ref type="bibr" target="#b6">[7]</ref>. We set the learning rate to be higher initially so that the gradient descent is faster at the early stages of training; then, we reduced the learning rate so that the gradient descent algorithm will not overshoot and skip the global minimum error. We set the hyperparameters of the Adam optimizer <ref type="bibr" target="#b32">[32]</ref>, 1 and 2 to the default values of 0.9 and 0.999, respectively. We initialized all the weights using the Xavier uniform initialization <ref type="bibr" target="#b33">[33]</ref> except the network bias values, which were initialized to zero. We applied these settings to the GD network as well as to the other baseline methods to ensure that fair performance comparisons could be made across the different methods except Multi-Crop, whereby we set the hyperparameters as described in ref. <ref type="bibr" target="#b14">[15]</ref>.</p><p>We validated all nine methods using a ten-fold crossvalidation method whereby the sum of the 406 malignant and 442 benign nodules where randomly divided into 10 exclusive partitions (or subgroups). In the random division, we maintained the ratio of benign to malignant nodules as much as possible across all 10 folds to ensure that the distribution of labels in each fold was approximately equal. Thus, after the division into 10 folds, the ratio of benign examples in each fold was around 51%. In each validation (training and testing) cycle, nine subgroups were used to train the network, and the trained network was then applied to the remaining subgroup. For each testing sample/nodule, the network generated an output score ranging from 0 to 1. A higher score indicates a higher probability/likelihood of the nodule being malignant. This process was iteratively executed 10 times using the 10 different combinations of subgroups. In this way, each of the 848 nodules was tested once with a corresponding networkgenerated probability score. <ref type="figure">Figure 3</ref> displays four ROC curves of the GD network, Resnet, Densenet, and Multi-Crop networks that were generated by a well-established software package for computing ROC curves and corresponding AUC values, namely ROCKIT TM <ref type="bibr" target="#b39">[39]</ref>. The ROC curves for the other five baseline methods were omitted to enhance the readability/ visibility of the four curves in <ref type="figure">Figure 3</ref>; however, the corresponding AUC, accuracy, precision, and sensitivity results of all methods are summarized and tabulated in <ref type="table">Table 1</ref>. We observe from <ref type="figure">Figure  3</ref> that Resnet, Densenet, and Multi-Crop have almost identical ROC curves and very similar true positive rates across all false positive rates with Densenet slightly outperforming Resnet and Multi-Crop. The results also show the superiority of the GD network, which significantly outperforms all other methods across all false positive rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV . RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Comparisons of four receiver operating characteristic (ROC) curves of our GD network, Resnet, Densenet and a Multi-Crop network with very similar hyperparameters and network architecture as our GD network. It can be observed that the ROC curve of our GD network is very competitive compared to the other baseline methods.</head><p>The AUC, accuracy, precision, and sensitivity results of all nine methods are tabulated and compared in <ref type="table">Table 1</ref>. The results in <ref type="table">Table 1</ref> show that the GD network outperforms the other eight baseline methods by significant margins in terms of accuracy = 92.57%, AUC = 0.9514, and precision = 91.85%. Although the CNN network has a marginally higher sensitivity result than the GD network, the difference in performance is very small (i.e., 92.67%-92.21%=0.46%). Densenet outperforms CNN and Resnet across all performance metrics except sensitivity and precision, whereby it is outperformed by CNN and Resnet, respectively. If we compare the results of CNN with Resnet, we observe that Resnet outperforms CNN across all performance metrics except sensitivity. Additionally, we classified the nodules without the segmentation mask and the results changed marginally (i.e., &lt;1%). <ref type="table">Table 1</ref>. Classification performance comparisons of the proposed GD network with the eight baseline comparison methods. We also adopted the reported results of DenseBTNET, GBRT, and HSCN from the published papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>AUC Accuracy Precision Sensitivity DenseBTNET <ref type="bibr" target="#b40">[40]</ref> 0.9315 88.31% ------GBRT <ref type="bibr" target="#b9">[10]</ref> 0.91 84% 0.874 0.886 HSCNN <ref type="bibr" target="#b41">[41]</ref> 0 The ablation studies in <ref type="table">Table 1</ref> (i.e., GD-No-Dilation and GD-No-Gate) show the importance of applying dilation and gating on the overall results: GD outperforms both GD-No-Dilation and GD-No-Gate across all performance metrics. The results also demonstrate the superiority of Resnet and Densenet over Resnet-Full and Densenet-Full, respectively. Resnet and Densenet outperform Resnet-Full and Densenet-Full across all performance metrics, which means that fine tuning the last layer and keeping all other layers fixed is better than tuning all the layers. This is especially true in this study, whereby the dataset size is comparatively smaller to the size of the Resnet and Densenet models.</p><p>To analyze the performance of our GD network on nodules of different diameters within the LIDC-IDRI dataset, we compared the accuracies of all models on different nodule diameters. Similar to <ref type="figure">Figure 3</ref>, to enhance the readability/ visibility of the results, we only plotted the accuracies of the GD network, Resnet, Densenet, and Multi-Crop in <ref type="figure" target="#fig_2">Figure 4</ref>. As expected, the nodules with large diameters of 13 to 25 mm are easily classified by all four models except Multi-Crop. This is because malignant nodules generally have bigger diameters/larger sizes than benign nodules as shown in <ref type="figure" target="#fig_0">Figure  1</ref>, and also confirmed by a recent study conducted on a very big dataset <ref type="bibr" target="#b4">[5]</ref>. Similarly, benign nodules generally have smaller sizes than malignant nodules; thus, the very small nodules of 3 to 4 mm in diameter were accurately classified by all four methods in <ref type="figure" target="#fig_2">Figure 4</ref> except Multi-Crop. <ref type="figure" target="#fig_2">Figure 4</ref> also shows that the nodules that are difficult to classify are those that have diameters between 5 to 12 mm. The results show that our GD model outperforms all other methods on the difficult nodules within this diameter range. The accuracy of the GD network also exceeds or performs at least as well as all other methods across the range of small, medium and large-sized nodules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EXPLORATORY ANALYSIS OF THE ATTENTION SIGNAL</head><p>The attention signal α that we introduced in equation <ref type="formula" target="#formula_1">(2)</ref> and used in equations <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_2">(4)</ref> has a critical effect on the overall output of the GD layer. As explained in Section III.A, α is used to guide the features between the different dilations based on the nodule size. Due to the significant/important role of α, we conducted an experiment to study the relationship between the nodule area size and α, and to also analyze whether α effectively guides the dilated convolution filters between smaller and bigger nodules, respectively. Thus, after training the GD model, we forwarded the 2D nodule images through the network and recorded the attention signal α for each image. We also estimated the nodule area by multiplying the width and the height of the nodule in each 2D image and examined the relationship between α and the estimated nodule area.</p><p>The results of our analysis performed on all five GD layers in our network architecture are depicted in <ref type="figure" target="#fig_3">Figure 5</ref> (layers 1 to 5 depict increasingly deeper GD layers as we progress through the GD network architecture in <ref type="figure" target="#fig_1">Figure 2</ref>). We can observe several interesting and unique findings of the results in <ref type="figure" target="#fig_3">Figure 5</ref>. First, we observe that the relationship between α and the nodule area is approximately linear for all layers except layer 4. The linearity between α and the nodule area follows/agrees with the linear relationship assumption that we hypothesized in equations <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_2">(4)</ref> of Section III.A. Another interesting observation is that the value of α never becomes close to zero or one; that is, for most nodules, α is between 0.4 and 0.6. This result means that both dilation rates 1 and 2 of our proposed model are useful to extract features from the nodules, which also explains and confirms the good classification results obtained for our GD model in <ref type="figure" target="#fig_2">Figure 4</ref>. We also observe that as the nodule area increases, α decreases, which means that the dilation rate of 2 gets more attention as the nodule size increases. This trend is observed for all the layers except layer 4. In layer 4, which is the second-last GD layer, α for all nodule sizes is always less than 0.5 (specifically, 0.46 or less), which means that the model prefers to give a higher attention to dilation rate 2 compared with dilation rate 1 as we go deeper in the network. As we progress to deeper layers, global features (extracted by dilation rate 2) are emphasized more than local features, which explains the higher attention signal given to dilation rate 2 in layers 4 and 5. We also measured Pearson's correlation coefficient between the attention signal, α and the nodule area. <ref type="table" target="#tab_1">Table 2</ref> tabulates Pearson's correlation coefficient between the two variables for all five GD layers. Except for layer 4, a strong negative linear correlation of -0.93 or less exists between the two variables. For layer 4, Pearson's correlation coefficient was around 0.58, which demonstrates only a moderate linear correlation between the two variables. These results confirm our previous finding that there is a linear relationship between the nodule area and α for all GD layers except layer 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this paper, we proposed a new GD Network to classify lung nodules as either malignant or benign. The intuition behind this novel architecture is to work around the high variation in the nodule diameter, which ranges from 3 to 30 mm. Extremely big or small nodules are easy to classify as very big nodules are usually malignant, whereas smaller ones are usually benign. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the nodules with diameters between 5 and 12 mm are harder to classify than the extremely big or small nodules. Thus, we proposed using multiple convolutional filters in parallel, whereby one filter captures the local features and the other the global features. Instead of using multiple pooling layers to obtain different scales of the feature representations, we used dilation convolution with different dilation rates to capture the wide range of nodule diameters. By increasing the dilation rate, we can cover a wide area of the features without the need to scale down the input features or reduce the feature resolution. The output from the two dilation convolutions combined using a concatenation function are passed to the next layer. In this way, different feature resolutions corresponding to different dilation rates are captured across the low and high level features as the network progresses to deeper layers.</p><p>Another novel and unique aspect of our network architecture is that we designed a Context-Aware sub-network to guide the features between the different dilation convolution filters. The Context-Aware sub-network is conditioned on the input features, which means that it first analyzes the input features. Based on the analysis, the Context-Aware subnetwork generates an attention signal that controls the attention gates that are located in front of each dilated convolution filter as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The intuition is to let the network decide which path the features should take and how much each dilation rate as proportioned by the attention signal, should contribute to the output. A linear relationship was observed between the nodule size and attention signal generated by our network architecture, which indicates that the Context-Aware sub-network works effectively to guide the features to the right attention gate/dilation rate based on the nodule size. This is further confirmed by the results in <ref type="figure" target="#fig_2">Figure  4</ref> that show that our GD network performs considerably better than the other baseline methods to classify nodules in the difficult size range (of 5-12 mm). Although we applied our GD network for lung nodule classification, the same principle could be applied to different problems where multi-scale objects exist.</p><p>Another contribution of this paper is that we used eight baseline comparison methods including Multi-Crop, Resnet, and Densenet, which are approaches that produced state-ofthe-art results in the literature for the nodule classification task. Our results demonstrate that our new GD network significantly outperformed Multi-Crop, Resnet, Densenet, and the conventional CNN for almost all performance metrics including AUC (0.9514±0.0078), accuracy (92.57%±2.47) and precision (0.9185±0.0454). We maintained all hyperparameter settings as much as possible between the nine examined methods to ensure that valid performance comparisons and conclusions could be derived from the obtained results.</p><p>The results of this study are very timely, especially in this era of big data and lung radiomics <ref type="bibr" target="#b42">[42]</ref>. The previous conventional lung nodule classification and detection methods used hand-crafted features, such as shape and texture based features to classify and detect nodules <ref type="bibr" target="#b43">[43]</ref>. However, the process of designing/selecting relevant features is difficult and time consuming and rely on researchers' prior knowledge/ expertise. Deep learning algorithms have the ability to address the limitations of current CAD schemes. First, deep learning algorithms can automatically extract meaningful/relevant features, thus eliminating the requirement of prior knowledge to derive useful hand-crafted features including heuristic descriptors. In our proposed method, our new GD network was able to extract relevant multiscale features without reducing the overall feature resolution. Second, in this era of big data, large amounts of data are readily available, which could extend to tomographic raw data (namely, "rawdiomics") <ref type="bibr" target="#b42">[42]</ref>. The ability of deep learning based methods to handle large-scale datasets and automatically generate features is a requirement in this ever-changing era.</p><p>The current CAD schemes for lung nodule detection and classification including R2 Technology's commercialized ImageChecker CT Lung CAD scheme also produce high false positive rates/many false detections that distract radiologists. Furthermore, the performance of current CAD schemes is very uncertain and varies from one dataset to another, thus making the validity/reproducibility of CAD schemes a highly controversial topic <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[44]</ref>- <ref type="bibr" target="#b46">[46]</ref>. Subjective evaluation of CT images also lag in specificity especially when it comes to differentiating benign from malignant lung etiologies <ref type="bibr" target="#b42">[42]</ref>.</p><p>Thus, new approaches incorporating deep learning methods and image-based radiomics are required to improve the performance of current CAD schemes for lung nodule classification so that they can be integrated into clinical practice. The results of our novel GD network are very encouraging. Namely, unlike the current CAD schemes that have high detection sensitivities, but have high false positive rates <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[44]</ref>- <ref type="bibr" target="#b46">[46]</ref>, our method has very high AUC, accuracy, precision, and sensitivity results.</p><p>Although the results are very encouraging, we recognize that this is a preliminary study. First, our method is not a fullyautomated method that can automatically extract/segment the nodule locations from whole CT scans and classify them as benign or malignant. Namely, our method currently requires an object detector model to identify the nodule locations before classifying them as benign/malignant. Although this is a current limitation of our method, other similar and recent studies for lung nodule classification also have this same limitation/drawback <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref>. In future, we plan to incorporate a fully-automated nodule classification scheme that can automatically detect the nodule candidates/locations similar to the CAD scheme for lung nodule detection that we proposed in our previous study <ref type="bibr" target="#b2">[3]</ref>. Second, the size of the LIDC-IDRI dataset is a limitation for training deep learning algorithms as many hyperparameters in each layer require very large datasets to obtain good training results. However, we recognize that the LIDC-IDRI dataset is still the biggest available public lung CT dataset, which has been used in other similar studies for the nodule classification task <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref>. Thus, its usage is beneficial for valid and comparable performance comparisons with other similar methods in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this study, we developed a new deep learning model based on GD networks for the challenging task of lung nodule benign/malignant classification. Our work focuses on the wide diameter variation of the nodules, which can range anywhere between 3 and 30 mm. To tackle this challenging problem, we proposed a GD Layer that has two dilated convolutions in each layer. Each of these convolutions has a different dilation rate to capture different nodule sizes. Moreover, the input features are guided between the two dilated convolutions by a new Context-Aware sub-network. This sub-network generates attention signals that guide the input features through the network architecture. The proposed model achieves state-ofthe-art results on the LIDC-LDRI dataset and outperforms eight baseline state-of-the-art methods including Multi-Crop, Densenet and Resnet in terms of AUC, precision and accuracy. The results also demonstrate significant improvements in terms of classification accuracy on "difficult" medium-sized nodules, specifically the nodules with diameters between 5 and 12 mm. Analysis of the relationship between the attention signal and nodule area/size demonstrates that the Context-Aware sub-network works effectively to guide the features to the right attention gate/dilation rate based on the nodule size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The nodule diameter distributions of all malignant and benign nodules within the LIDC-IDRI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our new Gated-Dilated (GD) network framework. The GD network consists of five GD layers, a Pooling layer, and a Fully-Connected layer. The design of each GD layer is shown in the magnified image (below). All hyperparameters in the GD layer are fixed except the S l value which controls the number of output channels in the l th layer. We set the S l value forGD1, GD2, GD3, GD4, and GD5 to 16, 16, 32, 32, and 32respectively. The bottom part of the diagram illustrates how the kernels and operate on 5x5 input features. The shaded area in blue depicts the convolution area/region. Two dilated convolutions, and are designed to extract image features of two scales for smaller and bigger nodules, respectively as shown by their receptive fields in this figure. Note that the kernels first pad the input with zeros to maintain the same spatial size after the convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Accuracies of the GD network, Resnet, Densenet, and Multi-Crop across small (3-4 mm), medium (5-12 mm) and big (13-25 mm) nodule diameters within the LIDC-IDRI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Analysis of the relationship between the area/size of the nodules and the attention signal, α across all GD layers of our proposed network architecture in Figure 2. Layers 1 to 5, respectively correspond to the increasingly deeper GD layers in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Pearson's correlation coefficient between the attention signal, and the nodule area for all five GD layers in Figure 2.</head><label>2</label><figDesc></figDesc><table><row><cell>Alpha</cell><cell cols="5">Conv1 Conv2 Conv3 Conv4 Conv5</cell></row><row><cell>Correlation</cell><cell>-0.96</cell><cell>-0.93</cell><cell>-0.97</cell><cell>0.57</cell><cell>-0.93</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. The authors also acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC-IDRI database used in this study.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">International Agency for Research on Cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Cancer Report</title>
		<editor>C.P.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA. Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note>Cancer statistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel computer-aided lung nodule detection system for CT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deklerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5630" to="5645" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lung Cancer : Bridging the gap between medical imaging and data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Cadrin-Chênevert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probability of Cancer in Pulmonary Nodules Detected on First Screening CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="910" to="919" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pulmonary nodule classification with deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wollersheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1799" to="1808" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lung nodule classification using deep Local-Global networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1815" to="1819" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble Learners of Multiple Deep CNNs for Pulmonary Nodules Classification Using CT Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="110358" to="110371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention to Scale: Scale-aware Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-crop Convolutional Neural Networks for lung nodule malignancy suspiciousness classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="663" to="673" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="286" to="297" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: wedding the a trous and Mallat algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dilated Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural Machine Translation in Linear Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic feature learning using multichannel ROI based on deep structured algorithms for computerized lung cancer diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Risk stratification of lung nodules using 3D CNN-based multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</title>
		<imprint>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-Scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lung nodules diagnosis based on evolutionary convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Da Silva Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>De Paiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gattass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="19039" to="19055" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICNN&apos;95 -International Conference on Neural Networks</title>
		<meeting>ICNN&apos;95 -International Conference on Neural Networks</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems : an introductory analysis with applications to biology, control, and artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Threshold Selection Method from Gray-Level Histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man. Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Random Search for Hyper-Parameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-view multi-scale CNNs for lung nodule type classification from CT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="262" to="275" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Computer-aided lung nodule detection on high-resolution CT data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rogalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zwartkruis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaffert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4684</biblScope>
			<biblScope unit="page">677</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The Lung Image Database Consortium (LIDC) Nodule Size Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Biancardi</surname></persName>
		</author>
		<ptr target="http://www.via.cornell.edu/lidc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Thorax disease diagnosis using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tervonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Silven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2287" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical Comparison of Two ROC-curve Estimates Obtained from Partially-paired Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Decis. Mak</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense Convolutional Binary-Tree Networks for Lung Nodule Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49080" to="49088" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Radiomics in lung cancer: Its time is here</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Orton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="997" to="1000" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Computer analysis of computed tomography scans of the lung: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sluimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="405" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Computer-Aided Detection in Screening CT for Pulmonary Nodules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Cooperberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1280" to="1287" />
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A large-scale evaluation of automatic pulmonary nodule detection in chest CT using local image features and k-nearest-neighbour classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M R</forename><surname>Schilham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>De Hoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Gietema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2009-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The effect of nodule segmentation on the accuracy of computerized lung nodule detection on CT scans: comparison on a data set annotated by multiple radiologists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sahiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6514</biblScope>
			<biblScope unit="page">65140</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
