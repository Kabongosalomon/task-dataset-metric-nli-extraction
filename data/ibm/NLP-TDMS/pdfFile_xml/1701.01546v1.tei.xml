<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abnormal Event Detection in Videos using Spatiotemporal Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-09">January 9, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shean</surname></persName>
							<email>yshean@1utar.mytayyh@utar.edu.my</email>
							<affiliation key="aff0">
								<orgName type="department">Chian Faculty of Engineering Science</orgName>
								<orgName type="institution">Universiti Tunku Abdul Rahman</orgName>
								<address>
									<postCode>43000</postCode>
									<settlement>Kajang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chian Faculty of Engineering Science</orgName>
								<orgName type="institution">Universiti Tunku Abdul Rahman</orgName>
								<address>
									<postCode>43000</postCode>
									<settlement>Kajang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haur</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chian Faculty of Engineering Science</orgName>
								<orgName type="institution">Universiti Tunku Abdul Rahman</orgName>
								<address>
									<postCode>43000</postCode>
									<settlement>Kajang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chian Faculty of Engineering Science</orgName>
								<orgName type="institution">Universiti Tunku Abdul Rahman</orgName>
								<address>
									<postCode>43000</postCode>
									<settlement>Kajang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Abnormal Event Detection in Videos using Spatiotemporal Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-09">January 9, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient method for detecting anomalies in videos. Recent applications of convolutional neural networks have shown promises of convolutional layers for object detection and recognition, especially in images. However, convolutional neural networks are supervised and require labels as learning signals. We propose a spatiotemporal architecture for anomaly detection in videos including crowded scenes. Our architecture includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features. Experimental results on Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of our method is comparable to state-of-the-art methods at a considerable speed of up to 140 fps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth of video data, there is an increasing need not only for recognition of objects and their behaviour, but in particular for detecting the rare, interesting occurrences of unusual objects or suspicious behaviour in the large body of ordinary data. Finding such abnormalities in videos is crucial for applications ranging from automatic quality control to visual surveillance.</p><p>Meaningful events that are of interest in long video sequences, such as surveillance footage, often have an extremely low probability of occurring. As such, manually detecting such events, or anomalies, is a very meticulous job that often requires more manpower than is generally available. This has prompted the need for automated detection and segmentation of sequences of interest. However, present technology requires an enormous amount of configuration efforts on each video stream prior to the deployment of the video analysis process, even with that, those events are based on some predefined heuristics, which makes the detection model difficult to generalize to different surveillance scenes.</p><p>Video data is challenging to represent and model due to its high dimensionality, noise, and a huge variety of events and interactions. Anomalies are also highly contextual, for example, running in a restaurant would be an anomaly, but running at a park would be normal. Moreover, the definition of anomaly can be ambiguous and often vaguely defined. A person may think walking around on a subway platform is normal, but some may think it should be flagged as an anomaly since it could be suspicious. These challenges have made it difficult for machine learning methods to identify video patterns that produce anomalies in real-world applications.</p><p>There are many successful cases in the related field of action recognition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>. However, these methods only applicable to labelled video footages where events of interest are clearly defined and does not involve highly occluded scenes, such as crowded scenes. Furthermore, the cost of labelling every type of event is extremely high. Even so, it is not guaranteed to cover every past and future events. The recorded video footage is likely not long enough to capture all types of activities, especially abnormal activities which rarely or never occurred.</p><p>Recent effort on detecting anomalies by treating the task as a binary classification problem (normal and abnormal) <ref type="bibr" target="#b32">[33]</ref> proved it being effective and accurate, but the practicality of such method is limited since footages of abnormal events are difficult to obtain due to its rarity. Therefore, many researchers have turned to models that can be trained using little to no supervision, including spatiotemporal features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, dictionary learning <ref type="bibr" target="#b30">[31]</ref> and autoencoders <ref type="bibr" target="#b22">[23]</ref>. Unlike supervised methods, these methods only require unlabelled video footages which contain little or no abnormal event, which are easy to obtain in real-world applications. A description of these methodologies and their limitations are discussed in the next section.</p><p>This paper presents a novel framework to represent video data by a set of general features, which are inferred automatically from a long video footage through a deep learning approach. Specifically, a deep neural network composed of a stack of convolutional autoencoders was used to process video frames in an unsupervised manner that captured spatial structures in the data, which, grouped together, compose the video representation. Then, this representation is fed into a stack of convolutional temporal autoencoders to learn the regular temporal patterns.</p><p>Our proposed method is domain free (i.e., not related to any specific task, no domain expert required), does not require any additional human effort, and can be easily applied to different scenes. To prove the effectiveness of the proposed method we apply the method to real-world datasets and show that our method consistently outperforms similar methods while maintaining a short running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>The main characteristics of our approach and also the contributions of this research are as follows:</p><p>• We wish to reduce the labor-intensive effort in feature engineering that results in a representation of the data that can support effective machine learning. This can be done by replacing low-level handcrafted features with learned hierarchical features. With the help of autoencoders, we are able to find representative features by learning from data instead of forming suitable features based on our knowledge.</p><p>• We replace traditional sparse coding methods with autoencoders. Unlike existing methods, there is no separation between extracting feature representation of videos and learning a model of features. In addition, by having multiple layers of hidden units in autoencoder, hierarchical feature learning can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most of these abnormal instances are beforehand unknown, as this would require predicting all the ways something could happen out of the norm. It is therefore simply impossible to learn a model for all that is abnormal or irregular. But how can we find an anomaly without what to look for? Since it is easier to get video data where the scene is normal in contrast to obtaining what is abnormal, we could focus on a setting where the training data contains only normal visual patterns. A popular approach adopted by researchers in this area is to first learn the normal patterns from the training videos, then anomalies are detected as events deviated from the normal patterns <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref>. The majority of the work on anomaly detection relies on the extraction of local features from videos, that are then used to train a normalcy model.</p><p>Trajectories have long been popular in video analysis and anomaly detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. A common characteristic of trajectory-based approaches is the deviation of nominal classes of object trajectories in a training phase, and the comparison of new test trajectories against the nominal classes in an evaluation phase. A statistically significant deviation from all classes indicates an anomaly. However, the accuracy of trajectory analysis relies heavily on tracking, which precise tracking still remains a significant challenge in computer vision, particularly in complex situations. Tracking-based approaches are suitable for scenes with few objects but are impractical for detecting abnormal patterns in a crowded or complex scene.</p><p>Non-tracking approaches that focus on spatiotemporal anomalies in videos also exist. These rely mainly on extracting and analyzing local low-level visual features, such as the histogram of oriented gradients <ref type="bibr" target="#b29">[30]</ref>, the histogram of oriented flows <ref type="bibr" target="#b10">[11]</ref> and optical flow <ref type="bibr" target="#b20">[21]</ref>, by employing spatiotemporal video volumes (dense sampling or interest point selection) <ref type="bibr" target="#b2">[3]</ref>. These local features are then grouped in clusters, i.e., bags of visual words (BOV), according to similarity metrics. Their popularity is due to their low computational cost, as well as their ability to focus on abnormal behaviour, even in extremely crowded scenes <ref type="bibr" target="#b9">[10]</ref>. Another similar technique is sparse reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. The fundamental underlying assumption of these methods is that any new feature representation of a normal/anomalous event can be approximately modeled as a (sparse) linear combination of feature representations (of previously observed events) in a trained dictionary. This assumes that all previously observed events are normal events.</p><p>However, since classical BOV approaches group similar volumes (summarize), they destroy all compositional information in the process of grouping visual words. It is also required to pre-determine the number of clusters, which can only be found through trial-and-error during testing time. In addition, codebook models require searching over a large space <ref type="bibr" target="#b21">[22]</ref> even during the time of testing, making it impractical for real-time anomaly detection.</p><p>The success of deep learning methods in various applications consequently caused the rise of such methods in anomaly detection. The term deep learning refers to learning a hierarchical set of features through multiple layers of hidden nodes in an artificial neural network. Unlike previously stated methods, there is no need to define a specific set of features to extract from the dataset -deep learning methods learn the useful features directly from the data with minimal preprocessing. Specifically, convolutional neural networks (ConvNet) have proved its effectiveness in a wide range of applications such as object recognition <ref type="bibr" target="#b25">[26]</ref>, person detection <ref type="bibr" target="#b27">[28]</ref>, and action recognition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25]</ref>. ConvNet consists of a stack of convolutional layers with a fully-connected layer and a softmax classifier, and convolutional autoencoder is essentially a Con-vNet with its fully-connected layer and classifier replaced by a mirrored stack of convolutional layers. The authors of <ref type="bibr" target="#b32">[33]</ref> applied a 3D ConvNet on classifying anomalies, whereas <ref type="bibr" target="#b4">[5]</ref> used an end-to-end convolutional autoencoder to detect anomalies in surveillance videos. Their reported result proves the usefulness of learned representation on videos through a stack of convolutional layers. On the other hand, long short term memory (LSTM) model is well-known for learning temporal patterns and predicting time series data. <ref type="bibr" target="#b14">[15]</ref> has recently proposed to apply convolutional LSTMs for learning the regular temporal patterns in videos and his findings show great promise of what deep neural network can learn.</p><p>Despite its simplicity, some limitations remain in these recently proposed methods. Though 3D ConvNet performed excellently in learning discriminative features between the anomalies and the normal events, it is impractical to apply in real-world scenarios due to the absence of video segments containing abnormal events. Meanwhile, in the convolutional autoencoder proposed by <ref type="bibr" target="#b4">[5]</ref>, convolution and pooling operations are performed only spatially, even though the proposed network takes multiple frames as input, because of the 2D convolutions, after the first convolution layer, temporal information is collapsed completely <ref type="bibr" target="#b26">[27]</ref>. Besides, convolutional LSTM layers applied by <ref type="bibr" target="#b14">[15]</ref> are memory-intensive -the training will need to be executed on very small minibatches, which results in slow training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The method described here is based on the principle that when an abnormal event occurs, the most recent frames of video will be significantly different than the older frames. Inspired by <ref type="bibr" target="#b4">[5]</ref>, we train an end-to-end model that consists of a spatial feature extractor and a temporal encoder-decoder which together learns the temporal patterns of the input volume of frames. The model is trained with video volumes consists of only normal scenes, with the objective to minimize the reconstruction error between the input video volume and the output video volume reconstructed by the learned model. After the model is properly trained, normal video volume is expected to have low reconstruction error, whereas video volume consisting of abnormal scenes is expected to have high reconstruction error. By thresholding on the error produced by each testing input volumes, our system will be able to detect when an abnormal event occurs.</p><p>Our approach consists of three main stages:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The task of this stage is to convert raw data to the aligned and acceptable input for the model. Each frame is extracted from the raw videos and resized to 227 × 227. To ensure that the input images are all on the same scale, the pixel values are scaled between 0 and 1 and subtracted every frame from its global mean image for normalization. The mean image is calculated by averaging the pixel values at each location of every frame in the training dataset. After that, the images are converted to grayscale to reduce dimensionality. The processed images are then normalized to have zero mean and unit variance.</p><p>The input to the model is video volumes, where each volume consists of 10 consecutive frames with various skipping strides. As the number of parameters in this model is large, large amount of training data is needed. Following <ref type="bibr" target="#b4">[5]</ref>s practice, we perform data augmentation in the temporal dimension to increase the size of the training dataset. To generate these volumes, we concatenate frames with stride-1, stride-2, and stride-3. For example, the first stride-1 sequence is made up of frame {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, whereas the first stride-2 sequence contains frame number {1, <ref type="bibr">3, 5, 7, 9, 11, 13, 15, 17, 19}</ref>, and stride-3 sequence would contain frame number {1, 4, 7, 10, 13, 16, 19, 22, 25, 28}. Now the input is ready for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Learning</head><p>We propose a convolutional spatiotemporal autoencoder to learn the regular patterns in the training videos. Our proposed architecture consists of two parts -spatial autoencoder for learning spatial structures of each video frame, and temporal encoder-decoder for learning temporal patterns of the encoded spatial structures. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and 2, the spatial encoder and decoder have two convolutional and deconvolutional layers respectively, while the temporal encoder is a three-layer convolutional long short term memory (LSTM) model. Convolutional layers are well-known for its superb performance in object recognition, while LSTM model is widely used for sequence learning and time-series modelling and has proved its performance in applications such as speech translation and handwriting recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Autoencoder</head><p>Autoencoders, as the name suggests, consist of two stages: encoding and decoding. It was first used to reduce dimensionality by setting the number of encoder output units less than the input. The model is usually trained using back-propagation in an unsupervised manner, by minimizing the reconstruction error of the decoding results from the original inputs. With the activation function chosen to be nonlinear, an autoencoder can extract more useful features than some common linear transformation methods such as PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spatial Convolution</head><p>The primary purpose of convolution in case of a convolutional network is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. Mathematically, convolution operation performs dot products between the  filters and local regions of the input. Suppose that we have some n × n square input layer which is followed by the convolutional layer. If we use an m × m filter W , the convolutional layer output will be of size (n − m + 1) × (n − m + 1).</p><p>A convolutional network learns the values of these filters on its own during the training process, although we still need to specify parameters such as the number of filters, filter size, the number of layers before training. With more number of filters we have, more image features get extracted and the better the network becomes at recognizing patterns in unseen images. However, more filters would add to computational time and exhaust memory faster, so we need to find balance by not setting the number of filters too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Recurrent Neural Network (RNN)</head><p>In a traditional feedforward neural network, we assume that all inputs (and outputs) are independent of each other. However, learning temporal dependencies between inputs are important in tasks involving sequences, for example, a word predictor model should be able to derive information from the past inputs. RNN works just like a feedforward network, except that the values of its output vector are influenced not only by the input vector but also on the entire history of inputs. In theory, RNNs can make use of information in arbitrarily long sequences, but in practice, they are limited to looking back only a few steps due to vanishing gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Long Short Term Memory (LSTM)</head><p>To overcome this problem, a variant of RNN is introduced: long short term memory (LSTM) model which incorporates a recurrent gate called forget gate. With the new structure, LSTMs prevent backpropagated errors from vanishing or exploding, thus can work on long sequences and they can be stacked together to capture higher level information. The formulation of a typical LSTM unit is summarized with <ref type="figure">Figure 3</ref> and equations (1) through <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_0">f t = σ(W f ⊗ [h t−1 , x t ] + b f ) (1) i t = σ(W i ⊗ [h t−1 , x t ] + b i ) (2) C t = tanh(W C ⊗ [h t−1 , x t ] + b C ) (3) C t = f t ⊗ C t−1 + i t ⊗Ĉ t<label>(4)</label></formula><formula xml:id="formula_1">o t = σ(W o ⊗ [h t−1 , x t ] + b o ) (5) h t = o t ⊗ tanh(C t )<label>(6)</label></formula><p>Equation <ref type="formula">(1)</ref> represents the forget layer, <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula">(3)</ref> are where new information is added, (4) combines old and new information, whereas <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_1">(6)</ref> output what has been learned so far to the LSTM unit at the next timestep. The variable x t denotes the input vector, h t denotes the hidden state, and C t denotes the cell state at time t. W are the trainable weight matrices, b are the bias vectors, and the symbol ⊗ denotes the Hadamard product. <ref type="figure">Figure 3</ref>: The structure of a typical LSTM unit. The blue line represents an optional peephole structure, which allows the internal state to look back (peep) at the previous cell state C t−1 for a better decision. Best viewed in colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Convolutional LSTM</head><p>A variant of the LSTM architecture, namely Convolutional Long Short-term Memory (ConvLSTM) model was introduced by Shi et al. in <ref type="bibr" target="#b23">[24]</ref> and has been recently utilized by Patraucean et al. in <ref type="bibr" target="#b18">[19]</ref> for video frame prediction. Compared to the usual fully connected LSTM (FC-LSTM), ConvLSTM has its matrix operations replaced with convolutions. By using convolution for both input-to-hidden and hidden-to-hidden connections, ConvLSTM requires fewer weights and yield better spatial feature maps. The formulation of the ConvL-STM unit can be summarized with <ref type="bibr" target="#b6">(7)</ref> through <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_2">f t = σ(W f * [h t−1 , x t , C t−1 ] + b f ) (7) i t = σ(W i * [h t−1 , x t , C t−1 ] + b i ) (8) C t = tanh(W C * [h t−1 , x t ] + b C ) (9) C t = f t ⊗ C t−1 + i t ⊗Ĉ t<label>(10)</label></formula><formula xml:id="formula_3">o t = σ(W o * [h t−1 , x t , C t−1 ] + b o ) (11) h t = o t ⊗ tanh(C t )<label>(12)</label></formula><p>While the equations are similar in nature to (1) through (6), the input is fed in as images, while the set of weights for every connection is replaced by convolutional filters (the symbol * denotes a convolution operation). This allows ConvLSTM work better with images than the FC-LSTM due to its ability to propagate spatial characteristics temporally through each ConvLSTM state.</p><p>Note that this convolutional variant also adds an optional 'peephole' connections to allow the unit to derive past information better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularity Score</head><p>Once the model is trained, we can evaluate our models performance by feeding in testing data and check whether it is capable of detecting abnormal events while keeping false alarm rate low. To better compare with <ref type="bibr" target="#b4">[5]</ref>, we used the same formula to calculate the regularity score for all frames, the only difference being the learned model is of a different kind. The reconstruction error of all pixel values I in frame t of the video sequence is taken as the Euclidean distance between the input frame and the reconstructed frame:</p><formula xml:id="formula_4">e(t) = ||x(t) − f W (x(t))|| 2<label>(13)</label></formula><p>where f W is the learned weights by the spatiotemporal model. We then compute the abnormality score s a (t) by scaling between 0 and 1. Subsequently, regularity score s r (t) can be simply derived by subtracting abnormality score from 1:</p><formula xml:id="formula_5">s a (t) = e(t) − e(t) min e(t) max (14) s r (t) = 1 − s a (t)<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Anomaly Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Thresholding</head><p>It is straightforward to determine whether a video frame is normal or anomalous. The reconstruction error of each frame determines whether the frame is classified as anomalous. The threshold determines how sensitive we wish the detection system to behave -for example, setting a low threshold makes the system become sensitive to the happenings in the scene, where more alarms would be triggered. We obtain the true positive and false positive rate by setting at different error threshold in order to calculate the area under the receiver operating characteristic (ROC) curve (AUC). The equal error rate (EER) is obtained when false positive rate equals to the false negative rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Event count</head><p>Following the practice in <ref type="bibr" target="#b4">[5]</ref>, to reduce the noisy and unmeaningful minima in the regularity score, we applied Persistence1D <ref type="bibr" target="#b8">[9]</ref> algorithm to group local minima with a fixed temporal window of 50 frames. We assume local minima within 50 frames belong to the same abnormal event. This is a reasonable length of the temporal window as an abnormal event should be at least 2-3 seconds long to be meaningful (videos are captured at 24-25 fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We train our model on five most commonly used benchmarking datasets: Avenue <ref type="bibr" target="#b12">[13]</ref>, UCSD Ped1 and Ped2 <ref type="bibr" target="#b13">[14]</ref>, Subway entrance and exit datasets <ref type="bibr" target="#b0">[1]</ref>. All videos are taken from a fixed position for each dataset. All training videos contain only normal events. Testing videos have both normal and abnormal events.</p><p>In Avenue dataset, there are total 16 training and 21 testing video clips. Each clips duration vary between less than a minute to two minutes long. The normal scenes consist of people walking between staircase and subway entrance, whereas the abnormal events are people running, walking in opposite direction, loitering and etc. The challenges of this dataset include camera shakes and a few outliers in the training data. Also, some normal pattern seldom appears in the training data.</p><p>UCSD Ped1 dataset has 34 training and 36 testing video clips, where each clip contains 200 frames. The videos consist of groups of people walking towards and away from the camera. UCSD Ped2 dataset has 16 training and 12 testing video clips, where the number of frames of each clip varies. The videos consist of walking pedestrians parallel to the camera plane. Anomalies of the two datasets include bikers, skaters, carts, wheelchairs and people walking in the grass area.</p><p>Subway entrance dataset is 1 hour 36 minutes long with 66 unusual events of five different types: walking in the wrong direction (WD), no payment (NP), loitering (LT), irregular interactions between people (II), and miscellaneous (e.g. sudden stop, running fast). First 20 minutes of the video is used for training.</p><p>Subway exit dataset is 43 minutes long with 19 unusual events of three types: walking in the wrong direction (WD), loitering (LT), and miscellaneous (e.g. sudden stop, looking around, a janitor cleaning the wall, gets off the train and gets on the train again quickly. First 5 minutes of the video is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Parameters</head><p>We train the model by minimizing the reconstruction error of the input volume. We use Adam optimizer to allow it taking the role of setting the learning rate automatically based on the models weight update history. We use mini-batches of size 64 and each training volume is trained for a maximum of 50 epochs or until the reconstruction loss of validation data stop decreasing after 10 consecutive epochs. Hyperbolic tangent is chosen as the activation function of spatial encoder and decoder. To ensure the symmetry of the encoding and decoding function, we did not use rectified linear unit (ReLU) despite its regularization ability because activated values from ReLU have no upper bound. <ref type="table" target="#tab_0">Table 1</ref> shows the frame-level AUC and EER of our and of other methods on all five datasets. We outperform all other considered methods in respect to framelevel EER. We also provide the event count comparison for Avenue dataset and  the entrance and exit scenes in the Subway dataset in <ref type="table" target="#tab_1">Table 2</ref>. For the entrance scenes, we are better than <ref type="bibr" target="#b4">[5]</ref> since we detect the same number of anomalies with less false alarms. For the exit scenes, we detected more abnormal events compared to <ref type="bibr" target="#b4">[5]</ref> but at the expense of higher false alarm rate. The event count breakdown according to type of event is presented in <ref type="table" target="#tab_2">Table 3</ref>, 4 and 5 for Avenue dataset, Subway entrance and exit datasets respectively. All throwing, loitering (LT) and irregular interaction (II) events are well captured by our proposed system. These are strong abnormalities that are significantly different from what was captured in the normal scenes. However, our system does have difficulties in detecting certain types of event. Missed detection of running and walking in opposite direction events are due to (1) the crowded activities where multiple foreground events take place; and (2) the object of interest is far away from the camera. Meanwhile, in Subway entrance and exit scenes, some wrong direction events are missed. On the other hand, some no payment (NP) events in Subway entrance scene are difficult to detect due to their similar motion compared to others walking through the barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Quantitative Analysis: ROC and Anomalous Event Count</head><p>We also present a run-time analysis on our proposed abnormal event detec-     tion system, on CPU (Intel Xeon E5-2620) and GPU (NVIDIA Maxwell Titan X) respectively, in <ref type="table" target="#tab_5">Table 6</ref>. The total time taken is well less than a quarter second per frame for both CPU and GPU configuration. Due to computational intensive multiplication operations when feeding the input through the convolutional autoencoders, it is recommended to run on GPU for a better speed of nearly 30 times faster than CPU. <ref type="figure" target="#fig_2">Figure 4</ref>, 5, and 6 illustrate the output of the proposed system on samples of the Avenue dataset, Subway entrance and exit scenes respectively; our method detects anomalies correctly in these cases even in crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Qualitative Analysis: Visualising Frame Regularity</head><p>Almost all anomalies produce strong downward spikes which indicate a low regularity score, including a difficult-to-detect skateboarding activity as illus-       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Comparing Our Method with 2D Convolutional Autoencoder (ConvAE)</head><p>From <ref type="figure" target="#fig_7">Figure 9</ref> and 10, it is easy to see that our method has detected more abnormal events with fewer false alarms compared to <ref type="bibr" target="#b4">[5]</ref>. As observed in <ref type="figure" target="#fig_0">Figure  11</ref>, our method is able to produce higher regularity score during normal activities and lower scores when there are abnormalities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this research, we have successfully applied deep learning to the challenging video anomaly detection problem. We formulate anomaly detection as a spatiotemporal sequence outlier detection problem and applied a combination of spatial feature extractor and temporal sequencer ConvLSTM to tackle the problem. The ConvLSTM layer not only preserves the advantages of FC-LSTM but is also suitable for spatiotemporal data due to its inherent convolutional structure. By incorporating convolutional feature extractor in both spatial and temporal space into the encoding-decoding structure, we build an end-to-end trainable model for video anomaly detection. The advantage of our model is that it is semi-supervised -the only ingredient required is a long video segment containing only normal events in a fixed view. Despite the models ability to detect abnormal events and its robustness to noise, depending on the activity complexity in the scene, it may produce more false alarms compared to other methods. For future work, we will investigate how to improve the result of video anomaly detection by active learning -having human feedback to update the learned model for better detection and reduced false alarms. One idea is to add a supervised module to the current system, which the supervised module works only on the video segments filtered by our proposed method, then train a discriminative model to classify anomalies when enough video data has been acquired.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed network architecture. It takes a sequence of length T as input, and output a reconstruction of the input sequence. The numbers at the rightmost denote the output size of each layer. The spatial encoder takes one frame at a time as input, after which T = 10 frames have been processed, the encoded features of 10 frames are concatenated and fed into temporal encoder for motion encoding. The decoders mirror the encoders to reconstruct the video volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The zoomed-in architecture at time t, where t is the input vector at this time step. The temporal encoder-decoder model has 3 convolutional LSTM (ConvLSTM) layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Regularity score of video #5 (top) and #15 (bottom) from the Avenue dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Regularity score of frames 115000-120000 from the Subway Entrance video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Regularity score of frames 22500-37500 from the Subway Entrance video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Regularity score of video #1, #8, #24 and #32 (from top to bottom) from UCSD Ped1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Regularity score of video #2, #4, #5 and #7 (from top to bottom) from UCSD Ped2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Comparing our method with ConvAE [5] on Avenue dataset video #7 (top) and #8 (bottom). Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Comparing our method with ConvAE [5] on Subway Exit video frames 10000-22500. Best viewed in colour. trated inFigure 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Comparing our method with ConvAE [5] on Subway Entrance video frames 120000-144000. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of area under ROC curve (AUC) and Equal Error Rate (EER) of different methods. Higher AUC and lower EER are better. Most papers did not publish their AUC/EER for avenue, subway entrance and exit dataset.</figDesc><table><row><cell>Method</cell><cell>Ped1</cell><cell>Ped2</cell><cell>AUC/EER (%) Avenue</cell><cell>Subway Entrance</cell><cell>Subway Exit</cell></row><row><cell>Adam [1]</cell><cell>77.1/38.0</cell><cell>-/42.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SF [16] MPPCA [14]</cell><cell>67.5/31.0 66.8/40.0</cell><cell>55.6/42.0 69.3/30.0</cell><cell></cell><cell>N/A</cell><cell></cell></row><row><cell>MPPCA+SF [14]</cell><cell>74.2/32.0</cell><cell>61.3/36.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOFME [29]</cell><cell>72.7/33.1</cell><cell>87.5/20.0</cell><cell>N/A</cell><cell>81.6/22.8</cell><cell>84.9/17.8</cell></row><row><cell>ConvAE [5]</cell><cell>81.0/27.9</cell><cell>90.0/21.7</cell><cell>70.2/25.1</cell><cell>94.3/26.0</cell><cell>80.7/9.9</cell></row><row><cell>Ours</cell><cell>89.9/12.5</cell><cell>87.4/12.0</cell><cell>80.3/20.7</cell><cell>84.7/23.7</cell><cell>94.0/9.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Anomalous event and false alarm count detected by different methods. GT denotes groundtruth values of event count.</figDesc><table><row><cell>Method</cell><cell cols="3">Anomalous Event Detected / False Alarm Avenue (GT: 47, smaller set GT: 14) Subway Subway Exit Entrance (GT: 66) (GT: 19)</cell></row><row><cell>Sparse combination [13]</cell><cell>12/1 (smaller set)</cell><cell>57/4</cell><cell>19/2</cell></row><row><cell>Space-time MRF [8]</cell><cell>N/A</cell><cell>56/3</cell><cell>18/0</cell></row><row><cell>Online [4]</cell><cell>N/A</cell><cell>60/5</cell><cell>19/2</cell></row><row><cell>ConvAE [5]</cell><cell>45/4</cell><cell>61/15</cell><cell>17/5</cell></row><row><cell>Ours</cell><cell>44/6</cell><cell>61/9</cell><cell>18/10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Anomalous event and false alarm count detected by different methods on various event type in Avenue dataset.</figDesc><table><row><cell></cell><cell>Run</cell><cell>Loiter</cell><cell>Throw</cell><cell>Opposite Direction</cell><cell>False Alarm</cell></row><row><cell>Groundtruth</cell><cell>12</cell><cell>8</cell><cell>19</cell><cell>8</cell><cell>0</cell></row><row><cell>Ours</cell><cell>10</cell><cell>8</cell><cell>19</cell><cell>7</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Anomalous event and false alarm count detected by different methods on various event type in Subway Entrance dataset. WD: wrong direction; NP: no payment; LT: loitering; II: irregular interaction; Misc.: miscellaneous.</figDesc><table><row><cell></cell><cell>WD</cell><cell>NP</cell><cell>LT</cell><cell>II</cell><cell>Misc.</cell><cell>False Alarm</cell></row><row><cell>Groundtruth</cell><cell>26</cell><cell>13</cell><cell>14</cell><cell>4</cell><cell>9</cell><cell>0</cell></row><row><cell>Ours</cell><cell>24</cell><cell>10</cell><cell>14</cell><cell>4</cell><cell>9</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Anomalous event and false alarm count detected by different methods on various event type in Subway Exit dataset. WD: wrong direction; LT: loitering; Misc.: miscellaneous.</figDesc><table><row><cell></cell><cell>WD</cell><cell>LT</cell><cell>Misc.</cell><cell>False Alarm</cell></row><row><cell>Groundtruth</cell><cell>9</cell><cell>3</cell><cell>7</cell><cell>0</cell></row><row><cell>Ours</cell><cell>8</cell><cell>3</cell><cell>7</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Details of run-time during testing (second/frame).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Time (in sec)</cell><cell></cell></row><row><cell></cell><cell cols="2">Preprocessing Representation</cell><cell>Classifying</cell><cell>Total</cell></row><row><cell>CPU</cell><cell>0.0010</cell><cell>0.2015</cell><cell>0.0002</cell><cell>0.2027 (∼5fps)</cell></row><row><cell>GPU</cell><cell>0.0010</cell><cell>0.0058</cell><cell>0.0002</cell><cell>0.0070 (∼143fps)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<meeting>-2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Online detection of abnormal events using incremental coding length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9923" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/22392705" />
		<title level="m">3D Convolutional Neural Networks for Human Action Recognition. Pami</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Persistence1d: Extracting and filtering minima and maxima of 1d functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weinkauf</surname></persName>
		</author>
		<ptr target="http://people.mpi-inf.mpg.de/weinkauf/notes/persistence1d.html" />
		<imprint>
			<biblScope unit="page" from="2017" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abnormal behavior detection via sparse reconstruction analysis of trajectory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -6th International Conference on Image and Graphics</title>
		<meeting>-6th International Conference on Image and Graphics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="807" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Anomaly Detection Using Predictive Convolutional Long Short-Term Memory Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<ptr target="http://scholarworks.rit.edu/theses/9319" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Rochester Institute of</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technology</note>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive sparse representations for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="645" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action and Event Recognition with Fisher Vectors on a Compact Feature Set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751336" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1817" to="1824" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06309" />
		<title level="m">Spatio-temporal video autoencoder with differentiable memory. International Conference On Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011 WORKSHOPS</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An on-line, real-time learning method for detecting anomalies in videos using spatio-temporal compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1436" to="1452" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2969239.2969329" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems. pp. 802-810. NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems. pp. 802-810. NIPS&apos;15<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="j">ImageNet Challenge</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context-aware cnns for person head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2893" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Histograms of optical flow orientation for abnormal events detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation of Tracking and Surveillance, PETS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anomaly detection via local coordinate factorization and spatio-temporal pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<biblScope unit="volume">9007</biblScope>
			<biblScope unit="page" from="66" to="82" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abnormal event detection using HOSF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on IT Convergence and Security</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatialtemporal convolutional neural networks for anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="358" to="368" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unusual event detection in crowded scenes by trajectory analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings. vol. 2015-Augus</title>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="1300" to="1304" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers Inc.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
