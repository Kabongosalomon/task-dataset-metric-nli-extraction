<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences † DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling assigns each token with a label in a sequence. Tasks such as Named Entity Recognition (NER) <ref type="bibr" target="#b19">(Sundheim, 1995)</ref>, Part-Of-Speech (POS) tagging <ref type="bibr" target="#b7">(DeRose, 1988)</ref> and chunking <ref type="bibr" target="#b21">(Tjong Kim Sang and Buchholz, 2000)</ref> can all be formulated as sequence labeling tasks. BiLSTM-CRF <ref type="bibr" target="#b10">(Huang et al., 2015;</ref><ref type="bibr" target="#b13">Lample et al., 2016;</ref><ref type="bibr" target="#b14">Ma and Hovy, 2016)</ref> is one of the most successful neural sequence labeling architectures. It feeds pretrained (contextual) word representations into a single layer bi-directional LSTM (BiLSTM) encoder to extract contextual features and then feeds * Yong Jiang and Kewei Tu are the corresponding authors.</p><p>‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy.</p><p>: {wangxy1, tukw}@shanghaitech.edu.cn, <ref type="bibr">† : {yongjiang.jy, nguyen.bach, leeo.wangt, z.huang, f.huang}@alibaba-inc.com</ref> these features into a CRF <ref type="bibr" target="#b12">(Lafferty et al., 2001)</ref> decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently.</p><p>In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn <ref type="bibr" target="#b16">(Pan et al., 2017)</ref>) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Strubell et al., 2017)</ref>, distill larger encoders into smaller ones <ref type="bibr" target="#b23">(Tsai et al., 2019;</ref><ref type="bibr" target="#b15">Mukherjee and Awadallah, 2020)</ref> or in other settings <ref type="bibr" target="#b24">(Tu and Gimpel, 2018;</ref><ref type="bibr">Yang et al., 2018;</ref><ref type="bibr" target="#b25">Tu and Gimpel, 2019;</ref><ref type="bibr" target="#b6">Cui and Zhang, 2019)</ref>. The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks.</p><p>In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different positions in the sequence, achieving time complexity that is constant in n with full parallelization. Previous work <ref type="bibr">(Zheng et al., 2015)</ref> showed that such an algorithm can be unfolded as an RNN for grid CRF struc- ture. We expand on the work for the linear-chain CRF structure and unfold the algorithm as an RNN which can be connected with the encoder to form an end-to-end neural network that is amenable to parallelization for both training and prediction. We call the unfolded RNN an approximate inference network (AIN). In addition to linear-chain CRFs, we also apply AIN to factorized second-order CRF models, which consider relations between more neighboring labels. Our empirical results show that AIN significantly improves the speed and achieves competitive accuracy against the traditional CRF approach on 4 tasks with 15 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches</head><p>Given an input sequence with n tokens x = [x 1 , x 2 , . . . , x n ] and a corresponding label sequence y = [y 1 , y 2 , . . . , y n ] with a label set of size L, the conditional probability of y given x specified by a CRF with position-wise factorization is:</p><formula xml:id="formula_0">P (y|x) = exp{ n i=1 ψ(x, y, i)} y ∈Y(x) exp{ n i=1 ψ(x, y , i))}</formula><p>where Y(x) is the set of all possible label sequences for x and ψ(x, y, i) is a potential function.</p><p>In the simplest case, the potential function is just a softmax function that outputs the distribution of each label independently. We call it the MaxEnt approach. In a typical linear-chain CRF, the potential function is decomposed into a unary potential ψ u and a binary potential ψ b (called the emission and transition functions respectively):</p><formula xml:id="formula_1">ψ(x, y, i) = ψ u (x, y i ) + ψ b (y i−1 , y i ) (1) ψ u (x, y i ) = r i Wv y i ψ b (y i−1 , y i ) = U y i−1 ,y i<label>(2)</label></formula><p>where r i is the contextual feature of x i output from the CNN or BiLSTM encoder with dimension d, v y i is a one-hot vector for label y i , W is a d × L matrix and U is an L × L matrix containing the transition scores between two labels. The factor graph of a linear-chain CRF is shown at the top of <ref type="figure" target="#fig_0">Figure 1</ref>. The exact probabilistic inference algorithms (Viterbi and forward-backward) for the CRF layer are significantly slower than the MaxEnt approach. They take O(nL 2 ) and O(n log L) time on CPU and GPU 1 respectively, while the decoder in Max-Ent takes O(nL) and O(log L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AIN on Linear-Chain CRF</head><p>In order to speed up the training and prediction time of the CRF layer, we propose the approximate inference network (AIN), which is a neural network derived from MFVI for approximate decoding in linear-chain CRF.</p><p>MFVI approximates the distribution P (y|x) with a factorized distribution</p><formula xml:id="formula_2">Q(y|x) = n i=1 Q i (y i |x) and update it itera- tively to minimize the KL divergence KL(Q||P ). The update formula of Q i (y i |x) at iteration m is: s(i, j, k):= L y i =1 Q k−1 i (y i |x)ψ b (y min{i,j} , y max{i,j} ) Q m i (y i |x)∝ exp{ψ u (x, y i )+s(i−1, i, m) +s(i+1, i, m)}</formula><p>where s(i, j, k) represents the message from node i to node j at time step k. Q 0 i (y i |x) is set by normalizing the unary potential ψ u (x, y i ). Upon convergence, the label sequence with the highest approximate probability Q(y|x) can be found by optimizing Q i (y i |x) at each position i:</p><formula xml:id="formula_3">y i = argmax y i ∈{1,...,L} Q i (y i |x)</formula><p>Similar to Zheng et al. <ref type="formula" target="#formula_1">(2015)</ref>, we unfold the MFVI algorithm as a recurrent neural network that is parameterized by the linear-chain CRF potential functions. We fix the number of iterations to M and call the resulting network AIN. AIN can be connected with the encoding network that computes the potential functions and together they form an end-to-end neural network.</p><p>Note that, different from previous work (Krähenbühl and Koltun, 2011; Zheng et al., 2015; Baqué using the MFVI algorithm for solving intractable problems of densely connected probabilistic models to get better accuracy, we propose to employ the MFVI algorithm to accelerate tractable inference of sequence-structured probabilistic models. As far as we know, this is the first attempt of using approximate inference on tractable models for speedup with GPU parallelization. The time complexity of each iteration of the MFVI algorithm is O(nL 2 ), which is on par with the time complexity of the exact probabilistic inference algorithms. However, in each iteration, the update of each distribution Q i (y i |x) depends only on its two neighboring distributions from the previous iteration, so each iteration can be parallelized over positions. A comparison between the Viterbi algorithm and the MFVI algorithm is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The time complexity of our AIN decoder with full GPU parallelization is O(M log L), while the time complexity of the exact probabilistic inference algorithms with GPU parallelization is O(n log L). We set the value of M to 3s , which is much smaller than the typical value of sequence length n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AIN on Factorized Second-Order CRF</head><p>We can extend AIN to the second-order CRF with a ternary potential function over every three consecutive labels. In the second-order CRF, the potential function in Eq. 1 becomes:</p><formula xml:id="formula_4">ψ(x, y, i) = ψ u (x, y i ) + ψ t (y i−2 , y i−1 , y i )</formula><p>However, the second-order CRF has space and time complexity that is cubic in L. Therefore, we factorize its ternary potential function and reduce its complexity to be quadratic in L:</p><formula xml:id="formula_5">ψ t (y i−2 , y i−1 , y i ) = ψ b (y i−2 , y i ) + ψ b (y i−1 , y i ) ψ b (y i−2 , y i ) =Ũ y i−2 ,y i</formula><p>where the matrixŨ has the same shape as U in Eq. 2. The factor graph of our factorized secondorder CRF is shown at the bottom of <ref type="figure" target="#fig_0">Figure 1</ref>. The update formula is similar to that of our first-order approach but with more neighbors:</p><formula xml:id="formula_6">Q m i (y i |x)∝ exp{ψ u (x, y i )+s (i−2, i, m) +s(i−1, i, m)+s(i+1, i, m)+s (i+2, i, m)}</formula><p>where s (i, j, k) has a similar definition as s(i, j, k) by replacing ψ b with ψ b . The time complexity of this approach is also O(nL 2 ) for each iteration and O(M log L) with full GPU parallelization for M iterations. Following the first approach, we also unfold MFVI of this approach as an AIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning</head><p>Given a sequence x with corresponding gold labels y * = {y * 1 , · · · , y * n }, the learning objective of our approaches is:</p><formula xml:id="formula_7">L NLL = − n i=1 log Q M i (y * i |x)</formula><p>Since AINs are end-to-end neural networks, the objective function can be optimized by any gradientbased method in an end-to-end manner.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Named Entity Recognition (NER) We use the corpora from the CoNLL 2002 and CoNLL 2003 shared tasks <ref type="bibr" target="#b20">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b22">Tjong Kim Sang and De Meulder, 2003)</ref>, which contain four languages in total. We use the standard training/development/test split for experiments. 2</p><p>Chunking The chunking datasets are also from the CoNLL 2003 shared task <ref type="bibr" target="#b22">(Tjong Kim Sang and De Meulder, 2003)</ref> that contains English and German datasets. We use the same standard split as in NER.  Part-Of-Speech (POS) Tagging Universal Dependencies 3 (UD) <ref type="bibr" target="#b1">(Nivre et al., 2018)</ref> contains syntactically annotated corpora of over 70 languages. We use universal POS tag annotations with 8 languages for experiments. The list of treebanks is shown in <ref type="table" target="#tab_3">Table 3</ref>. We use the standard training/development/test split for experiments.</p><p>Slot Filling Slot filling is a task that interprets user commands by extracting relevant slots, which can be formulated as a sequence labeling task. We use the Air Travel Information System (ATIS) <ref type="bibr" target="#b9">(Hemphill et al., 1990)</ref> dataset for the task 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>Embeddings For word embeddings in the NER, chunking and slot filling experiments, we use the same word embedding as in <ref type="bibr" target="#b13">Lample et al. (2016)</ref> except that we use fastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> embedding for Dutch which we find significantly improves the accuracy (more than 5 F1 scores on CoNLL NER). We use fastText embeddings for all UD tagging experiments. For character embedding, we use a single layer character CNN with a hidden size of 50, because Yang et al. (2018) empirically showed that it has competitive performance with character LSTM. We concatenate the word embedding and character CNN output for the final word representation.</p><p>Encoder In our experiments, we use three types of encoders. The first is a BiLSTM fed with word and character embeddings, which captures contextual information globally. The second is a single layer CNN with only word embedding as input, which captures contextual information locally. The third is a single linear layer with word embeddings as input, which does not capture any contextual information. We use these settings for a better understanding of how the decoders perform on each task when the encoders capture different levels of contextual information.</p><p>Decoder We use the MaxEnt approach, the traditional CRF approach and AINs with the first-order and factorized second-order CRFs for decoding. We denote these approaches by MaxEnt, CRF, AIN-1O and AIN-F2O respectively. We set the iteration number M to 3 in AINs because we find that more iterations do not result in further improvement in accuracy.</p><p>Hyper-parameters For the hyper-parameters, we follow the settings of previous work <ref type="bibr" target="#b0">(Akbik et al., 2018)</ref>. We use Stochastic Gradient Descent for optimization with a fixed learning rate of 0.1 and a batch size of 32. We fix the hidden size of the CNN and BiLSTM layer to 512 and 256 respectively, and the kernel size of CNN to 3. We anneal the learning rate by 0.5 if there is no improvement in the development sets for 10 epochs when training.</p><p>Evaluation We use F1 score to evaluate the NER, slot filling and chunking tasks and use accuracy to evaluate the POS tagging task. We convert the BIO format into BIOES format for NERs, slot filling and chunking datasets and use the official release of CoNLL evaluation script 5 to evaluate the F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Speed We report the relative speed improvements over the CRF model based on our PyTorch <ref type="bibr" target="#b17">(Paszke et al., 2019)</ref> implementation run on a GPU server with Nvidia Tesla V100. Following <ref type="bibr" target="#b23">Tsai et al. (2019)</ref>, we report the training and prediction speed with 10,000 sentences of 32 and 128 words, respectively. The results <ref type="table" target="#tab_0">(Table 1)</ref> show that AINs are significantly faster than CRF in terms of both the full model speed and the decoder speed. The speed advantage of AINs is especially prominent with long sentences, suggesting their usefulness in tasks like document-level NER.</p><p>Accuracy We run each approach on each dataset for 5 times and compute its average accuracy. Because of space limit, we report the accuracy averaged over all the datasets for each task in <ref type="table" target="#tab_1">Table 2</ref>. Please refer to the supplementary material for the complete results. AINs achieve competitive overall accuracy with CRF, even though AINs take significantly less time than CRF. With the BiLSTM encoder which has the capability to capture global contextual information, AINs achieves almost the same average accuracy as CRF, demonstrating that AINs performing approximate inference with local contextual information are competitive with CRF with globally exact decoding. With the CNN encoder that encodes local contextual information, AINs are inferior to CRF because both the CNN layer and our approaches utilize only local information. Without any contextual encoders (Word Only), the accuracy of these decoders vary significantly over tasks. For NER and chunking, CRF is the strongest, but our approaches only marginally underperform CRF while significantly outperform MaxEnt. For POS tagging and slot filling, our approaches outperform CRF, which implies that local information might be more beneficial for these tasks. Comparing AIN-1O and AIN-F2O, AIN-F2O is stronger when the encoder is weak, but their performance gap becomes smaller and eventually disappears when the encoder gets stronger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion on Transformers</head><p>Recently, the Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> encoder has significantly improved the performance of tasks such as neural machine translation. The Transformer can be parallelized over the input words while the BiLSTM layer needs sequential computation. However, the transformer structure is rarely applied in sequence labeling tasks. One possible reason is that the performance of models with Transformers encoders are inferior to the performance of models with the BiLSTM encoders.</p><p>In our experiments, a six-layer transformer with a MaxEnt decoder achieves an F1 score of only 80.00 on CoNLL English NER, which is significantly lower than the 91.00 F1 score of our BiL-STM+MaxEnt model <ref type="table">(Table 5)</ref>. For the speed, the six-layer transformer model with a MaxEnt decoder is 1.58/1.14 times slower than the singlelayer BiLSTM model with a MaxEnt decoder with sentences of 32/128 words respectively. Therefore, we do not include the Transformer encoder in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose approximate inference networks (AIN) that use Mean-Field Variational Inference (MFVI) instead of exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms for training and prediction on the conditional random field for sequence labeling. The MFVI algorithm can be unfolded as a recurrent neural network and connected with the encoder to form an end-to-end neural network. AINs can be parallelized over different positions in the sequence. Empirical results show that AINs are significantly faster than traditional CRF and do very well in tasks that require more local information. Our approaches achieve competitive accuracy on 4 tasks with 15 datasets over three encoder types. Our code is publicly available at https://github.com/Alibaba-NLP/AIN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Detailed Results</head><p>The detailed results for the four tasks are shown in <ref type="table" target="#tab_5">Table 4</ref> and 5. We use ISO 639-1 codes 6 to represent each language.   <ref type="table">Table 5</ref>: Averaged F1 scores on NER, chunking and slot filling for each language. SF represents the slot filling task. : for reference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FactorizedFigure 1 :</head><label>1</label><figDesc>Factor graphs of different CRFs. Y i is the random variable representing the i-th label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the computation graphs for the Viterbi decoding and one iteration of our MFVI inference on the CRF model. Y i is the random variable representing the i-th label with three possible values. The illustrated vectors represent Viterbi scores and Q i distributions respectively.<ref type="bibr" target="#b4">Chen et al., 2018;</ref><ref type="bibr" target="#b27">Wang et al., 2019)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Relative speedup over the CRF model with 10,000 sentences of 32/128 words. All represents the speed of the full model. Dec. represents the speed of decoder. : For reference. .91 92.88 95.52 91.87 79.44 94.26 89.21 92.24 88.79 72.28 92.79 89.39 76.82 82.82 AIN-1O 84.22 94.97 92.87 95.59 91.91 78.47 94.29 88.86 92.18 88.45 70.23 92.84 88.69 88.76 85.13 AIN-F2O 84.11 94.91 92.85 95.58 91.86 78.71 94.32 88.75 92.26 88.51 71.16 93.03 88.80 88.86 85.46</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">WORD-CHAR-BILSTM</cell><cell></cell><cell></cell><cell></cell><cell cols="2">WORD-CNN</cell></row><row><cell></cell><cell></cell><cell cols="2">Training</cell><cell></cell><cell></cell><cell cols="2">Prediction</cell><cell></cell><cell cols="2">Training</cell><cell cols="2">Prediction</cell></row><row><cell># Words</cell><cell>32</cell><cell></cell><cell cols="2">128</cell><cell>32</cell><cell></cell><cell>128</cell><cell></cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>128</cell></row><row><cell></cell><cell>All</cell><cell>Dec.</cell><cell>All</cell><cell>Dec.</cell><cell>All</cell><cell>Dec.</cell><cell>All</cell><cell>Dec.</cell><cell>All</cell><cell>All</cell><cell>All</cell><cell>All</cell></row><row><cell>MaxEnt</cell><cell>6.8×</cell><cell>-</cell><cell>13.1×</cell><cell>-</cell><cell>3.0×</cell><cell>-</cell><cell>5.9×</cell><cell>-</cell><cell cols="4">12.9× 40.1× 6.3× 18.6×</cell></row><row><cell>AIN-1O</cell><cell cols="8">4.3× 7.7× 10.2× 31.4× 1.7× 2.4× 4.4× 12.7×</cell><cell>5.6×</cell><cell cols="2">21.5× 2.4×</cell><cell>6.8×</cell></row><row><cell>AIN-F2O</cell><cell cols="2">3.5× 5.3×</cell><cell>8.7×</cell><cell cols="5">20.1× 1.5× 1.9× 4.1× 10.6×</cell><cell>4.4×</cell><cell cols="2">16.7× 1.8×</cell><cell>5.5×</cell></row><row><cell></cell><cell cols="4">WORD-CHAR-BILSTM</cell><cell></cell><cell cols="2">WORD-CNN</cell><cell></cell><cell></cell><cell cols="2">WORD ONLY</cell></row><row><cell></cell><cell cols="12">NER POS Chunk SF Avg. NER POS Chunk SF Avg. NER POS Chunk SF Avg.</cell></row><row><cell>MaxEnt</cell><cell cols="12">83.74 94.84 92.58 95.47 91.65 75.19 94.00 87.05 91.07 86.83 52.27 90.53 78.17 62.93 70.98</cell></row><row><cell>CRF</cell><cell>84.17 94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Averaged F1 score and accuracy on four tasks. SF represents the slot filling task. : For reference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The list of treebank that we used in UD POS tagging.</figDesc><table><row><cell>Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-</cell></row><row><cell>Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,</cell></row><row><cell>Chang Huang, and Philip HS Torr. 2015. Condi-</cell></row><row><cell>tional random fields as recurrent neural networks. In</cell></row><row><cell>Proceedings of the IEEE international conference on</cell></row><row><cell>computer vision, pages 1529-1537.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>19±0.04 95.70±0.07 96.44±0.05 98.00±0.06 92.76±0.17 95.09±0.13 90.96±0.69 95.56±0.07 94.84 CRF 94.27±0.11 95.71±0.06 96.37±0.09 98.06±0.04 92.87±0.15 95.10±0.17 91.38±1.12 95.55±0.09 94.91 AIN-1 94.23±0.06 95.73±0.05 96.39±0.10 98.04±0.10 93.13±0.19 95.10±0.20 91.42±0.28 95.69±0.05 94.97 AIN-F2 94.11±0.22 95.76±0.05 96.34±0.05 97.99±0.11 92.87±0.20 95.24±0.16 91.38±0.44 95.59±0.07 94.91 WORD CNN MaxEnt 92.36±0.19 93.99±0.12 95.91±0.06 97.62±0.05 92.49±0.08 94.51±0.08 91.39±0.18 93.76±0.15 94.00 CRF 93.06±0.17 94.22±0.10 96.09±0.08 97.68±0.07 92.63±0.05 94.63±0.16 91.65±0.23 94.15±0.17 94.26 AIN-1 93.11±0.14 94.21±0.05 96.02±0.06 97.73±0.05 92.64±0.06 94.58±0.07 91.77±0.20 94.26±0.11 94.29 AIN-F2 92.99±0.12 94.17±0.13 96.00±0.04 97.75±0.03 92.69±0.06 94.68±0.04 91.84±0.23 94.47±0.10 94.32 WORD ONLY MaxEnt 89.44±0.08 87.57±0.12 93.02±0.05 94.82±0.07 89.23±0.08 91.63±0.17 88.56±0.24 90.01±0.06 90.53 CRF 91.55±0.13 91.04±0.22 94.64±0.05 96.65±0.10 91.56±0.05 93.28±0.12 90.02±0.24 93.55±0.09 92.79 AIN-1 91.53±0.08 91.47±0.09 94.77±0.05 96.67±0.05 91.62±0.03 93.46±0.03 89.65±0.37 93.54±0.10 92.84 AIN-F2 91.75±0.09 91.76±0.12 94.82±0.03 96.95±0.05 91.63±0.06 93.32±0.13 90.17±0.23 93.86±0.09 93.03</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>POS TAGGING</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>nl</cell><cell>sl</cell><cell>sv</cell><cell>avg</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WORD-CHAR-BILSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MaxEnt 94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Averaged accuracy scores on POS tagging. 63±0.23 91.00±0.23 84.53±0.50 83.78±0.38 83.74 93.80±0.14 91.36±0.10 92.58 95.47±0.06 CRF 76.46±0.24 91.14±0.16 85.29±0.36 83.80±0.33 84.17 94.06±0.07 91.70±0.08 92.88 95.52±0.10 AIN-1O 76.34±0.34 91.07±0.10 85.37±0.07 84.12±0.53 84.22 94.03±0.02 91.71±0.05 92.87 95.59±0.11 AIN-F2O 76.17±0.28 91.22±0.20 85.30±0.32 83.76±0.57 84.11 94.02±0.04 91.69±0.08 92.85 95.58±0.14 WORD CNN MaxEnt 69.40±0.15 84.86±0.41 70.02±0.62 76.46±0.28 75.19 88.29±0.10 85.80±0.65 87.05 91.07±0.01 CRF 71.12±0.25 87.58±0.21 80.34±0.58 78.70±0.30 79.44 89.68±0.21 88.73±0.18 89.21 92.24±0.27 AIN-1O 70.00±0.28 86.94±0.43 78.95±0.51 77.98±0.38 78.47 89.21±0.11 88.51±0.15 88.86 92.18±0.14 AIN-F2O 70.08±0.92 87.01±0.22 79.80±0.38 77.95±0.47 78.71 89.33±0.12 88.16±0.30 88.75 92.26±0.26 WORD ONLY MaxEnt 36.24±1.77 63.68±1.08 52.42±1.73 56.73±0.77 52.27 81.21±0.33 75.14±0.41 78.17 62.93±0.33 CRF 55.10±2.87 81.76±0.39 76.53±0.80 75.71±0.39 72.28 90.56±0.24 88.21±0.34 89.39 76.82±0.57 AIN-1O 57.25±2.16 79.68±0.25 70.44±0.72 73.55±0.21 70.23 90.04±0.18 87.35±0.29 88.69 88.76±0.65 AIN-F2O 56.36±5.97 81.16±0.37 73.03±1.86 74.09±0.24 71.16 90.04±0.15 87.56±0.24 88.8 88.86±0.41</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>NER</cell><cell></cell><cell></cell><cell></cell><cell>CHUNK</cell><cell></cell><cell>SF</cell></row><row><cell>Models</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>nl</cell><cell>avg</cell><cell>de</cell><cell>en</cell><cell>avg</cell><cell>en</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WORD-CHAR-BILSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MaxEnt</cell><cell>75.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We assume that the number of threads is enough for full parallelization on GPU and the parallel reduction (e.g., sum and max) for a L elements vector takes O(log L) time<ref type="bibr" target="#b8">(Harris et al., 2007)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.clips.uantwerpen.be/conll2003/ner/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://lindat.mff.cuni.cz/repository/xmlui/handle/ 11234/1-28374  We use the same dataset split as https://github.com/sz128/ slot_filling_and_intent_detection_of_SLU/tree/master/data/ atis-2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/chakki-works/seqeval/blob/master/ tests/conlleval.pl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (61976139). This work also was supported by Alibaba Group through Alibaba Innovative Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal dependencies 2.2. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Faculty of Mathematics and Physics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principled parallel mean-field inference for discrete random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baqué</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5848" to="5857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchicallyrefined label attention network for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1422</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4119" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grammatical category disambiguation by statistical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Optimizing parallel reduction in cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Nvidia developer technology 2.4.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA. Morgan</addrLine></address></meeting>
		<imprint>
			<publisher>Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tinymbert: Multi-stage distillation framework for massive multi-lingual ner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05686</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Named entity task definition, version 2.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Message Understanding Conference</title>
		<meeting>the Sixth Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="319" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2000 shared task chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Small and practical BERT models for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelia</forename><surname>Archer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3632" to="3636" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning approximate inference networks for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benchmarking approximate inference methods for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3313" to="3324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Second-order semantic dependency parsing with end-to-end neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4609" to="4618" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
