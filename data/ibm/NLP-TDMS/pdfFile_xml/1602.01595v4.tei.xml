<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Many Languages, One Parser</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
							<email>wammar@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">♠ NLP Group</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Pompeu Fabra University</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
							<email>miguel.ballesteros@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">♠ NLP Group</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Pompeu Fabra University</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Many Languages, One Parser</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The parsing model uses (i) multilingual word clusters and embeddings; (ii) token-level language information; and (iii) language-specific features (finegrained POS tags). This input representation enables the parser not only to parse effectively in multiple languages, but also to generalize across languages based on linguistic universals and typological similarities, making it more effective to learn from limited annotations. Our parser's performance compares favorably to strong baselines in a range of data scenarios, including when the target language has a large treebank, a small treebank, or no treebank for training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Developing tools for processing many languages has long been an important goal in NLP <ref type="bibr" target="#b21">(Rösner, 1988;</ref><ref type="bibr" target="#b8">Heid and Raab, 1989</ref>), 1 but it was only when statistical methods became standard that massively multilingual NLP became economical. The mainstream approach for multilingual NLP is to design language-specific models. For each language of interest, the resources necessary for training the model are obtained (or created), and separate parameters are fit for each language separately. This approach is simple and grants the flexibility of customizing the model and features to the needs of each language, but it is suboptimal for theoretical and practical reasons. Theoretically, the study of linguistic typology tells us that many languages share morphological, phonological, and syntactic phenomena <ref type="bibr" target="#b1">(Bender, 2011)</ref>; therefore, the mainstream approach misses an opportunity to exploit relevant supervision from typologically related languages. Practically, it is inconvenient to deploy or distribute NLP tools that are customized for many different languages because, for each language of interest, we need to configure, train, tune, monitor, and occasionally update the model. Furthermore, code-switching or code-mixing (mixing more than one language in the same discourse), which is pervasive in some genres, in particular social media, presents a challenge for monolingually-trained NLP models <ref type="bibr" target="#b0">(Barman et al., 2014)</ref>. <ref type="bibr">2</ref> In parsing, the availability of homogeneous syntactic dependency annotations in many languages <ref type="bibr">Nivre et al., 2015b;</ref><ref type="bibr" target="#b0">Agić et al., 2015;</ref><ref type="bibr" target="#b17">Nivre et al., 2015a)</ref> has created an opportunity to develop a parser that is capable of parsing sentences in multiple languages, addressing these theoretical and practical concerns. 3 A multilingual parser can potentially replace an array of language-specific monolingually-trained parsers (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality <ref type="bibr" target="#b2">(Cohen et al., 2011;</ref><ref type="bibr" target="#b16">McDonald et al., 2011;</ref><ref type="bibr">Zhang and Barzilay, 2015;</ref><ref type="bibr" target="#b3">Duong et al., 2015a;</ref><ref type="bibr" target="#b3">Duong et al., 2015b;</ref><ref type="bibr" target="#b7">Guo et al., 2016)</ref>, but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios.</p><p>We propose a parsing architecture that takes as input sentences in several languages, 4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets , cross-lingual word clusters <ref type="bibr">(Täckström et al., 2012)</ref>, selective sharing <ref type="bibr" target="#b17">(Naseem et al., 2012)</ref>, universal dependency annotations <ref type="bibr">Nivre et al., 2015b;</ref><ref type="bibr" target="#b0">Agić et al., 2015;</ref><ref type="bibr" target="#b17">Nivre et al., 2015a)</ref>, advances in neural network architectures <ref type="bibr" target="#b2">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b5">Dyer et al., 2015)</ref>, and multilingual word embeddings <ref type="bibr">(Gardner et al., 2015;</ref><ref type="bibr" target="#b7">Guo et al., 2016;</ref><ref type="bibr">Ammar et al., 2016)</ref>. We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank <ref type="table" target="#tab_3">(Table 3)</ref>, a small treebank <ref type="table" target="#tab_9">(Table 7)</ref>, or no treebank <ref type="table" target="#tab_8">(Table 8</ref>). Our parser is publicly available. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Our goal is to train a dependency parser for a set of target languages L t , given universal dependency annotations in a set of source languages L s . Ideally, we would like to have training data in all target languages (i.e., L t ⊆ L s ), but we are also interested in the case where the sets of source and target languages are disjoint (i.e., L t ∩ L s = ∅). When all languages in L t have a large treebank, the mainstream approach has been to train one monolingual parser per target language and route sentences of a <ref type="bibr">4</ref> We discuss data requirements in the next section. 5 https://github.com/clab/ language-universal-parser given language to the corresponding parser at test time. In contrast, our approach is to train one parsing model with the union of treebanks in L s , then use this single trained model to parse text in any language in L t , hence the name "Many Languages, One Parser" (MALOPA). MALOPA strikes a balance between: (1) enabling cross-lingual model transfer via language-invariant input representations; i.e., coarse POS tags, multilingual word embeddings and multilingual word clusters, and (2) tweaking the behavior of the parser depending on the current input language via language-specific representations; i.e., fine-grained POS tags and language embeddings.</p><p>In addition to universal dependency annotations in source languages (see <ref type="table" target="#tab_1">Table 1</ref>), we use the following data resources for each language in L = L t ∪L s :</p><p>• universal POS annotations for training a POS tagger, 6</p><p>• a bilingual dictionary with another language in L for adding cross-lingual lexical information, 7</p><p>• language typology information, 8</p><p>• language-specific POS annotations, 9 and</p><p>• a monolingual corpus. 10 Novel contributions of this paper include: (i) using one parser instead of an array of monolinguallytrained parsers without sacrificing accuracy on languages with a large treebank, (ii) an effective neural network architecture for using language embeddings to improve multilingual parsing, and (iii) a study of how automatic language identification affects the performance of a multilingual dependency parser.</p><p>While not the primary focus of this paper, we also show that a variant of our parser outperforms previous work on multi-source cross-lingual parsing in 6 See §3.6 for details. 7 Our best results make use of this resource. We require that all languages in L are (transitively) connected. The bilingual dictionaries we used are based on unsupervised word alignments of parallel corpora, as described in <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. See §3.3 for details. 8 See §3.4 for details. 9 Our best results make use of this resource. See §3.5 for details.</p><p>10 This is only used for training word embeddings with 'mul-tiCCA,' 'multiCluster' and 'translation-invariance' methods in <ref type="table">Table 6</ref>. We do not use this resource when we compare to previous work.  low resource scenarios, where languages in L t have a small treebank (see <ref type="table" target="#tab_9">Table 7</ref>) or where L t ∩ L s = ∅ (see <ref type="table" target="#tab_8">Table 8</ref>). In the small treebank setup with 3,000 token annotations, we show that our parser consistently outperforms a strong monolingual baseline with 5.7 absolute LAS (labeled attachment score) points per language, on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parsing Model</head><p>Recent advances suggest that recurrent neural networks, especially long short-term memory (LSTM) architectures, are capable of learning useful representations for modeling problems of sequential nature <ref type="bibr">(Graves et al., 2013;</ref><ref type="bibr" target="#b22">Sutskever et al., 2014)</ref>.</p><p>In this section, we describe our language-universal parser, which extends the stack LSTM (S-LSTM) parser of <ref type="bibr" target="#b5">Dyer et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transition-based Parsing with S-LSTMs</head><p>This section briefly reviews Dyer et al.'s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures:</p><p>• a buffer (from which we read the token sequence),</p><p>• a stack (which contains partially-built parse trees), and</p><p>• a list of actions previously taken by the parser.</p><p>The parser uses the arc-standard transition system <ref type="bibr" target="#b18">(Nivre, 2004)</ref>. 11 At each timestep t, a transition action is applied that alters these data structures according to <ref type="table">Table 2</ref>.</p><p>Along with the discrete transitions of the arcstandard system, the parser computes vector representations for the buffer, stack and list of actions at time step t denoted b t , s t , and a t , respectively. <ref type="bibr">12</ref> The parser state at time t is given by:</p><formula xml:id="formula_0">p t = max {0, W[s t ; b t ; a t ] + W bias }<label>(1)</label></formula><p>where the matrix W and the vector W bias are learned parameters. The matrix W is multiplied by the vector [s t ; b t ; a t ] created by the concatenation of s t , b t , a t . The parser state p t is then used to define a categorical distribution over possible next actions z: 13</p><formula xml:id="formula_1">p(z | p t ) = exp g z p t + q z z exp g z p t + q z<label>(2)</label></formula><p>where g z and q z are parameters associated with action z. The selected action is then used to update the buffer, stack and list of actions, and to compute b t+1 , s t+1 and a t+1 accordingly. The model is trained to maximize the loglikelihood of correct actions. At test time, the parser greedily chooses the most probable action in every time step until a complete parse tree is produced.</p><p>The following sections describe our extensions of the core parser. More details about the core parser can be found in <ref type="bibr" target="#b5">Dyer et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token Representations</head><p>The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in <ref type="bibr" target="#b5">Dyer et al. (2015)</ref>. <ref type="bibr">13</ref> The total number of actions is 1+2× the number of unique dependency labels in the treebank used for training, but we only consider actions which meet the arc-standard preconditions in Stack t Buffer t Action <ref type="table">Table 2</ref>: Parser transitions indicating the action applied to the stack and buffer at time t and the resulting stack and buffer at time t + 1.</p><formula xml:id="formula_2">Dependency Stack t+1 Buffer t+1 u, v, S B REDUCE-RIGHT(r) u r → v u, S B u, v, S B REDUCE-LEFT(r) u r ← v v, S B S u, B SHIFT - u, S B</formula><p>For monolingual parsing, we represent each token by concatenating the following vectors:</p><p>• a fixed, pretrained embedding of the word type,</p><p>• a learned embedding of the word type,</p><p>• a learned embedding of the Brown cluster,</p><p>• a learned embedding of the fine-grained POS tag,</p><p>• a learned embedding of the coarse POS tag.</p><p>For multilingual parsing with MALOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in MALOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical Embeddings</head><p>Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser <ref type="bibr" target="#b2">(Cohen et al., 2011;</ref><ref type="bibr">Täckström et al., 2012;</ref><ref type="bibr" target="#b25">Tiedemann, 2015;</ref>. Therefore, we extend the token representation in MALOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types.</p><p>Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the 'projected clusters' method in <ref type="bibr">Täckström et al. (2012)</ref>. To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster.</p><p>Multilingual word embeddings. We also use <ref type="bibr">Guo et al.'s (2016)</ref> 'robust projection' method to pretrain multilingual word embeddings. The first step in 'robust projection' is to learn embeddings for English words using the skip-gram model <ref type="bibr" target="#b16">(Mikolov et al., 2013)</ref>. Then, we compute an embedding of non-English words as the weighted average of English word embeddings, using word alignment probabilities as weights. The last step computes an embedding of non-English words which are not aligned to any English words by averaging the embeddings of all words within an edit distance of 1 in the same language. We experiment with two other methods-'multiCCA' and 'multiCluster,' both proposed by Ammar et al. (2016)-for pretraining multilingual word embeddings in §4.1. 'MultiCCA' uses a linear operator to project pretrained monolingual embeddings in each language (except English) to the vector space of pretrained English word embeddings, while 'multiCluster' uses the same embedding for translationally-equivalent words in different languages. The results in <ref type="table">Table 6</ref> illustrate that the three methods perform similarly on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Language Embeddings</head><p>While many languages, especially ones that belong to the same family, exhibit some similar syntactic phenomena (e.g., all languages have subjects, verbs, and objects), substantial syntactic differences abound. Some of these differences are easy to characterize (e.g., subject-verb-object vs. verb-subjectobject, prepositions vs. postpositions, adjectivenoun vs. noun-adjective), while others are subtle (e.g., number and positions of negation morphemes). It is not at all clear how to translate descriptive facts about a language's syntax into features for a parser.</p><p>Consequently, training a language-universal parser on treebanks in multiple source languages requires caution. While exposing the parser to a diverse set of syntactic patterns across many languages has the potential to improve its performance in each, dependency annotations in one language will, in some ways, contradict those in typologically different languages.</p><p>For instance, consider a context where the next word on the buffer is a noun, and the top word on the stack is an adjective, followed by a noun. Treebanks of languages where postpositive adjectives are typical (e.g., French) will often teach the parser to predict REDUCE-LEFT, while those of languages where prepositive adjectives are more typical (e.g., English) will teach the parser to predict SHIFT.</p><p>Inspired by <ref type="bibr" target="#b17">Naseem et al. (2012)</ref>, we address this problem by informing the parser about the input language it is currently parsing. Let l be the input vector representation of a particular language. We consider three definitions for l: 14</p><p>• one-hot encoding of the language ID,</p><p>• one-hot encoding of individual word-order properties, 15 and</p><p>• averaged one-hot encoding of WALS typological properties (including word-order properties). <ref type="bibr">16</ref> It is worth noting that the first definition (language ID) turns out to work best in our experiments.</p><p>We use a hidden layer with tanh nonlinearity to compute the language embedding l as:</p><formula xml:id="formula_3">l = tanh(Ll + L bias )</formula><p>where the matrix L and the vector L bias are additional model parameters. We modify the parsing architecture as follows:</p><p>• include l in the token representation (which feeds into the stack-LSTM modules of the buffer and the stack as described in §3.1), <ref type="bibr">14</ref> The files which contain these definitions are available at https://github.com/clab/ language-universal-parser/tree/master/ typological_properties. <ref type="bibr">15</ref> The World Atlas of Language Structures (WALS; Dryer and Haspelmath, 2013) is an online portal documenting typological properties of 2,679 languages (as of July 2015). We use the same set of WALS features used by <ref type="bibr">Zhang and Barzilay (2015)</ref>, namely 82A (order of subject and verb), 83A (order of object and verb), 85A (order of adposition and noun phrase), 86A (order of genitive and noun), and 87A (order of adjective and noun). <ref type="bibr">16</ref> Some WALS features are not annotated for all languages. Therefore, we use the average value of all languages in the same genus. We rescale all values to be in the range [−1, 1].</p><p>• include l in the action vector representation (which feeds into the stack-LSTM module that represents previous actions as described in §3.1), and</p><p>• redefine the parser state at time t as</p><formula xml:id="formula_4">p t = max {0, W[s t ; b t ; a t ; l ] + W bias }.</formula><p>Intuitively, the first two modifications allow the input language to influence the vector representation of the stack, the buffer and the list of actions. The third modification allows the input language to influence the parser state which in turn is used to predict the next action. In preliminary experiments, we found that adding the language embeddings at the token and action level is important. We also experimented with computing more complex functions of (s t , b t , a t , l ) to define the parser state, but they did not help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-grained POS Tag Embeddings</head><p>Tiedemann <ref type="formula" target="#formula_0">(2015)</ref> shows that omitting fine-grained POS tags significantly hurts the performance of a dependency parser. However, those fine-grained POS tagsets are defined monolingually and are only available for a subset of the languages with universal dependency treebanks.</p><p>We extend the token representation to include a fine-grained POS embedding (in addition to the coarse POS embedding). We stochastically dropout the fine-grained POS embedding for each token with 50% probability <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> so that the parser can make use of fine-grained POS tags when available but stay reliable when the fine-grained POS tags are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Predicting POS Tags</head><p>The model discussed thus far conditions on the POS tags of words in the input sentence. However, gold POS tags may not be available in real applications (e.g., parsing the web). Here, we describe two modifications to (i) model both POS tagging and dependency parsing, and (ii) increase the robustness of the parser to incorrect POS predictions.</p><p>Tagging model. Let x 1 , . . . , x n , y 1 , . . . , y n , z 1 , . . . , z 2n be the sequence of words, POS tags, and parsing actions, respectively, for a sentence of length n. We define the joint distribution of a POS tag sequence and parsing actions given a sequence of words as follows:</p><formula xml:id="formula_5">p(y 1 , . . . , y n , z 1 , . . . , z 2n | x 1 , . . . , x n ) = n i=1 p(y i | x 1 , . . . , x n ) × 2n j=1 p(z j | x 1 , . . . , x n , y 1 , . . . , y n , z 1 , . . . , z j−1 )</formula><p>where p(z j | . . .) is defined in Eq. 2, and p(y i | x 1 , . . . , x n ) uses a bidirectional LSTM <ref type="bibr">(Graves et al., 2013)</ref>. <ref type="bibr" target="#b11">Huang et al. (2015)</ref> show that the performance of a bidirectional LSTM POS tagger is on par with a conditional random field tagger.</p><p>We use slightly different token representations for tagging and parsing in the same model. For tagging, we construct the token representation by concatenating the embeddings of the word type (pretrained), the Brown cluster and the input language. This token representation feeds into the bidirectional LSTM, followed by a softmax layer (at each position) which defines a categorical distribution over possible POS tags. For parsing, we construct the token representation by further concatenating the embeddings of predicted POS tags. This token representation feeds into the stack-LSTM modules of the buffer and stack components of the transition-based parser. This multi-task learning setup enables us to predict both POS tags and dependency trees in the same model. We note that pretrained word embeddings, cluster embeddings and language embeddings are shared for tagging and parsing.</p><p>Block dropout. We use an independently developed variant of word dropout <ref type="bibr" target="#b13">(Iyyer et al., 2015)</ref>, which we call block dropout. The token representation used for parsing includes the embedding of predicted POS tags, which may be incorrect. We introduce another modification which makes the parser more robust to incorrect POS tag predictions, by stochastically zeroing out the entire embedding of the POS tag. While training the parser, we replace the POS embedding vector e with another vector (of the same dimensionality) stochastically computed as: e = (1 − b)/µ × e, where b ∈ {0, 1} is a Bernoulli-distributed random variable with parameter µ which is initialized to 1.0 (i.e., always dropout, setting b = 1, e = 0), and is dynamically updated to match the error rate of the POS tagger on the development set. At test time, we never dropout the predicted POS embedding, i.e., e = e. Intuitively, this method extends the dropout method <ref type="bibr" target="#b22">(Srivastava et al., 2014)</ref> to address structured noise in the input layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the MALOPA approach in three data scenarios: when the target language has a large treebank <ref type="table" target="#tab_3">(Table 3)</ref>, a small treebank <ref type="table" target="#tab_9">(Table 7)</ref> or no treebank <ref type="table" target="#tab_8">(Table 8)</ref>.</p><p>Data. For experiments where the target language has a large treebank, we use the standard data splits for German (de), English (en), Spanish (es), French (fr), Italian (it), Portuguese (pt) and Swedish (sv) in the latest release (version 1.2) of Universal Dependencies <ref type="bibr" target="#b17">(Nivre et al., 2015a)</ref>, and experiment with both gold and predicted POS tags. For experiments where the target language has no treebank, we use the standard splits for these languages in the older universal dependency treebanks v2.0  and use gold POS tags, following the baselines (Zhang and Barzilay, 2015; <ref type="bibr" target="#b7">Guo et al., 2016)</ref>. <ref type="table" target="#tab_1">Table 1</ref> gives the number of sentences and words annotated for each language in both versions. In a preprocessing step, we lowercase all tokens and remove multi-word annotations and language-specific dependency relations. We use the same multilingual Brown clusters and multilingual embeddings of <ref type="bibr" target="#b7">Guo et al. (2016)</ref>, kindly provided by the authors.</p><p>Optimization. We follow <ref type="bibr" target="#b5">Dyer et al. (2015)</ref> in parameter initialization and optimization. 17 However, when training the parser on multiple languages <ref type="bibr">17</ref> We use stochastic gradient updates with an initial learning rate of η0 = 0.1 in epoch #0, update the learning rate in following epochs as ηt = η0/(1 + 0.1t). We clip the 2 norm of the gradient to avoid "exploding" gradients. Unlabeled attachment score (UAS) on the development set determines early stopping. Parameters are initialized with uniform samples in ± 6/(r + c) where r and c are the sizes of the previous and following layer in the nueral network <ref type="bibr" target="#b6">(Glorot and Bengio, 2010)</ref>. The standard deviations of the labeled attachment score (LAS) due to random initialization in individual target languages are 0.36 (de), 0.40 (en), 0.37 (es), 0.46 (fr), 0.47 (it), 0.41 (pt) and 0.24 (sv). The standard deviation of the average LAS scores across languages is 0.17.  in MALOPA, instead of updating the parameters with the gradient of individual sentences, we use mini-batch updates which include one sentence sampled uniformly (without replacement) from each language's treebank, until all sentences in the smallest treebank are used (which concludes an epoch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAS</head><p>We repeat the same process in following epochs. We found this to help prevent one source language with a larger treebank (e.g., German) from dominating parameter updates at the expense of other source languages with a smaller treebank (e.g., Swedish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Target Languages with a Treebank</head><formula xml:id="formula_6">(L t = L s )</formula><p>Here, we evaluate our MALOPA parser when the target language has a treebank.</p><p>Baseline. For each target language, the strong baseline we use is a monolingually-trained S-LSTM parser with a token representation which concatenates: pretrained word embeddings (50 dimensions), 18 learned word embeddings (50 dimensions), coarse (universal) POS tag embeddings (12 dimensions), fine-grained (language-specific, when available) POS tag embeddings (12 dimensions), and embeddings of Brown clusters (12 dimensions), and uses a two-layer S-LSTM for each of the stack, the buffer and the list of actions. We independently train one baseline parser for each target language, and share no model parameters. This baseline, denoted 18 These embeddings are treated as fixed inputs to the parser, and are not optimized towards the parsing objective. We use the same embeddings used in <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. <ref type="table" target="#tab_3">Tables 3 and 7</ref>, achieves UAS score 93.0 and LAS score 91.5 when trained on the English Penn Treebank, which is comparable to <ref type="bibr" target="#b5">Dyer et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'monolingual' in</head><p>MALOPA. We train MALOPA on the concantenation of training sections of all seven languages. To balance the development set, we only concatenate the first 300 sentences of each language's development section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token</head><p>representations. The first MAL-OPA parser we evaluate uses only coarse POS embeddings to construct the token representation. <ref type="bibr">19</ref> As shown in <ref type="table" target="#tab_3">Table 3</ref>, this parser consistently underperforms the monolingual baselines, with a gap of 12.5 LAS points on average.</p><p>Augmenting the token representation with lexical embeddings to the token representation (both multilingual word clusters and pretrained multilingual word embeddings, as described in §3.3) substantially improves the performance of MALOPA, recovering 83% of the gap in average performance.</p><p>We experimented with three ways to include language information in the token representation, namely: 'language ID', 'word order' and 'full typology' (see §3.4 for details), and found all three to improve the performance of MALOPA giving LAS scores 83.5, 83.2 and 82.5, respectively. It is noteworthy that the model benefits more from lan-  guage ID than from typological properties. Using 'language ID,' we recover another 12% of the original gap. Finally, the best configuration of MALOPA adds fine-grained POS embeddings to the token representation. 20 Surprisingly, adding fine-grained POS embeddings improves the performance even for some languages where fine-grained POS tags are not available (e.g., Spanish). This parser outperforms the monolingual baseline in five out of seven target languages, and wins on average by 0.3 LAS points. We emphasize that this model is only trained once on all languages, and the same model is used to parse the test set of each language, which simplifies the distribution or deployment of multilingual parsing software.</p><p>Qualitative analysis. To gain a better understanding of the model behavior, we analyze certain classes of dependency attachments/relations in German, which has notably flexible word order, in Table 4. We consider the recall of left attachments (where the head word precedes the dependent word in the sentence), right attachments, root attachments, short-attachments (with distance = 1), longattachments (with distance &gt; 6), as well as the following relation groups: nsubj* (nominal subjects: 20 Fine-grained POS tags were only available for English, Italian, Portuguese and Swedish. Other languages reuse the coarse POS tags as fine-grained tags instead of padding the extra dimensions in the token representation with zeros. nsubj, nsubjpass), dobj (direct object: dobj), conj (conjunct: conj), *comp (clausal complements: ccomp, xcomp), case (clitics and adpositions: case), *mod (modifiers of a noun: nmod, nummod, amod, appos), neg (negation modifier: neg). 21</p><p>Findings. We found that each of the three improvements (lexical embeddings, language embeddings and fine-grained POS embeddings) tends to improve recall for most classes. MALOPA underperforms (compared to the monolingual baseline) in some classes: nominal subjects, direct objects and modifiers of a noun. Nevertheless, MAL-OPA outperforms the baseline in some important classes such as: root, long attachments and conjunctions.</p><p>Predicting language IDs and POS tags. In Table 3, we assume that both gold language ID of the input language and gold POS tags are given at test time. However, this assumption is not realistic in practical applications. Here, we quantify the degradation in parsing accuracy when language ID and POS tags are only given at training time, but must be predicted at test time. We do not use fine-grained POS tags in these experiments because some languages use a very large fine-grained POS tag set (e.g., 866 unique tags in Portuguese).</p><p>In order to predict language ID, we use the langid.py library <ref type="bibr">(Lui and Baldwin, 2012)</ref>  <ref type="bibr">22</ref> and classify individual sentences in the test sets to one of the seven languages of interest, using the default models included in the library. The macro average language ID prediction accuracy on the test set across sentences is 94.7%. In order to predict POS tags, we use the model described in §3.6 with both input and hidden LSTM dimensions of 60, and with block dropout. The macro average accuracy of the POS tagger is 93.3%. <ref type="table" target="#tab_5">Table 5</ref> summarizes the four configurations: {gold language ID, predicted language ID} × {gold POS tags, predicted POS tags}. The performance of the parser suffers mildly (-0.8 LAS points) when using predicted language IDs, but more (-5.1 LAS points) when using predicted POS tags. As an alternative approach to predicting POS tags, we trained the Stanford POS tagger, for each target language, on the coarse POS tag annotations in the training section of the universal dependency treebanks, 23 then replaced the gold POS tags in the test set of each language with predictions of the monolingual tagger. The resulting degradation in parsing performance between gold vs. predicted POS tags is -6.0 LAS points (on average, compared to a degradation of -5.1 LAS points in <ref type="table" target="#tab_5">Table 5</ref>). The disparity in parsing results with gold vs. predicted POS tags is an important open problem, and has been previously discussed by <ref type="bibr" target="#b25">Tiedemann (2015)</ref>.</p><p>The predicted POS results in <ref type="table" target="#tab_5">Table 5</ref> use block dropout. Without using block dropout, we lose an extra 0.2 LAS points in both configurations using predicted POS tags.</p><p>Different multilingual embeddings. Several methods have been proposed for pretraining multilingual word embeddings. We compare three of them:</p><p>• multiCCA (Ammar et al., 2016) uses a lin-22 https://github.com/saffsd/langid.py <ref type="bibr">23</ref> We used version 3.6.0 of the Stanford POS tagger, with the following pre-packaged configuration files: german-fast-caseless.tagger.props (de), english-caseless-left3words-distsim.tagger.props (en), spanish.tagger.props (es), french.tagger.props (fr). We reused french.tagger.props for <ref type="bibr">(it, pt, sv</ref>  <ref type="table">Table 6</ref>: Effect of multilingual embedding estimation method on the multilingual parsing with MAL-OPA. UAS and LAS scores are macro-averaged across seven target languages. ear operator to project pretrained monolingual embeddings in each language (except English) to the vector space of pretrained English word embeddings.</p><p>• multiCluster <ref type="bibr">(Ammar et al., 2016)</ref> uses the same embedding for translationally-equivalent words in different languages.</p><p>• robust projection  first pretrains monolingual English word embeddings, then defines the embedding of a non-English word as the weighted average embedding of English words aligned to the non-English words (in a parallel corpus). The embedding of a non-English word which is not aligned to any English words is defined as the average embedding of words with a unit edit distance in the same language (e.g., 'playz' is the average of 'plays' and 'play'). 24</p><p>All embeddings are trained on the same data and use the same number of dimensions (100). 25 <ref type="table">Table 6</ref> illustrates that the three methods perform similarly on this task. Aside from <ref type="table">Table 6</ref>, in this paper, we exclusively use the robust projection multilingual embeddings trained in <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. <ref type="bibr">26</ref> The "robust projection" result in <ref type="table">Table 6</ref> (which uses 100 dimensions) is comparable to the last row in <ref type="table" target="#tab_3">Table 3</ref> (which uses 50 dimensions). <ref type="bibr">24</ref> Our implementation of this method can be found at https://github.com/gmulcaire/ average-embeddings. <ref type="bibr">25</ref> We share the embedding files at https://github. com/clab/language-universal-parser/tree/ master/pretrained_embeddings. <ref type="bibr">26</ref> The embeddings were kindly provided by the authors of <ref type="bibr" target="#b7">Guo et al. (2016)</ref>     <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>, which shares most of the parameters between English and the target language, and uses an 2 regularizer to tie the lexical embeddings of translationally-equivalent words. While not the primary focus of this paper, 27 we compare our proposed method to that of <ref type="bibr" target="#b3">Duong et al. (2015b)</ref> on five target languages for which multilingual Brown clusters are available from <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. For each target language, we train the parser on the English training data in the UD version 1.0 corpus <ref type="bibr">(Nivre et al., 2015b</ref>) and a small treebank in the target language. 28 Following <ref type="bibr" target="#b3">Duong et al. (2015b)</ref>, in this setup, we only use gold coarse POS tags, we do not use any development data in the target languages (we use the English development set instead), and we subsample the English training data in each epoch to the same number of sentences in the target language. We use the same hyperparameters specified before for the single MALOPA parser and each of the monolingual baselines. In this section, we evaluate the performance of our parser in this setup. We use two strong baseline multi-source model transfer parsers with no supervision in the target language:</p><p>• Zhang and Barzilay <ref type="formula" target="#formula_0">(2015)</ref> is a graph-based arcfactored parsing model with a tensor-based scoring function. It takes typological properties of a language as input. We compare to the best reported configuration (i.e., the column titled "OURS" in <ref type="table" target="#tab_5">Table 5</ref> of Zhang and Barzilay, 2015).</p><p>• <ref type="bibr" target="#b7">Guo et al. (2016)</ref> is a transition-based neuralnetwork parsing model based on <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. It uses a multilingual embeddings and Brown clusters as lexical features. We compare to the best reported configuration (i.e., the column titled "MULTI-PROJ" in <ref type="table" target="#tab_1">Table 1</ref> of <ref type="bibr" target="#b7">Guo et al., 2016)</ref>.</p><p>Following <ref type="bibr" target="#b7">Guo et al. (2016)</ref>, for each target language, we train the parser on six other languages in the Google universal dependency treebanks version 2.0 29 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags. Our parser uses the same word embeddings and word clusters used in <ref type="bibr" target="#b7">Guo et al. (2016)</ref>, and does not use any typology information. <ref type="bibr">30</ref> The results in <ref type="table" target="#tab_8">Table 8</ref> show that, on average, our parser outperforms both baselines by more than 1 point in LAS, and gives the best LAS results in four (out of six) languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work builds on the model transfer approach, which was pioneered by Zeman and Resnik <ref type="formula" target="#formula_1">(2008)</ref> who trained a parser on a source language treebank then applied it to parse sentences in a target language. <ref type="bibr" target="#b2">Cohen et al. (2011) and</ref><ref type="bibr" target="#b16">McDonald et al. (2011)</ref> trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. <ref type="bibr" target="#b17">Naseem et al. (2012)</ref>, , and Zhang and Barzilay <ref type="formula" target="#formula_0">(2015)</ref> used language typology to improve model transfer. To add lexical information, <ref type="bibr">Täckström et al. (2012)</ref> used multilingual word clusters, while <ref type="bibr" target="#b27">Xiao and Guo (2014)</ref>, , <ref type="bibr" target="#b22">Søgaard et al. (2015)</ref> and <ref type="bibr" target="#b7">Guo et al. (2016)</ref> used multilingual word embeddings. <ref type="bibr" target="#b3">Duong et al. (2015b)</ref> used a neural network based model, sharing most of the parameters between two languages, and used an 2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout <ref type="bibr" target="#b13">(Iyyer et al., 2015)</ref> for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to <ref type="bibr">Vilares et al. (2016)</ref>.</p><p>Neural network parsing models which preceded <ref type="bibr" target="#b5">Dyer et al. (2015)</ref> include <ref type="bibr" target="#b10">Henderson (2003)</ref>, Titov and <ref type="bibr">Henderson (2007)</ref>, <ref type="bibr" target="#b9">Henderson and Titov (2010)</ref> 29 https://github.com/ryanmcd/uni-dep-tb/ 30 In preliminary experiments, we found language embeddings to hurt the performance of the parser for target languages without a treebank. and <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. Related to lexical features in cross-lingual parsing is <ref type="bibr" target="#b4">Durrett et al. (2012)</ref> who defined lexico-syntactic features based on bilingual lexicons. Other related work include <ref type="bibr">Östling (2015)</ref>, which may be used to induce more useful typological properties to inform multilingual parsing.</p><p>Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel corpus <ref type="bibr" target="#b27">(Yarowsky et al., 2001;</ref><ref type="bibr" target="#b12">Hwa et al., 2005)</ref> or via automatically-translated sentences <ref type="bibr" target="#b24">(Tiedemann et al., 2014)</ref>. <ref type="bibr" target="#b15">Ma and Xia (2014)</ref> used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins <ref type="formula" target="#formula_0">(2015)</ref> trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented MALOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank. This pattern of results is quite encouraging. Although languages may share underlying syntactic properties, individual parsing models must behave quite differently, and our model allows this while sharing parameters across languages. The value of this sharing is more pronounced in scenarios where the target language's training treebank is small or non-existent, where our parser outperforms previous cross-lingual multisource model transfer methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Number of sentences (tokens) in each treebank split in Universal Dependency Treebanks (UDT)</cell></row><row><cell>version 2.0 and Universal Dependencies version (UD) 1.2 for the languages we experiment with. The last</cell></row><row><cell>row gives the number of unique language-specific fine-grained POS tags used in a treebank.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dependency parsing: labeled attachment scores (LAS) for monolingually-trained parsers and MALOPA in the fully supervised scenario where L t = L s . Note that we use the universal dependencies verson 1.2 which only includes annotations for ∼13,000 English sentences, which explains the relatively low scores in English. When we instead use the universal dependency treebanks version 2.0 which includes annotations for ∼40,000 English sentences (originally from the English Penn Treebank), we achieve UAS score 93.0 and LAS score 91.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of automatically predicting language ID and POS tags with MALOPA on LAS scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where L t ∩ L s = ∅.</figDesc><table><row><cell>LAS</cell><cell></cell><cell cols="2">target language</cell><cell></cell></row><row><cell></cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>sv</cell></row><row><cell cols="6">monolingual 58.0 64.7 63.0 68.7 57.6</cell></row><row><cell cols="6">Duong et al. 61.8 70.5 67.2 71.3 62.5</cell></row><row><cell>MALOPA</cell><cell cols="5">63.4 70.5 69.1 74.1 63.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Small (3,000 token) target treebank setting:</cell></row><row><cell>language-universal dependency parser performance.</cell></row><row><cell>Small target treebank. Duong et al. (2015b) con-</cell></row><row><cell>sidered a setup where the target language has a small</cell></row><row><cell>treebank of ∼3,000 tokens, and the source language</cell></row><row><cell>(English) has a large treebank of ∼205,000 tokens.</cell></row><row><cell>The parser proposed in Duong et al. (2015b) is a</cell></row><row><cell>neural network parser based on</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>shows</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As of 2007, the total number of native speakers of the hundred most popular languages only accounts for 85% of the world's population<ref type="bibr" target="#b26">(Wikipedia, 2016)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While our parser can be used to parse input with codeswitching, we have not evaluated this capability due to the lack of appropriate data.3 Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks(Buchholz and Marsi, 2006;<ref type="bibr" target="#b17">Nivre et al., 2007)</ref>, the treebank of each language was annotated independently and with its own annotation conventions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the "baseline" scheme in<ref type="bibr" target="#b17">(Nivre and Nilsson, 2005)</ref>. We evaluate against the original nonprojective test set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">We use the same number of dimensions for the coarse POS embeddings as in the monolingual baselines. The same applies to all other types of embeddings used in MALOPA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">For each group, we report recall of both the attachment and relation weighted by the number of instances in the gold annotation. A detailed description of each relation can be found at http://universaldependencies.org/ u/dep/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27">The setup cost involved in recruiting linguists, developing and revising annotation guidelines to annotate a new language ought to be higher than the cost of annotating 3,000 tokens. After investing much resources in a language, we believe it is unrealistic to stop the annotation effort after only 3,000 tokens.28  We thank Long Duong for sharing the processed, subsampled training corpora in each target language at https://github.com/longdt219/universal_ dependency_parser/tree/master/data/ universal-dep/universal-dependencies-1.0.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Waleed Ammar is supported by the Google fellowship in natural language processing. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Part of this material is based upon work supported by a subcontract with Raytheon BBN Technologies Corp. under DARPA Prime Contract No. HR0011-15-C-0013, and part of this research was supported by a Google research award to Noah Smith. We thank Jiang Guo for sharing the multilingual word embeddings and multilingual word clusters. We thank Lori Levin, Ryan Mc-Donald, Jörg Tiedemann, Yulia Tsvetkov, and Yuan Zhang for helpful discussions. Last but not least, we thank the anonymous TACL reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal dependencies 1.1. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925v2</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop on Computational Approaches to Code Switching</title>
		<editor>Ammar et al.2016] Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith</editor>
		<meeting><address><addrLine>Barbara Plank; Maria Simi, Kiril Simov, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague</orgName>
		</respStmt>
	</monogr>
	<note>Massively multilingual word embeddings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On achieving and evaluating language-independence in NLP. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<editor>Buchholz and Marsi2006] Sabine Buchholz and Erwin Marsi</editor>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
	<note>CoNLL-X shared task on multilingual dependency parsing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised structure prediction with non-parallel multilingual guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2014] Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP. Dryer and Haspelmath2013] Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP<address><addrLine>Steven Bird, and Paul Cook</addrLine></address></meeting>
		<imprint>
			<publisher>Trevor Cohn</publisher>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Max Planck Institute for Evolutionary Anthropology, Leipzig</orgName>
		</respStmt>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syntactic transfer using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Durrett et al.2012</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<editor>Gardner et al.2015] Matt Gardner, Kejun Huang, Evangelos Papalexakis, Xiao Fu, Partha Talukdar, Christos Faloutsos, Nicholas Sidiropoulos, and Tom Mitchell</editor>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alan Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio2010] Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>Graves et al.2013</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of AIS-TATS</title>
		<meeting>of AIS-TATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Proc. of ICASSP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A representation learning framework for multi-source transfer parsing</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collocations in multilingual generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raab1989] Ulrich</forename><surname>Heid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sybille</forename><surname>Raab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental sigmoid belief networks for grammar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titov2010] James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3541" to="3570" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<editor>Huang, Wei Xu, and Kai Yu</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2015</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">2012. langid.py: An off-the-shelf language identification tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldwin2012] Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia2014] Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Núria Bertomeu Castelló, and Jungmee Lee</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Proc. of ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naseem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL. [Nivre and Nilsson2005] Joakim Nivre and Jens Nilsson</title>
		<editor>Štěpánek, Alane Suhr, Zsolt Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Uematsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zdeněk Žabokrtský, Daniel Zeman, and Hanzhi Zhu</editor>
		<meeting>of ACL. [Nivre and Nilsson2005] Joakim Nivre and Jens Nilsson<address><addrLine>Teresa Lynn, Christopher Manning, Cȃtȃlina Mȃrȃnduc, David Mareček, Héctor Martínez Alonso; Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis, Sampo Pyysalo, Loganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker; Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Kiril Simov, Aaron Smith; Cristina Bosco, Jinho Choi, Marie-Catherine de Marneffe, Timothy Dozat, Richárd Farkas, Jennifer Foster, Filip Ginter, Yoav Goldberg; Veronika Laippala, Alessandro Lenci, Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missilä; Maria Simi, Aaron Smith, Reut Tsarfaty</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague ; Charles University in Prague</orgName>
		</respStmt>
	</monogr>
	<note>Jenna Kanerva. and Daniel Zeman. 2015b. Universal dependencies 1.0. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word order typology through multilingual word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Östling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Density-driven crosslingual transfer of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<editor>Mohammad Sadegh Rasooli and Michael Collins</editor>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Proc. of LREC</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The generation system of the semsyn project: Towards a taskindependent generator for german</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deitmar</forename><surname>Rösner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Generation</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<editor>NIPS. [Täckström et al.2012] Oscar Täckström, Ryan McDonald, and Jakob Uszkoreit</editor>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Treebank translation for crosslingual parser induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing with universal dependencies and predicted POS labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08449v2</idno>
	</analytic>
	<monogr>
		<title level="m">One model, two languages: training bilingual parsers with harmonized treebanks</title>
		<editor>David Vilares, Carlos Gómez-Rodríguez, and Miguel A. Alonso</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Proc. of DepLing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">List of languages by number of native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="http://bit.ly/1LUP5kJ" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2016" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inducing multilingual text analysis tools via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo2014] Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo ; David Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<editor>Zhang and Regina Barzilay</editor>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
