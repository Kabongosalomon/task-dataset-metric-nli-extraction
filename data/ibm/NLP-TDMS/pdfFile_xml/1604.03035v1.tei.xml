<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Global Features for Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
							<email>swiseman@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<email>srush@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
							<email>shieber@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Global Features for Coreference Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative clusterlevel features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DA</head><p>: um and [I] 1 think that is what's -Go ahead [Linda] 2 . LW: Well and uh thanks goes to [you] 1 and to [the media] 3 to help [us] 4 ...So [our] 4 hat is off to all of [you] 5 as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While structured, non-local coreference models would seem to hold promise for avoiding many common coreference errors (as discussed further in Section 3), the results of employing such models in practice are decidedly mixed, and state-of-the-art results can be obtained using a completely local, mention-ranking system.</p><p>In this work, we posit that global context is indeed necessary for further improvements in coreference resolution, but argue that informative cluster, rather than mention, level features are very difficult to devise, limiting their effectiveness. Accordingly, we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network (shown in Section 4). Our model has no manually defined cluster features, but instead learns a global representation from the individual mentions present in each cluster. We incorporate these representations into a mention-ranking style coreference system.</p><p>The entire model, including the recurrent neural network and the mention-ranking sub-system, is trained end-to-end on the coreference task. We train the model as a local classifier with fixed context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model.</p><p>Experiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems <ref type="bibr" target="#b5">(Durrett and Klein, 2013;</ref><ref type="bibr" target="#b9">Kummerfeld and Klein, 2013;</ref><ref type="bibr" target="#b22">Wiseman et al., 2015;</ref><ref type="bibr" target="#b10">Martschat and Strube, 2015)</ref>. Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>Coreference resolution is fundamentally a clustering task. Given a sequence (x n ) N n=1 of (intra-document) mentions -that is, syntactic units that can refer or be referred to -coreference resolution involves partitioning (x n ) into a sequence of clusters (X (m) ) M m=1 such that all the mentions in any particular cluster X (m) refer to the same underlying entity. Since the mentions within a particular cluster may be ordered linearly by their appearance in the document, 1 we will use the notation X (m) j to refer to the j'th mention in the m'th cluster.</p><p>A valid clustering places each mention in exactly one cluster, and so we may represent a clustering with a vector z ∈ {1, . . . , M } N , where z n = m iff x n is a member of X <ref type="bibr">(m)</ref> . Coreference systems attempt to find the best clustering z * ∈ Z under some scoring function, with Z the set of valid clusterings.</p><p>One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention x n ; because x n may not be anaphoric (and therefore have no antecedents), a "dummy" antecedent may also be predicted. The aforementioned strategy is adopted by "mention-ranking" systems <ref type="bibr" target="#b4">(Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b16">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b5">Durrett and Klein, 2013)</ref>, which, formally, predict an antecedentŷ ∈ Y(x n ) for each mention x n , where Y(x n ) = {1, . . . , n − 1, }. Through transitivity, these decisions induce a clustering over the document.</p><p>Mention-ranking systems make their antecedent predictions with a local scoring function f (x n , y) defined for any mention x n and any antecedent y ∈ Y(x n ). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-right pass through a document's mentions during which a mention's antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention x n to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., <ref type="bibr">Bengtson and Roth (2008)</ref>), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in <ref type="bibr">Culotta et al. (2007)</ref>). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model as linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions. Accordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing the model to utilize global, structural information relating to z in making its predictions. We therefore investigate objective functions of the form arg max y1,...,y N N n=1 f (x n , y n ) + g(x n , y n , z 1:n−1 ) , where g is a global function that, in making predictions for x n , may examine (features of) the clustering z 1:n−1 induced by the antecedent predictions made through y n−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Role of Global Features</head><p>Here we motivate the use of global features for coreference resolution by focusing on the issues that may arise when resolving pronominal mentions in a purely local way. See <ref type="bibr" target="#b3">Clark and Manning (2015)</ref> and <ref type="bibr" target="#b21">Stoyanov and Eisner (2012)</ref> for more general motivation for using global models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pronoun Problems</head><p>Recent empirical work has shown that the resolution of pronominal mentions accounts for a substantial percentage of the total errors made by modern mention-ranking systems. <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref> show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions. Martschat and <ref type="bibr" target="#b10">Strube (2015)</ref> found a similar pattern in their comparison of mention-ranking, mention-pair, and latent-tree models. To see why pronouns can be so problematic, consider the following passage from the "Broadcast Conversation" portion of the CoNLL development set (bc/msnbc/0000/018); below, we enclose mentions in brackets and give the same subscript to coclustered mentions. (This example is also shown in <ref type="figure" target="#fig_3">Figure 2</ref>.) This example is typical of Broadcast Conversation, and it is difficult because local systems learn to myopically link pronouns such as [you] 5 to other instances of the same pronoun that are close by, such as [you] 1 . While this is often a reasonable strategy, in this case predicting [you] 1 to be an antecedent of [you] 5 would result in the prediction of an incoherent cluster, since [you] 1 is coreferent with the singular [I] 1 , and [you] 5 , as part of the phrase "all of you," is evidently plural. Thus, while there is enough information in the text to correctly predict [you] 5 , doing so crucially depends on having access to the history of predictions made so far, and it is precisely this access to history that local models lack.</p><p>More empirically, there are non-local statistical regularities involving pronouns we might hope models could exploit. For instance, in the CoNLL training data over 70% of pleonastic "it" instances and over 74% of pleonastic "you" instances follow (respectively) previous pleonastic "it" and "you" instances. Similarly, over 78% of referential "I" instances and over 68% of referential "he" instances corefer with previous "I" and "he" instances, respectively.</p><p>Accordingly, we might expect non-local models with access to global features to perform significantly better. However, models incorporating nonlocal features have a rather mixed track record. For instance, <ref type="bibr" target="#b1">Björkelund and Kuhn (2014)</ref> found that cluster-level features improved their results, whereas Martschat and <ref type="bibr" target="#b10">Strube (2015)</ref> found that they did not. <ref type="bibr" target="#b3">Clark and Manning (2015)</ref> found that incorporating cluster-level features beyond those involving the precomputed mention-pair and mention-ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance. Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts <ref type="bibr" target="#b6">(Durrett and Klein, 2014;</ref><ref type="bibr" target="#b3">Clark and Manning, 2015;</ref><ref type="bibr" target="#b22">Wiseman et al., 2015;</ref><ref type="bibr" target="#b12">Peng et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Issues with Global Features</head><p>We believe a major reason for the relative ineffectiveness of global features in coreference problems is that, as noted by <ref type="bibr" target="#b3">Clark and Manning (2015)</ref>, cluster-level features can be hard to define. Specif-ically, it is difficult to define discrete, fixed-length features on clusters, which can be of variable size (or shape). As a result, global coreference features tend to be either too coarse or too sparse. Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defined on the mentions (or pairs of mentions) in a cluster <ref type="bibr">(Culotta et al., 2007;</ref><ref type="bibr" target="#b17">Rahman and Ng, 2011)</ref>. For example, a cluster would have the feature 'most-female=true' if more than half the mentions (or pairs of mentions) in the cluster have a 'female=true' feature.</p><p>On the other extreme, <ref type="bibr" target="#b1">Björkelund and Kuhn (2014)</ref> define certain cluster-level features by concatenating the mention-level features of a cluster's constituent mentions in order of the mentions' appearance in the document. For example, if a cluster consists, in order, of the mentions (the president, he, he), they would define a cluster-level "type" feature 'C-P-P=true', which indicates that the cluster is composed, in order, of a common noun, a pronoun, and a pronoun. While very expressive, these concatenated features are often quite sparse, since clusters encountered during training can be of any size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Global Features</head><p>To circumvent the aforementioned issues with defining global features, we propose to learn cluster-level feature representations implicitly, by identifying the state of a (partial) cluster with the hidden state of an RNN that has consumed the sequence of mentions composing the (partial) cluster. Before providing technical details, we provide some preliminary evidence that such learned representations capture important contextual information by displaying in <ref type="figure" target="#fig_0">Figure 1</ref> the learned final states of all clusters in the CoNLL development set, projected using T-SNE (van der Maaten and Hinton, 2012). Each point in the visualization represents the learned features for an entity cluster and the head words of mentions are shown for representative points. Note that the model learns to roughly separate clusters by simple distinctions such as predominant type <ref type="bibr">(nominal, proper, pronominal)</ref> and number (it, they, etc), but also captures more subtle relationships such as grouping geographic terms and long strings of pronouns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recurrent Neural Networks</head><p>A recurrent neural network is a parameterized nonlinear function RNN that recursively maps an input sequence of vectors to a sequence of hidden states. Let (m j ) J j=1 be a sequence of J input vectors m j ∈ R D , and let h 0 = 0. Applying an RNN to any such sequence yields</p><formula xml:id="formula_0">h j ← RNN(m j , h j−1 ; θ) ,</formula><p>where θ is the set of parameters for the model, which are shared over time.</p><p>There are several varieties of RNN, but by far the most commonly used in natural-language processing is the Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), particularly for language modeling (e.g., <ref type="bibr" target="#b24">Zaremba et al. (2014)</ref>) and machine translation (e.g., Sutskever et al. <ref type="formula" target="#formula_4">(2014)</ref>), and we use LSTMs in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RNNs for Cluster Features</head><p>Our main contribution will be to utilize RNNs to produce feature representations of entity clusters which will provide the basis of the global term g. Recall that we view a cluster X (m) as a sequence of mentions (X (m) j ) J j=1 (ordered in linear document or-der). We therefore propose to embed the state(s) of X (m) by running an RNN over the cluster in order.</p><p>In order to run an RNN over the mentions we need an embedding function h c to map a mention to a real vector. First, following <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref> define φ a (x n ) : X → {0, 1} F as a standard set of local indicator features on a mention, such as its head word, its gender, and so on. (We elaborate on features below.) We then use a non-linear feature embedding h c to map a mention x n to a vector-space representation. In particular, we define</p><formula xml:id="formula_1">h c (x n ) tanh(W c φ a (x n ) + b c ) ,</formula><p>where W c and b c are parameters of the embedding. We will refer to the j'th hidden state of the RNN corresponding to X (m) as h (m) j , and we obtain it according to the following formula</p><formula xml:id="formula_2">h (m) j ← RNN(h c (X (m) j ), h (m) j−1 ; θ) ,</formula><p>again assuming that h (m) 0 = 0. Thus, we will effectively run an RNN over each (sequence of mentions corresponding to a) cluster X (m) in the document, and thereby generate a hidden state h </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Coreference with Global Features</head><p>We now describe how the RNN defined above is used within an end-to-end coreference system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Full Model and Training</head><p>Recall that our inference objective is to maximize the score of both a local mention ranking term as well as a global term based on the current clusters: f (x n , y n ) + g(x n , y n , z 1:n−1 )</p><p>We begin by defining the local model f (x n , y) with the two layer neural network of <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref>, which has a specialization for the nonanaphoric case, as follows:</p><formula xml:id="formula_3">f (x n , y) u T ha(xn) hp(xn,y) + u 0 if y = v T h a (x n ) + v 0 if y =<label>.</label></formula><p>DA: um and [I], h</p><p>(1) 2</p><p>[Linda], h</p><p>(2) 1</p><p>[you], h  . There are currently four entity clusters in scope X (1) , X (2) , X (3) , X (4) based on unseen previous decisions (y). Each cluster has a corresponding RNN state, two of which (h (1) and h (4) ) have processed multiple mentions (with X (1) notably including a singular mention <ref type="bibr">[I]</ref>). At the bottom, we show the complete mention-ranking process. Each previous mention is considered as an antecedent, and the global term considers the antecedent clusters' current hidden state. Selecting is treated with a special case NA(x n ).</p><p>Above, u and v are the parameters of the model, and h a and h p are learned feature embeddings of the local mention context and the pairwise affinity between a mention and an antecedent, respectively. These feature embeddings are defined similarly to h c , as</p><formula xml:id="formula_6">h a (x n ) tanh(W a φ a (x n ) + b a ) h p (x n , y) tanh(W p φ p (x n , y) + b p ) ,</formula><p>where φ a (mentioned above) and φ p are "raw" (that is, unconjoined) features on the context of x n and on the pairwise affinity between mentions x n and antecedent y, respectively <ref type="bibr" target="#b22">(Wiseman et al., 2015)</ref>. Note that h a and h c use the same raw features; only their weights differ. We now specify our global scoring function g based on the history of previous decisions. Define h (m) &lt;n as the hidden state of cluster m before a decision is made for x n -that is, h (m) &lt;n is the state of cluster m's RNN after it has consumed all mentions in the cluster preceding x n . We define g as</p><formula xml:id="formula_7">g(x n , y,z 1:n−1 ) h c (x n ) T h (zy) &lt;n if y = NA(x n ) if y = ,</formula><p>where NA gives a score for assigning based on a non-linear function of all of the current hidden states:</p><formula xml:id="formula_8">NA(x n ) = q T tanh W s φ a (xn) M m=1 h (m) &lt;n + b s .</formula><p>See <ref type="figure" target="#fig_3">Figure 2</ref> for a diagram. The intuition behind the first case in g is that in considering whether y is a good antecedent for x n , we add a term to the score that examines how well x n matches with the mentions already in X (zy) ; this matching score is expressed via a dot-product. 2 In the second case, when predicting that x n is non-anaphoric, we add the NA term to the score, which examines the (sum of) the current states h (m) &lt;n of all clusters. This information is useful both because it allows the non-anaphoric score to incorporate information about potential antecedents, and because the occurrence of certain singleton-clusters often predicts the occurrence of future singleton-clusters, as noted in Section 3.</p><p>The whole system is trained end-to-end on coreference using backpropagation. For a given training document, let z (o) be the oracle mapping from mention to cluster, which induces an oracle clustering. While at training time we do have oracle clusters, we do not have oracle antecedents (y) N n=1 , so following past work we treat the oracle antecedent as latent <ref type="bibr" target="#b23">(Yu and Joachims, 2009;</ref><ref type="bibr">Fernandes et al., 2012;</ref><ref type="bibr">Chang et al., 2013;</ref><ref type="bibr" target="#b5">Durrett and Klein, 2013)</ref>. We train with the following slack-rescaled, margin objective:</p><formula xml:id="formula_9">N n=1 max y∈Y(xn) ∆(x n ,ŷ)(1 + f (x n ,ŷ) + g(x n ,ŷ, z (o) ) − f (x n , y n ) − g(x n , y n , z (o) )),</formula><p>where the latent antecedent y n is defined as</p><formula xml:id="formula_10">y n arg max y∈Y(xn):z (o) y =z (o) n f (x n , y) + g(x n , y, z (o) )</formula><p>if x n is anaphoric, and is otherwise. The term ∆(x n ,ŷ) gives different weight to different error types. We use a ∆ with 3 different weights (α 1 , α 2 , α 3 ) for "false link" (FL), "false new" (FN), and "wrong link" (WL) mistakes <ref type="bibr" target="#b5">(Durrett and Klein, 2013)</ref>, which correspond to predicting an antecedent when non-anaphoric, when anaphoric, and the wrong antecedent, respectively. Note that in training we use the oracle clusters z (o) . Since these are known a priori, we can precompute all the hidden states h (m) j in a document, which makes training quite simple and efficient. This approach contrasts in particular with the work of Björkelund and Kuhn (2014) -who also incorporate global information in mention-ranking -in that they train against latent trees, which are not annotated and must be searched for during training. On the other hand, training on oracle clusters leads to a mismatch between training and test, which can hurt performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Search</head><p>When moving from a strictly local objective to one with global features, the test-time search problem becomes intractable. The local objective requires O(n 2 ) time, whereas the full clustering problem is NP-Hard. Past work with global features has used integer linear programming solvers for exact search <ref type="bibr">(Chang et al., 2013;</ref><ref type="bibr" target="#b12">Peng et al., 2015)</ref>, or beam search with (delayed) early update training for an approximate solution <ref type="bibr" target="#b1">(Björkelund and Kuhn, 2014)</ref>. In contrast, we simply use greedy search at test time, which also requires O(n 2 ) time. <ref type="bibr">3</ref> The full algorithm Algorithm 1 Greedy search with global RNNs 1: procedure GREEDYCLUSTER(x1, . . . , xN ) 2:</p><p>Initialize clusters X (1) . . . as empty lists, hidden states h (0) , . . . as 0 vectors in R D , z as map from mention to cluster, and cluster counter M ← 0 3:</p><p>for n = 2 . . . N do 4: y * ← arg max <ref type="bibr">y∈Y(xn)</ref> f (xn, y) + g(xn, y, z1:n−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>m ← zy * 6:</p><p>if y * = then 7:</p><formula xml:id="formula_11">M ← M + 1 8: m ← M 9:</formula><p>append xn to X (m) 10:</p><p>zn ← m 11:</p><formula xml:id="formula_12">h (m) ← RNN(hc(xn), h (m) ) 12: return X (1) , . . . , X (M )</formula><p>is shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring.</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methods</head><p>We run experiments on the CoNLL 2012 English shared task <ref type="bibr" target="#b14">(Pradhan et al., 2012)</ref>. The task uses the OntoNotes corpus <ref type="bibr" target="#b8">(Hovy et al., 2006)</ref>, consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System <ref type="bibr" target="#b5">(Durrett and Klein, 2013)</ref> for mention extraction and to compute features φ a and φ p .</p><p>Features We use the raw BASIC+ feature sets described by <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref>, with the following modifications:</p><p>• We remove all features from φ p that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features.</p><p>• We add true-cased head features, a current speaker indicator feature, and a 2-character they underperformed. We also experimented with training approaches and model variants that expose the model to its own predictions <ref type="bibr" target="#b4">(Daumé III et al., 2009;</ref><ref type="bibr" target="#b18">Ross et al., 2011;</ref><ref type="bibr" target="#b0">Bengio et al., 2015)</ref>, but found that these yielded a negligible performance improvement.  <ref type="formula" target="#formula_4">(2015)</ref>, <ref type="bibr" target="#b3">Clark and Manning (2015)</ref>, <ref type="bibr" target="#b12">Peng et al. (2015)</ref>, and <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref>. F 1 gains are significant (p &lt; 0.05 under the bootstrap resample test <ref type="bibr" target="#b9">(Koehn, 2004)</ref>) compared with <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref> for all metrics.</p><p>genre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to φ p and φ a .</p><p>• We add features indicating if a mention has a substring overlap with the current speaker (φ p and φ a ), and if an antecedent has a substring overlap with a speaker distinct from the current mention's speaker (φ p ).</p><p>• We add a single centered, rescaled document position feature to each mention when learning h c . We calculate a mention x n 's rescaled document position as 2n−N −1 N −1 . These modifications result in there being approximately 14K distinct features in φ a and approximately 28K distinct features in φ p , which is far fewer features than has been typical in past work.</p><p>For training, we use document-size minibatches, which allows for efficient pre-computation of RNN states, and we minimize the loss described in Section 5 with AdaGrad <ref type="bibr" target="#b5">(Duchi et al., 2011)</ref> (after clipping LSTM gradients to lie (elementwise) in (−10, 10)). We find that the initial learning rate chosen for AdaGrad has a significant impact on results, and we choose learning rates for each layer out of {0.1, 0.02, 0.01, 0.002, 0.001}.</p><p>In experiments, we set h a (x n ), h c (x n ), and h (m) to be ∈ R 200 , and h p (x n , y) ∈ R 700 . We use a single-layer LSTM (without "peep-hole" connections), as implemented in the element-rnn library <ref type="bibr">(Léonard et al., 2015)</ref>. For regularization, we apply Dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref> with a rate of 0.4 before applying the linear weights u, and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot-product scores.  Following <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref> we use the costweights α = 0.5, 1.2, 1 in defining ∆, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script <ref type="bibr" target="#b10">(Pradhan et al., 2014;</ref><ref type="bibr" target="#b10">Luo et al., 2014)</ref>. Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>In <ref type="table">Table 1</ref> we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F 1 scores to date on all three CoNLL metrics. We now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC, B 3 , and CEAF e F 1scores in <ref type="table" target="#tab_4">Table 2</ref> as well as errors broken down by mention type and by whether the mention is anaphoric or not in <ref type="table" target="#tab_6">Table 3</ref>. <ref type="table" target="#tab_6">Table 3</ref>   are defined in Section 5.1. We typically think of FL and WL as representing precision errors, and FN as representing recall errors. Our experiments consider several different settings.</p><p>First, we consider an oracle setting ("RNN, OH" in tables), in which the model receives z (o) 1:n−1 , the oracle partial clustering of all mentions preceding x n in the document, and is therefore not forced to rely on its own past predictions when predicting x n . This provides us with an upper bound on the performance achievable with our model. Next, we consider the performance of the model under a greedy inference strategy (RNN, GH), as in Algorithm 1. Finally, for baselines we consider the mention-ranking system (MR) of <ref type="bibr" target="#b22">Wiseman et al. (2015)</ref> using our updated feature-set, as well as a non-local baseline with oracle history (Avg, OH), which averages the representations h c (x j ) for all x j ∈ X (m) , rather than feed them through an RNN; errors are still backpropagated through the h c representations during learning.</p><p>In <ref type="table" target="#tab_6">Table 3</ref> we see that the RNN improves performance overall, with the most dramatic improve- ments on non-anaphoric pronouns, though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head. While WL errors also decrease for both these mention-categories under the RNN model, FN errors increase. Importantly, the RNN performance is significantly better than that of the Avg baseline, which barely improves over mention-ranking, even with oracle history. This suggests that modeling the sequence of mentions in a cluster is advantageous. We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting, we are still able to recover a significant portion of the possible performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Qualitative Analysis</head><p>In this section we consider in detail the impact of the g term in the RNN scoring function on the two error categories that improve most under the RNN model (as shown in <ref type="table" target="#tab_6">Table 3</ref>), namely, pronominal WL errors and pronominal FL errors. We consider an example from the CoNLL development set in each category on which the baseline MR model makes an error but the greedy RNN model does not.</p><p>The example in <ref type="figure" target="#fig_4">Figure 3</ref> involves the resolution of the ambiguous pronoun "his," which is bracketed and in bold in the figure. Whereas the baseline MR model incorrectly predicts "his" to corefer with the closest gender-consistent antecedent "Justin"thus making a WL error -the greedy RNN model correctly predicts "his" to corefer with "Mr. Kaye" in the previous sentence. (Note that "the official" also refers to Mr. Kaye). To get a sense of the greedy RNN model's decision-making on this example, we color the mentions the greedy RNN model has predicted to corefer with "Mr. Kaye" in green, and the mentions it has predicted to corefer with "Justin" in blue. (Note that the model incorrectly predicts the initial "I" mentions to corefer with "Justin.") Letting X (1) refer to the blue cluster, X (2) refer to the green cluster, and x n refer to the ambiguous mention "his," we further shade each mention x j in X (1) so that its intensity corresponds to h c (x n ) T h (1) &lt;k , where k = j + 1; mentions in X (2) are shaded analogously. Thus, the shading shows how highly g scores the compatibility between "his" and a cluster X (i) as each of X (i) 's mentions is added. We see that when the initial "Justin" mentions are added to X (1) the g-score is relatively high. However, after "The company" is correctly predicted to corefer with "Justin," the score of X (1) drops, since companies are generally not coreferent with pronouns like "his." <ref type="figure" target="#fig_5">Figure 4</ref> shows an example (consisting of a telephone conversation between "A" and "B") in which the bracketed pronoun "It's" is being used pleonastically. Whereas the baseline MR model predicts "It's" to corefer with a previous "it" -thus making a FL error -the greedy RNN model does not. In <ref type="figure" target="#fig_5">Figure 4</ref> the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention. This visualization resembles the "saliency" technique of <ref type="bibr">Li et al. (2016)</ref>, and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score.</p><p>We see that the potential antecedent "S-Bahn" has a large gradient, but also that the initial, obviously pleonastic use of "it's" has a large gradient, which may suggest that earlier, easier predictions of pleonasm can inform subsequent predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not <ref type="bibr">(Soon et al., 2001;</ref><ref type="bibr" target="#b11">Ng and Cardie, 2002;</ref><ref type="bibr">Bengtson and Roth, 2008)</ref>, and mention-ranking models, which select a single antecedent for each anaphoric mention <ref type="bibr" target="#b4">(Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b16">Rahman and Ng, 2009;</ref><ref type="bibr" target="#b5">Durrett and Klein, 2013;</ref><ref type="bibr">Chang et al., 2013;</ref><ref type="bibr" target="#b22">Wiseman et al., 2015)</ref>. Structured approaches typically divide between those that induce a clustering of mentions <ref type="bibr">(McCallum and Wellner, 2003;</ref><ref type="bibr">Culotta et al., 2007;</ref><ref type="bibr" target="#b13">Poon and Domingos, 2008;</ref><ref type="bibr" target="#b7">Haghighi and Klein, 2010;</ref><ref type="bibr" target="#b21">Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b2">Cai and Strube, 2010)</ref>, and, more recently, those that learn a latent tree of mentions <ref type="bibr">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b1">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b10">Martschat and Strube, 2015)</ref>.</p><p>There have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, <ref type="bibr" target="#b17">Rahman and Ng (2011)</ref> rank clusters rather than mentions; <ref type="bibr" target="#b3">Clark and Manning (2015)</ref> use the output of both mention-ranking and mention pair systems to learn a clustering.</p><p>The application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of <ref type="bibr" target="#b7">Dyer et al. (2015)</ref>, who use LSTMs to embed the state of a transition based parser's stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a simple, state of the art approach to incorporating global information in an end-to-end coreference system, which obviates the need to define global features, and moreover allows for simple (greedy) inference. Future work will examine improving recall, and more sophisticated approaches to global training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>T-SNE visualization of learned entity representations on the CoNLL development set. Each point shows a gold cluster of size &gt; 1. Yellow, red, and purple points represent predominantly common noun, proper noun, and pronoun clusters, respectively. Captions show head words of representative clusters' mentions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to each step of each cluster in the document. Concretely, this can be implemented by maintaining M RNNs -one for each cluster -that all share the parameters θ. The process is illustrated in the top portion ofFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>2 x n = [you] , NA(x n ) Full RNN example for handling the mention x n = [you]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Cluster predictions of greedy RNN model; coclustered mentions are of the same color, and intensity of mention x j corresponds to h c (x n ) T h (i) &lt;k , where k = j+1, i ∈ {1, 2}, and x n = "his." See text for full description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Magnitudes of gradients of NA score applied to bold "It's" with respect to final mention in three preceding clusters. See text for full description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[I] 1 think that is what's -Go ahead [Linda] 2 . LW: Well and thanks goes to [you] 1 and to [the media] 3 to help [us] 4 ...So [our] 4 hat is off to all of [you] 5 ...</figDesc><table><row><cell></cell><cell></cell><cell>X (1)</cell><cell cols="2">X (2)</cell><cell>X (3)</cell><cell></cell><cell>X (4)</cell></row><row><cell>h</cell><cell>(1) 1</cell><cell>h (1) 2</cell><cell>h</cell><cell>(2) 1</cell><cell>h (3) 1</cell><cell>h (4) 1</cell><cell>(4) h 2</cell></row><row><cell cols="2">[I]</cell><cell>[you]</cell><cell cols="2">[Linda]</cell><cell>[the media]</cell><cell>[us]</cell><cell>[our]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>67.46 70.72 62.71 54.96 58.58 59.40 52.27 55.61 61.63 M&amp;S (2015) 76.72 68.13 72.17 66.12 54.22 59.58 59.47 52.33 55.67 62.47 C&amp;M (2015) 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02</figDesc><table><row><cell>System</cell><cell>P</cell><cell>MUC R</cell><cell>F 1</cell><cell>P</cell><cell>B 3 R</cell><cell>F 1</cell><cell>P</cell><cell>CEAF e R</cell><cell>F 1</cell><cell>CoNLL</cell></row><row><cell>B&amp;K (2014)</cell><cell cols="10">74.30 63.02</cell></row><row><cell>Peng et al. (2015)</cell><cell>-</cell><cell>-</cell><cell>72.22</cell><cell>-</cell><cell>-</cell><cell>60.50</cell><cell>-</cell><cell>-</cell><cell>56.37</cell><cell>63.03</cell></row><row><cell cols="10">Wiseman et al. (2015) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05</cell><cell>63.39</cell></row><row><cell>This work</cell><cell cols="9">77.49 69.75 73.42 66.83 56.95 61.50 62.14 53.85 57.70</cell><cell>64.21</cell></row><row><cell cols="11">Table 1: Results on CoNLL 2012 English test set. We compare against recent state of the art systems, including (in</cell></row><row><cell cols="5">order) Bjorkelund and Kuhn (2014), Martschat and Strube</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>F 1 scores of models described in text on CoNLL 2012 development set. Rows in grey highlight models using oracle history.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Number of "false link" (FL) errors on non-</cell></row><row><cell>anaphoric mentions (top) and number of "false new" (FN)</cell></row><row><cell>and "wrong link" (WL) errors on anaphoric mentions</cell></row><row><cell>(bottom) on CoNLL 2012 development set. Mentions</cell></row><row><cell>are categorized as nominal or proper with (previous) head</cell></row><row><cell>match (Nom. HM), nominal or proper with no head match</cell></row><row><cell>(Nom. No HM), and pronominal. Models are described</cell></row><row><cell>in the text, and rows in grey highlight models using oracle</cell></row><row><cell>history.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We assume nested mentions are ordered by their syntactic heads.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also experimented with other non-linear functions, but dot-products performed best.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">While beam search is a natural way to decrease search error at test time, it may fail to help if training involves a local margin objective (as in our case), since scores need not be calibrated across local decisions. We accordingly attempted to train various locally normalized versions of our model, but found that</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of a Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Bengtson and Roth2008] Eric Bengtson and Dan Roth</editor>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Constrained Latent Variable Model for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Strube2010] Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Chang et al.2013] Kai-Wei Chang, Rajhans Samdani, and Dan Roth</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
	<note>23rd International Conference on Computational Linguistics (COLING)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">First-order Probabilistic Models for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (NAACL HLT)</title>
		<meeting><address><addrLine>Aron Culotta, Michael Wick, Robert Hall, and Andrew McCallum</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
	<note>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Specialized Models and Ranking for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
	<note>Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
	<note>Easy Victories and Uphill Battles in Coreference Resolution</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">A Joint Model for Entity Analysis: Coreference, Typing, and Linking. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
		</imprint>
	</monogr>
	<note>Durrett and Klein2014</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution</title>
	</analytic>
	<monogr>
		<title level="m">The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the human language technology conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical Significance Tests for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07889</idno>
	</analytic>
	<monogr>
		<title level="m">Nicholas Léonard, Yand Waghmare, Sagar ad Wang, and Jin-Hwa Kim. 2015. rnn: Recurrent Library for Torch</title>
		<editor>Li et al.2016] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky</editor>
		<meeting><address><addrLine>Seattle, WA, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NAACL HLT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<editor>McCallum and Wellner2003] Andrew McCallum and Ben Wellner</editor>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie2002] Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A joint framework for coreference resolution and mention head detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 19th Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint unsupervised coreference resolution with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domingos2008] Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conll-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Marta Recasens</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Sameer Pradhan, Xiaoqiang Luo</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised Models for Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng2009] Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Narrowing the modeling gap: A cluster-ranking approach to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="469" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Machine Learning Approach to Coreference Resolution of Noun Phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing non-metric similarities in multiple maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner ; Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Laurens van der Maaten and Geoffrey E. Hinton</editor>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="33" to="55" />
		</imprint>
	</monogr>
	<note>Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Structural SVMs with Latent Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachims2009] Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent neural network regularization. CoRR, abs/1409.2329</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
