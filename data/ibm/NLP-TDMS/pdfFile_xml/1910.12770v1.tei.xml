<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skip-Clip: Self-Supervised Spatiotemporal Representation Learning by Future Clip Order Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Canadian Institute for Advanced Research</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Apple Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Skip-Clip: Self-Supervised Spatiotemporal Representation Learning by Future Clip Order Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks require collecting and annotating large amounts of data to train successfully. In order to alleviate the annotation bottleneck, we propose a novel self-supervised representation learning approach for spatiotemporal features extracted from videos. We introduce Skip-Clip, a method that utilizes temporal coherence in videos, by training a deep model for future clip order ranking conditioned on a context clip as a surrogate objective for video future prediction. We show that features learned using our method are generalizable and transfer strongly to downstream tasks. For action recognition on the UCF101 dataset, we obtain 51.8% improvement over random initialization and outperform models initialized using inflated Im-ageNet parameters. Skip-Clip also achieves results competitive with state-of-the-art self-supervision methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of deep Convolutional Neural Networks (CNN)s relies heavily on the availability of human annotations to power supervised learning. While visual data like images and videos are abundant, providing semantic labels for such large amounts of data can be very expensive and time-consuming. There is thus a need for methods that can utilize huge amounts of available unlabeled data. This will improve the scalability of deep learning methods and make them more accessible to new domains that suffer from high annotation costs. Researchers have studied different approaches to enable learning with less dependence on labels.</p><p>There is a whole class of algorithms that follow the unsupervised learning paradigm. One interesting example of such approaches is self-supervised learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. For self-supervision methods, a pretext task is designed to exploit structure in data. Pseudo-labels are generated automatically from the data structure and a deep model * Work was performed during an internship with Apple Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordered Frames</head><p>Reveresed Frames <ref type="figure">Figure 1</ref>. A short 4 frame clip of a person performing a pull-up action. Left are the frames in the correct order from left to right. On the Right the same frames but in the reverse order. It is clear that both set of frames are temporally plausible when observed out of context.</p><p>can be trained to minimize the loss with respect to the generated labels, using standard supervised learning methods.</p><p>The study of the video domain in computer vision is of great importance. It powers computer-human interaction applications as well as enables implicit learning of laws of motion and physics. However, providing annotations for videos is particularly difficult due to its temporal dimension. Additionally, models used for video-related tasks like action recognition are usually heavily parameterized 3D CNNs <ref type="bibr" target="#b3">[4]</ref> that enable simultaneous modelling of spatial and temporal dimensions. Training high capacity models requires a huge amount of data to prevent over-fitting. Until recently, training of 3D CNNs have not been very successful due to the lack of large scale datasets. Considering the high cost of annotations and the strong need for huge amounts of data, learning from videos can gain significantly from a strong self-supervision method. Videos are usually temporally coherent and there is a high correlation in the spatial information across nearby frames. This property can be a source of self-supervision signals as shown by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>. Some efforts utilize the temporal order signal by training a model to either verify the order of frames <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref> or to sort a shuffled set of frames <ref type="bibr" target="#b9">[10]</ref>. Such tasks can provide a strong supervisory signal, however, setting up the pretext task this way can lead to many noisy samples. For example in Figure 1 we show two sets of ordered and reversed frames of a person doing a pull-up action, both sets are temporally plausible since pull-up is a cyclic action. Training a model for an ambiguous label can hinder the strength and the generality of the representation. The reason this task can be ambiguous is that we train a model to sort frames out of context. There is no information about how the current state of the scene came to be.</p><p>In this work, we propose a method that alleviates the shortcomings of sorting and future frame prediction approaches by combining both ideas in a single method. From a given video, we sparsely sample a set of frames to be sorted as well as a set of contiguous surrounding frames as a context. Instead of training a model to predict the correct ordering out of a disjoint set of different possible orderings, the model is trained to predict the correct relative position of every frame given the context using a ranking objective. Ranking of future frames encourages the model to learn about scene dynamics and tracking without requiring expensive full frame prediction. Training the model to rank future frames given a context is a softer, more controlled instantiation of future prediction in latent space.</p><p>We summarize our contribution as follows:</p><p>• We propose a novel pretext task for spatiotemporal representation learning. Our method combines the two ideas of predicting future frames and exploiting temporal coherence, by learning to sort frames in a simple and efficient framework.</p><p>• In order to demonstrate the quality of the learned representations, we provide strong results for the downstream task of action recognition using the UCF101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the language domain, self-supervised methods for learning word or sentence representations have been dominant. One of the most successful approaches learns an encoding per word that maximizes the probability of its surrounding words. Representations learned using this objective encode strong semantics about language. <ref type="bibr" target="#b10">[11]</ref> proposed the Skip-gram model in which a context word is encoded in a way that allows predicting surrounding words. The same idea has been extended to sentences <ref type="bibr" target="#b8">[9]</ref>. Our method can be seen as an instantiation of this idea adapted to the video domain, hence we name our method Skip-Clip. In this work, we focus on predicting future clips only, though the method can easily be extended to consider both future and past clips.</p><p>One of the related methods to our approach is Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b14">[15]</ref>. It studies unsupervised representation learning by predicting the future in the latent space by using a contrastive loss between positive future samples and negative samples sampled from a random different sequence. While this method has shown promising results on different domains like speech, images, and text, applying this method to videos reduces to a trivial task since there is a strong visual similarity between the positive samples and the context. In our method, we also predict the future in a latent space. However, our objective is a hinge rank loss in which we rank positive future samples with respect to their temporal distance from the context. This task is significantly harder since we train the model to learn a latent space that represents differences in very similar frames in order to be successful at the ranking task. Accordingly, an encoder that learns such a latent space capturing the subtleties needed for fine-grained frame ranking will be useful for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We propose a simple and powerful framework for selfsupervised spatiotemporal representation learning by combining two core ideas: predicting the future based on context, and temporal consistency by ordering clips. An overview of our method is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Skip-Clip</head><p>Given a video consisting of a set of N frames V = {v 1 , v 2 , · · · , v N }, we randomly seek to time step t in the video 1 . We densely sample K context frames c = {v t , v t+1 , · · · , v t+K }. We sparsely sample M target clips T = {x 1 , x 2 , · · · , x M } using fixed sampling rate r such that all target clips are subsequent to the context frames. The context clip and the first target clip are r frames apart. Each target clip contains d contiguous frames (target clip length is d). Additionally, we sample another M clips from different videos to create a set of clips Q = {x 1 , x 2 , · · · , x M } that can be used as negative samples.</p><p>We define encoding functions for the context and the target clips to obtain a compressed representation in the latent space. For the context, we have h = g(c) where h is the latent representation of the context clip. Similarly each clip in the target set T and negative set Q is encoded as z i = f (x i ) and z i = f (x i ) respectively. Our goal is to train the encoding functions g(c) and f (x) to enable a successful ranking of target clips conditioned on the context clip. We then need to define a scoring function s = Γ(h, z i ) that describes the relationship between the context and target encodings. The scores for high ranking targets should be higher than low ranking ones. We implement this objective using a hinge rank loss: <ref type="figure">Figure 2</ref>. A context clip c is sampled as well as M clips from the following frames {x1, · · · , xM }. Latent representations h and zi are extracted using encoding functions g(c) and f (x) for the context and target clips respectively. A scoring function Γ measures the ranking score between the target representations relative to the context. Finally, the scores are used in hinge rank losses (equation 1 and 2).</p><formula xml:id="formula_0">L rank = M −1 i=0 M j=i+1 max(0, −Γ(h, z i ) + Γ(h, z j ) + δ rank ).<label>(1)</label></formula><p>where δ rank is the margin. This objective is challenging since the target clips share high spatial similarity with the context. For successful ranking, the latent representations need to encode information about the various motions of objects and scene and be aware of physical concepts like velocity to determine the correct position of a target clip relative to the context clip.</p><p>Additionally, scores for target clips should be higher than negative clips given the context encoding. Therefore we introduce a contrastive loss:</p><formula xml:id="formula_1">L contrastive = M i=0 max(0, −Γ(h, z i )+E z∈Q [Γ(h, z)]+δ neg ).<label>(2)</label></formula><p>This is important because the ranking objective can choose to focus mainly on temporal cues since it is more strongly correlated with motion. An objective that forces the encodings to discriminate between two spatially dissimilar clips is required to also pick up on spatial subtleties. If we train our model using only the contrastive objective in equation 2, our model will be equivalent to the the CPC model <ref type="bibr" target="#b14">[15]</ref>.</p><p>Finally, to enrich the spatial signal, we add a rotation prediction auxiliary task. In experiments presented below, we found that by training f (x) solely for the ranking and contrastive objectives, it achieved strong ranking performance that did not translate to better generalization to the downstream task of action recognition, indicating that with the large capacity of g(c) and f (x), the model was able to learn trivial solutions to the ranking task. Adding an auxiliary ob-jective helped restrict the representations learned by f (x) such that it retains the spatial information. Training the two encoders for ranking with the presence of these restrictions led to better correlation between the ranking task and generalization to the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-supervision</head><p>For the context clip, we sample K = 16 contiguous frames. g(c) is implemented using a 3D CNN, specifically, a 3D ResNet-18 <ref type="bibr" target="#b3">[4]</ref> up to the last convolutional layer. As for the target clips, we implement the special case where d = 1 such that each target clip is a single frame. Empirically, we found that using longer target clips gives the model more room to exploit trivial solutions for the ranking task that does not generalize well. Ranking is performed on M = 8 target clips. f (x i ) is implemented using a 2D ResNet-18 <ref type="bibr" target="#b4">[5]</ref>. Both encoders are trained from scratch (random initialization). Both the context and target clip encodings h and z i are tensors of dimensionality R C×H×W where number of channels C = 512 and the spatial dimensions H × W = 7 × 7. We found that keeping the spatial dimension is more informative when used with the scoring function compared to having the encodings in vector form. For the scoring function Γ(h, z i ), given two R C×H×W tensors, we compute the average cosine similarity across every two aligned spatial cells in the H × W grid: </p><formula xml:id="formula_2">Γ(h, z i ) = 1 H * W</formula><p>where, h m,n indicates the vector in the m th row and n th column with dimension R C . For the hinge rank losses in equations 1 and 2.</p><p>The rotation auxiliary objective follows <ref type="bibr" target="#b2">[3]</ref> predicting a rotation angle out of the fixed set [0 • , 90 • , 180 • , 270 • ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Random Initialization <ref type="bibr" target="#b3">[4]</ref> 42.4 ImageNet Inflation <ref type="bibr" target="#b6">[7]</ref> 60.3 Skip-Clip 64.4 <ref type="table">Table 1</ref>. Top-1 accuracy comparison to standard initialization baselines performance for action recognition task on UCF-101 dataset.</p><p>All loss terms are summed without any weighting. More details about the hyper-parameters used for pre-training can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Action Recognition fine tuning</head><p>To demonstrate the quality of the representations learned using the pretext task, we fine tune the context clip encoder g(c) for the downstream task of action recognition. We add a global average pooling layer followed by a fullyconnected layer with softmax outputs to the encoder network. We use split 1 of the UCF101 dataset for training and testing. g(c) is a 3D CNN. We use an input of 16 frames with spatial dimensions of 112×112 after applying random cropping. We fine tune for 300 epochs. We use the Adam optimizer with weight decay of 1e −2 and learning rate of 5e −4 that is multiplied by 0.5 every 15 epochs up to the 60 th epoch. For testing, we use center cropping and apply a sliding window approach by averaging the softmax outputs over all the 16 frame non-overlapping windows in a given test video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>We evaluate our model Skip-Clip for the downstream task of action recognition using the UCF101 dataset. We compare our results to different initialization baselines as shown in <ref type="table">Table 1</ref>. First, we notice that using Skip-Clip is clearly superior to training from scratch. Our model achieves accuracy that is 51.8% higher than a model trained from random initialization of the weights. Secondly, our model outperforms a model initialized with inflated Im-ageNet <ref type="bibr" target="#b0">[1]</ref> weights while being completely unsupervised during pre-training.</p><p>We study the contribution of multiple components in Table 2. We observe that a basic Skip-Clip model without the auxiliary rotation objective can lead to relatively weaker performance. This is because the basic model is susceptible to learning trivial solutions to the ranking task, which does not necessarily transfer well to other semantic tasks like action recognition. By adding the rotation auxiliary objective to the target encoder f (x), we see a significant improvement of 3.6%. Furthermore, by adding the contrastive loss as described in equation 2, we obtain a strong performance of 64.4%.  <ref type="table">Table 3</ref>. Top-1 Accuracy performance for action recognition task on UCF-101 dataset. Different backbones used for by the methods can account for some of the performance difference.</p><p>We compare Skip-Clip to other self-supervision methods as shown in <ref type="table">Table.</ref> 3. Skip-Clip significantly outperforms all 2D CNN based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>. Additionally, it achieves competitive performance compared to the methods that used UCF101 for pre-training using the selfsupervised objective, outperforming <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> and being on par with <ref type="bibr" target="#b20">[21]</ref>. The strong performance of <ref type="bibr" target="#b20">[21]</ref> suggests that there is potential in using target clips larger than a single frame in future work. As for the methods that used the large scale Kinetics dataset for pre-training, Skip-Clip outperforms <ref type="bibr" target="#b5">[6]</ref> despite using a significantly smaller dataset for pre-training. Finally, we fall behind only the 3D Cubic Puzzles <ref type="bibr" target="#b6">[7]</ref> approach; however, this method takes advantage of pre-training using Kinetics, which we will explore in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We have presented a method, Skip-Clip, for selfsupervised spatiotemporal representation learning. We combine the strengths of two popular ideas, future frame prediction and temporal sorting of video frames. We demonstrate the strengths and the generalization of the representations learned using our method by finetuning the encoders for the downstream task of action recognition. We show that our method is competitive compared to other selfsupervised approaches. Additionally, our method outperforms the strong inflated ImageNet baseline and beats some models that leverage the much larger-scale Kinetics dataset. These results demonstrate the value of our proposition to pose frame sorting as a predictive task that relies on a con-text. <ref type="figure">Figure 3</ref>. Visualizations of the cosine similarity per aligned cell between context and target representations that are part of computing the scores Γ(h, z). We can observe that the highlighted regions correspond to salient motions in the frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>n · z m,n i ||h m,n || · ||z m,n i ||</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablation Study comparing the base model to models with additional auxiliary objectives.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">UCF101</cell></row><row><cell>Skip-Clip</cell><cell></cell><cell></cell><cell>59.5</cell></row><row><cell>Skip-Clip + rotation</cell><cell></cell><cell></cell><cell>63.1</cell></row><row><cell cols="3">Skip-Clip + rotation + negative sampling</cell><cell>64.4</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Source</cell><cell cols="2">UCF101</cell></row><row><cell>Shuffle and Learn [12]</cell><cell>AlexNet</cell><cell cols="2">UCF101</cell><cell>50.9</cell></row><row><cell>Arrow of time [20]</cell><cell>AlexNet</cell><cell cols="2">UCF101</cell><cell>55.3</cell></row><row><cell>OPN [10]</cell><cell>AlexNet</cell><cell cols="2">UCF101</cell><cell>56.3</cell></row><row><cell>VideoGAN [16]</cell><cell>C3D</cell><cell cols="2">UCF101</cell><cell>52.1</cell></row><row><cell>Motion &amp; Appearance [18]</cell><cell>C3D</cell><cell cols="2">UCF101</cell><cell>58.8</cell></row><row><cell>Motion &amp; Appearance [18]</cell><cell>C3D</cell><cell cols="2">Kinetics</cell><cell>61.2</cell></row><row><cell>3DRotNet [6]</cell><cell>3D ResNet-18</cell><cell cols="2">Kinetics</cell><cell>62.9</cell></row><row><cell>Video Clip Ordering [21]</cell><cell>R3D</cell><cell cols="2">UCF101</cell><cell>64.9</cell></row><row><cell>3DCubicPuzzles [7]</cell><cell>3D ResNet-18</cell><cell cols="2">Kinetics</cell><cell>65.8</cell></row><row><cell>Skip-Clip</cell><cell cols="3">3D ResNet-18 UCF101</cell><cell>64.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Random seeking for t has to be a certain distance from the end of the video to enable sampling context and target clips</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Qualitative Analysis</head><p>To understand which regions of each frame contribute more to frame ranking relative to the context, we visualize the distribution of the cosine similarity scores between g(c) and f (x) feature maps across different spatial locations in <ref type="figure">Figure 3</ref>. From these samples, it is clear that higher scores correlate with more salient regions in the frame in terms of object/person motion. We hypothesize that these regions are good cues for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-training Implementation Details</head><p>We used videos from split 1 of the training set of UCF101 <ref type="bibr" target="#b13">[14]</ref> for training on the pretext task. The input frames are resized to 224×224 pixels. We used the same hyperparameters to train all the encoder parameters. We trained for 600 epochs using a batch size of 128. We used the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with a learning rate of 3e −4 that is multiplied by 0.1 every 200 epochs and weight decay of 1e −7 . For data augmentation, we randomly reversed the video frames before sampling the context to double the motion patterns explored by the model. Additionally, we applied random horizontal flipping and random cropping consistent for the context and target clips. The target sampling rate was set to r = 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.07750</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>abs/1811.09795</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by sorting sequences. CoRR, abs/1708.01246</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised learning using sequential verification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno>abs/1603.08561</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
