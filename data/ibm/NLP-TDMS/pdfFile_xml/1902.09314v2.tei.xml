<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentional Encoder Network for Targeted Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
							<email>raoyangh@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attentional Encoder Network for Targeted Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Targeted sentiment classification aims at determining the sentimental tendency towards specific targets. Most of the previous approaches model context and target words with RNN and attention. However, RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long-term patterns. To address this issue, this paper proposes an Attentional Encoder Network (AEN) which eschews recurrence and employs attention based encoders for the modeling between context and target. We raise the label unreliability issue and introduce label smoothing regularization. We also apply pretrained BERT to this task and obtain new stateof-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Targeted sentiment classification is a fine-grained sentiment analysis task, which aims at determining the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over "opinion targets" that explicitly appear in the sentence. For example, given a sentence "I hated their service, but their food was great", the sentiment polarities for the target "service" and "food" are negative and positive respectively. A target is usually an entity or an entity aspect.</p><p>In recent years, neural network models are designed to automatically learn useful lowdimensional representations from targets and contexts and obtain promising results <ref type="bibr" target="#b4">(Dong et al., 2014;</ref><ref type="bibr" target="#b15">Tang et al., 2016a)</ref>. However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task.</p><p>Attention mechanism, which has been successfully used in machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>, is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target. There are already some studies use attention to generate targetspecific sentence representations <ref type="bibr" target="#b18">(Wang et al., 2016;</ref><ref type="bibr" target="#b10">Ma et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref> or to transform sentence representations according to target words <ref type="bibr" target="#b9">(Li et al., 2018)</ref>. However, these studies depend on complex recurrent neural networks (RNNs) as sequence encoder to compute hidden semantics of texts.</p><p>The first problem with previous works is that the modeling of text relies on RNNs. RNNs, such as LSTM, are very expressive, but they are hard to parallelize and backpropagation through time (BPTT) requires large amounts of memory and computation. Moreover, essentially every training algorithm of RNN is the truncated BPTT, which affects the model's ability to capture dependencies over longer time scales <ref type="bibr" target="#b19">(Werbos, 1990)</ref>. Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information, this usually requires a large amount of training data. Another problem that previous studies ignore is the label unreliability issue, since neutral sentiment is a fuzzy sentimental state and brings difficulty for model learning. As far as we know, we are the first to raise the label unreliability issue in the targeted sentiment classification task. This paper propose an attention based model to solve the problems above. Specifically, our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words. To deal with the label unreliability issue, we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> to this task and show our model enhances the performance of basic BERT model. Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models.</p><p>The main contributions of this work are presented as follows:</p><p>1. We design an attentional encoder network to draw the hidden states and semantic interactions between target and context words.</p><p>2. We raise the label unreliability issue and add an effective label smoothing regularization term to the loss function for encouraging the model to be less confident with the training labels.</p><p>3. We apply pre-trained BERT to this task, our model enhances the performance of basic BERT model and obtains new state-of-the-art results.</p><p>4. We evaluate the model sizes of the compared models and show the lightweight of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The research approach of the targeted sentiment classification task including traditional machine learning methods and neural networks methods. Traditional machine learning methods, including rule-based methods <ref type="bibr" target="#b3">(Ding et al., 2008)</ref> and statistic-based methods <ref type="bibr" target="#b6">(Jiang et al., 2011)</ref>, mainly focus on extracting a set of features like sentiment lexicons features and bag-of-words features to train a sentiment classifier <ref type="bibr" target="#b13">(Rao and Ravichandran, 2009</ref>). The performance of these methods highly depends on the effectiveness of the feature engineering works, which are labor intensive.</p><p>In recent years, neural network methods are getting more and more attention as they do not need handcrafted features and can encode sentences with low-dimensional word vectors where rich semantic information stained. In order to incorporate target words into a model, <ref type="bibr" target="#b15">Tang et al. (2016a)</ref> propose TD-LSTM to extend LSTM by using two single-directional LSTM to model the left context and right context of the target word respectively. <ref type="bibr" target="#b16">Tang et al. (2016b)</ref> design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concerning the given target. Multiple attention is paid to the memory represented by word embeddings to build higher semantic information. <ref type="bibr" target="#b18">Wang et al. (2016)</ref> propose ATAE-LSTM which concatenates target embeddings with word representations and let targets participate in computing attention weights. <ref type="bibr" target="#b1">Chen et al. (2017)</ref> propose RAM which adopts multiple-attention mechanism on the memory built with bidirectional LSTM and nonlinearly combines the attention results with gated recurrent units (GRUs). <ref type="bibr" target="#b10">Ma et al. (2017)</ref> propose IAN which learns the representations of the target and context with two attention networks interactively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><formula xml:id="formula_0">Given a context sequence w c = {w c 1 , w c 2 , ..., w c n } and a target sequence w t = {w t 1 , w t 2 , ..., w t m }, where w t is a sub-sequence of w c .</formula><p>The goal of this model is to predict the sentiment polarity of the sentence w c over the target w t . <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">GloVe Embedding</head><p>Let L ∈ R d emb ×|V | to be the pre-trained GloVe <ref type="bibr" target="#b11">(Pennington et al., 2014)</ref> embedding matrix, where d emb is the dimension of word vectors and |V | is the vocabulary size. Then we map each word w i ∈ R |V | to its corresponding embedding vector e i ∈ R d emb ×1 , which is a column in the embedding matrix L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">BERT Embedding</head><p>BERT embedding uses the pre-trained BERT to generate word vectors of sequence. In order to facilitate the training and fine-tuning of BERT model, we transform the given context and target to "[CLS] + context + [SEP]" and "[CLS] + target + [SEP]" respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attentional Encoder Layer</head><p>The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Multi-Head Attention</head><p>Multi-Head Attention (MHA) is the attention that can perform multiple attention function in parallel. Different from Transformer <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>, we use Intra-MHA for introspective context words modeling and Inter-MHA for contextperceptive target words modeling, which is more lightweight and target is modeled according to a given context.</p><p>An attention function maps a key sequence k = {k 1 , k 2 , ..., k n } and a query sequence q = {q 1 , q 2 , ..., q m } to an output sequence o:</p><formula xml:id="formula_1">Attention(k, q) = sof tmax(f s (k, q))k (1)</formula><p>where f s denotes the alignment function which learns the semantic relevance between q j and k i :</p><formula xml:id="formula_2">f s (k i , q j ) = tanh([k i ; q j ] · W att )<label>(2)</label></formula><p>where W att ∈ R 2d hid are learnable weights. MHA can learn n head different scores in parallel child spaces and is very powerful for alignments. The n head outputs are concatenated and projected to the specified hidden dimension d hid , namely,</p><formula xml:id="formula_3">M HA(k, q) = [o 1 ; o 2 ...; o n head ] · W mh (3) o h = Attention h (k, q) (4) where ";" denotes vector concatenation, W mh ∈ R d hid ×d hid , o h = {o h 1 , o h 2 , ..., o h m } is the output of the h-th head attention and h ∈ [1, n head ].</formula><p>Intra-MHA, or multi-head self-attention, is a special situation for typical attention mechanism that q = k. Given a context embedding e c , we can get the introspective context representation c intra by:</p><formula xml:id="formula_4">c intra = M HA(e c , e c )<label>(5)</label></formula><p>The learned context representation c intra = {c intra 1 , c intra 2 , ..., c intra n } is aware of long-term dependencies.</p><p>Inter-MHA is the generally used form of attention mechanism that q is different from k. Given a context embedding e c and a target embedding e t , we can get the context-perceptive target representation t inter by:</p><formula xml:id="formula_5">t inter = M HA(e c , e t )<label>(6)</label></formula><p>After this interactive procedure, each given target word e t j will have a composed representation selected from context embeddings e c . Then we get the context-perceptive target words modeling t inter = {t inter 1 , t inter 2 , ..., t inter m }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Point-wise Convolution Transformation</head><p>A Point-wise Convolution T ransformation (PCT) can transform contextual information gathered by the MHA. Point-wise means that the kernel sizes are 1 and the same transformation is applied to every single token belonging to the input. Formally, given a input sequence h, PCT is defined as:</p><formula xml:id="formula_6">P CT (h) = σ(h * W 1 pc + b 1 pc ) * W 2 pc + b 2 pc (7)</formula><p>where σ stands for the ELU activation, * is the convolution operator, W 1 pc ∈ R d hid ×d hid and W 2 pc ∈ R d hid ×d hid are the learnable weights of the two convolutional kernels, b 1 pc ∈ R d hid and b 2 pc ∈ R d hid are biases of the two convolutional kernels.</p><p>Given c intra and t inter , PCTs are applied to get the output hidden states of the attentional encoder layer h c = {h c 1 , h c 2 , ..., h c n } and h t = {h t 1 , h t 2 , ..., h t m } by:</p><formula xml:id="formula_7">h c = P CT (c intra ) (8) h t = P CT (t inter )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Target-specific Attention Layer</head><p>After we obtain the introspective context representation h c and the context-perceptive target representation h t , we employ another MHA to obtain the target-specific context representation h tsc = {h tsc 1 , h tsc 2 , ..., h tsc m } by:</p><formula xml:id="formula_8">h tsc = M HA(h c , h t )<label>(10)</label></formula><p>The multi-head attention function here also has its independent parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Output Layer</head><p>We get the final representations of the previous outputs by average pooling, concatenate them as the final comprehensive representationõ, and use a full connected layer to project the concatenated vector into the space of the targeted C classes.</p><formula xml:id="formula_9">o = [h c avg ; h t avg ; h tsc avg ] (11) x =W o Tõ +b o (12) y = sof tmax(x) (13) = exp(x) C k=1 exp(x)<label>(14)</label></formula><p>where y ∈ R C is the predicted sentiment polarity distribution,W o ∈ R 1×C andb o ∈ R C are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Regularization and Model Training</head><p>Since neutral sentiment is a very fuzzy sentimental state, training samples which labeled neutral are unreliable. We employ a Label Smoothing Regularization (LSR) term in the loss function. which penalizes low entropy output distributions <ref type="bibr" target="#b14">(Szegedy et al., 2016)</ref>. LSR can reduce overfitting by preventing a network from assigning the full probability to each training example during training, replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9. For a training sample x with the original ground-truth label distribution q(k|x), we replace q(k|x) with</p><formula xml:id="formula_10">q(k|x) = (1 − )q(k|x) + u(k)<label>(15)</label></formula><p>where u(k) is the prior distribution over labels , and is the smoothing parameter. In this paper, we set the prior label distribution to be uniform u(k) = 1/C. LSR is equivalent to the KL divergence between the prior label distribution u(k) and the network's predicted distribution p θ . Formally, LSR term is defined as:</p><formula xml:id="formula_11">L lsr = −D KL (u(k) p θ )<label>(16)</label></formula><p>The objective function (loss function) to be optimized is the cross-entropy loss with L lsr and L 2 regularization, which is defined as:</p><formula xml:id="formula_12">L(θ) = − C i=1ŷ c log(y c ) + L lsr + λ θ∈Θ θ 2 (17)</formula><p>whereŷ ∈ R C is the ground truth represented as a one-hot vector, y is the predicted sentiment distribution vector given by the output layer, λ is the coefficient for L 2 regularization term, and Θ is the parameter set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Settings</head><p>We conduct experiments on three datasets: Se-mEval 2014 Task 4 2 <ref type="bibr" target="#b12">(Pontiki et al., 2014)</ref> dataset composed of Restaurant reviews and Laptop reviews, and ACL 14 Twitter dataset gathered by <ref type="bibr" target="#b4">Dong et al. (2014)</ref>. These datasets are labeled with three sentiment polarities: positive, neutral and negative. <ref type="table" target="#tab_0">Table 1</ref> shows the number of training and test instances in each category.</p><p>Word embeddings in AEN-GloVe do not get updated in the learning process, but we fine-tune pre-trained BERT 3 in AEN-BERT. Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT. Dimension of hidden states d hid is set to 300. The weights of our model are initialized with Glorot initialization <ref type="bibr" target="#b5">(Glorot and Bengio, 2010)</ref>. During training, we set label smoothing parameter to 0.2 <ref type="bibr" target="#b14">(Szegedy et al., 2016)</ref>, the coefficient λ of L 2 regularization item is 10 −5 and dropout rate is 0.1. Adam optimizer (Kingma and Ba, 2014) is applied to update all the parameters. We adopt the Accuracy and Macro-F1 metrics to evaluate the performance of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Comparisons</head><p>In order to comprehensively evaluate and analysis the performance of AEN-GloVe, we list 7 baseline models and design 4 ablations of AEN-GloVe. We also design a basic BERT-based model to evaluate the performance of AEN-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-RNN based baselines:</head><p>• Feature-based SVM <ref type="bibr" target="#b8">(Kiritchenko et al., 2014)</ref> is a traditional support vector machine based model with extensive feature engineering.</p><p>• Rec-NN <ref type="bibr" target="#b4">(Dong et al., 2014)</ref> firstly uses rules to transform the dependency tree and put the opinion target at the root, and then learns the sentence representation toward target via semantic composition using Recursive NNs.</p><p>• MemNet <ref type="bibr" target="#b16">(Tang et al., 2016b)</ref> uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN based baselines:</head><p>• TD-LSTM (Tang et al., 2016a) extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively. The left and right target-dependent representations are concatenated for predicting the sentiment polarity of the target.</p><p>• ATAE-LSTM <ref type="bibr" target="#b18">(Wang et al., 2016)</ref> strengthens the effect of target embeddings, which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification.</p><p>• IAN <ref type="bibr" target="#b10">(Ma et al., 2017)</ref> learns the representations of the target and context with two LSTMs and attentions interactively, which generates the representations for targets and contexts with respect to each other.</p><p>• RAM <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> strengthens Mem-Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AEN-GloVe ablations:</head><p>• AEN-GloVe w/o PCT ablates PCT module.</p><p>• AEN-GloVe w/o MHA ablates MHA module.</p><p>• AEN-GloVe w/o LSR ablates label smoothing regularization.</p><p>• AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM.</p><p>Basic BERT-based model:</p><p>• BERT-SPC feeds sequence "[CLS] + context + [SEP] + target + [SEP]" into the basic BERT model for sentence pair classification task. <ref type="table" target="#tab_1">Table 2</ref> shows the performance comparison of AEN with other models. BERT-SPC and AEN-BERT obtain substantial accuracy improvements, which shows the power of pre-trained BERT on small-data task. The overall performance of AEN-BERT is better than BERT-SPC, which suggests that it is important to design a downstream network customized to a specific task. As the prior knowledge in the pre-trained BERT is not specific to any particular domain, further fine-tuning on the specific task is necessary for releasing the true power of BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The overall performance of TD-LSTM is not good since it only makes a rough treatment of the target words. ATAE-LSTM, IAN and RAM are attention based models, they stably exceed the TD-LSTM method on Restaurant and Laptop datasets. RAM is better than other RNN based models, but it does not perform well on Twitter dataset, which might because bidirectional LSTM is not good at modeling small and ungrammatical text.</p><p>Feature-based SVM is still a competitive baseline, but relying on manually-designed features. Rec-NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments. Like AEN, MemNet also eschews recurrence, but its overall performance is not good since it does not model the hidden semantic of embeddings, and the result of the last attention is essentially a linear combination of word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, the performances of AEN-GloVe ablations are incomparable with AEN- GloVe in both accuracy and macro-F1 measure. This result shows that all of these discarded components are crucial for a good performance.</p><p>Comparing the results of AEN-GloVe and AEN-GloVe w/o LSR, we observe that the accuracy of AEN-GloVe w/o LSR drops significantly on all three datasets. We could attribute this phenomenon to the unreliability of the training samples with neutral sentiment. The overall performance of AEN-GloVe and AEN-GloVe-BiLSTM is relatively close, AEN-GloVe performs better on the Restaurant dataset. More importantly, AEN-GloVe has fewer parameters and is easier to parallelize.</p><p>To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in <ref type="table" target="#tab_2">Table 3</ref>. We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU 4 .</p><p>RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all attention based RNN models, memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms. MemNet has the lowest model size as it only has one shared attention layer and two linear layers, it does not calculate hidden states 4 NVIDIA GTX 1080ti. of word embeddings. AEN-GloVe's lightweight level ranks second, since it takes some more parameters than MemNet in modeling hidden states of sequences. As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of the proposed AEN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Positive</cell><cell cols="2">Neural</cell><cell cols="2">Negative</cell></row><row><cell></cell><cell cols="6">Train Test Train Test Train Test</cell></row><row><cell>Twitter</cell><cell cols="6">1561 173 3127 346 1560 173</cell></row><row><cell cols="3">Restaurant 2164 728</cell><cell>637</cell><cell>196</cell><cell>807</cell><cell>196</cell></row><row><cell>Laptop</cell><cell>994</cell><cell>341</cell><cell>464</cell><cell>169</cell><cell>870</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Main results. The results of baseline models are retrieved from published papers. "-" means not reported. Top 3 scores are in bold.</figDesc><table><row><cell></cell><cell>Models</cell><cell cols="2">Twitter</cell><cell cols="2">Restaurant</cell><cell cols="2">Laptop</cell></row><row><cell></cell><cell></cell><cell cols="6">Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1</cell></row><row><cell></cell><cell>TD-LSTM</cell><cell>0.7080</cell><cell>0.6900</cell><cell>0.7563</cell><cell>-</cell><cell>0.6813</cell><cell>-</cell></row><row><cell>RNN baselines</cell><cell>ATAE-LSTM IAN</cell><cell>--</cell><cell>--</cell><cell>0.7720 0.7860</cell><cell>--</cell><cell>0.6870 0.7210</cell><cell>--</cell></row><row><cell></cell><cell>RAM</cell><cell>0.6936</cell><cell>0.6730</cell><cell>0.8023</cell><cell>0.7080</cell><cell>0.7449</cell><cell>0.7135</cell></row><row><cell></cell><cell>Feature-based SVM</cell><cell>0.6340</cell><cell>0.6330</cell><cell>0.8016</cell><cell>-</cell><cell>0.7049</cell><cell>-</cell></row><row><cell>Non-RNN baselines</cell><cell>Rec-NN</cell><cell>0.6630</cell><cell>0.6590</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MemNet</cell><cell>0.6850</cell><cell>0.6691</cell><cell>0.7816</cell><cell>0.6583</cell><cell>0.7033</cell><cell>0.6409</cell></row><row><cell></cell><cell>AEN-GloVe w/o PCT</cell><cell>0.7066</cell><cell>0.6907</cell><cell>0.8017</cell><cell>0.7050</cell><cell>0.7272</cell><cell>0.6750</cell></row><row><cell>AEN-GloVe ablations</cell><cell>AEN-GloVe w/o MHA AEN-GloVe w/o LSR</cell><cell>0.7124 0.7080</cell><cell>0.6953 0.6920</cell><cell>0.7919 0.8000</cell><cell>0.7028 0.7108</cell><cell>0.7178 0.7288</cell><cell>0.6650 0.6869</cell></row><row><cell></cell><cell>AEN-GloVe-BiLSTM</cell><cell>0.7210</cell><cell>0.7042</cell><cell>0.7973</cell><cell>0.7037</cell><cell>0.7312</cell><cell>0.6980</cell></row><row><cell></cell><cell>AEN-GloVe</cell><cell>0.7283</cell><cell>0.6981</cell><cell>0.8098</cell><cell>0.7214</cell><cell>0.7351</cell><cell>0.6904</cell></row><row><cell>Ours</cell><cell>BERT-SPC</cell><cell>0.7355</cell><cell>0.7214</cell><cell>0.8446</cell><cell>0.7698</cell><cell>0.7899</cell><cell>0.7503</cell></row><row><cell></cell><cell>AEN-BERT</cell><cell>0.7471</cell><cell>0.7313</cell><cell>0.8312</cell><cell>0.7376</cell><cell>0.7993</cell><cell>0.7631</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold.</figDesc><table><row><cell>Models</cell><cell cols="2">Model size</cell></row><row><cell></cell><cell cols="2">Params ×10 6 Memory (MB)</cell></row><row><cell>TD-LSTM</cell><cell>1.44</cell><cell>12.41</cell></row><row><cell>ATAE-LSTM</cell><cell>2.53</cell><cell>16.61</cell></row><row><cell>IAN</cell><cell>2.16</cell><cell>15.30</cell></row><row><cell>RAM</cell><cell>6.13</cell><cell>31.18</cell></row><row><cell>MemNet</cell><cell>0.36</cell><cell>7.82</cell></row><row><cell>AEN-BERT</cell><cell>112.93</cell><cell>451.84</cell></row><row><cell>AEN-GloVe-BiLSTM</cell><cell>3.97</cell><cell>22.52</cell></row><row><cell>AEN-GloVe</cell><cell>1.16</cell><cell>11.04</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The detailed introduction of this task can be found at http://alt.qcri.org/semeval2014/task4.3  We use uncased BERT-base from https://github. com/google-research/bert.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose an attentional encoder network for the targeted sentiment classification task. which employs attention based encoders for the modeling between context and target. We raise the the label unreliability issue add a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of the proposed model.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 international conference on web search and data mining</title>
		<meeting>the 2008 international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised polarity lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
