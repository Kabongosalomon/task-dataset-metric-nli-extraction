<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distance-based Self-Attention Network for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbae</forename><surname>Im</surname></persName>
							<email>jinbae@dm.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">Seoul National University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">Seoul National University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distance-based Self-Attention Network for Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanism has been used as an ancillary means to help RNN or CNN. However, the Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> recently recorded the state-of-theart performance in machine translation with a dramatic reduction in training time by solely using attention. Motivated by the Transformer, Directional Self Attention Network (Shen et al., 2017), a fully attention-based sentence encoder, was proposed. It showed good performance with various data by using forward and backward directional information in a sentence. But in their study, not considered at all was the distance between words, an important feature when learning the local dependency to help understand the context of input text. We propose Distance-based Self-Attention Network, which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent. Our model shows good performance with NLI data, and it records the new state-of-the-art result with SNLI data. Additionally, we show that our model has a strength in long sentences or documents. * The NLI task can be solved through two different approaches: sentence encoding-based models and joint models. The former separately encode each sentence, whereas the latter take into account the direct relationship between two sentences. Between them, sentence-encoding based models focus on training sentence encoder that can represent sentences in vector form well. We focus on the former approach, since the objective of our work is to develop an advanced sentenceencoding model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence modeling has been employing Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) mostly. More recently, models incorporating attention mechanisms have shown good performance in machine translation <ref type="bibr" target="#b1">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b22">Sutskever et al., 2014)</ref>, Natural Language Inference (NLI) <ref type="bibr" target="#b12">(Liu et al., 2016)</ref>, and Question Answering (QA) <ref type="bibr" target="#b9">(Hermann et al., 2015;</ref><ref type="bibr" target="#b21">Sukhbaatar et al., 2015)</ref> etc. Attention mechanisms used to be exploited in conjunction with RNN or CNN as an ancillary means to help improve performance. Lately, <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> presented the first fully attention-based model, which recorded the state-of-the-art result in machine translation. As a fully attention-based model can consider all words in a sentence at once, parallelization leads to great reduction in training time.</p><p>Motivated by <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, <ref type="bibr" target="#b20">Shen et al. (2017)</ref> proposed the first fully attention-based sentence encoder. <ref type="bibr" target="#b20">Shen et al. (2017)</ref> recorded good performance in a variety of tasks. In particular, they recorded the state-of-the-art result with Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> which is a representative dataset of NLI. The NLI task aims to classify the relationship between two sentences as entailment, contradiction, or neutral. One of the approaches to solving the NLI task is to use sentence-encoding based models. <ref type="bibr" target="#b20">* Shen et al. (2017)</ref> presented a sentence-encoding based model reflecting directional information in a sentence. However, the distance between words was not considered at all in their model, and the directional information simply involved words before and after the reference word. Altogether, positional information of words was not fully taken into account. As a result, the difference of importance between the distant words and the nearby words was not appropriately reflected. Hence lo-cal dependency was not properly modeled, which in turn failed to capture the context information in long sentences.</p><p>To tackle this limitation, we propose Distancebased Self-Attention Network which introduces a distance mask which models the relative distance between words. In conjunction with a directional mask, the distance mask allows us to incorporate complete positional information of words in our model. Our Distance-based Self-Attention Network achieved good performance with NLI data, and recorded the state-of-the-art result with SNLI. Our model worked exceptionally well with long sentences, in particular. We also visualized the effect of the distance mask to show that our model can grasp both local dependency and global dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>NLI tasks have been studied through models of various structures. Most of all, models combining attention with Long Short-Term Memory (LSTM) have performed well. <ref type="bibr" target="#b12">Liu et al. (2016)</ref> improved the performance by adding the mean pooling vector to the conventional attention model in which attention is applied to hidden states of LSTM. <ref type="bibr" target="#b5">Chen et al. (2017)</ref> used the input gates of the LSTM as attention weights to simplify the model structure. In <ref type="bibr" target="#b5">Chen et al. (2017)</ref> and <ref type="bibr" target="#b17">Ni and Bansal (2017)</ref>, short-cut connections in stacked LSTM, in combination with max-pooling originally suggested by <ref type="bibr" target="#b7">Conneau et al. (2017)</ref>, were proven effective in improving performance, recording the state-of-the-art performance in MultiNLI. And <ref type="bibr" target="#b14">Munkhdalai and Yu (2016a)</ref> used the memory for sentence encoding motivated by Neural Turing Machine <ref type="bibr" target="#b8">(Graves et al., 2014)</ref>. <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> was the first study to construct an end-to-end model with attention alone, and recorded the state-of-the-art performance in machine translation tasks. <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>'s encoder-decoder framework consists of a multihead attention and a position-wise feed forward network as a basic building block which is deeply stacked combined with residual connection. The multi-head attention projects the input sentences to multiple subspaces and then computes the scaled dot-product attention in each subspace. The results in each subspace are then concatenated and projected again. Position-wise feed forward network adds non-linearity to vector representations of each position. In this way, the fully attentionbased model was constructed without using RNN or CNN, and the training cost was greatly reduced. <ref type="bibr" target="#b20">Shen et al. (2017)</ref>, a very recent work, constructed a fully attention-based sentence encoder motivated by <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>. They proposed a multi-dimensional attention mechanism that computes the attention by each dimension through modification of additive attention. In addition, their model exploits directional attention as well as fusion gate motivated by bi-directional LSTM. Directional information was reflected by introducing a simple directional mask. By adding a directional mask to the logit of attention, words in a specific direction in the sentence were masked to avoid attention. The extent to which attention results are ultimately reflected was determined through fusion gate. In our study, we construct our model based on <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>'s basic building block, as well as <ref type="bibr" target="#b20">Shen et al. (2017)</ref>'s key model structures. In order to model the distance between words, which was not considered in their works, we transform the multi-head attention in <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, in particular, to fit our objective. Details can be found in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, the attention function is defined as follows by introducing the concept of query, key, and value. "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>." The two most commonly used attentions are additive attention <ref type="bibr" target="#b1">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b19">Shang et al., 2015)</ref> and dot-product attention <ref type="bibr" target="#b10">(Kim et al., 2016;</ref><ref type="bibr" target="#b21">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Additive Attention</head><p>Let query, ith key, and ith value be q, k i , and v i respectively.</p><formula xml:id="formula_0">(q ∈ R d k , k i ∈ R d k , and v i ∈ R dv )</formula><p>Compatibility function of the query with the ith key is represented by the following equation 1.</p><formula xml:id="formula_1">f (q, k i ) = l i = u T σ(q + k i ),<label>(1)</label></formula><p>where u ∈ R d k , and σ(·) is an activation function usually chosen as tanh.</p><p>And attention weight assigned to each ith value is computed by applying the softmax function to l i and final output is weighted sum of value as following equations.</p><formula xml:id="formula_2">w i = exp(l i ) j=1 exp(l j ) (2) Output = i=1 w i v i (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dot-product Attention</head><p>Dot-product attention is the same as additive attention except for compatibility function. In dotproduct attention, compatibility function is computed by the following equation 4 in place of the equation 1.</p><formula xml:id="formula_3">f (q, k i ) = l i = q, k i<label>(4)</label></formula><p>On implementation, dot-product attention is much faster and more space-efficient than additive attention due to optimized matrix multiplication.</p><p>In practice, however, additive attention outperforms dot product attention for large values of d k . So <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> used scaled dot-product attention instead of normal dot-product attention to prevent performance loss in large dimension as following equation 5.</p><formula xml:id="formula_4">f (q, k i ) = l i = q, k i √ d k<label>(5)</label></formula><p>4 Proposed Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Architecture</head><p>Our model's overall architecture is shown in <ref type="figure">Figure</ref> 1. We follow the conventional architecture for training NLI data. First, the two input sentences, premise and hypothesis, are encoded as vectors, u and v respectively, through identical sentence encoders. For the encoded vectors u and v, the representation of relation between the two vectors is generated by the concatenation of u, v, |u − v|, and u * v. Thereafter, a probability for each of the 3-class is generated through the 300D ReLU layer and the 3-way softmax output layer. We configured the model with the setting of 1layer 300D as in <ref type="bibr" target="#b20">Shen et al. (2017)</ref> to focus on the performance evaluation of the sentence encoder itself. Layer normalization <ref type="bibr">(Ba et al., 2016)</ref> and dropout are applied to 300D ReLU layer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Word Embedding Layer</head><p>Let an input sentence be a sequence of discrete words x = [x 1 , x 2 , · · ·, x n ], where x i ∈ R N is a one-hot representation of the word i, and N is the vocabulary size. These one-hot representations are transformed into dense representations by us-ing the pre-trained word embedding.</p><p>Let W e ∈ R de×N be a pre-trained word embedding matrix. Then a sequence of dense word representations can be written as w = W e x = [w 1 , w 2 , · · ·, w n ], where w i ∈ R de is dense representation of the word i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Masked Multi-Head Attention</head><p>The masked multi-head attention is a variation of the multi-head attention employed by <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>. The scaled dot-product attention of <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> is expressed as following:</p><formula xml:id="formula_5">Attention(Q, K, V ) = softmax( QK T √ d k )V (6)</formula><p>where Q, K, V are matrices composed of a set of queries, keys, and values, respectively. We transform equation 6 and express the masked attention as following:</p><formula xml:id="formula_6">Masked(Q, K, V ) = softmax( QK T √ d k + M dir + αM dis )V<label>(7)</label></formula><p>Here, M dir ∈ R n×n is the directional mask as proposed in <ref type="bibr" target="#b20">Shen et al. (2017)</ref>, while M dis ∈ R n×n is the distance mask proposed in this model. Hyper parameter α is the distance-alpha tuned through validation data.</p><p>M dir consists of the forward mask and backward mask as explained in <ref type="figure">Figure 3</ref>. In the Forward Masked Multi-Head Attention phase, the forward mask is selected, and in the Backward Masked Multi-Head Attention phase, the backward mask. The forward masks prevent words that appear after a given word from being considered in the attention process, while backward masks prevent words that appear before from consideration by adding −∞ to the logits before taking the softmax at the attention phase. The diagonal component of M dir is also set to −∞ so that each token does not consider itself to attention, and the information of each token is later transmitted through the fusion gate of section 4.2.3 M dis is shown in the <ref type="figure">Figure 4</ref>. The (i, j) component of the distance mask is −|i − j|, representing the distance between (i + 1)th word and (j + 1)th word multiplied by −1. By multiplying this value by α and adding it to logit, the attention weight becomes smaller as distance increases. That is, the distance mask serves to concentrate on the local words around the reference word. Such a structure may appear similar to a CNN filter extracting a local feature. Yet, the big difference is that CNN only uses information in the window size, whereas our model considers all words in a sentence at once, concentrating on the local words by taking account of the relative distance between words. By using the distance mask, the distance between words, not considered through the directional mask of <ref type="bibr" target="#b20">Shen et al. (2017)</ref>, was considered additionally, so the complete positional information of words was taken into consideration. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Distance mask</head><p>The masked multi-head attention can be expressed as following:</p><formula xml:id="formula_7">Masked Multi-Head(Q, K, V ) = concat(head 1 , · · · , head h )W O (8) where head i = Masked(QW Q i , KW K i , V W V i ), with h as the number of heads, W Q i , W K i , W V i ∈ R de×de/h , and W O ∈ R de×de . Q, K, V ∈ R n×de</formula><p>are matrices created from n word embedding vectors of sentences and expressed as equation 9.</p><formula xml:id="formula_8">Q = K = V =      − w 1 − − w 2 − . . . − w n −     <label>(9)</label></formula><p>The masked multi-head attention first projects Q, K, V into h subspaces, respectively, and performs masked attention of equation 7 for each Q, K, V projection combination. The h attention result is concatenated before projection. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Fusion Gate</head><p>At the fusion gate, raw word embedding S ∈ R n×de and the result of masked multi-head attention H ∈ R n×de in equation 10 are used as input.</p><formula xml:id="formula_9">S =      − w 1 − − w 2 − . . . − w n −      H =      − h 1 − − h 2 − . . . − h n −     <label>(10)</label></formula><p>First, we generate S F , H F by projecting S, H using W S , W H ∈ R de×de . Mathematically:</p><formula xml:id="formula_10">S F = SW S H F = HW H<label>(11)</label></formula><p>Then create gate F as shown in equation 12</p><formula xml:id="formula_11">where b F ∈ R de . Gate(S, H) = F S F + (1 − F ) H F where F = sigmoid(S F + H F + b F )<label>(12)</label></formula><p>Finally, we obtain the gated sum by using F . It is common in many papers including <ref type="bibr" target="#b20">Shen et al. (2017)</ref> to use raw S and H in gated sum. We, however, use the gated sum of S F and H F which resulted in a significant increase in accuracy. * Multi-head attention <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> is fast and efficient because it is based on dot-product attention. However, multi-dimensional attention <ref type="bibr" target="#b20">(Shen et al., 2017)</ref> has a disadvantage in that it consumes a lot of gpu memory because it requires several 4-dimensional tensors on implementation. So, in our model, the multi-head attention was used as a base structure instead of the multi-dimensional attention. In addition, the performance of the actual implementation was also better with multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Position-wise Feed Forward Networks</head><p>We used position-wise feed forward network structure of <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> as it is. The position-wise feed forward network employs the same fully connected network to each position of sentence, in which the fully connected layer consists of two linear transformations, with the ReLU activation in between. Mathematically:</p><formula xml:id="formula_12">FFN(x) = max(0, xW P 1 + b P 1 )W P 2 + b P 2 (13) where x ∈ R 1×de , W P 1 ∈ R de×d f f , W P 2 ∈ R d f f ×de , b P 1 ∈ R d f f , and b P 2 ∈ R de .</formula><p>The FFN function of the above equation 13 is applied to each position of the result of the fusion gate. Note that position-wise feed forward network is combined with the residual connection as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. That is, FFN learns the residuals. In our model, d f f was set to 4d e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Pooling Layer</head><p>The vector representation of input sentence is generated through the pooling layer after the concatenation of the results of forward directional self attention and backward directional self attention. That is, the input of pooling layer is</p><formula xml:id="formula_13">U = [U f w ; U bw ] ∈ R n×2de where each directional self attention output is U f w ∈ R n×de , U bw ∈ R n×de .</formula><p>We use the multi-dimensional source2token self-attention of <ref type="bibr" target="#b20">Shen et al. (2017)</ref> for our multidimensional self-attention.</p><p>For ith row vector of U , u i , logit l(u i ) is computed as following:</p><formula xml:id="formula_14">l(u i ) = ELU (u i W M 1 + b M 1 )W M 2 + b M 2<label>(14)</label></formula><p>where</p><formula xml:id="formula_15">u i = U i * ∈ R 1×2de , W M 1 , W M 2 ∈ R 2de×2de , and b M 1 , b M 2 ∈ R 2de</formula><p>. The calculations of logit consist of two linear transformations, with the Exponential Linear Units (ELU) activation function <ref type="bibr" target="#b6">(Clevert et al., 2015)</ref> in between. Multi-dimensional attention differs from general attention in that the logit for an input vector is not a scalar but a vector with dimensions equal to the dimensions of the input vector. This allows each dimension of the input vector to have a scalar logit, and we can perform attention to n word tokens in each dimension, as illustrated below by equation 15, 16. Note that softmax is performed on the row dimension of L, not the column dimension.</p><formula xml:id="formula_16">M = softmax(L) U where L =      − l(u 1 ) − − l(u 2 ) − . . . − l(u n ) −      (15) Multi-dimensional(U ) = n i=1 M i *<label>(16)</label></formula><p>The 2d e -dimensional output vector of multidimensional attention and the 2d e -dimensional vector obtained by applying max pooling to U are concatenated to encode the input sentence as a 4d e -dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>The dataset used in the experiments are SNLI <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> and MultiNLI  datasets. The SNLI dataset consists of 549,367 / 9,842 / 9,824 (train / valid / test) premise and hypothesis pairs; and the MultiNLI dataset, 392,702 / 9,815 / 9,832 / 9,796 / 9,847 (train / valid matched / valid mismatched / test matched / test mismatched) sentence pairs. The two datasets have the same format, but sentences in the MultiNLI dataset are much longer than those in SNLI dataset. In addition, MultiNLI dataset consists of various genre information. If genres included in the train data are also found in valid (test) data, then the dataset is called "matched"; if valid (test) data includes genres that are not in the train data, then the dataset is called "mismatched".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Details</head><p>We used the Glove 840B 300D 1 (d e = 300) for the pre-trained word embedding without any finetuning. This is to train the more universally usable sentence encoder.</p><p>Layer normalization <ref type="bibr">(Ba et al., 2016)</ref> was applied to all linear projections of masked multihead attention, fusion gate, and multi-dimensional attention. We applied residual dropout as used in <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, with dropout to the output of masked multi-head attention and S F +H F +b F of fusion gate. 1 https://nlp.stanford.edu/projects/glove/ We set h = 5, α = 1.5 in the masked multi-head attention, and the dropout probability was set to 0.1. Batch size was 64, and the model was trained with Adam <ref type="bibr" target="#b11">(Kingma et al., 2014)</ref> optimizer, with a learning rate of 0.001. All models were implemented via Tensorflow on single Nvidia Geforce GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SNLI Results</head><p>Experimental results of SNLI data compared with the existing models on the SNLI leader-board 2 are shown in <ref type="table" target="#tab_2">Table 1</ref>. Compared with the existing state-of-the-art model <ref type="bibr" target="#b20">(Shen et al., 2017)</ref>, the number of parameters and the training time increased, but our results show the new state-of-theart record. We also looked at the model with distance mask removed to verify the effect of the distance mask proposed in this paper. Results show that the addition of the distance mask improved the performance without significantly affecting the training time or increasing the number of parameters.    <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> 49.4 50.4 +Unigram and bigram features <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> 99.7 78.2 Sentence encoding-based models 100D LSTM encoders <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> 220k 84.8 77.6 300D LSTM encoders <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref> 3.0m 83.9 80.6 1024D GRU encoders <ref type="bibr" target="#b24">(Vendrov et al., 2015)</ref> 15m 98.8 81.4 300D Tree-based CNN encoders <ref type="bibr" target="#b13">(Mou et al., 2015)</ref> 3.5m 83.3 82.1 300D SPINN-PI encoders <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref> 3.7m 89.2 83.2 600D Bi-LSTM encoders <ref type="bibr" target="#b12">(Liu et al., 2016)</ref> 2.0m 86.4 83.3 300D NTI-SLSTM-LSTM encoders <ref type="bibr" target="#b15">(Munkhdalai and Yu, 2016b)</ref> 4.0m 82.5 83.4 600D Bi-LSTM encoders+intra-attention <ref type="bibr" target="#b12">(Liu et al., 2016)</ref> 2.8m 84.5 84.2 300D NSE encoders <ref type="bibr" target="#b14">(Munkhdalai and Yu, 2016a)</ref> 3.0m 86.2 84.6 600D Deep Gated Attn. BiLSTM encoders <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> 11.6m 90.5 85.5 600D Directional Self-Attention Network <ref type="bibr" target="#b20">(Shen et al., 2017)</ref> 2  The improvement of the test accuracy by introducing the distance mask is only by 0.3% point, potentially because SNLI data mostly consist of short sentences. Hence, we additionally examined how the effect of the distance mask changes as the average length of the two sentences of premise and hypothesis pair changes. The distribution of the average length of the two sentences of the SNLI test data is shown in <ref type="figure" target="#fig_3">Figure 5</ref>, and the effect of the distance mask according to the average length change can be seen from <ref type="figure" target="#fig_4">Figure 6</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> shows that the accuracy is similar until the average length is less than 25, yet the test accuracy of the model without the distance mask deteriorates drastically for data of an average length exceeding 25. This demonstrates that the distance mask has an advantage with long sentences or documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MultiNLI Results</head><p>The results of applying SNLI best model to MultiNLI dataset without additional parameter tuning are presented in  <ref type="bibr" target="#b20">(Shen et al., 2017)</ref>. This once again confirms our model's advantage in long sentences, given that the sentence is much longer in MultiNLI.</p><p>Compared with the result of RepEVAL 2017 , we can see that the Distance-based Self-Attention Network performs well. When compared with the model of <ref type="bibr" target="#b5">Chen et al. (2017)</ref>, our model showed similar average test accuracy with much lower number of parameters. Also, considering that the model of <ref type="bibr" target="#b5">Chen et al. (2017)</ref> is a complex LSTM model, our model has an advantage in training time as a fully attention-based model. <ref type="bibr" target="#b17">Ni and Bansal (2017)</ref> showed the best performance with 74.5% accuracy in Matched Test. However, it is a very deep structured LSTM model with 140.2m parameters. In our model, the inference layer is simply composed of 1 layer of 300D in order to focus on the training of sentence encoder. Both in <ref type="bibr" target="#b5">Chen et al. (2017)</ref> and <ref type="bibr" target="#b17">Ni and Bansal (2017)</ref> models, the inference layer was set very complex in order to improve the MultiNLI accuracy. Taking this into consideration, it can be seen that our Distance-based Self-Attention Network performs competitively given its simpler structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>A case study was conducted to investigate the role of each structure of the Distance-based Self-Attention Network. For this, a sentence "A lady stands outside of a Mexican market." is picked   O 67.5 67.1 RepEval 2017  Cha-level Intra-attention BiLSTM encoders <ref type="bibr" target="#b27">(Yang et al., 2017)</ref> O 67.9 68.2 BiLSTM + enhanced embedding + max pooling <ref type="bibr">(Vu et al., 2017)</ref> X 70.7 70.8 BiLSTM + Inner-attention <ref type="bibr" target="#b2">(Balazs et al., 2017)</ref> O 72.1 72.1 Deep Gated Attn. BiLSTM encoders <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> X 11.6m 73.5 73.6 Shortcut-Stacked BiLSTM encoders <ref type="bibr" target="#b17">(Ni and Bansal, 2017)</ref> O 140.2m 74.5 73.5 Fully attention-based models Directional Self-Attention Network <ref type="bibr" target="#b20">(Shen et al., 2017)</ref> X 2.4m 71.0 71.4 Our Distance-based Self-Attention Network X 4.7m 74.1 72.9 among the premise sentences of SNLI test data. We focused on training encoders that can represent each sentence in a vector form well. Therefore, a case study was conducted on a single sentence, not a sentence pair.</p><p>Masked Multi-Head Attention We first look at the attention weights in masked multi-head attention. Attention weights represent a n by n matrix corresponding to softmax( QK T √ d k + M dir + αM dis ) of equation 7, which is different for each head.</p><p>Here we look at the average attention weights obtained by averaging the attention weights of each head. The attention weights for each head can be found in Appendix. The row of the matrix of <ref type="figure" target="#fig_5">Figure 7</ref> represents each word of the sentence, and the column represents the attention weights for each word at each row. It can be seen that the attention weights are heavier to the nearby words as compared to those distant from the reference word. At the same time, 'outside' in the forward mask and 'Mexican' in the backward mask have high attention weights for several words. From this, it can be seen that important word is considered in the attention process. However, in panels (c) and (d), the neighboring words are seen more intensively, which im-plies that the local dependency has been well captured by our model. In addition, as shown in panel (c), even if the word is far apart, it is still considered in the attention process if it is important. This demonstrates the effectiveness of the distance mask to identify local dependencies without losing the ability to grasp the global dependency.</p><p>Fusion Gate We visualize the role of the fusion gate F ∈ R n×de at forward directional self attention. <ref type="figure" target="#fig_7">Figure 9</ref> represents the average gate value that averages d e -dimensional gate value for each word. If look at the results of both extremes, keyword 'Mexican' has a low gate value, resulting in an output that greatly reflects the multi-head attention result. In contrast, 'of', '.', the words of little importance, have large gate values, which indicates that the original word embedding is greatly reflected, not the multi-head attention result. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, position-wise ffn is used in conjunction with a residual connection. That is, the final output of position-wise ffn for input x is the d e -dimensional vector of LayerNorm(x + FFN(x)). <ref type="figure" target="#fig_0">Figure 10(b)</ref> visualizes the maximum value of this final output vector.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref>, keywords with a high deactivation ratio is shown in panel (a) and a high final max value in panel <ref type="bibr">(b)</ref>. In case of a word corresponding to a keyword, deactivation occurs frequently in (a), and residual learning is hardly achieved in the position-wise ffn, so that the output of the fusion gate is almost maintained. On the other hand, in case of non-important words, residual learning is performed in position-wise ffn because there is less deactivation in (a), so that the max value of final output becomes smaller in (b). This results in preventing non-important words from consideration in the subsequent pooling layer. In summary, position-wise ffn plays a key role in ensuring that non-critical words are paid less attention to in pooling layers. Pooling Layer For the multi-dimensional attention corresponding to <ref type="figure" target="#fig_0">Figure 11(a)</ref>, we visualized the attention weights averaged for each word, where attention weights correspond to softmax(L) ∈ R n×2de in equation 15.</p><p>In max pooling, the max value is selected for each column of U ∈ R n×2de . Thus, in <ref type="figure" target="#fig_0">Figure  11</ref>(b), we visualize the percentage at which each word is selected in the max pooling operation for the 2d e dimension.</p><p>It can be seen that panels (a) and (b) of <ref type="figure" target="#fig_0">Figure  11</ref> are similar on the whole. In other words, both multi-dimensional attention and max pooling utilize information about key words intensively. A similar result can be expected by using only one of the pooling layers. However, experiment results show that using both multi-dimensional attention and max pooling layer gives better performance. In this paper, we propose the Distance-based Self-Attention Network reflecting the distance between words. By reflecting the word distance information, our model learns the local dependency without losing the ability to capture the global dependency. This was achieved through a simple distance mask, so that the performance of the NLI task could be improved while maintaining the number of parameters and training time. In particular, we recorded the new state-of-the-art performance for SNLI data. The introduction of the distance mask improves the performance with longer sentences.</p><p>As the research on universal sentence encoders using NLI data was proposed by <ref type="bibr" target="#b7">Conneau et al. (2017)</ref>, we plan to carry out research on fully attention-based networks for universal sentence embedding as future work. We will also study the fully attention-based network in image data and speech data. Especially, regarding image data, capsule network <ref type="bibr" target="#b18">(Sabour et al., 2017)</ref> recently proposed, and as research on new structure to replace CNN is going on, our future work will move in similar directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture4.2 Sentence EncoderThe sentence encoder structure proposed in this paper is shown inFigure 2. The term "Norm" inFigure 2stands for layer normalization. The sentence encoder ofFigure 2encodes the premise and hypothesis in a vector form. We describe each component of our sentence encoder in detail in the following subsections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sentence encoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Directional mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>SNLI average sentence length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>With distance mask vs. Without distance mask. Change of test accuracy on SNLI data w.r.t average length of sentence pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Masked multi-head average attention weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Masked multi-head average attention weights : without/with distance mask. (a), (b) : without distance mask. (c), (d) : with distance mask Distance Mask We compared the masked multi-head average attention weights for the longest sentence example in the SNLI test data, with length of 57 words to further verify the effect of the distance mask. Panels (a) and (b) of Figure 8 show results without considering distance, while (c) and (d) show the results with the distance mask. In panels (a) and (b), very distant words are considered in the attention and the overall attention weights were reduced. This implies that each word does not focus on the important words in the attention process, but rather takes into account almost every word, resulting in noisier figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Fusion gate (forward) Position-wise FFN For the FFN function of equation 13, Figure 10(a) represents the deactivation ratio in the first hidden layer of position-wise ffn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) First hidden layer deactivation ratio (b) Final output max value (+residual connection) Figure 10: Position-wise ffn (forward)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 11: Pooling layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of different models on SNLI data. | θ| : number of parameters (ex-</figDesc><table /><note>cluding word embedding part). T(s)/epoch : average training time (second) per epoch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Note that</cell></row><row><cell>matched-test accuracy and mismatched-test accu-</cell></row><row><cell>racy were obtained by submitting our test results</cell></row><row><cell>to Kaggle open evaluation platforms: MultiNLI</cell></row><row><cell>Matched Open Evaluation 3 and MultiNLI Mis-</cell></row><row><cell>matched Open Evaluation 4 . First, the average</cell></row><row><cell>test accuracy difference is greater than 2% when</cell></row><row><cell>compared to the Directional Self-Attention Net-</cell></row><row><cell>3 https://www.kaggle.com/c/</cell></row><row><cell>multinli-matched-open-evaluation</cell></row><row><cell>4 https://www.kaggle.com/c/</cell></row><row><cell>multinli-mismatched-open-evaluation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of different models on MultiNLI data. SNLI Mix : use of SNLI training dataset. | θ| : number of parameters (excluding word embedding part).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* In<ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, the positional information of the word was used through positional encoding. By adding the positional encoding vector to the word embedding vector, the embedding changed according to the absolute position of the word in the sentence. However, in sentence modeling, the relative position with respect to the other words is important, not the absolute position of the word. In other words, what words are placed in order before and after the word is important, not the absolute position of the word in a sentence. Therefore, we take the relative position directly into account in our model through the distance mask instead of the positional encoding which considers the relative position indirectly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://nlp.stanford.edu/projects/snli/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Hyejin Lee, Hyunjoong Kim, Taewook Kim, Jinwon An, Inbeom Park, Minki Chung, and many others in SNUDM center, for critical feedback and discussions. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Masked Multi-Head Attention The attention weights for each head in the masked multi-head attention are shown in <ref type="figure">Figures 12 and 13</ref>. <ref type="figure">Figure  12</ref> shows the result of using a forward directional mask, and <ref type="figure">Figure 13</ref> is the result of using a backward directional mask. It can be seen that the attention weights are different for each head. This allows our model to capture various dependencies between words in a sentence. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hinton. 2016. Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Deep Learning Symposium</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Refining raw sentence representations for textual entailment recognition via attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP. Association for Computational Linguistics</title>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP. Association for Computational Linguistics</title>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Djork-Arn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2015</title>
		<meeting>NIPS 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2016</title>
		<meeting>AAAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional lstm model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
	<note>Proceedings of EACL 2017</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
	<note>Proceedings of EACL 2017</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The repeval 2017 shared task: Multi-genre natural language inference with sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcut-stacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09829</idno>
		<title level="m">Dynamic routing between capsules</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2015</title>
		<meeting>NIPS 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2014</title>
		<meeting>NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lct-maltas submission to repeval 2017 shared task</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</title>
		<editor>Hoa Trong Vu, Thuong-Hai Pham, Xiaoyu Bai Marc Tanti</editor>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Lonneke van der Plas, and Albert Gatt</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bowman</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-level intra attention network for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos</forename><forename type="middle">A R</forename><surname>Costa-Juss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>RepEval 2017: The Second Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Fonollosa</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
