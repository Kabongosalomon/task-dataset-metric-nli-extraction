<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistic Features for Readability Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tovly</forename><surname>Deutsch</surname></persName>
							<email>tdeutsch@college.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Jasbi</surname></persName>
							<email>masoudjasbi@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
							<email>shieber@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistic Features for Readability Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Readability assessment aims to automatically classify text by the level appropriate for learning readers. Traditional approaches to this task utilize a variety of linguistically motivated features paired with simple machine learning models. More recent methods have improved performance by discarding these features and utilizing deep learning models. However, it is unknown whether augmenting deep learning models with linguistically motivated features would improve performance further. This paper combines these two approaches with the goal of improving overall model performance and addressing this question. Evaluating on two large readability corpora, we find that, given sufficient training data, augmenting deep learning models with linguistically motivated features does not improve state-of-the-art performance. Our results provide preliminary evidence for the hypothesis that the state-of-theart deep learning models represent linguistic features of the text related to readability. Future research on the nature of representations formed in these models can shed light on the learned features and their relations to linguistically motivated ones hypothesized in traditional approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Readability assessment poses the task of identifying the appropriate reading level for text. Such labeling is useful for a variety of groups including learning readers and second language learners. Readability assessment systems generally involve analyzing a corpus of documents labeled by editors and authors for reader level. Traditionally, these documents are transformed into a number of linguistic features that are fed into simple models like SVMs and MLPs <ref type="bibr" target="#b17">(Schwarm and Ostendorf, 2005;</ref><ref type="bibr" target="#b18">Vajjala and Meurers, 2012)</ref>.</p><p>More recently, readability assessment models utilize deep neural networks and attention mechanisms <ref type="bibr" target="#b14">(Martinc et al., 2019)</ref>. While such models achieve state-of-the-art performance on readability assessment corpora, they struggle to generalize across corpora and fail to achieve perfect classification. Often, model performance is improved by gathering additional data. However, readability annotations are time-consuming and expensive given lengthy documents and the need for qualified annotators. A different approach to improving model performance involves fusing the traditional and modern paradigms of linguistic features and deep learning. By incorporating the inductive bias provided by linguistic features into deep learning models, we may be able to reduce the limitations posed by the small size of readability datasets. In this paper, we evaluate the joint use of linguistic features and deep learning models. We achieve this fusion by simply taking the output of deep learning models as features themselves. Then, these outputs are joined with linguistic features to be further fed into some other model like an SVM. We select linguistic features based on a broad psycholinguistically-motivated composition <ref type="bibr" target="#b19">by Vajjala Balakrishna (2015)</ref>. Transformers and Hierarchical attention networks were selected as the deep learning models because of their state-ofart performance in readability assessment. Models were evaluated on two of the largest available corpora for readability assessment: WeeBit and Newsela. We also evaluate with different sized training sets to investigate the use of linguistic features in data-poor contexts. Our results find that, given sufficient training data, the linguistic features do not provide a substantial benefit over deep learning methods.</p><p>The rest of this paper is organized as follows. Related research is described in section 2. Section 3 details our preprocessing, features, and model construction. Section 4 presents model evaluations on two corpora. Section 5 discusses the implications of our results.</p><p>We provide a publicly available version of the code used for our experiments. 1 2 Related Work Work on readability assessment has involved progress on three core components: corpora, features, and models. While early work utilized small corpora, limited feature sets, and simple models, modern research has experimented with a broad set of features and deep learning techniques.</p><p>Labeled corpora can be difficult to assemble given the time and qualifications needed to assign a text a readability level. The size of readability corpora expanded significantly with the introduction of the WeeklyReader corpus by <ref type="bibr" target="#b17">Schwarm and Ostendorf (2005)</ref>. Composed of articles from an educational magazine, the WeeklyReader corpus contains roughly 2,400 articles. The WeeklyReader corpus was then built upon by <ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> by adding data from the BBC Bitesize website to form the WeeBit corpus. This WeeBit corpus is larger, containing roughly 6,000 documents, while also spanning a greater range of readability levels. Within these corpora, topic and readability are highly correlated. Thus, <ref type="bibr" target="#b23">Xia et al. (2016)</ref> constructed the Newsela corpus in which each article is represented at multiple reading levels thereby diminishing this correlation.</p><p>Early work on readability assessment, such as that of <ref type="bibr" target="#b4">Flesch (1948)</ref>, extracted simple textual features like character count. More recently, <ref type="bibr" target="#b17">Schwarm and Ostendorf (2005)</ref> analyzed a broader set of features including out-of-vocabulary scores and syntactic features such as average parse tree height. <ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> assembled perhaps the broadest class of features. They incorporated measures shown by <ref type="bibr" target="#b13">Lu (2010)</ref> to correlate well with second language acquisition measures, as well as psycholinguistically relevant features from the Celex Lexical database and MRC Psycholinguistic Database <ref type="bibr" target="#b1">(Baayen et al., 1995;</ref><ref type="bibr" target="#b21">Wilson, 1988)</ref>.</p><p>Traditional feature formulas, like the Flesch formula, relied on linear models. Later work progressed to more complex related models like SVMs <ref type="bibr" target="#b17">(Schwarm and Ostendorf, 2005)</ref>. Most recently, state-of-art-performance has been achieved on readability assessment with deep neural network incor-porating attention mechanisms. These approaches ignore linguistic features entirely and instead feed the raw embeddings of input words, relying on the model itself to extract any relevant features. Specifically, <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> found that a pretrained transformer model achieved state-of-the-art performance on the WeeBit corpus while a hierarchical attention network (HAN) achieved state-of-the-art performance on the Newsela corpus.</p><p>Deep learning approaches generally exclude any specific linguistic features. In general, a "featureless" approach is sensible given the hypothesis that, with enough data, training, and model complexity, a model should learn any linguistic features that researchers might attempt to precompute. However, precomputed linguistic features may be useful in data-poor contexts where data acquisition is expensive and error-prone. For this reason, in this paper we attempt to incorporate linguistic features with deep learning methods in order to improve readability assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpora</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">WeeBit</head><p>The WeeBit corpus was assembled by <ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> by combining documents from the WeeklyReader educational magazine and the BBC Bitesize educational website. They selected classes to assemble a broad range of readability levels intended for readers aged 7 to 16. To avoid classification bias, they undersampled classes in order to equalize the number of documents in each class to 625. We term this downsampled corpus "WeeBit downsampled". Following the methodologies of <ref type="bibr" target="#b23">Xia et al. (2016)</ref> and <ref type="bibr" target="#b14">Martinc et al. (2019)</ref>, we applied additional preprocessing to the WeeBit corpus in order to remove extraneous material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Newsela</head><p>The Newsela corpus <ref type="bibr" target="#b23">(Xia et al., 2016)</ref> consists of 1,911 news articles each re-written up to 4 times in simplified manners for readers at different reading levels. This simplification process means that, for any given topic, there exist examples of material on that topic suited for multiple reading levels. This overlap in topic should make the corpus more challenging to label than the WeeBit corpus. In a similar manner to the WeeBit corpus, the Newsela corpus is labeled with grade levels ranging from grade 2 to grade 12. As with WeeBit, these labels can either be treated as classes or transformed into numeric labels for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Labeling Approaches</head><p>Often, readability classes within a corpus are treated as unrelated. These approaches use raw labels as distinct unordered classes. However, readability labels are ordinal, ranging from lower to higher readability. Some work has addressed this issue such as the readability models of <ref type="bibr" target="#b5">Flor et al. (2013)</ref> which predict grade levels via linear regression. To test different approaches to acknowledging this ordinality, we devised three methods for labeling the documents: "classification", "age regression", and "ordered class regression".</p><p>The classification approach uses the classes originally given. This approach does not suppose any ordinality of the classes. Avoiding such ordinality may be desirable for the sake of simplicity.</p><p>"Age regression" applies the mean of the age ranges given by the constituent datasets. For instance, in this approach Level 2 documents from Weekly Reader would be given the label of 7.5 as they are intended for readers of ages 7-8. The advantage of age regression over standard classification is that it provides more precise information about the magnitude of readability differences.</p><p>Finally, "ordered class regression" assigns the classes equidistant integers ordered by difficulty. The least difficult class would be labeled "0", the second least difficult class would be labeled "1" and so on. As with age regression, this labeling results in a regression rather than classification problem. This method retains the advantage of age regression in demonstrating ordinality. However, ordered regression labeling removes information about the relative differences in difficulty between the classes, instead asserting that they are equidistant in difficulty. The motivation behind this loss of information is that such age differences between classes may not directly translate into differences of difficulty. For instance, the readability difference between documents intended for 7 or 8 yearolds may be much greater than between documents intended for 15 or 16 year-olds because reading development is likely accelerated in younger years.</p><p>For final model inferences, we used the classification approach for comparison to previous work. For intermediary CNN models, all three approaches were tested. As the different approaches with CNN models produced insubstantial differences, other model types were restricted to the simple classifi-cation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>Motivated by the success in using linguistic features for modeling readability, we considered a large range of textual analyses relevant to readability. In addition to utilizing features posed in the existing readability research, we investigated formulating new features with a focus on syntactic ambiguity and syntactic diversity. This challenging aspect of language appeared to be underutilized in existing readability literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Existing Features</head><p>To capture a variety of features, we utilized existing linguistic feature computation software 2 developed by Vajjala Balakrishna (2015) based on 86 feature descriptions in existing readability literature. Given the large number of features, in this section we will focus on the categories of features and their psycholinguistic motivations (where available) and properties. The full list of features used can be found in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Features</head><p>The most basic features involve what <ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> refer to as "traditional features" for their use in long-standing readability formulae. They include characters per word, syllables per word, and traditional formulas based on such features like the Flesch-Kincaid formula <ref type="bibr" target="#b8">(Kincaid et al., 1975)</ref>.</p><p>Another set of feature types consists of counts and ratios of part-of-speech tags, extracted using the Stanford parser <ref type="bibr" target="#b10">(Klein and Manning, 2003)</ref>. In addition to basic parts of speech like nouns, some features include phrase level constituent counts like noun phrases and verb phrases. All of these counts are normalized by either the number of word tokens or number of sentences to make them comparable across documents of differing lengths. These counts are not provided with any psycholinguistic motivation for their use; however, it is not an unreasonable hypothesis that the relative usage of these constituents varies across reading levels. Empirically, these features were shown to have some predictive power for readability. In addition to parts of speech counts, we also utilized word type counts as a simple baseline feature, that is, counting the number of instances of each possible word in the vocabulary. These counts are also divided by document length to generate proportions.</p><p>Becoming more abstract than parts of speech, some features count complex syntactic constituent like clauses and subordinated clauses. Specifically, <ref type="bibr" target="#b13">Lu (2010)</ref> found ratios involving sentences, clauses, and t-units 3 that correlated with second language learners' abilities to read a document. For many of the multi-word syntactic constituents previously described, such as noun phrases and clauses, features were also constructed of their mean lengths. Finally, properties of the syntactic trees themselves were analyzed such as their mean heights.</p><p>Moving beyond basic features from syntactic parses, Vajjala Balakrishna (2015) also incorporated "word characteristic" features from linguistic databases. A significant source was the Celex Lexical Database <ref type="bibr" target="#b1">Baayen et al. (1995)</ref> which "consists of information on the orthography, phonology, morphology, syntax and frequency for more than 50,000 English lemmas". The database appears to have a focus on morphological data such as whether a word may be considered a loan word and whether it contains affixes. It also contains syntactic properties that may not be apparent from a syntactic parse, e.g. whether a noun is countable. The MRC Psycholinguistic Database <ref type="bibr" target="#b21">Wilson (1988)</ref> was also used with a focus on its age of acquisition ratings for words, an clear indicator of the appropriateness of a document's vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Novel Syntactic Features</head><p>We investigated additional syntactic features that may be relevant for readability but whose qualities were not targeted by existing features. These features were used in tandem with the existing linguistic features described previously; future work could utilize these novel feature independently to investigate their particular effect on readability information extraction. For generating syntactic parses, we used the PCFG (probabilistic context-free grammar) parser <ref type="bibr" target="#b10">(Klein and Manning, 2003)</ref> from the Stanford Parser package.</p><p>Syntactic Ambiguity Sentences can have multiple grammatical syntactic parses. Therefore, syntactic parsers produce multiple parses annotated with parse likelihood. It may seem sensible to use the number of parses generated as a measure of ambiguity. However, this measure is extremely sensitive to sentence length as longer sentences tend to have more possible syntactic parses. Instead, if this list of probabilities is viewed as a distribution, the standard deviation of this distribution is likely to correlate with perceptions of syntactic ambiguity.</p><formula xml:id="formula_0">Definition 3.1. P D x</formula><p>The parse deviation, P D x (s), of sentence s is the standard deviation of the distribution of the x most probable parse log probabilities for s. If s has less than x valid parses, the distribution is taken from all the valid parses.</p><p>For large values of x, P D x (s) can be significantly sensitive to sentence length: longer sentences are likely to have more valid syntactic parses and thus create low probability tails that increase standard deviation. To reduce this sensitivity, an alternative involves measuring the difference between the largest and mean parse probability.</p><formula xml:id="formula_1">Definition 3.2. P DM x P DM x (s)</formula><p>is the difference between the largest parse log probability and the mean of the log probabilities of the x most probable parses for a sentence s. If s has less than x valid parses, the mean is taken over all the valid parses.</p><p>As a compromise between parse investigation and the noise of implausible parses, we selected P DM 10 , P D 10 , and P D 2 as features to use in the models of this paper.</p><p>Part-of-Speech Divergence To capture the grammatical makeup of a sentence or document, we can count the usage of each part of speech ("POS"), phrase, or clause. The counts can be collected into a distribution. Then, the standard deviation of this distribution, P OSD dev , measures a sentence's grammatical heterogeneity.</p><formula xml:id="formula_2">Definition 3.3. P OSD dev P OSD dev (d) is the standard deviation of the distribution of POS counts for document d.</formula><p>Similarly, we may want to measure how this grammatical makeup differs from the composition of the document as a whole, a concept that might be termed syntactic uniqueness. To capture this concept, we measure the Kullback-Leibler divergence <ref type="bibr" target="#b11">(Kullback and Leibler, 1951)</ref> between the sentence POS count distribution and the document POS count distribution.</p><p>Definition 3.4. P OS div Let P (s) be the distribution of POS counts for sentence s in document d. Let Q be the distribution of POS counts for document d. Let |d| be the number of sentences in d.</p><formula xml:id="formula_3">P OS div (d) = s∈d D KL (P (s) Q) |d|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Models</head><p>A large range of model complexities were evaluated in order to ascertain the performance improvements, or lack thereof, of additional model complexity. In this section we will describe the specific construction and usage of these models for the experiments conducted in this paper, ordered roughly by model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVMs, Linear Models, and Logistic Regression</head><p>We used the Scikit-Learn library <ref type="bibr" target="#b16">(Pedregosa et al., 2011)</ref> for constructing SVM models. Hyperparameter optimization was performed using the guidelines suggested by <ref type="bibr" target="#b6">Hsu et al. (2003)</ref>. From the Scikit-Learn library, we also utilized the linear support vector classifier (an SVM with a linear kernel) and logistic regression classifier. As simplicity was the aim for these evaluations, no hyperparameter optimization was performed. The logistic regression classifier was trained using the stochastic average gradient descent ("sag") optimizer.</p><p>CNN Convolutional neural networks were selected for their demonstrated performance on sentence classification <ref type="bibr" target="#b7">(Kim, 2014)</ref>. The CNN model used in this paper is based on the one described by <ref type="bibr" target="#b7">Kim (2014)</ref> and implemented using the Keras (Chollet and others, 2015), Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, and Magpie libraries.</p><p>Transformer The transformer <ref type="bibr" target="#b20">(Vaswani et al., 2017</ref>) is a neural-network-based model that has achieved state-of-the-art results on a wide array of natural language tasks including readability assessment <ref type="bibr" target="#b14">(Martinc et al., 2019)</ref>. Transformers utilize the mechanism of attention which allows the model to attend to specific parts of the input when constructing the output. Although they are formulated as sequence-to-sequence models, they can be modified to complete a variety of NLP tasks by placing an additional linear layer at the end of the network and training that layer to produce the desired output. This approach often achieves state-of-the-art results when combined with pretraining. In this paper, we use the BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> transformer-based model that is pretrained on BooksCorpus (800M words) <ref type="bibr" target="#b25">(Zhu et al., 2015)</ref> and English Wikipedia. The model is then fine-tuned on a specific readability corpus such as WeeBit. The pretrained BERT model is sourced from the Huggingface transformers library <ref type="bibr" target="#b22">(Wolf et al., 2019)</ref> and is composed of 12 hidden layers each of size 768 and 12 self-attention heads. The fine-tuning step utilizes an implementation by <ref type="bibr" target="#b14">Martinc et al. (2019)</ref>. Among the pretrained transformers in the Huggingface library, there are transformers that can accept sequences of size 128, 256, and 512. The 128 sized model was chosen based on the finding by <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> that it achieved the highest performance on the WeeBit and Newsela corpora. Documents that exceeded the input sequence size were truncated.</p><p>HAN The Hierarchical attention network involves feeding the input through two bidirectional RNNs each accompanied by a separate attention mechanism. One attention mechanism attends to the different words within each sentence while the second mechanism attends to the sentences within the document. These hierarchical attention mechanisms are thought to better mimic the structure of documents and consequently produce superior classification results. The implementation of the model used in this paper is identical to the original architecture described by <ref type="bibr" target="#b24">Yang et al. (2016)</ref> and was provided by the authors of <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> based on code by <ref type="bibr" target="#b15">Nguyen (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Linguistic Features with Neural Models</head><p>The neural network models thus far described take either the raw text or word vector embeddings of the text as input. They make no use of linguistic features such as those described in section 3.2. We hypothesized that combining these linguistic features with the deep neural models may improve their performance on readability assessment. Although these models theoretically represent similar features to those prescribed by the linguistic features, we hypothesized that the amount of data and model complexity may be insufficient to capture them. This can be evidenced in certain models failure to generalize across readability corpora. <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> found that the BERT model performed well on the WeeBit corpus, achieving a weighted F1 score of 0.8401, but performed poorly on the Newsela corpus only achieving an F1 score of 0.5759. They posit that this disparity occurred "because BERT is pretrained as a language model, [therefore] it tends to rely more on semantic than structural differences during the classification phase and therefore performs better on problems with distinct semantic differences between readability classes". Similarly a HAN was able to achieve better performance than BERT on the Newsela but performed substantially worse on the WeeBit corpus. Thus, under some evaluations the models have deficiencies and fail to generalize. Given these deficiencies, we hypothesized that the inductive bias provided by linguistic features may improve generalizability and overall model performance.</p><p>In order to weave together the linguistic features and neural models, we take the simple approach of using the single numerical output of a neural model as a feature itself, joined with linguistic features, and then fed into one of the simpler non-neural models such as SVMs. SVMs were chosen as the final classification model for their simplicity and frequent use in integrating numerical features. The output of the neural model could be any of the label approaches such as grade classes or age regressions described in section 3.1. While all these labeling approaches were tested for CNNs, insubstantial differences in final inferences led us to restrict intermediary results to simple classification for other model types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Evaluation Details</head><p>All experiments involved 5-fold cross validation. All neural-network-based models were trained with the Adam optimizer (Kingma and Ba, 2015) with learning rates of 10 −3 ,10 −4 , and 2 −5 for the CNN, HAN, and transformer respectively. The HAN and CNN models were trained for 20 and 30 epochs. The transformer models were fine-tuned for 3 epochs.</p><p>All results are reported as either a weighted F1 or macro F1 score. To calculate weighted F1, first the F1 score is calculated for each class independently, as if each class was a case of binary classification. Then, these F1 score are combined in a weighted mean in which each class is weighted by the number of samples in that class. Thus, the weighted F1 score treats each sample equally but prioritizes the most common classes. The macro F1 is similar to the weighted F1 score in that F1 scores are first calculated for each class independently. However, for the macro F1 score, the class F1 scores are com- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we report the experimental results of incorporating linguistic features into readability assessment models. The two corpora, WeeBit and Newsela, are analyzed individually and then compared. Our results demonstrate that, given sufficient data, linguistic features provide little to no benefit compared to independent deep learning models. While the corpus experiment results demonstrate a portion of the approaches tested, the full results are available in appendix B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Newsela Experiments</head><p>For the Newsela corpus, while linguistic features were able to improve the performance of some models, the top performers did not utilize linguistic features. The results from the top performing models are presented in table 1. While the HAN performance was not surpassed by models with linguistic features, the transformer models were. This improvement indicates that lin-guistic features capture readability information that transformers cannot capture or have insufficient data to learn. The outsize effect of adding the linguistic features to the transformer models, resulting in a weighted F1 score improvement of 0.22, may reveal what types of information they address. <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> hypothesize that a pretrained language model "tends to rely more on semantic than structural differences" indicating that these features are especially suited to providing non-semantic information such as syntactic qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WeeBit Experiments</head><p>The WeeBit corpus was analyzed in two perspectives: the downsampled dataset and the full dataset. Raw results and model rankings were largely comparable between the two dataset sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Downsampled WeeBit Experiments</head><p>As with the Newsela corpus, the downsampled WeeBit corpus demonstrates no gains from being analyzed with linguistic features. The best performing model, a transformer, did not utilize linguistic features. The results for some of the best performing models are shown in table 2.</p><p>Differing with the Newsela corpus, the word type models performed near the top results on the WeeBit corpus comparably to the transformer models. Word type models have no access to word order, thus semantic and topic analysis form their core analysis. Therefore, this result supports the hypothesis of <ref type="bibr" target="#b14">Martinc et al. (2019)</ref> that the pretrained transformer is especially attentive to semantic content. This result also indicates that the word type features can provide a significant portion of the information needed for successful readability assessment.</p><p>The differing best performing model types between the two corpora are likely due to differing compositions. Unlike the Newsela corpus, the WeeBit corpus shows strong correlation between topic and difficulty. Extracting this topic and semantic content is thought to be a particular strength of the transformer <ref type="bibr" target="#b14">(Martinc et al., 2019)</ref> leading to its improved results on this corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Full WeeBit Experiments</head><p>All of the models were also tested on the full imbalanced WeeBit corpus, the top performing results of which are shown in  tribution of this imbalanced dataset. Additionally, the ranking of models between the downsampled and standard WeeBit corpora showed little change.</p><p>Although the SVM with transformer and linguistic features performed better than the transformer alone, this difference is extremely small (&lt; 0.005) and thus not likely to be statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of Training Set Size</head><p>One hypothesis explaining the lack of effect of linguistic features is that models learn to extract   those features given enough data. Thus, perhaps in more data-poor environments the linguistic features would prove more useful. To test this hypothesis, we evaluated two CNN-based models, one with linguistic features and one without, with various sized training subsets of the downsampled WeeBit corpus. The macro F1 at these various dataset sizes is shown in figure 1. Across the trials at different training set sizes, the test set is held constant thereby isolating the impact of training set size.</p><p>The hypothesis holds true for extremely small subsets of training data, those with fewer than 200 documents. Above this training set size, the addition of linguistic features results in insubstantial changes in performance. Thus, either the patterns exposed by the linguistic features are learnable with very little data or the patterns extracted by deep learning models differ significantly from the linguistic features. The latter appears more likely given that linguistic features are shown to improve performance for certain corpora (Newsela) and model types (transformers).</p><p>This result indicates that the use of linguistic features should be considered for small datasets. However, the dataset size at which those features lose utility is extremely small. Therefore, collecting additional data would often be more efficient than investing the time to incorporate linguistic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of Linguistic Features</head><p>Overall, the failure of linguistic features to improve state-of-the-art deep learning models indicates that, given the available corpora, model complexity, and model structures, they do not add information over and beyond what the state-of-the-art models have already learned. However, in certain data-poor contexts, they can improve the performance of deep learning models. Similarly, with more diverse and more accurately and consistently labeled corpora, the linguistic features could prove more useful. It may be the case that the best performing models already achieve near the maximal possible performance on this corpus. The reason the maximal performance may be below a perfect score (an F1 score of 1) is disagreement and inconsistency in dataset labeling. Presumably the dataset was assessed by multiple labelers who may not have always agreed with one another or even with themselves. Thus, if either a new set of human labelers or the original labelers are tasked with labeling readability in this corpus, they may only achieve performance similar to the best performance seen in these experiments. Performing this human experiment would be a useful analysis of corpus validity and consistency. Similarly, a more diverse corpus (differing in length, topic, writing style, etc.) may prove more difficult for the models to label alone without additional training data; in this case, the linguistic features may prove more helpful in providing inductive bias.</p><p>Additionally, the lack of improvement from adding linguistic features indicates that deep learning models may already be representing those features. Future work could probe the models for different aspects of the linguistic features, thereby investigating what properties are most relevant for readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we explored the role of linguistic features in deep learning methods for readability assessment, and asked: can incorporating linguistic features improve state-of-the-art models? We constructed linguistic features focused on syntactic properties ignored by existing features. We incorporated these features into a variety of model types, both those commonly used in readability research and more modern deep learning methods. We evaluated these models on two distinct corpora that posed different challenges for readability assess-ment. Additional evaluations were performed with various training set sizes to explore the inductive bias provided by linguistic features. While linguistic features occasionally improved model performance, particularly at small training set sizes, these models did not achieve state-of-the-art performance.</p><p>Given that linguistic features did not generally improve deep learning models, these models may be already implicitly capturing the features that are useful for readability assessment. Thus, future work should investigate to what degree the models represent linguistic features, perhaps via probing methods.</p><p>Although this work supports disusing linguistic features in readability assessment, this assertion is limited by available corpora. Specifically, ambiguity in the corpora construction methodology limits our ability to measure label consistency and validity. Therefore, the maximal possible performance may already be achieved by state-of-the-art models. Thus, future work should explore constructing and evaluating readability corpora with rigorous consistent methodology; such corpora may be assessed most effectively using linguistic features. For instance, accuracy could be improved by averaging across multiple labelers.</p><p>Overall, linguistic features do not appear to be useful for readability assessment. While often used in traditional readability assessment models, these features generally fail to improve the performance of deep learning methods. Thus, this paper provides a starting point to understanding the qualities and abilities of deep learning models in comparison to linguistic features. Through this comparison, we can analyze what types of information these models are well-suited to learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Feature Definitions</head><p>For the following definitions, if the a ratio is undefined (i.e. the denominator is zero) the result is treated as zero. <ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> define complex nominals to be: "a) nouns plus adjective, possessive, prepositional phrase, relative clause, participle or appositive, b) nominal clauses, c) gerunds and infinitives in subject positions." Here polysyllabic means more than two syllables and "long words" means a word with seven or more characters. Descriptions of the norms of age of acquisition ratings can be found in <ref type="bibr" target="#b12">Kuperman et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Name Definition</head><p>P D x (s) The parse deviation, P D x (s), of sentence s is the standard deviation of the distribution of the x most probable parse log probabilities for s. If s has less than x valid parses, the distribution is taken from all the valid parses. P DM x P DM x (s) is the difference between the largest parse log probability and the mean of the log probabilities of the x most probable parses for a sentence s. If s has less than x valid parses, the mean is taken over all the valid parses. P OSD dev P OSD dev (d) is the standard deviation of the distribution of POS counts for document d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P OS div</head><p>Let P (s) be the distribution of POS counts for sentence s in document d. Let Q be the distribution of POS counts for document d. Let |d| be the number of sentences in         </p><formula xml:id="formula_4">d. P OS div (d) = s∈d D KL (P (s) Q) |d|</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance differences across different training set sizes on the downsampled WeeBit corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table 3 .</head><label>3</label><figDesc>Most performance figures increased modestly. However, these gains may not be seen if documents do not match the dis-</figDesc><table><row><cell>Features</cell><cell>Weighted F1</cell></row><row><cell>Transformer</cell><cell>0.8387</cell></row><row><cell>SVM with transformer, Flesch</cell><cell>0.8381</cell></row><row><cell>features, and linguistic features</cell><cell></cell></row><row><cell>SVM with transformer and</cell><cell>0.8359</cell></row><row><cell>Flesch features</cell><cell></cell></row><row><cell>SVM with transformer and lin-</cell><cell>0.8344</cell></row><row><cell>guistic features</cell><cell></cell></row><row><cell>SVM with transformer</cell><cell>0.8343</cell></row><row><cell>Logistic regression classifier</cell><cell>0.8135</cell></row><row><cell>with word types, Flesch features,</cell><cell></cell></row><row><cell>and linguistic features</cell><cell></cell></row><row><cell>Logistic regression classifier</cell><cell>0.7894</cell></row><row><cell>with word types</cell><cell></cell></row><row><cell>Logistic regression classifier</cell><cell>0.7934</cell></row><row><cell>with word types, word count,</cell><cell></cell></row><row><cell>and Flesch features</cell><cell></cell></row><row><cell>SVM with CNN classifier and lin-</cell><cell>0.7923</cell></row><row><cell>guistic features</cell><cell></cell></row><row><cell>Logistic regression classifier</cell><cell>0.7908</cell></row><row><cell>with word types and word count</cell><cell></cell></row><row><cell>CNN</cell><cell>0.7859</cell></row><row><cell>HAN</cell><cell>0.7507</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Top 10 performing model results, CNN, and HAN on the downsampled WeeBit corpus</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top 5 performing model results on the WeeBit corpus</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Novel syntactic feature definitions</figDesc><table><row><cell>Feature Name</cell><cell>Definition</cell></row><row><cell>mean t-unit lenght</cell><cell>number of words / number of t-units</cell></row><row><cell cols="2">mean parse tree height per sentence mean parse tree height / number of sentences</cell></row><row><cell>subtrees per sentence</cell><cell>number of subtrees / number of sentences</cell></row><row><cell>SBARs per sentence</cell><cell>number of SBARs / number of sentences</cell></row><row><cell>NPs per sentence</cell><cell>number of NPs / number of sentences</cell></row><row><cell>VPs per sentence</cell><cell>number of VPs / number of sentences</cell></row><row><cell>PPs per sentence</cell><cell>number of PPs / number of sentences</cell></row><row><cell>mean NP size</cell><cell>number of children of NPs / number of NPs</cell></row><row><cell>mean VP size</cell><cell>number of children of VPs / number of VPs</cell></row><row><cell>mean PP size</cell><cell>number of children of PPs / number of PPs</cell></row><row><cell>WHPs per sentence</cell><cell>number of wh-phrases / number of sentences</cell></row><row><cell>RRCs per sentence</cell><cell>number of reduced relative clauses / number of sentences</cell></row><row><cell>ConjPs per sentence</cell><cell>number of conjunction phrases / number of sentences</cell></row><row><cell>clauses per sentence</cell><cell>number of clauses / number of sentences</cell></row><row><cell>t-units per sentence</cell><cell>number of t-units / number of sentences</cell></row><row><cell>clauses per t-unit</cell><cell>number of clauses / number of t-units</cell></row><row><cell>complex t-unit ratio</cell><cell>number of t-units that contain a dependent clause / number of t-units</cell></row><row><cell>dependent clauses per clause</cell><cell>number of dependent clauses / number of clauses</cell></row><row><cell>dependent clauses per t-unit</cell><cell>number of dependent clauses / number of t-units</cell></row><row><cell>coordinate clauses per clause</cell><cell>number of coordinate clauses / number of clauses</cell></row><row><cell>coordinate clauses per t-unit</cell><cell>number of coordinate clauses / number of t-units</cell></row><row><cell>complex nominals per clauses</cell><cell>number of complex nominals / number of clauses</cell></row><row><cell>complex nominals per t-unit</cell><cell>number of complex nominals / number of t-units</cell></row><row><cell>VPs per t-unit</cell><cell>number of VP / number of t-units</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Existing syntactic-parse-based feature definitions</figDesc><table><row><cell>Feature Name</cell><cell>Definition</cell></row><row><cell>nouns per word</cell><cell>number of nouns / number of words</cell></row><row><cell>proper nouns per word</cell><cell>number of proper nouns / number of words</cell></row><row><cell>pronouns per word</cell><cell>number of pronouns / number of words</cell></row><row><cell>conjuctions per word</cell><cell>number of conjuctions / number of words</cell></row><row><cell>adjectives per word</cell><cell>number of adjectives / number of words</cell></row><row><cell>verbs per word</cell><cell>number of verbs / number of words</cell></row><row><cell>adverbs per word</cell><cell>number of adverbs / number of words</cell></row><row><cell>modal verbs per word</cell><cell>number of modal verbs / number of words</cell></row><row><cell>prepositions per word</cell><cell>number of prepositions / number of words</cell></row><row><cell>interjections per word</cell><cell>number of interjections / number of words</cell></row><row><cell cols="2">personal pronouns per word number of personal pronouns / number of words</cell></row><row><cell>wh-pronouns per word</cell><cell>number of wh-pronouns / number of words</cell></row><row><cell>lexical words per word</cell><cell>number of lexical words / number of words</cell></row><row><cell>function words per word</cell><cell>number of function words / number of words</cell></row><row><cell>determiners per word</cell><cell>number of determiners / number of words</cell></row><row><cell>VBs per word</cell><cell>number of base form verbs / number of words</cell></row><row><cell>VBDs per word</cell><cell>number of past tense verbs / number of words</cell></row><row><cell>VBGs per word</cell><cell>number of gerund or present participle verbs / number of words</cell></row><row><cell>VBNs per word</cell><cell>number of past participle verbs / number of words</cell></row><row><cell>VBPs per word</cell><cell>number of non-3rd person singular present verbs / number of</cell></row><row><cell></cell><cell>words</cell></row><row><cell>VBZs per word</cell><cell>number of 3rd person singular present verbs / number of words</cell></row><row><cell>adverb variation</cell><cell>number of adverbs / number of lexical words</cell></row><row><cell>adjective variation</cell><cell>number of adjectives / number of lexical words</cell></row><row><cell>modal verb variation</cell><cell>number of adverbs and adverbs / number of lexical words</cell></row><row><cell>noun variation</cell><cell>number of nouns / number of lexical words</cell></row><row><cell>verb variation-I</cell><cell>number of verbs / number of unique verbs</cell></row><row><cell>verb variation-II</cell><cell>number of verbs / number of lexical words</cell></row><row><cell>squared verb variation-I corrected verb variation-I</cell><cell>(number of verbs) 2 / number of unique verbs number of verbs / √ 2  *  number of unique verbs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Existing POS-tag-based feature definitions</figDesc><table><row><cell>Feature Name</cell><cell>Definition</cell></row><row><cell>AoA Kuperman</cell><cell>Mean age of acquisition of words (Kuperman database)</cell></row><row><cell>AoA Kuperman lemmas</cell><cell>Mean age of acquisition of lemmas</cell></row><row><cell>AoA Bird lemmas</cell><cell>Mean age of acquisition of lemmas, Bird norm</cell></row><row><cell>AoA Bristol lemmas</cell><cell>Mean age of acquisition of lemmas, Bristol norm</cell></row><row><cell cols="2">AoA Cortese and Khanna lemmas Mean age of acquisition of lemmas, Cortese and Khanna norm</cell></row><row><cell>MRC familiarity</cell><cell>Mean word familiarity rating</cell></row><row><cell>MRC concreteness</cell><cell>Mean word concreteness rating</cell></row><row><cell>MRC Imageability</cell><cell>Mean word imageability rating</cell></row><row><cell>MRC Colorado Meaningfulness</cell><cell>mean word Colorado norms meaningfulness rating</cell></row><row><cell>MRC Pavio Meaningfulness</cell><cell>mean word Pavio norms meaningfulness rating</cell></row><row><cell>MRC AoA</cell><cell>Mean age of acquisition of words (MRC database)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Existing psycholinguistic feature definitions * syllables per word + 0.39 * words per sentence − 15.59 Flesch Fomula 206.835 − 1.015 * words per sentence − 84.6 * syllables per word Automated Readability Index 4.71 * characters per word + 0.5 * words per sentence − 21.43</figDesc><table><row><cell>Feature Name</cell><cell>Definition</cell></row><row><cell>number of sentences</cell><cell>number of sentences</cell></row><row><cell>mean sentence length</cell><cell>number of words / number of sentences</cell></row><row><cell>number of characters</cell><cell>number of characters</cell></row><row><cell>number of syllables</cell><cell>number of syllables</cell></row><row><cell cols="2">Flesch-Kincaid Formula 11.8  Coleman Liau Formula −29.5873 * sentences per word+5.8799 * characters per word−15.8007 SMOG Formula 1.0430  *  √ 30.0  *  polysyllabic words per sentence + 3.1291</cell></row><row><cell>Fog Fomula</cell><cell>(words per sentence + proportion of words that are polysylabic)  *  0.4</cell></row><row><cell cols="2">FORCAST Readability Formula 20 − 15  *  monosylabic words per word</cell></row><row><cell>LIX Readability Formula</cell><cell>words per sentence + long words perword  *  100.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Existing traditional feature definitions</figDesc><table><row><cell>Feature Name</cell><cell>Definition</cell></row><row><cell>type token ratio corrected type token ratio root type token ratio</cell><cell>number of word types / number of word tokens number of word types / √ 2  *  number of word tokens number of word types / √ number of word tokens</cell></row><row><cell>bilogorathmic type token ratio</cell><cell>log(number of word types)/log(number of word tokens)</cell></row><row><cell>uber index</cell><cell>(log(number of word types)) 2 /log( number of word tokens number of word types )</cell></row><row><cell cols="2">measure of textual lexical diversity (MTLD) see McCarthy and Jarvis, 2010</cell></row><row><cell>number of senses</cell><cell>total number of senses across all words / number of word tokens</cell></row><row><cell>hyeprnyms per word</cell><cell>number of hypernyms / number of word tokens</cell></row><row><cell>hyponyms per word</cell><cell>total number of senses hyponyms / number of word tokens</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Existing traditional feature definitions B Full Model Results</figDesc><table><row><cell>Features</cell><cell>Weighted</cell><cell cols="2">Macro F1 SD</cell><cell>SD macro</cell></row><row><cell></cell><cell>F1</cell><cell></cell><cell>weighted</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F1</cell><cell></cell></row><row><cell>Linear classifier with Flesch Score</cell><cell>0.2147</cell><cell>0.2156</cell><cell>0.0347</cell><cell>0.0253</cell></row><row><cell>Linear classifier with Flesch features</cell><cell>0.3973</cell><cell>0.3976</cell><cell>0.0154</cell><cell>0.0087</cell></row><row><cell>SVM with HAN</cell><cell>0.5531</cell><cell>0.5499</cell><cell>0.1944</cell><cell>0.1928</cell></row><row><cell>SVM with Flesch features</cell><cell>0.5908</cell><cell>0.5905</cell><cell>0.0157</cell><cell>0.0168</cell></row><row><cell>SVM with CNN ordered class regression</cell><cell>0.6703</cell><cell>0.6700</cell><cell>0.0360</cell><cell>0.0334</cell></row><row><cell>SVM with CNN age regression</cell><cell>0.6743</cell><cell>0.6742</cell><cell>0.0339</cell><cell>0.0314</cell></row><row><cell>Linear classifier with word types</cell><cell>0.7202</cell><cell>0.7189</cell><cell>0.0063</cell><cell>0.0085</cell></row><row><cell>SVM with CNN ordered classes regression,</cell><cell>0.7265</cell><cell>0.7262</cell><cell>0.0326</cell><cell>0.0297</cell></row><row><cell>and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.7382</cell><cell>0.7376</cell><cell>0.0710</cell><cell>0.0684</cell></row><row><cell>types, Flesch features, and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with CNN age regression and linguistic</cell><cell>0.7384</cell><cell>0.7376</cell><cell>0.0361</cell><cell>0.0346</cell></row><row><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HAN</cell><cell>0.7507</cell><cell>0.7501</cell><cell>0.0306</cell><cell>0.0302</cell></row><row><cell>SVM with linguistic features and Flesch fea-</cell><cell>0.7664</cell><cell>0.7667</cell><cell>0.0109</cell><cell>0.0114</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with linguistic features</cell><cell>0.7665</cell><cell>0.7666</cell><cell>0.0146</cell><cell>0.0153</cell></row><row><cell>CNN</cell><cell>0.7859</cell><cell>0.7852</cell><cell>0.0171</cell><cell>0.0166</cell></row><row><cell>SVM with HAN and linguistic features</cell><cell>0.7862</cell><cell>0.7864</cell><cell>0.0631</cell><cell>0.0633</cell></row><row><cell>SVM with CNN classifier</cell><cell>0.7882</cell><cell>0.7879</cell><cell>0.0217</cell><cell>0.0195</cell></row><row><cell>Logistic regression with word types</cell><cell>0.7894</cell><cell>0.7887</cell><cell>0.0151</cell><cell>0.0202</cell></row><row><cell>Logistic regression classification with word</cell><cell>0.7908</cell><cell>0.7899</cell><cell>0.0130</cell><cell>0.0182</cell></row><row><cell>types and word count</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with CNN classifier and linguistic fea-</cell><cell>0.7923</cell><cell>0.7919</cell><cell>0.0210</cell><cell>0.0193</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.7934</cell><cell>0.7926</cell><cell>0.0135</cell><cell>0.0187</cell></row><row><cell>types, word count, and Flesch features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression with word types, Flesch</cell><cell>0.8135</cell><cell>0.8130</cell><cell>0.0131</cell><cell>0.0169</cell></row><row><cell>features, and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with transformer</cell><cell>0.8343</cell><cell>0.8340</cell><cell>0.0131</cell><cell>0.0135</cell></row><row><cell cols="2">SVM with transformer and linguistic features 0.8344</cell><cell>0.8347</cell><cell>0.0106</cell><cell>0.0091</cell></row><row><cell>SVM with transformer and Flesch features</cell><cell>0.8359</cell><cell>0.8358</cell><cell>0.0151</cell><cell>0.0154</cell></row><row><cell>SVM with transformer, Flesch features, and</cell><cell>0.8381</cell><cell>0.8377</cell><cell>0.0128</cell><cell>0.0118</cell></row><row><cell>linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>0.8387</cell><cell>0.8388</cell><cell>0.0097</cell><cell>0.0073</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>WeeBit downsampled model results sorted by weighted F1 score</figDesc><table><row><cell>Features</cell><cell>Weighted</cell><cell cols="2">Macro F1 SD</cell><cell>SD</cell></row><row><cell></cell><cell>F1</cell><cell></cell><cell>weighted</cell><cell>Macro F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F1</cell><cell></cell></row><row><cell>Linear classifier with Flesch Score</cell><cell>0.3357</cell><cell>0.1816</cell><cell>0.0243</cell><cell>0.0079</cell></row><row><cell>SVM with HAN</cell><cell>0.3625</cell><cell>0.2134</cell><cell>0.0400</cell><cell>0.0331</cell></row><row><cell>Linear classifier with Flesch features</cell><cell>0.3939</cell><cell>0.2639</cell><cell>0.0239</cell><cell>0.0305</cell></row><row><cell>SVM with Flesch features</cell><cell>0.4776</cell><cell>0.3609</cell><cell>0.0222</cell><cell>0.0190</cell></row><row><cell>SVM with CNN age regression</cell><cell>0.7279</cell><cell>0.6431</cell><cell>0.0198</cell><cell>0.0205</cell></row><row><cell>SVM with CNN ordered class regression</cell><cell>0.7316</cell><cell>0.6482</cell><cell>0.0142</cell><cell>0.0141</cell></row><row><cell>SVM with CNN age regression and linguistic</cell><cell>0.7779</cell><cell>0.7088</cell><cell>0.0156</cell><cell>0.0194</cell></row><row><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with CNN ordered classes regression,</cell><cell>0.7797</cell><cell>0.7114</cell><cell>0.0130</cell><cell>0.0120</cell></row><row><cell>and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear classifier with word types</cell><cell>0.7821</cell><cell>0.7109</cell><cell>0.0162</cell><cell>0.0127</cell></row><row><cell>SVM with Linguistic features and Flesch fea-</cell><cell>0.7952</cell><cell>0.7367</cell><cell>0.0121</cell><cell>0.0157</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with Linguistic features</cell><cell>0.7952</cell><cell>0.7366</cell><cell>0.0130</cell><cell>0.0164</cell></row><row><cell>HAN</cell><cell>0.8065</cell><cell>0.7435</cell><cell>0.0123</cell><cell>0.0220</cell></row><row><cell>Logistic regression classification with word</cell><cell>0.8088</cell><cell>0.7497</cell><cell>0.0127</cell><cell>0.0152</cell></row><row><cell>types</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.8088</cell><cell>0.7497</cell><cell>0.0121</cell><cell>0.0148</cell></row><row><cell>types and word count</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.8098</cell><cell>0.7505</cell><cell>0.0130</cell><cell>0.0163</cell></row><row><cell>types, word count, and Flesch features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.8206</cell><cell>0.7664</cell><cell>0.0428</cell><cell>0.0500</cell></row><row><cell>types, Flesch features, and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN</cell><cell>0.8282</cell><cell>0.7748</cell><cell>0.0211</cell><cell>0.0183</cell></row><row><cell>SVM with CNN classifier and linguistic fea-</cell><cell>0.8286</cell><cell>0.7753</cell><cell>0.0222</cell><cell>0.0209</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression classification with word</cell><cell>0.8293</cell><cell>0.7760</cell><cell>0.0152</cell><cell>0.0172</cell></row><row><cell>types, Flesch features, and ling features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with CNN classifier</cell><cell>0.8296</cell><cell>0.7754</cell><cell>0.0163</cell><cell>0.0136</cell></row><row><cell>SVM with HAN and linguistic features</cell><cell>0.8441</cell><cell>0.7970</cell><cell>0.0643</cell><cell>0.0827</cell></row><row><cell>SVM with transformer, Flesch features, and</cell><cell>0.8721</cell><cell>0.8273</cell><cell>0.0095</cell><cell>0.0121</cell></row><row><cell>linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>0.8721</cell><cell>0.8272</cell><cell>0.0071</cell><cell>0.0102</cell></row><row><cell>SVM with transformer</cell><cell>0.8729</cell><cell>0.8288</cell><cell>0.0064</cell><cell>0.0090</cell></row><row><cell>SVM with transformer and Flesch features</cell><cell>0.8746</cell><cell>0.8305</cell><cell>0.0054</cell><cell>0.0107</cell></row><row><cell cols="2">SVM with transformer and linguistic features 0.8769</cell><cell>0.8343</cell><cell>0.0077</cell><cell>0.0129</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>WeeBit model results sorted by weighted F1 score</figDesc><table><row><cell>Features</cell><cell>Weighted</cell><cell cols="2">Macro F1 SD</cell><cell>SD</cell></row><row><cell></cell><cell>F1</cell><cell></cell><cell>weighted</cell><cell>Macro F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F1</cell><cell></cell></row><row><cell>Linear classifier with Flesch Score</cell><cell>0.1668</cell><cell>0.0915</cell><cell>0.0055</cell><cell>0.0043</cell></row><row><cell>SVM with Flesch score</cell><cell>0.2653</cell><cell>0.1860</cell><cell>0.0053</cell><cell>0.0086</cell></row><row><cell>Logistic regression with word types</cell><cell>0.2964</cell><cell>0.2030</cell><cell>0.0144</cell><cell>0.0103</cell></row><row><cell>Logistic regression with word types and word</cell><cell>0.2969</cell><cell>0.2039</cell><cell>0.0145</cell><cell>0.0095</cell></row><row><cell>count</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Logistic regression with word types, word</cell><cell>0.3006</cell><cell>0.2097</cell><cell>0.0139</cell><cell>0.0088</cell></row><row><cell>count, and Flesch features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear classifier with Flesch features</cell><cell>0.3080</cell><cell>0.2060</cell><cell>0.0110</cell><cell>0.0077</cell></row><row><cell>Logistic regression with word types, Flesch</cell><cell>0.3333</cell><cell>0.2489</cell><cell>0.0118</cell><cell>0.0162</cell></row><row><cell>features, and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linear classifier with word types</cell><cell>0.3368</cell><cell>0.2485</cell><cell>0.0089</cell><cell>0.0153</cell></row><row><cell>CNN</cell><cell>0.3379</cell><cell>0.2574</cell><cell>0.0038</cell><cell>0.0111</cell></row><row><cell>SVM with CNN classifier</cell><cell>0.3407</cell><cell>0.2616</cell><cell>0.0079</cell><cell>0.0142</cell></row><row><cell>SVM with CNN ordered class regression</cell><cell>0.5207</cell><cell>0.4454</cell><cell>0.0092</cell><cell>0.0193</cell></row><row><cell>SVM with CNN age regression</cell><cell>0.5223</cell><cell>0.4469</cell><cell>0.0149</cell><cell>0.0244</cell></row><row><cell>SVM with transformer</cell><cell>0.5430</cell><cell>0.4711</cell><cell>0.0095</cell><cell>0.0258</cell></row><row><cell>Transformer</cell><cell>0.5435</cell><cell>0.4713</cell><cell>0.0106</cell><cell>0.0264</cell></row><row><cell>Linear classifier with linguistic features</cell><cell>0.5573</cell><cell>0.4748</cell><cell>0.0053</cell><cell>0.0140</cell></row><row><cell>SVM with CNN classifier, and linguistic fea-</cell><cell>0.7058</cell><cell>0.5510</cell><cell>0.0079</cell><cell>0.0357</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with Flesch features</cell><cell>0.7177</cell><cell>0.6257</cell><cell>0.0079</cell><cell>0.0292</cell></row><row><cell>SVM with transformer and Flesch features</cell><cell>0.7186</cell><cell>0.6305</cell><cell>0.0074</cell><cell>0.0282</cell></row><row><cell>SVM with CNN ordered classes regression</cell><cell>0.7231</cell><cell>0.6053</cell><cell>0.0062</cell><cell>0.0331</cell></row><row><cell>and linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with CNN age regression and linguistic</cell><cell>0.7281</cell><cell>0.6104</cell><cell>0.0057</cell><cell>0.0337</cell></row><row><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with linguistic features</cell><cell>0.7582</cell><cell>0.6432</cell><cell>0.0089</cell><cell>0.0379</cell></row><row><cell>SVM with transformer, Flesch features, and</cell><cell>0.7627</cell><cell>0.6263</cell><cell>0.0075</cell><cell>0.0301</cell></row><row><cell>linguistic features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SVM with transformer and linguistic features 0.7678</cell><cell>0.6656</cell><cell>0.0230</cell><cell>0.0385</cell></row><row><cell>SVM with linguistic features and Flesch Fea-</cell><cell>0.7694</cell><cell>0.6446</cell><cell>0.0060</cell><cell>0.0406</cell></row><row><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVM with HAN</cell><cell>0.7931</cell><cell>0.6724</cell><cell>0.0448</cell><cell>0.0449</cell></row><row><cell>SVM with HAN and linguistic features</cell><cell>0.8014</cell><cell>0.6751</cell><cell>0.0263</cell><cell>0.0379</cell></row><row><cell>HAN</cell><cell>0.8024</cell><cell>0.6775</cell><cell>0.1116</cell><cell>0.1825</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Newsela model results sorted by weighted F1 score</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/TovlyDeutsch/ Linguistic-Features-for-Readability</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This code can be found at https://bitbucket. org/nishkalavallabhi/complexity-features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Defined by<ref type="bibr" target="#b18">Vajjala and Meurers (2012)</ref> to be "one main clause plus any subordinate clause or non-clausal structure that is attached to or embedded in it".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="TensorFlow:large-scalema-chinelearningonheterogeneoussystems" />
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The CELEX lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><forename type="middle">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gulikers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">François Chollet and others</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Keras</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new readability yardstick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Flesch</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0057532</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical tightness and text complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beata</forename><forename type="middle">Beigman</forename><surname>Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Sheehan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility</title>
		<meeting>the Workshop on Natural Language Processing for Improving Textual Accessibility<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogers</forename><surname>Fishburne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chissom</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.21236/ADA006655</idno>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Defense Technical Information Center</publisher>
			<pubPlace>Fort Belvoir, VA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075096.1075150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -ACL &apos;03</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics -ACL &apos;03<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On Information and Sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177729694</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Age-of-acquisition ratings for 30,000 English words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Stadthagen-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0210-4</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="978" to="990" />
		</imprint>
	</monogr>
	<note type="report_type">Behavior Research Methods</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic analysis of syntactic complexity in second language writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1075/ijcl.15.4.02lu</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Corpus Linguistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="474" to="496" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised neural approaches to text readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Martinc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senja</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
		<idno type="arXiv">arXiv:1503.06733</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Version 2</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Nguyen</surname></persName>
		</author>
		<idno>Original-date: 2019- 01-31T18:56:40Z</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duchesnay</forename><surname>Andédouard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reading Level Assessment Using Support Vector Machines and Statistical Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219905</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA; Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On Improving the Accuracy of Readability Classification using Insights from Second Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Analyzing Text Complexity and Text Simplification: Connecting Linguistics, Processing and Educational Applications. Dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sowmya Vajjala Balakrishna</surname></persName>
		</author>
		<idno type="DOI">10.15496/publikation-5781</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Universität Tübingen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 30</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MRC psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior Research Methods, Instruments, &amp; Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03202594</idno>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s transformers: state-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text Readability Assessment for Second Language Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-0502</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 11th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06724</idno>
		<title level="m">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
