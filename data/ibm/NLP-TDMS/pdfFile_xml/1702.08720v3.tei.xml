<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Discrete Representations via Information Maximizing Self-Augmented Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
						</author>
						<title level="a" type="main">Learning Discrete Representations via Information Maximizing Self-Augmented Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the informationtheoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of unsupervised discrete representation learning is to obtain a function that maps similar (resp. dissimilar) data into similar (resp. dissimilar) discrete representations, where the similarity of data is defined according to applications of interest. It is a central machine learning task because of the compactness of the representations and ease  <ref type="figure">Figure 1</ref>. Basic idea of our proposed method for unsupervised discrete representation learning. We encourage the prediction of a neural network to remain unchanged under data augmentation (Red arrows), while maximizing the information-theoretic dependency between data and their representations (Blue arrow). of interpretation. The task includes two important machine learning tasks as special cases: clustering and unsupervised hash learning. Clustering is widely applied to data-driven application domains <ref type="bibr">(Berkhin, 2006)</ref>, while hash learning is popular for an approximate nearest neighbor search for large scale information retrieval <ref type="bibr" target="#b46">(Wang et al., 2016)</ref>.</p><p>Deep neural networks are promising to be used thanks to their scalability and flexibility of representing complicated, non-linear decision boundaries. However, their model complexity is huge, and therefore, regularization of the networks is crucial to learn meaningful representations of data. Particularly, in unsupervised representation learning, target representations are not provided and hence, are unconstrained. Therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest (e.g., invariance to small perturbations or affine transformation). Na√Øve regularization to use is a weight decay <ref type="bibr" target="#b10">(Erin Liong et al., 2015)</ref>. Such regularization, however, encourages global smoothness of the function prediction; thus, may not necessarily impose the intended invariance on the predicted discrete representations.</p><p>arXiv:1702.08720v3 [stat.ML] 14 Jun 2017</p><p>Instead, in this paper, we use data augmentation to model the invariance of learned data representations. More specifically, we map data points into their discrete representations by a deep neural network and regularize it by encouraging its prediction to be invariant to data augmentation. The predicted discrete representations then exhibit the invariance specified by the augmentation. Our proposed regularization method is illustrated as red arrows in <ref type="figure">Figure 1</ref>. As depicted, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. We term such regularization Self-Augmented Training (SAT). SAT is inspired by the recent success in regularization of neural networks in semi-supervised learning <ref type="bibr" target="#b0">(Bachman et al., 2014;</ref><ref type="bibr" target="#b33">Miyato et al., 2016;</ref><ref type="bibr" target="#b40">Sajjadi et al., 2016)</ref>. SAT is flexible to impose various types of invariances on the representations predicted by neural networks. For example, it is generally preferred for data representations to be locally invariant, i.e., remain unchanged under local perturbations on data points. Using SAT, we can impose the local invariance on the representations by pushing the predictions of perturbed data points to be close to those of the original data points.</p><p>For image data, it may also be preferred for data representations to be invariant under affine distortion, e.g., rotation, scaling and parallel movement. We can similarly impose the invariance via SAT by using the affine distortion for the data augmentation.</p><p>We then combine the SAT with the Regularized Information Maximization (RIM) for clustering <ref type="bibr" target="#b12">(Gomes et al., 2010;</ref><ref type="bibr" target="#b4">Bridle et al., 1991)</ref>, and arrive at our Information Maximizing Self-Augmented Training (IMSAT), an information-theoretic method for learning discrete representations using deep neural networks. We illustrate the basic idea of IMSAT in <ref type="figure">Figure 1</ref>. Following the RIM, we maximize information theoretic dependency between inputs and their mapped outputs, while regularizing the mapping function. IMSAT differs from the original RIM in two ways. First, IMSAT deals with a more general setting of learning discrete representations; thus, is also applicable to hash learning. Second, it uses a deep neural network for the mapping function and regularizes it in an end-to-end fashion via SAT. Learning with our method can be performed by stochastic gradient descent (SGD); thus, scales well to large datasets.</p><p>In summary, our contributions are: 1) an informationtheoretic method for unsupervised discrete representation learning using deep neural networks with the end-to-end regularization, and 2) adaptations of the method to clustering and hash learning to achieve the state-of-the-art performance on several benchmark datasets.</p><p>The rest of the paper is organized as follows. Related work is summarized in Section 2, while our method, IMSAT, is presented in Section 3. Experiments are provided in Section 4 and conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various methods have been proposed for clustering and hash learning. The representative ones include K-means clustering and hashing <ref type="bibr" target="#b15">(He et al., 2013)</ref>, Gaussian mixture model clustering, iterative quantization <ref type="bibr" target="#b13">(Gong et al., 2013)</ref>, and minimal-loss hashing <ref type="bibr" target="#b37">(Norouzi &amp; Blei, 2011)</ref>. However, these methods can only model linear boundaries between different representations; thus, cannot fit to nonlinear structures of data. Kernel-based <ref type="bibr" target="#b51">(Xu et al., 2004;</ref><ref type="bibr" target="#b24">Kulis &amp; Darrell, 2009</ref>) and spectral <ref type="bibr" target="#b36">(Ng et al., 2001;</ref><ref type="bibr" target="#b47">Weiss et al., 2009</ref>) methods can model the non-linearity of data, but they are difficult to scale to large datasets.</p><p>Recently, clustering and hash learning using deep neural networks have attracted much attention. In clustering, <ref type="bibr" target="#b49">Xie et al. (2016)</ref> proposed to use deep neural networks to simultaneously learn feature representations and cluster assignments, while <ref type="bibr" target="#b8">Dilokthanakul et al. (2016)</ref> and <ref type="bibr" target="#b54">Zheng et al. (2016)</ref> proposed to model the data generation process by using deep generative models with Gaussian mixture models as prior distributions.</p><p>Regarding hashing learning, a number of studies have used deep neural networks for supervised hash learning and achieved state-of-the-art results on image and text retrievals <ref type="bibr" target="#b48">(Xia et al., 2014;</ref><ref type="bibr" target="#b25">Lai et al., 2015;</ref><ref type="bibr" target="#b53">Zhang et al., 2015;</ref><ref type="bibr" target="#b50">Xu et al., 2015;</ref><ref type="bibr" target="#b31">Li et al., 2015)</ref>. Relatively few studies have focused on unsupervised hash learning using deep neural networks. The pioneering work is semantic hashing, which uses stacked RBM models to learn compact binary representations <ref type="bibr" target="#b41">(Salakhutdinov &amp; Hinton, 2009</ref>). Erin <ref type="bibr" target="#b10">Liong et al. (2015)</ref> recently proposed to use deep neural networks for the mapping function and achieved state-of-the-art results. These unsupervised methods, however, did not explicitly intended impose the invariance on the learned representations. Consequently, the predicted representations may not be useful for applications of interest.</p><p>In supervised and semi-supervised learning scenarios, data augmentation has been widely used to regularize neural networks. <ref type="bibr" target="#b29">Leen (1995)</ref> showed that applying data augmentation to a supervised learning problem is equivalent to adding a regularization to the original cost function. <ref type="bibr" target="#b0">Bachman et al. (2014)</ref>; <ref type="bibr" target="#b33">Miyato et al. (2016)</ref>; <ref type="bibr" target="#b40">Sajjadi et al. (2016)</ref> showed that such regularization can be adapted to semisupervised learning settings to achieve state-of-the-art performance.</p><p>In unsupervised representation learning scenarios, <ref type="bibr" target="#b9">Dosovitskiy et al. (2014)</ref> proposed to use data augmentation to model the invariance of learned representations. Our IM-SAT is different from <ref type="bibr" target="#b9">Dosovitskiy et al. (2014)</ref> in two im- Relation of our work to denoising and contractive autoencoders <ref type="bibr" target="#b45">(Vincent et al., 2008;</ref><ref type="bibr" target="#b39">Rifai et al., 2011)</ref> is discussed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Let X and Y denote the domains of inputs and discrete representations, respectively. Given training samples, {x 1 , x 2 , . . . , x N }, the task of discrete representation learning is to obtain a function, f : X ‚Üí Y, that maps similar inputs into similar discrete representations. The similarity of data is defined according to applications of interest.</p><p>We organize Section 3 as follows. In Section 3.1, we review the RIM for clustering <ref type="bibr" target="#b12">(Gomes et al., 2010)</ref>. In Section 3.2, we present our proposed method, IMSAT, for discrete representation learning. In Sections 3.3 and 3.4, we adapt IMSAT to the tasks of clustering and hash learning, respectively. In Section 3.5, we discuss an approximation technique for scaling up our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Regularized Information Maximization for Clustering</head><p>The RIM <ref type="bibr" target="#b12">(Gomes et al., 2010</ref>) learns a probabilistic classifier p Œ∏ (y|x) such that mutual information <ref type="bibr" target="#b7">(Cover &amp; Thomas, 2012)</ref> between inputs and cluster assignments is maximized. At the same time, it regularizes the complexity of the classifier. Let X ‚àà X and Y ‚àà Y ‚â° {0, . . . , K ‚àí 1} denote random variables for data and cluster assignments, respectively, where K is the number of clusters. The RIM minimizes the objective:</p><formula xml:id="formula_0">R(Œ∏) ‚àí ŒªI(X; Y ),<label>(1)</label></formula><p>where R(Œ∏) is the regularization penalty, and I(X; Y ) is mutual information between X and Y , which depends on Œ∏ through the classifier, p Œ∏ (y|x). Mutual information measures the statistical dependency between X and Y , and is 0 iff they are independent. Hyper-parameter Œª ‚àà R trades off the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Information Maximizing Self-Augmented Training</head><p>Here, we present two components that make up our IMSAT. We present the Information Maximization part in Section 3.2.1 and the SAT part in Section 3.2.2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">INFORMATION MAXIMIZATION FOR LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCRETE REPRESENTATIONS</head><p>We extend the RIM and consider learning M -dimensional discrete representations of data. Let the output domain be</p><formula xml:id="formula_1">Y = Y 1 √ó ¬∑ ¬∑ ¬∑ √ó Y M , where Y m ‚â° {0, 1, . . . , V m ‚àí 1}, 1 ‚â§ m ‚â§ M . Let Y = (Y 1 , . . . , Y M ) ‚àà Y be a random variable</formula><p>for the discrete representation. Our goal is to learn a multioutput probabilistic classifier p Œ∏ (y 1 , . . . , y M |x) that maps similar inputs into similar representations. For simplicity, we model the conditional probability p Œ∏ (y 1 , . . . , y M |x) by using the deep neural network depicted in <ref type="figure">Figure 1</ref>. Under the model, {y 1 , . . . , y M } are conditionally independent given x:</p><formula xml:id="formula_2">p Œ∏ (y 1 , . . . , y M |x) = M m=1 p Œ∏ (y m |x).<label>(2)</label></formula><p>Following the RIM <ref type="bibr" target="#b12">(Gomes et al., 2010)</ref>, we maximize the mutual information between inputs and their discrete representations, while regularizing the multi-output probabilistic classifier. The resulting objective to minimize looks exactly the same as Eq. <ref type="formula" target="#formula_0">(1)</ref>, except that Y is multi-dimensional in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">REGULARIZATION OF DEEP NEURAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIA SELF-AUGMENTED TRAINING</head><p>We present an intuitive and flexible regularization objective, termed Self-Augmented Training (SAT). SAT uses data augmentation to impose the intended invariance on the data representations. Essentially, SAT penalizes representation dissimilarity between the original data points and augmented ones. Let T : X ‚Üí X denote a pre-defined data augmentation under which the data representations should be invariant. The regularization of SAT made on data point x is</p><formula xml:id="formula_3">R SAT (Œ∏; x, T (x)) = ‚àí M m=1 Vm‚àí1 ym=0 p Œ∏ (y m |x) log p Œ∏ (y m |T (x)),<label>(3)</label></formula><p>where p Œ∏ (y m |x) is the prediction of original data point x, and Œ∏ is the current parameter of the network. In Eq. <ref type="formula" target="#formula_3">(3)</ref>, the representations of the augmented data are pushed to be close to those of the original data. Since probabilistic classifier p Œ∏ (y|x) is modeled using a deep neural network, it is flexible enough to capture a wide range of invariances specified by the augmentation function T . The regularization by SAT is then the average of R SAT (Œ∏; x, T (x)) over all the training data points:</p><formula xml:id="formula_4">R SAT (Œ∏; T ) = 1 N N n=1 R SAT (Œ∏; x n , T (x n )).<label>(4)</label></formula><p>The augmentation function T can either be stochastic or deterministic. It can be designed specifically for the applications of interest. For example, for image data, affine distortion such as rotation, scaling and parallel movement can be used for the augmentation function.</p><p>Alternatively, more general augmentation functions that do not depend on specific applications can be considered. A representative example is local perturbations, in which the augmentation function is</p><formula xml:id="formula_5">T (x) = x + r,<label>(5)</label></formula><p>where r is a small perturbation that does not alter the meaning of the data point. The use of local perturbations in SAT encourages the data representations to be locally invariant. The resulting decision boundaries between different representations tend to lie in low density regions of a data distribution. Such boundaries are generally preferred and follow the low-density separation principle <ref type="bibr" target="#b14">(Grandvalet et al., 2004)</ref>.</p><p>The two representative regulariztion methods based on local perturbations are: Random Perturbation Training (RPT) <ref type="bibr" target="#b0">(Bachman et al., 2014)</ref> and Virtual Adversarial Training (VAT) <ref type="bibr" target="#b33">(Miyato et al., 2016)</ref>. In RPT, perturbation r is sampled randomly from hyper-sphere ||r|| 2 = , where is a hyper-parameter that controls the range of the local perturbation. On the other hand, in VAT, perturbation r is chosen to be an adversarial direction:</p><formula xml:id="formula_6">r = arg max r {R SAT ( Œ∏; x, x + r ); ||r || 2 ‚â§ }.<label>(6)</label></formula><p>The solution of Eq. (6) can be approximated efficiently by a pair of forward and backward passes. For further details, refer to <ref type="bibr" target="#b33">Miyato et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">IMSAT for Clustering</head><p>In clustering, we can directly apply the RIM <ref type="bibr" target="#b12">(Gomes et al., 2010)</ref> reviewed in Section 3.1. Unlike the original RIM, however, our method, IMSAT, uses deep neural networks for the classifiers and regularizes them via SAT. By representing mutual information as the difference between marginal entropy and conditional entropy <ref type="bibr" target="#b7">(Cover &amp; Thomas, 2012)</ref>, we have the objective to minimize:</p><formula xml:id="formula_7">R SAT (Œ∏; T ) ‚àí Œª [H(Y ) ‚àí H(Y |X)] ,<label>(7)</label></formula><p>where H(¬∑) and H(¬∑|¬∑) are entropy and conditional entropy, respectively. The two entropy terms can be calculated as</p><formula xml:id="formula_8">H(Y ) ‚â° h(p Œ∏ (y)) = h 1 N N i=1 p Œ∏ (y|x) ,<label>(8)</label></formula><formula xml:id="formula_9">H(Y |X) ‚â° 1 N N i=1 h(p Œ∏ (y|x i )),<label>(9)</label></formula><p>where h(p(y)) ‚â° ‚àí y p(y ) log p(y ) is the entropy function. Increasing the marginal entropy H(Y ) encourages the cluster sizes to be uniform, while decreasing the conditional entropy H(Y |X) encourages unambiguous cluster assignments <ref type="bibr" target="#b4">(Bridle et al., 1991)</ref>.</p><p>In practice, we can incorporate our prior knowledge on cluster sizes by modifying H(Y ) <ref type="bibr" target="#b12">(Gomes et al., 2010)</ref>.</p><formula xml:id="formula_10">Note that H(Y ) = log K ‚àí KL[p Œ∏ (y)|| U], where K is the number of clusters, KL[¬∑||¬∑] is the Kullback-Leibler divergence, and U is a uniform distribution. Hence, maximization of H(Y ) is equivalent to minimization of KL[p Œ∏ (y)|| U]</formula><p>, which encourages predicted cluster distribution p Œ∏ (y) to be close to U. <ref type="bibr" target="#b12">Gomes et al. (2010)</ref> replaced U in KL[p Œ∏ (y)|| U] with any specified class prior q(y) so that p Œ∏ (y) is encouraged to be close to q(y). In our preliminary experiments, we found that the resulting p Œ∏ (y) could still be far apart from pre-specified q(y). To ensure that p Œ∏ (y) is actually close to q(y), we consider the following constrained optimization problem:</p><formula xml:id="formula_11">min Œ∏ R SAT (Œ∏; T ) + ŒªH(Y |X), subject to KL[p Œ∏ (y)|| q(y)] ‚â§ Œ¥,<label>(10)</label></formula><p>where Œ¥ &gt; 0 is a tolerance hyper-parameter that is set sufficiently small so that predicted cluster distribution p Œ∏ (y) is the same as class prior q(y) up to Œ¥-tolerance. Eq. (10) can be solved by using the penalty method <ref type="bibr" target="#b3">(Bertsekas, 1999)</ref>, which turns the original constrained optimization problem into a series of unconstrained optimization problems. Refer to Appendix B for the detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">IMSAT for Hash Learning</head><p>In hash learning, each data point is mapped into a D-bitbinary code. Hence, the original RIM is not directly applicable. Instead, we apply our method for discrete representation learning presented in Section 3.2.1.</p><p>The computation of mutual information I(Y 1 , . . . , Y D ; X), however, is intractable for large D because it involves a summation over an exponential number of terms, each of which corresponds to a different configuration of hash bits. <ref type="bibr" target="#b5">Brown (2009)</ref> showed that mutual information I(Y 1 , . . . , Y D ; X) can be expanded as the sum of interaction information <ref type="bibr" target="#b32">(McGill, 1954)</ref>:</p><formula xml:id="formula_12">I(Y 1 , . . . , Y D ; X) = C‚äÜS Y I(C ‚à™ X), |C| ‚â• 1, (11) where S Y ‚â° {Y 1 , . . . , Y D }.</formula><p>Note that I denotes interaction information when its argument is a set of random variables. Interaction information is a generalization of mutual information and can take a negative value. When the argument is a set of two random variables, the interaction information reduces to mutual information between the two random variables. Following <ref type="bibr" target="#b5">Brown (2009)</ref>, we only retain terms involving pairs of output dimensions in Eq. (11), i.e., all terms where |C| ‚â§ 2. This gives us</p><formula xml:id="formula_13">D d=1 I(Y d ; X) + 1‚â§d =d ‚â§D I({Y d , Y d , X}).<label>(12)</label></formula><p>This approximation ignores the interactions among hash bits beyond the pairwise interactions. It is related to the orthogonality constraint that is widely used in the literature to remove redundancy among hash bits <ref type="bibr" target="#b46">(Wang et al., 2016)</ref>.</p><p>In fact, the orthogonality constraint encourages the covariance between a pair of hash bits to 0. Thus, it also takes into account the pairwise interactions.</p><p>It follows from the definition of interaction information and the conditional independence in Eq.</p><p>(2) that</p><formula xml:id="formula_14">I({Y d , Y d , X}) ‚â° I(Y d ; Y d |X) ‚àí I(Y d ; Y d ) = ‚àíI(Y d ; Y d ).<label>(13)</label></formula><p>In summary, our approximated objective to minimize is</p><formula xml:id="formula_15">R SAT (Œ∏; T ) ‚àí Œª Ô£´ Ô£≠ D d=1 I(X; Y d ) ‚àí 1‚â§d =d ‚â§D I(Y d ; Y d ) Ô£∂ Ô£∏ .<label>(14)</label></formula><p>The first term regularizes the neural network. The second term maximizes the mutual information between data and each hash bit, and the third term removes the redundancy among the hash bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Approximation of the Marginal Distribution</head><p>To scale up our method to large datasets, we would like the objective in Eq.</p><p>(1) to be amenable to optimization based on mini-batch SGD. For the regularization term, we use the SAT in Eq. (4), which is the sum of per sample penalties and can be readily adapted to mini-batch computation.</p><p>For the approximated mutual information in Eq. <ref type="formula" target="#formula_0">(14)</ref>, we can decompose it into three parts: (i) conditional entropy</p><formula xml:id="formula_16">H(Y d |X), (ii) marginal entropy H(Y d ), and (iii) mutual in- formation between a pair of output dimensions I(Y d ; Y d ).</formula><p>The conditional entropy only consists of a sum over per example entropies (see Eq. <ref type="formula" target="#formula_9">(9)</ref>); thus, can be easily adapted to mini-batch computation. However, the marginal entropy (see Eq. (8)) and the mutual information involve the marginal distribution over a subset of target dimensions, i.e., p Œ∏ (c) ‚â° 1 N N n=1 p Œ∏ (c|x n ), where c ‚äÜ {y 1 , . . . , y M }. Hence, the marginal distribution can only be calculated using the entire dataset and is not amenable to the mini-batch setting. Following <ref type="bibr" target="#b42">Springenberg (2015)</ref>, we approximate the marginal distributions using mini-batch data: where B is a set of data in the mini-batch. In the case of clustering, the approximated objective that we actually minimize is an upper bound of the exact objective that we try to minimize. Refer to Appendix C for the detailed discussion.</p><formula xml:id="formula_17">p Œ∏ (c) ‚âà 1 |B| x‚ààB p Œ∏ (c|x) ‚â° p Œ∏ (B) (c),<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate IMSAT for clustering and hash learning using benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation. Therefore, in all the experiments with benchmark datasets, we used commonly reported parameter values for deep neural networks and avoided dataset-specific tuning as much as possible. Specifically, inspired by <ref type="bibr" target="#b18">Hinton et al. (2012)</ref>, we set the network dimensionality to d-1200-1200-M for clustering across all the datasets, where d and M are input and output dimensionality, respectively. For hash learning, we used smaller network sizes to ensure fast computation of mapping data into hash codes. We used rectified linear units <ref type="bibr" target="#b20">(Jarrett et al., 2009;</ref><ref type="bibr" target="#b34">Nair &amp; Hinton, 2010;</ref><ref type="bibr" target="#b11">Glorot et al., 2011)</ref> for all the hidden activations and applied batch normalization <ref type="bibr" target="#b19">(Ioffe &amp; Szegedy, 2015)</ref> to each layer to accelerate training. For the output layer, we used the softmax for clustering and the sigmoids for hash learning. Regarding optimization, we used Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref> with the step size 0.002. Refer to Appendix D for further details. Our implementation based on Chainer <ref type="bibr" target="#b43">(Tokui et al., 2015)</ref> is available at https://github.com/weihua916/ imsat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">DATASETS AND COMPARED METHODS</head><p>We evaluated our method for clustering presented in Section 3.3 on eight benchmark datasets. We performed experiments with two variants of the RIM and three variants of IMSAT, each of which uses different classifiers and regularization. <ref type="table" target="#tab_1">Table 1</ref> summarizes these variants. We also compared our IMSAT with existing clustering methods including K-means, DEC , denoising Auto-Encoder (dAE)+K-means .   40000 100 441 1% STL  13000 10 2048 10% CIFAR10  60000 10 2048 10% CIFAR100  60000 100 2048 1% SVHN  99289 10 960 19% Reuters  10000 4 2000 43% 20news <ref type="bibr" target="#b27">(Lang, 1995)</ref> 18040 20 2000 5% A brief summary of dataset statistics is given in <ref type="table" target="#tab_2">Table 2</ref>. In the experiments, our goal was to discover clusters that correspond well with the ground-truth categories. For the STL, CIFAR10 and CIFAR100 datasets, raw pixels are not suited for our goal because color information is dominant. We therefore applied 50-layer pre-trained deep residual networks  to extract features and used them for clustering. Note that since the residual network was trained on ImageNet, each class of the STL dataset (which is a subset of ImageNet) was expected to be well-separated in the feature space. For Omniglot, 100 types of characters were sampled, each containing 20 data points. Each data point was augmented 20 times by the stochastic affine distortion described in Appendix F. For SVHN, each image was represented as a 960-dimensional GIST feature <ref type="bibr" target="#b38">(Oliva &amp; Torralba, 2001)</ref>. For Reuters and 20news, we removed stop words and retained the 2000 most frequent words. We then used tf-idf features. Refer to Appendix E for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">EVALUATION METRIC</head><p>Following <ref type="bibr" target="#b49">Xie et al. (2016)</ref>, we set the number of clusters to the number of ground-truth categories and evaluated clustering performance with unsupervised clustering accuracy (ACC):</p><formula xml:id="formula_18">ACC = max m N n=1 1{l n = m(c n )} N ,<label>(16)</label></formula><p>where l n and c n are the ground-truth label and cluster assignment produced using the algorithm for x n , respectively. The m ranges over all possible one-to-one mappings between clusters and labels. The best mapping can be efficiently computed using the Hungarian algorithm <ref type="bibr" target="#b23">(Kuhn, 1955)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">HYPER-PARAMETER SELECTION</head><p>In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation. Hence, we fixed hyper-parameters across all the datasets unless there was an objective way to select them. For K-means, we tried 12 different initializations and reported the results with the best objectives. For dAE+K-means and DEC , we used the recommended hyper-parameters for the network dimensionality and annealing speed.</p><p>Inspired by the automatic kernel width selection in spectral clustering <ref type="bibr" target="#b52">(Zelnik-Manor &amp; Perona, 2004)</ref>, we set the perturbation range, , on data point x in VAT and RPT as</p><formula xml:id="formula_19">(x) = Œ± ¬∑ œÉ t (x),<label>(17)</label></formula><p>where Œ± is a scalar and œÉ t (x) is the Euclidian distance to the t-th neighbor of x. In our experiments, we fixed t = 10. For Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), we fixed Œ± = 0.4, 2.5 and 0.25, respectively, which performed well across the datasets.</p><p>For the methods shown in <ref type="table" target="#tab_1">Table 1</ref>, we varied one hyper- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Omniglot <ref type="bibr">IMSAT (VAT)</ref> 24.0 (0.9) IMSAT (affine) 45.1 (2.0) IMSAT (VAT &amp; affine) 70.0 (2.0) parameter and chose the best one that performed well across the datasets. More specifically, for Linear RIM and Deep RIM, we varied the decay rate over 0.0025 ¬∑ 2 i , i = 0, 1, . . . , 7. For the three variants of IMSAT, we varied Œª in Eq. (19) for 0.025 ¬∑ 2 i , i = 0, 1, . . . , 7. We set q to be the uniform distribution and let Œ¥ = 0.01 ¬∑ h(q(y)) in Eq. <ref type="formula" target="#formula_0">(10)</ref> for the all experiments.</p><p>Consequently, we chose 0.005 for decay rates in both Linear RIM and Deep RIM. Also, we set Œª = 1.6, 0.05 and 0.1 for Linear IMSAT (VAT), IMSAT (RPT) and IM-SAT (VAT), respectively. We hereforth fixed these hyperparameters throughout the experiments for both clustering and hash learning. In Appendix G, we report all the experimental results and the criteria to choose the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">EXPERIMENTAL RESULTS</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare clustering performance across eight benchmark datasets. We see that IMSAT (VAT) performed well across the datasets. The fact that our IMSAT outperformed Linear RIM, Deep RIM and Linear IMSAT (VAT) for most datasets suggests the effectiveness of using deep neural networks with an end-to-end regularization via SAT. Linear IMSAT (VAT) did not perform well even with the end-to-end regularization probably because the linear classifier was not flexible enough to model the intended invariance of the representations. We also see from <ref type="table" target="#tab_3">Table 3</ref> that IMSAT (VAT) consistently outperformed IMSAT (RPT) in our experiments. This suggests that VAT is an effective regularization method in unsupervised learning scenarios.</p><p>We further conducted experiments on the Omniglot dataset to demonstrate that clustering performance can be improved by incorporating domain-specific knowledge in the augmentation function of SAT. Specifically, we used the affine distortion in addition to VAT for the augmented function of SAT. We compared the clustering accuracy of IM-SAT with three different augmentation functions: VAT, affine distortion, and the combination of VAT &amp; affine distortion, in which we simply set the regularization to be</p><formula xml:id="formula_20">1 2 ¬∑ R SAT (Œ∏; T VAT ) + 1 2 ¬∑ R SAT (Œ∏; T affine ),<label>(18)</label></formula><p>where T VAT and T affine are augmentation functions of VAT and affine distortion, respectively. For T affine , we used the stochastic affine distortion function defined in Appendix F.</p><p>We report the clustering accuracy of Omniglot in <ref type="table" target="#tab_4">Table 4</ref>. We see that including affine distortion in data augmentation significantly improved clustering accuracy. <ref type="figure" target="#fig_2">Figure 2</ref> shows ten randomly selected clusters of the Omniglot dataset that were found using IMSAT (VAT) and IMSAT (VAT &amp; affine distortion). We observe that IMSAT (VAT &amp; affine distortion) was able to discover cluster assignments that are invariant to affine distortion as we intended. These results suggest that our method successfully captured the invariance in the hand-written character recognition in an unsupervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hash Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">DATASETS AND COMPARED METHODS</head><p>We evaluated our method for hash learning presented in Section 3.4 on two benchmark datasets: MNIST and CI-FAR10 datasets. Each data sample of CIFAR10 is represented as a 512-dimensional GIST feature <ref type="bibr" target="#b38">(Oliva &amp; Torralba, 2001)</ref>. Our method was compared against several unsupervised hash learning methods: spectral hashing <ref type="bibr" target="#b47">(Weiss et al., 2009</ref>), PCA-ITQ <ref type="bibr" target="#b13">(Gong et al., 2013)</ref>, and Deep Hash (Erin <ref type="bibr" target="#b10">Liong et al., 2015)</ref>. We also compared our method to the hash versions of Linear RIM and Deep RIM. For our IMSAT, we used VAT for the regularization. We used the same hyper-parameters as in Section 4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">EVALUATION METRIC</head><p>Following Erin <ref type="bibr" target="#b10">Liong et al. (2015)</ref>, we used three evaluation metrics to measure the performance of the different methods: 1) mean average precision (mAP); 2) precision at N = 500 samples; and 3) Hamming look-up result where the hamming radius is set as r = 2. We used the class labels to define the neighbors. We repeated the experiments ten times and took the average as the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">EXPERIMENTAL RESULTS</head><p>The MNIST and CIFAR10 datasets both have 10 classes, and contain 70000 and 60000 data points, respectively. Following Erin <ref type="bibr" target="#b10">Liong et al. (2015)</ref>, we randomly sampled 1000 samples, 100 per class, as the query data and used the remaining data as the gallery set.</p><p>We tested performance for 16 and 32-bit hash codes. In practice, fast computation of hash codes is crucial for fast information retrieval. Hence, small networks are preferable. We therefore tested our method on three different network sizes: the same ones as     <ref type="bibr" target="#b10">Liong et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Hamming ranking (mAP) precision @ sample = 500 precision @ r = 2 (Dimensions of hidden layers) MNIST CIFAR10 MNIST CIFAR10 MNIST CIFAR10 Spectral hash <ref type="bibr" target="#b47">(Weiss et al., 2009)</ref> 26.6 12.6 56.3 18.8 57.5 18.5 PCA-ITQ <ref type="bibr" target="#b13">(Gong et al., 2013)</ref> 41 Appendix H, but the results showed a similar tendency as that of 16-bit hash codes. We see from <ref type="table" target="#tab_5">Table 5</ref> that IMSAT with the largest network sizes (400-400) achieved competitive performance in both datasets. The performance of IM-SAT improved significantly when slightly bigger networks  were used, while the performance of Deep RIM did not improve much with the larger networks. We deduce that this is because we can better model the local invariance by using more flexible networks. Deep RIM, on the other hand, did not significantly benefit from the larger networks, because the additional flexibility of the networks was not used by the global function regularization via weight-decay. 1 In Appendix I, our deduction is supported using a toy dataset.</p><p>1 Hence, we deduce that Deep Hash, which is only regularized by weight-decay, would not benefit much by using larger networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion &amp; Future Work</head><p>In this paper, we presented IMSAT, an informationtheoretic method for unsupervised discrete representation learning using deep neural networks. Through extensive experiments, we showed that intended discrete representations can be obtained by directly imposing the invariance to data augmentation on the prediction of neural networks in an end-to-end fashion. For future work, it is interesting to apply our method to structured data, i.e., graph or sequential data, by considering appropriate data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relation to Denoising and Contractive Auto-encoders</head><p>Our method is related to denoising auto-encoders <ref type="bibr" target="#b45">(Vincent et al., 2008)</ref>. Auto-encoders maximize a lower bound of mutual information <ref type="bibr" target="#b7">(Cover &amp; Thomas, 2012)</ref> between inputs and their hidden representations <ref type="bibr" target="#b45">(Vincent et al., 2008)</ref>, while the denoising mechanism regularizes the auto-encoders to be locally invariant. However, such a regularization does not necessarily impose the invariance on the hidden representations because the decoder network also has the flexibility to model the invariance to data perturbations. SAT is more direct in imposing the intended invariance on hidden representations predicted by the encoder network.</p><p>Contractive auto-encoders <ref type="bibr" target="#b39">(Rifai et al., 2011)</ref> directly impose the local invariance on the encoder network by minimizing the Frobenius norm of the Jacobian with respect to the weight matrices. However, it is empirically shown that such regularization attained lower generalization performance in supervised and semi-supervised settings than VAT, which regularizes neural networks in an end-to-end fashion <ref type="bibr" target="#b33">(Miyato et al., 2016)</ref>. Hence, we adopted the end-to-end regularization in our unsupervised learning. In addition, our regularization, SAT, has the flexibility of modeling other types invariance such as invariance to affine distortion, which cannot be modeled with the contractive regularization. Finally, compared with the auto-encoders approaches, our method does not require learning the decoder network; thus, is computationally more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Penalty Method and its Implementation</head><p>Our goal is to optimize the constrained objective of Eq. <ref type="formula" target="#formula_0">(10)</ref>:</p><formula xml:id="formula_21">min Œ∏ R SAT (Œ∏; T ) + ŒªH(Y |X), subject to KL[p Œ∏ (y)|| q(y)] ‚â§ Œ¥.</formula><p>We use the penalty method <ref type="bibr" target="#b3">(Bertsekas, 1999)</ref> to solve the optimization. We introduce a scalar parameter ¬µ and consider minimizing the following unconstrained objective:</p><formula xml:id="formula_22">R SAT (Œ∏; T ) + ŒªH(Y |X) + ¬µ max{KL[p Œ∏ (y)|| q(y)] ‚àí Œ¥, 0}.<label>(19)</label></formula><p>We gradually increase ¬µ and solve the optimization of Eq. <ref type="formula" target="#formula_0">(19)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. On the Mini-batch Approximation of theMmarginal Distribution</head><p>The mini-batch approximation can be validated for the clustering scenario in Eq. (10) as follows. By the convexity of the KL divergence <ref type="bibr" target="#b7">(Cover &amp; Thomas, 2012)</ref> and Jensen's inequality, we have</p><formula xml:id="formula_23">E B [KL[ p Œ∏ (B) (y)||q(y)]] ‚â• KL[p Œ∏ (y)||q(y)] ‚â• 0,<label>(20)</label></formula><p>where the first expectation is taken with respect to the randomness of the mini-batch selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Detail</head><p>We set the size of mini-batch to 250, and ran 50 epochs for each dataset. We initialized weights following <ref type="bibr" target="#b16">He et al. (2015)</ref>: each element of the weight is initialized by the value drawn independently from Gaussian distribution whose mean is 0, and standard deviation is scale √ó 2/f an in , where f an in is the number of input units. We set the scale to be 0.1-0.1-0.0001 for weight matrices from the input to the output. The bias terms were all initialized with 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Hyper-parameter Selection</head><p>In <ref type="figure" target="#fig_5">Figure 4</ref> we report the experimental results for different hyper-parameter settings. We used Eq. (21) as a criterion to select hyper-parameter, Œ≤ * , which performed well across the datasets.</p><formula xml:id="formula_24">Œ≤ * = arg max Œ≤ dataset ACC(Œ≤, dataset) ACC(Œ≤ * dataset , dataset) ,<label>(21)</label></formula><p>where Œ≤ * dataset is the best hyper-parameter for the dataset, and ACC(Œ≤, dataset) is the clustering accuracy when hyperparameter Œ≤ is used for the dataset. According to the criterion, we set 0.005 for decay rates in both Linear RIM and Deep RIM. Also, we set Œª = 1.6, 0.05 and 0.1 for Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), respectively. <ref type="table" target="#tab_10">Table 6</ref> lists the results on hash learning when 32-bit hash codes were used. We observe that IMSAT with the largest network sizes (400-400) exhibited competitive performance in both datasets. The performance of IMSAT improved significantly when we used slightly larger networks , while the performance of Deep RIM did not improve much with the larger networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experimental Results on Hash Learning with 32-bit Hash Codes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparisons of Hash Learning with Different Regularizations and Network Sizes Using Toy Dataset</head><p>We used a toy dataset to illustrate that IMSAT can benefit from larger networks sizes by better modeling the local invariance. We also illustrate that weight-decay does not benefit much from the increased flexibility of neural networks.</p><p>For the experiments, we generated a spiral-shaped dataset, each arc containing 300 data points. For IMSAT, we used VAT regularization and set = 0.3 for all the data points. We compared IMSAT with Deep RIM, which also uses neural networks but with weight-decay regularization. We set the decay rate to 0.0005. We varied three settings for the network dimensionality of the hidden layers: 5-5, 10-10, and 20-20. <ref type="figure">Figure 5</ref> shows the experimental results. We see that IMSAT (VAT) was able to model the complicated decision boundaries by using the increased network dimensionality. On the contrary, the decision boundaries of Deep RIM did not adapt to the non-linearity of data even when the network dimensionality was increased. This observation may suggest why IMSAT (VAT) benefited from the large networks in the benchmark datasets, while Deep RIM did not.</p><p>IMSAT <ref type="formula">(</ref> : 00 : 01 : 10 : 11 <ref type="figure">Figure 5</ref>. Comparisons of hash learning with the different regularizations and network sizes using toy datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Deep Hash (Erin Liong et al., 2015), d-200-200-M , and d-400-400-M . Note that Deep Hash used d-60-30-M and d-80-50-M for learning 16 and 32-bit hash codes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Randomly sampled clusters of Omniglot discovered using (a) IMSAT (VAT) and (b) IMSAT (VAT &amp; affine). Each row contains randomly sampled data points in same cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples of the random affine distortion used in our experiments. Images in the top left side are stochastically transformed using the affine distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Relationship between hyper-parameters and clustering accuracy for 8 benchmark datasets with different methods: (a) Linear RIM, (b) Deep RIM, (c) Linear IMSAT (VAT), (d) IMSAT (RPT), and (e) IMSAT (VAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>portant aspects: 1) IMSAT directly imposes the invariance on the learned representations, while<ref type="bibr" target="#b9">Dosovitskiy et al. (2014)</ref> imposes invariance on surrogate classes, not directly on the learned representations. 2) IMSAT focuses on learning discrete representations that are directly usable for clustering and hash learning, while<ref type="bibr" target="#b9">Dosovitskiy et al. (2014)</ref> focused on learning continuous representations that are then used for other tasks such as classification and clustering.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of the variants.</figDesc><table><row><cell>Method</cell><cell>Used classifier</cell><cell>Regularization</cell></row><row><cell>Linear RIM</cell><cell>Linear</cell><cell>Weight-decay</cell></row><row><cell>Deep RIM</cell><cell>Deep neural nets</cell><cell>Weight-decay</cell></row><row><cell>Linear IMSAT (VAT)</cell><cell>Linear</cell><cell>VAT</cell></row><row><cell>IMSAT (RPT)</cell><cell>Deep neural nets</cell><cell>RPT</cell></row><row><cell>IMSAT (VAT)</cell><cell>Deep neural nets</cell><cell>VAT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Summary of dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Points #Classes Dimension %Largest class</cell></row><row><cell>MNIST (LeCun et al., 1998)</cell><cell>70000</cell><cell>10</cell><cell>784</cell><cell>11%</cell></row><row><cell>Omniglot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of clustering accuracy on eight benchmark datasets (%). Averages and standard deviations over twelve trials were reported. Results marked with ‚Ä† were excerpted from<ref type="bibr" target="#b49">Xie et al. (2016)</ref>.</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>Omniglot</cell><cell>STL</cell><cell cols="2">CIFAR10 CIFAR100</cell><cell>SVHN</cell><cell>Reuters</cell><cell>20news</cell></row><row><cell>K-means</cell><cell>53.2</cell><cell>12.0</cell><cell>85.6</cell><cell>34.4</cell><cell>21.5</cell><cell>17.9</cell><cell>54.1</cell><cell>15.5</cell></row><row><cell>dAE+K-means</cell><cell>79.8  ‚Ä†</cell><cell>14.1</cell><cell>72.2</cell><cell>44.2</cell><cell>20.8</cell><cell>17.4</cell><cell>67.2</cell><cell>22.1</cell></row><row><cell>DEC</cell><cell>84.3  ‚Ä†</cell><cell>5.7 (0.3)</cell><cell cols="2">78.1 (0.1) 46.9 (0.9)</cell><cell>14.3 (0.6)</cell><cell cols="3">11.9 (0.4) 67.3 (0.2) 30.8 (1.8)</cell></row><row><cell>Linear RIM</cell><cell cols="4">59.6 (2.3) 11.1 (0.2) 73.5 (6.5) 40.3 (2.1)</cell><cell>23.7 (0.8)</cell><cell cols="3">20.2 (1.4) 62.8 (7.8) 50.9 (3.1)</cell></row><row><cell>Deep RIM</cell><cell>58.5 (3.5)</cell><cell>5.8 (2.2)</cell><cell cols="2">92.5 (2.2) 40.3 (3.5)</cell><cell>13.4 (1.2)</cell><cell cols="3">26.8 (3.2) 62.3 (3.9) 25.1 (2.8)</cell></row><row><cell>Linear IMSAT (VAT)</cell><cell cols="4">61.1 (1.9) 12.3 (0.2) 91.7 (0.5) 40.7 (0.6)</cell><cell>23.9 (0.4)</cell><cell cols="3">18.2 (1.9) 42.9 (0.8) 43.9 (3.3)</cell></row><row><cell>IMSAT (RPT)</cell><cell cols="4">89.6 (5.4) 16.4 (3.1) 92.8 (2.5) 45.5 (2.9)</cell><cell>24.7 (0.5)</cell><cell cols="3">35.9 (4.3) 71.9 (6.5) 24.4 (4.7)</cell></row><row><cell>IMSAT (VAT)</cell><cell cols="4">98.4 (0.4) 24.0 (0.9) 94.1 (0.4) 45.6 (0.8)</cell><cell>27.5 (0.4)</cell><cell cols="3">57.3 (3.9) 71.0 (4.9) 31.1 (1.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of clustering accuracy on the Omniglot dataset using IMSAT with different types of SAT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>lists the results for 16-bit hash. Due to the space constraint, we report the results for 32-bit hash codes in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison of hash performance for 16-bit hash codes (%). Averages and standard deviations over ten trials were reported. Experimental results of Deep Hash and the previous methods were excerpted from Erin</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>for a fixed ¬µ. Let ¬µ * be the smallest value for which the solution of Eq. (19) satisfies the constraint of Eq. (10). The penalty method ensures that the solution obtained by solving Eq. (19) with ¬µ = ¬µ * is the same as that of the constrained optimization of Eq. (10).In experiments in Section 4.2, we increased ¬µ in the order of Œª, 2Œª, 4Œª, 6Œª, . . . until the solution of Eq. (19) satisfied the constraint of Eq. (10).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Therefore, in the penalty method, the constraint on the exact KL divergence, i.e., KL[p Œ∏ (y)|| q(y)] ‚â§ Œ¥ can be satisfied by minimizing its upper bound, which is the approximated KL divergence E B [KL[ p Œ∏ (B) (y)||q(y)]]. Obviously, the approximated KL divergence is amenable to the mini-batch setting; thus, can be minimized with SGD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Comparison of hash performance for 32-bit hash codes (%). Averages and standard deviations over ten trials were reported. Experimental results of Deep Hash and the previous methods are excerpted from Erin<ref type="bibr" target="#b10">Liong et al. (2015)</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">Hamming ranking (mAP) precision @ sample = 500</cell><cell cols="2">precision @ r = 2</cell></row><row><cell>(Network dimensionality)</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>MNIST</cell><cell>CIFAR10</cell><cell>MNIST</cell><cell>CIFAR10</cell></row><row><cell>Spectral hash (Weiss et al., 2009)</cell><cell>25.7</cell><cell>12.4</cell><cell>61.3</cell><cell>19.7</cell><cell>65.3</cell><cell>20.6</cell></row><row><cell>PCA-ITQ (Gong et al., 2013)</cell><cell>43.8</cell><cell>16.2</cell><cell>74.0</cell><cell>25.3</cell><cell>73.1</cell><cell>15.0</cell></row><row><cell>Deep Hash (80-50)</cell><cell>45.0</cell><cell>16.6</cell><cell>74.7</cell><cell>26.0</cell><cell>73.3</cell><cell>15.8</cell></row><row><cell>Linear RIM</cell><cell>29.7 (0.4)</cell><cell>21.2 (3.0)</cell><cell>68.9 (0.9)</cell><cell>16.7 (0.8)</cell><cell cols="2">60.9 (2.2) 15.2 (0.9)</cell></row><row><cell>Deep RIM (80-50)</cell><cell>34.8 (0.7)</cell><cell>14.2 (0.3)</cell><cell>72.7 (2.2)</cell><cell>24.0 (0.9)</cell><cell cols="2">72.6 (2.1) 23.5 (1.0)</cell></row><row><cell>Deep RIM (200-200)</cell><cell>36.5 (0.8</cell><cell>14.1 (0.2)</cell><cell>76.2 (1.7)</cell><cell>23.7 (0.7)</cell><cell cols="2">75.9 (1.6) 23.3 (0.7)</cell></row><row><cell>Deep RIM (400-400)</cell><cell>37.0 (1.2)</cell><cell>14.2 (0.4)</cell><cell>76.1 (2.2)</cell><cell>23.9 (1.3)</cell><cell cols="2">75.7 (2.3) 23.7 (1.2)</cell></row><row><cell>IMSAT (VAT) (80-50)</cell><cell>55.4 (1.4)</cell><cell>20.0 (5.5)</cell><cell>87.6 (1.3)</cell><cell>23.5 (3.4)</cell><cell cols="2">88.8 (1.3) 22.4 (3.2)</cell></row><row><cell>IMSAT (VAT) (200-200)</cell><cell>62.9 (1.1)</cell><cell>18.9 (0.7)</cell><cell>96.1 (0.6)</cell><cell>29.8 (1.6)</cell><cell cols="2">95.8 (0.4) 29.1 (1.4)</cell></row><row><cell>IMSAT (VAT) (400-400)</cell><cell>64.8 (0.8)</cell><cell>18.9 (0.5)</cell><cell>97.3 (0.4)</cell><cell>30.8 (1.2)</cell><cell cols="2">96.7 (0.6) 29.2 (1.2)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of Tokyo, Japan 2 RIKEN AIP, Japan 3 Preferred Networks, Inc., Japan 4 ATR Cognitive Mechanism Laboratories, Japan. Correspondence to: Weihua Hu &lt;hu@ms.k.utokyo.ac.jp&gt;, Takeru Miyato &lt;takeru.miyato@gmail.com&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Brian Vogel for helpful discussions and insightful reviews on the paper. This paper is based on results obtained from Hu's internship at Preferred Networks, Inc.</p><p>Learning Discrete Representations via Information Maximizing Self-Augmented Training</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of clustering data mining techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grouping multidimensional data</title>
		<imprint>
			<biblScope unit="page" from="25" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nonlinear programming. Athena scientific Belmont</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised classifiers, mutual information and &apos;phantom targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="1096" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new perspective for information theoretic feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1001</biblScope>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Am</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<title level="m">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svetlana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">K-means hashing: An affinity-preserving quantization method for learning binary compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc&amp;apos;aurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>CogSci</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L√©on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From data distributions to regularization in invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="974" to="981" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature learning based deep supervised hashing with pairwise labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu-Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang-Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multivariate information transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Takeru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin-Ichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masanori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised and semisupervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of workshop on machine learning systems (LearningSys) in the twentyninth annual conference on neural information processing systems (NIPS)</title>
		<meeting>workshop on machine learning systems (LearningSys) in the twentyninth annual conference on neural information processing systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freeman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning to hash for indexing big data-a survey. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shih-Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="34" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongkai</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanjiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for text hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guanhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Variational deep embedding: A generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huachun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bangsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Description ‚Ä¢ MNIST: A dataset of hand-written digit classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Datasets ; Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>The value of each pixel was transformed linearly into an interval. -1, 1</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">containing examples from 50 alphabets ranging from well-established international languages. We sampled 100 types of characters from four alphabets, Magi, Anglo-Saxon Futhorc, Arcadian, and Armenian. Each character contains 20 data points. Since the original data have high resolution (105-by-105 pixels), each data point was down-sampled to 21-by-21 pixels. We also augmented each data point 20 times by thestochastic affine distortion explained in Appendix F</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Omniglot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A dataset of hand-written character recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Features were extracted using 50-layer pre-trained deep residual networks (He et al., 2016) available online as a caffe model. Note that since the residual network is also trained on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Stl ; Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A dataset of 96-by-96 color images acquired from labeled examples on ImageNet</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>we expect that each class is separated well in the feature space</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Features were extracted using the 50-layer pre-trained deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Cifar10 ; Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A dataset of 32-by-32 color images with ten object classes, which are from the Tiny image dataset</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A dataset 32-by-32 color images with 100 refined object classes, which are from the Tiny image dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Cifar100 ; Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Features were extracted using the 50-layer pre-trained deep residual networks</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Training and test images were both used. Each image was represented as a 960-dimensional GIST feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Svhn ; Netzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A dataset with street view house numbers</title>
		<meeting><address><addrLine>Oliva &amp; Torralba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">we used four categories: corporate/industrial, government/social, markets, and economics as labels. The preprocessing was the same as that used by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>Reuters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A dataset with English news stories labeled with a category tree</title>
		<editor>Xie et al.</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Following DEC. except that we removed stop words. As Xie et al. (2016) did, 10000 documents were randomly sampled, and tf-idf features were used</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A dataset of newsgroup documents, partitioned nearly evenly across 20 different newsgroups 2 . As Reuters dataset, stop words were removed, and the 2000 most frequent words were retained. Documents with less than ten words were then removed, and tf-idf features were used</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‚Ä¢</forename><surname>20news</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">For the STL, CIFAR10 and CIFAR100 datasets, each image was first resized into a 224-by-224 image before its feature was extracted using the deep residual network</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Distortion for the Omniglot Dataset We applied stochastic affine distortion to data points in Omniglot. The affine distortion is similar to the one used by Koch (2015), except that we applied the affine distortion on down-sampled images in our experiments. The followings are the stochastic components of the affine distortion used in our experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Affine</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Our implementation of the affine distortion is based on scikit-image 3</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">‚Ä¢ Random scaling along x and y-axis by a factor of (s x , s y ), where s x and s y are drawn uniformly from interval</title>
		<imprint/>
	</monogr>
	<note>0.8, 1.2</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">‚Ä¢ Random translation along x and y-axis by (t x , t y ), where t x and t y are drawn uniformly from interval</title>
		<imprint/>
	</monogr>
	<note>‚àí0.4, 0.4</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">‚Ä¢ Random rotation by Œ∏, where Œ∏ is drawn uniformly from interval</title>
		<imprint/>
	</monogr>
	<note>‚àí10 ‚Ä¢ , 10 ‚Ä¢</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">‚Ä¢ Random shearing along x and y-axis by (œÅ x , œÅ y ), where œÅ x and œÅ y are drawn uniformly from interval</title>
		<imprint/>
	</monogr>
	<note>‚àí0.3, 0.3</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<ptr target="http://qwone.com/Àújason/20Newsgroups/3http://scikit-image.org/" />
		<title level="m">Figure. 3 shows examples of the random affine distortion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
