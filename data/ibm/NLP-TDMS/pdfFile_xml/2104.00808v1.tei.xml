<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Krivosheev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Italy</roleName><forename type="first">Fondazione</forename><forename type="middle">Bruno</forename><surname>Kessler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Project Page: https://roysubhankar.github.io/graph-coteaching-adaptation</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address multi-target domain adaptation (MTDA), where given one labeled source dataset and multiple unlabeled target datasets that differ in data distributions, the task is to learn a robust predictor for all the target domains. We identify two key aspects that can help to alleviate multiple domain-shifts in the MTDA: feature aggregation and curriculum learning. To this end, we propose Curriculum Graph Co-Teaching (CGCT) that uses a dual classifier head, with one of them being a graph convolutional network (GCN) which aggregates features from similar samples across the domains. To prevent the classifiers from over-fitting on its own noisy pseudo-labels we develop a co-teaching strategy with the dual classifier head that is assisted by curriculum learning to obtain more reliable pseudo-labels. Furthermore, when the domain labels are available, we propose Domain-aware Curriculum Learning (DCL), a sequential adaptation strategy that first adapts on the easier target domains, followed by the harder ones. We experimentally demonstrate the effectiveness of our proposed frameworks on several benchmarks and advance the state-of-the-art in the MTDA by large margins (e.g. +5.6% on the DomainNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning models suffer from the well known drawback of failing to generalize well when deployed in the real world. The gap in performance arises due to the difference in the distributions of the training (a.k.a source) and the test (a.k.a target) data, which is popularly referred to as domain-shift <ref type="bibr" target="#b48">[49]</ref>. Since, collecting labeled data for every new operating environment is prohibitive, a rich line of research, called Unsupervised Domain Adaptation (UDA), has evolved to tackle the task of leveraging the source data to learn a robust predictor on a desired target domain.</p><p>In the literature, UDA methods have predominantly been designed to adapt from a single source domain to a single target domain (STDA). Such methods include optimizing * Equal contribution † Corresponding author statistical moments <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39]</ref>, adversarial training <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27]</ref>, generative modelling <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, to name a few. However, given the proliferation in unlabeled data acquisition, the need to adapt to just a single target domain has lost traction in the real world scenarios. As the number of target domains grows, the number of models that need to be trained also scales linearly. For this reason, the research focus has very recently been steered to address a more practical scenario of adapting simultaneously to multiple target domains from a single source domain. This adaptation setting is formally termed as Multi-target Domain Adaptation (MTDA). The goal of the MTDA is to learn more compact representations with a single predictor that can perform well in all the target domains. Straightforward application of the STDA methods for the MTDA may be sub-optimal due to the presence of multiple domainshifts, thereby leading to negative transfer <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b8">9]</ref>. Thus, the desideratum to align multiple data distributions makes the MTDA considerably more challenging.</p><p>In this paper we build our framework for the MTDA pivoted around two key concepts: feature aggregation and curriculum learning. Firstly, we argue that given the intrinsic nature of the task, learning robust features in a unified space is a prerequisite for attaining minimum risk across multiple target domains. For this purpose we propose to represent the source and the target samples as a graph and then leverage Graph Convolutional Networks <ref type="bibr" target="#b19">[20]</ref> (GCN) to aggregate semantic information from similar samples in a neighbourhood across different domains. For the GCN to be operative, partial relationships among the samples (nodes) in the graph must at least be known apriori in the form of class labels. However, this information is absent for the target samples. To this end, we design a co-teaching framework where we train two classifiers: a MLP classifier and a GCN classifier that provide target pseudo-labels to each other. On the one hand, the MLP classifier is utilized to make the GCN learn the pairwise similarity between two nodes in the graph. While, on the other hand, the GCN classifier, due to its feature aggregation property, provides better pseudo-labels to assist the training of the MLP classifier. Given that co-teaching works on the assumption that differ-Method Domain labels Feature aggregation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curriculum learning</head><p>Coteaching AMEAN <ref type="bibr" target="#b8">[9]</ref> DADA <ref type="bibr" target="#b36">[37]</ref> MTDA-ITA <ref type="bibr" target="#b12">[13]</ref> HGAN <ref type="bibr" target="#b55">[56]</ref> CGCT (Ours) D-CGCT (Ours) <ref type="table" target="#tab_6">Table 1</ref>. Comparison with recent the state-of-the-art MTDA methods in terms of the operating regimes. ent networks capture different aspects of learning <ref type="bibr" target="#b1">[2]</ref>, it is beneficial for suppressing noisy pseudo-labels. his feature aggregation and/or co-teaching aspects are largely missing in existing MTDA methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56]</ref> (see Tab. 1).</p><p>Secondly, we make a crucial observation, very peculiar to the MTDA setting, i.e., during training as the network tries to adapt to multiple domain-shifts of varying degree, pseudo-labels obtained on-the-fly from the network for the target samples are very noisy. Self-training the network with unreliable pseudo-labeled target data further deteriorates the performance. To further combat the impact of noisy pseudo-labels, we propose to obtain pseudo-labels in an episodic fashion, and advocate the use of curriculum learning in the context of MTDA. In particular, when the domain labels of the target are latent, each episode or curriculum step consists of a fixed number of training iterations. Fairly consistent and reliable pseudo-labels are obtained from the GCN classifier at the end of each curriculum step. We call this proposed framework as Curriculum Graph Co-Teaching (CGCT) (see <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>).</p><p>Furthermore, when the domain labels of the target are available, we propose an Easy-To-Hard Domain Selection (EHDS) strategy where the feature alignment process begins with the target domain that is closest to the source and then gradually progresses towards the hardest one. This makes adaptation to multiple targets smoother. In this case, each curriculum step involves adaptation with a single new target domain. The CGCT when combined with this proposed Domain-aware Curriculum Learning (DCL) (see <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>) is referred to as D-CGCT. The Tab. 1 highlights the operating regimes of our frameworks versus the state-of-the-art MTDA methods. To summarize, the contributions of this work are threefold:</p><p>• We propose Curriculum Graph Co-Teaching (CGCT) for MTDA that exploits the co-teaching strategy with the dual classifier head, together with the curriculum learning, to learn more robust representations across multiple target domains. • To better utilize the domain labels, we propose a Domain-aware Curriculum Learning (DCL) strategy to make the feature alignment process smoother. • In the MTDA setting, we outperform the state-of-theart for several UDA benchmarks by significant margins (including +5.6% on the large scale DomainNet <ref type="bibr" target="#b35">[36]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Single-source single-target DA (STDA) refers to the task of adapting a classifier from a single labeled source dataset to a single unlabeled target dataset. In the UDA literature, a plethora of STDA methods have been proposed, which can be broadly classified into three major categories based upon the adaptation strategy. The first category uses first (Maximum Mean Discrepancy <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52]</ref>) or second order (correlation alignment <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>) statistics of the source and target features to align the marginal feature distributions. The second category of STDA methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref> adopts adversarial training strategy to align the marginal feature distributions of the two domains. Essentially, these methods use a gradient reversal layer <ref type="bibr" target="#b11">[12]</ref> to make the feature extractor network agnostic to domain specific information. The final category of STDA methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25]</ref> resort to pixel-level adaptation by generating synthetic target-like source images or source-like target images with the help of generative adversarial network (GAN) <ref type="bibr" target="#b13">[14]</ref>. However, practical applications go beyond the single-source and single-target setting and often involve multiple source <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">55]</ref> or target domains.</p><p>Multi-target DA aims to transfer knowledge from a single labeled source dataset to multiple unlabeled target datasets. While the research in STDA is quite mature, most STDA methods can not be trivially extended to a multi-target setting. So far only a handful of methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b55">56]</ref> for MTDA can be found in the literature. AMEAN <ref type="bibr" target="#b8">[9]</ref> performs clustering on the blended target domain samples to obtain sub-targets and then learns domain-invariant features from the source and the obtained sub-targets using a STDA method <ref type="bibr" target="#b45">[46]</ref>. The approaches introduced in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13]</ref> are derived from STDA and do not exploit any peculiarity of the MTDA setting. Conversely, our CGCT and D-CGCT are tailor-made for the multi-target setting as we propose to use feature aggregation of similar samples across multiple domains.</p><p>Curriculum for DA involves adopting an adaptive strategy that evolves over time to better address the adaptation across domains. Shu et. al. <ref type="bibr" target="#b46">[47]</ref> propose a strategy based on curriculum learning that exploits the loss of the network as weights to identify and eliminate unreliable source samples. An Easy-to-Hard Transfer Strategy (EHTS) is proposed in PFAN <ref type="bibr" target="#b6">[7]</ref> that progressively selects the pseudo-labeled target samples which have higher cosine similarity to the percategory source prototypes. Similarly, our CGCT is inspired by the EHTS strategy except we progressively recruit the pseudo-labeled targets <ref type="bibr" target="#b0">[1]</ref> from the robust GCN classification head to better train the MLP classifier, which in turn regularizes the GCN head (see Sec.3.2). For the multisource DA setting, CMSS <ref type="bibr" target="#b54">[55]</ref> trains a separate network to weigh the most relevant samples across several source domains for adapting to a single target domain. However, dif-ferently from CMSS, our proposed DCL utilizes the domain information to adapt over time from the easiest to the hardest target domain in the MTDA setting (see Sec. 3.3).</p><p>Graph Neural Networks (GNN) are neural network models applied on graph-structured data that can capture the relationships between the objects (nodes) in a graph via message passing through the edges <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53]</ref>. Relevant to our work are GNN-derived Graph Convolutional Networks (GCN) <ref type="bibr" target="#b19">[20]</ref> that have recently been applied for addressing DA <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">56]</ref>. For instance, Luo et. al. <ref type="bibr" target="#b29">[30]</ref> propose PGL for open-set DA to capture the relationship between the overlapping classes in the source and the target. Notably, Yang et. al. <ref type="bibr" target="#b55">[56]</ref> introduce heterogeneous Graph Attention Network (HGAN) for MTDA to learn the relationship of similar samples among multiple domains and then utilize the graph-based pseudo-labeled target samples to align their centroids with that of the source. Unlike <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56]</ref>, we incorporate the idea of co-teaching <ref type="bibr" target="#b15">[16]</ref> in a GCN framework for combating noisy pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we present our proposed Curriculum Graph Co-Teaching (CGCT) and thereafter Domain Curriculum Learning (DCL) for the task of MTDA. We also discuss some preliminaries that are used to address the task.</p><p>Problem Definition. In the MTDA scenario, we are provided with a single source dataset S = {(x s,i , y s,i )} ns i=1 , containing n s labeled samples, and N unlabeled target</p><formula xml:id="formula_0">datasets T = {T j } N j=1 , where T j = {x tj ,k } nj k=1</formula><p>with each containing n j unlabeled samples. As in any DA scenario, the fundamental assumption is that the underlying data distributions of the source and the targets are different from each other. It is also assumed that the label space of the source and targets are the same. Under these assumptions, the goal of the MTDA is to learn a single predictor for all the target domains by using the data in S ∪ {T j } N j=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Baseline for Multi-target Domain Adaptation. Domain Adversarial Network (DANN) <ref type="bibr" target="#b11">[12]</ref>, originally designed for STDA, aligns the feature distributions of the source and the target domains by using an adversarial training. DANN comprises of three networks: the feature extractor, the classifier and the domain discriminator. The classifier is responsible for classifying the features obtained from the feature extractor into n c classes. On the one hand, the domain discriminator distinguishes the source from the target features. While on the other hand, the feature extractor is trained to fool the discriminator and simultaneously learn good features for semantic classification.</p><p>Formally, let F θ : R 3xwxh → R d be the feature extractor network, parameterized by θ, that outputs a feature f = F (x) for a given sample x. The classifier network, parameterized by φ, is denoted by G φ : R d → R nc , which takes as input a feature f and outputs class logits g = G(f ). The discriminator network D ψ : R d → R 1 , parameterized by ψ, takes in the same feature f and outputs a single logit. By treating all the target domains as one combined target domain, the overall training objective of DANN for MTDA is given by: max</p><formula xml:id="formula_1">ψ min θ,φ ce − λ adv adv ,<label>(1)</label></formula><p>where ce = − E (xs,i,ys,i)∼S y s,i log G(F (x s,i )),</p><formula xml:id="formula_2">and adv = − E xs,i∼S log D(F (x s,i )) − E xt,j ∼T log [1 − D(F (x t,j ))].</formula><p>y s,i is the one-hot label for a source label y s,i . The first term, ce , in Eq. 1 is the cross-entropy loss computed on the source domain samples and minimized w.r.t. θ, φ. The second term, adv , in Eq. 1 is the adversarial loss that is maximized w.r.t ψ but minimized w.r.t θ. λ adv is the weighing factor for adv . To capture the multi-modal nature of the distributions, CDAN <ref type="bibr" target="#b26">[27]</ref> is proposed where D can be additionally conditioned on the classifier predictions g. In CDAN <ref type="bibr" target="#b26">[27]</ref>, the D takes as input h = (f , g), the joint variable of f and g, instead of just f . In this work we use CDAN for aligning the feature distributions. Graph Convolutional Network. For the GCN <ref type="bibr" target="#b19">[20]</ref> classifier we construct an undirected and fully-connected graph Γ = (V, E, A) from all samples in mini-batch. In details, given a mini-batch of images, we represent each image x i as a node v i ∈ V in the Γ. e i,j ∈ E indicates an edge between nodes v i and v j , and a i,j is the semantic similarity score for nodes (v i , v j ) forming an affinity matrix A.</p><p>Following <ref type="bibr" target="#b29">[30]</ref>, we compute the semantic similarity scoresâ <ref type="bibr">(l)</ref> i,j at the l-th layer for all pairs</p><formula xml:id="formula_3">(v i , v j ) ∈ E: a (l) i,j = f (l) edge (v (l−1) i , v (l−1) j ),<label>(2)</label></formula><p>where f (l) edge is a non-linear similarity function parameterized by ϕ, and v (l−1) i is features at l-1 GCN layer of a sample v i . The initial node features v i are instantiated with f i , the embedding obtained from F . Then, we add selfconnections for nodes in the graph and normalize the obtained similarity scores as:</p><formula xml:id="formula_4">A (l) = M − 1 2 (Â (l) + I)M − 1 2 ,<label>(3)</label></formula><p>where M is the degree matrix, I is the identity matrix, and A is the un-normalized affinity matrix. Finally, given the affinity matrix A (l) , we update the node features with the following propagation rule:</p><formula xml:id="formula_5">v (l) i = f (l) node [v (l−1) i , j∈B a (l) i,j · v (l−1) j ] ,<label>(4)</label></formula><p>where f</p><formula xml:id="formula_6">(l)</formula><p>node is a non-linear function parameterized by ϕ , B is a set of samples in the mini-batch, and [·, ·] is the feature node layer is the output layer with n c outputs. We slightly abuse the notations and drop the superscript l in our subsequent formulations for the sake of clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Curriculum Graph Co-Teaching</head><p>In this work we introduce the Curriculum Graph Co-Teaching (CGCT) that employs feature aggregation with a GCN and uses curriculum learning for pseudo-labeling. In details, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, it is composed of: a feature extractor F , a domain discriminator D, a MLP classifier G mlp and a GCN classifier G gcn . The G mlp is a fullyconnected output layer with n c output logits. The G gcn consists of an edge network f edge and a node classifier f node . The f node aggregates the features of the samples in B by considering the learnt pairwise similarity in the affinity ma-trixÂ produced by the f edge . The G gcn also outputs n c logits. Since, the G mlp and the G gcn capture different aspects of learning, they are exploited to provide feedback to each other in a co-teaching fashion. The CGCT is trained for Q curriculum steps where a curriculum step, t q cur , is an episode in which the network is trained for K training iterations. Each curriculum step t q cur is further decomposed into two stages: i) Adaptation stage and ii) Pseudo-labeling stage. Each stage in a t q cur is described below. Note that, as in <ref type="bibr" target="#b8">[9]</ref>, we assume that the domains labels of the target are latent and not observed during training.</p><p>Adaptation stage. In this stage we mainly perform the feature alignment using CDAN <ref type="bibr" target="#b26">[27]</ref>. In details, initially at step t 0 cur we start with a source setŜ 0 = {S} and a target set T . We sample mini-batches</p><formula xml:id="formula_7">B 0 = {B 0 s , B 0 t } = {B 0 s,i , B 0 t,i } B i=1 with size B such that B 0 s,i ∼Ŝ and B 0 t,i ∼ T . Each mini-batch of images is first fed to the feature ex- tractor F to obtain F 0 = {f 0 s,i , f 0 t,i } B i=1</formula><p>which are then simultaneously fed to both the G mlp and G gcn . When fed to</p><formula xml:id="formula_8">the G mlp it outputs the logitsĜ 0 = {ĝ 0 s,i ,ĝ 0 t,i } B i=1 .</formula><p>On the other hand, F 0 are input to the f edge to estimate the pairwise similarity of the samples in B 0 . Specifically, the f edge outputs an affinity matrixÂ following Eq. 2, where the entriesâ i,j inÂ denote the strength of similarity between samples i and j in B 0 . Intuitively, higher the value ofâ i,j , higher is the likelihood of samples i and j belonging to the same semantic category. Finally, following Eq. 4, the f node aggregates the features in F 0 based on the estimatedÂ such that for each node the most similar samples in the neighbourhood contribute more to its final representation. Subsequently, the f node outputs its logits asḠ</p><formula xml:id="formula_9">0 = {ḡ 0 s,i ,ḡ 0 t,i } B i=1</formula><p>. The elements inĜ 0 andḠ 0 are then passed through a softmax function to obtain the probabilities for each sample as p(ŷ = c|ĝ; c ∈ n c ) and p(ȳ = c|ḡ; c ∈ n c ), whereŷ andȳ are the predictions, respectively.</p><p>To guide the f edge to learn the pairwise similarity between the samples in B 0 we propose the concept of coteaching where the G mlp provides feedback to the f edge . Since, G mlp makes instance-level independent predictions on the samples in B 0 , it is not susceptible to the accumulation of potential noise from the dissimilar neighbours. To this end, for a B 0 we construct a "target" affinity matrix A tar and enforce the predictions of f edge to be as close as possible to theÂ tar . Each entryâ tar i,j in theÂ tar is given by:â</p><formula xml:id="formula_10">tar i,j = 1, if y i = y j = c 0, otherwise ,<label>(5)</label></formula><p>where c is the class label. While the class labels of B 0 s are provided as ground truth, we do not have access to the labels of B 0 t . Therefore, a target domain sample x t,j ∈ B 0 t is assigned a definitive pseudo-labelŷ t,j = c where c = argmax c∈nc p(ŷ t,j = c|ĝ t,j ) if the maximum likelihood max c∈nc p(ŷ t,j = c|ĝ t,j ) is greater than a threshold τ . The entriesâ tar i,j involving x t,j ∈ B 0 t not passing the τ are not optimized during training. We train the f edge using a binary cross-entropy loss as:</p><formula xml:id="formula_11">edge bce =â tar i,j log p(â i,j )+(1−â tar i,j ) log (1−p(â i,j )</formula><p>). (6) Finally, for training the G mlp and the f node in the G gcn we compute the standard cross-entropy loss with the samples in B 0 s as:</p><formula xml:id="formula_12">mlp ce = − 1 |B 0 s | |B 0 s | i=1ỹ i log p(ŷ s,i |ĝ 0 s,i ),<label>(7)</label></formula><p>node ce</p><formula xml:id="formula_13">= − 1 |B 0 s | |B 0 s | i=1ỹ i log p(ȳ s,i |ḡ 0 s,i ).<label>(8)</label></formula><p>We feed the features {ĥ 0</p><formula xml:id="formula_14">s,i ,ĥ 0 t,i } B i=1 = {(f 0 s,i ,ĝ 0 s,i ), (f 0 t,i ,ĝ 0 t,i )} B i=1</formula><p>, corresponding to B 0 , to the domain discriminator D and compute the conditional adversarial loss following Eq. 1. Thus, the final objective function for the CGCT can be written as: max</p><formula xml:id="formula_15">ψ min θ,φ,ϕ,ϕ mlp ce + λ edge edge bce + λ node node ce − λ adv adv ,<label>(9)</label></formula><p>where λ edge , λ node and λ adv are the weighing factors. Pseudo-labelling stage. Upon completion of the adaptation stage in a curriculum step t q cur we put the network in inference mode and obtain pseudo-labels ∀x t,j ∈ T . The G gcn is employed for this task because, owing to its aggregating characteristics, it learns more robust features <ref type="bibr" target="#b55">[56]</ref> than the G mlp . This is the curriculum aspect of our proposed co-teaching training strategy in CGCT where the obtained pseudo-labeled target samples are then used to train the G mlp , besides the f node .</p><p>At any step t q cur , the criterion for pseudo-label selection is formally written as:</p><formula xml:id="formula_16">∀x t,j ∈ T , w j = 1, if max c∈nc p(ȳ t,j = c|ḡ t,j ) &gt; τ 0, otherwise ,<label>(10)</label></formula><p>where w j = 1 signifies that x t,j is selected with a pseudolabelȳ t,j = c where c = argmax c∈nc p(ȳ t,j = c|ḡ t,j ), whereas w j = 0 denotes no pseudo-label is assigned. After the pseudo-labeling stage in a t q cur we obtain a pseudolabeled target set</p><formula xml:id="formula_17">D q t = {(x t,j ,ȳ t,j )}n t j=1</formula><p>wheren t is the number of recruited pseudo-labeled target samples. Post pseudo-labeling we update and prepare the source set for the succeeding step t q+1 cur as:</p><formula xml:id="formula_18">S q+1 = S ∪ D q t .<label>(11)</label></formula><p>The update rule in Eq. 11 allows us to compute the supervised losses node ce and mlp ce from Eq. 9 for x t,j ∼ D t . Note that we do not alter the domain labels in D q t and hence, the formulation for adv remains unchanged.</p><p>At the culmination of Q curriculum steps,Ŝ Q is obtained using Eq. 11 and the network is fine-tuned with only the supervised losses in Eq. 9 for K training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Domain-aware Curriculum Learning</head><p>Now we consider the case when the domain labels of the target are available, i.e. T = {T j } N j=1 , N being the number of target domains. In principle, when the domain labels are available, one can either train N domain discriminators or a (N + 1) way single domain discriminator. Apart from over-parameterization, it also suffers from limited gradients coming from the discriminator(s) due to single point estimates <ref type="bibr" target="#b20">[21]</ref>. Thus, we propose Domain-aware Curriculum Learning (DCL) as an alternate learning paradigm to better utilize the target domain labels in the MTDA setting.</p><p>To this end we design the DCL that is based on our proposed Easy-to-Hard Domain Selection (EHDS) strategy. Our proposal for the DCL stems from the observation that different target domains exhibit different domain shifts from the source domain, where some domain shifts are larger than the others. Evidently, the network will find it easier to adapt to the closest target domain while performing sub-optimally on the domain with the largest domain shift. When adaptation is performed with N domains at tandem then the large domain shifts of harder domains will interfere with the feature alignment on the easier target domains, thereby compromising the overall performance. To overcome this problem, in the EHDS strategy, as the name suggests, the network performs feature adaptation one domain at a time, starting from the easiest target domain and gradually moving towards the hardest. The "closeness" of a target domain from the source is measured by the uncertainty in the target predictions with a source-trained model. Lesser the uncertainty in predictions, closer the target from the source domain. Therefore, measuring the entropy on a target domain can serve as a good proxy for domain selection, and is defined as:</p><formula xml:id="formula_19">H(T j ) = − E x t j ,k ∼Tj |nc| c=1 p(ŷ tj ,k,c |x tj ,k )log p(ŷ tj ,k,c |x tj ,k ).<label>(12)</label></formula><p>Due to this step-by-step adaptation through domain traversal, the intermediate target domains help in reducing large domain shifts by making the farthest domain shift considerably closer than that at the start. Differently from the CGCT, in the DCL, each curriculum step, defined as t q dcl , consist in learning over one target domain, with a total of N steps. Since, the simulation of single-source and single-target adaptation inside the MTDA setup yields better domain-invariant features, at the end of each t q dcl we also consider extracting pseudo-labels for the target samples from the classifier and add them to the source set (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) for computing the supervised losses. This further reduces the domain gaps for the forthcoming harder domains. The t q dcl is split into three stages and are described below:</p><p>Domain selection stage. Given a source-trained model F θ * (G φ * ), where θ * and φ * are the trained parameters of F and G, and initial source and target setsŜ 0 = {S} and T 0 = {T j } N j=1 , the closest target domain is selected as:</p><formula xml:id="formula_20">D 0 = argmin j {H j (T j ) | ∀T j ∈T 0 },<label>(13)</label></formula><p>where D 0 is the target domain selected at step t 0 dcl and is used for performing adaptation in the subsequent stage.</p><p>Adaptation stage. This stage is similar to the one in t q cur , described in Sec. 3.2, except the feature adaptation at any step t q dcl is performed usingŜ q ∪ T D q , rather than the entire target set T . The model is trained using the losses described in Eq. 9.</p><p>Pseudo-labeling stage. The criterion for pseudo-label selection still remains the same, as described in Eq. 10, with the exception of target samples being drawn only from the current target domain D q , yielding a pseudo-labeled target set D D q t . Consequently, the source and target set update changes as following:</p><formula xml:id="formula_21">S q+1 =Ŝ q ∪ D D q t ,<label>(14)</label></formula><formula xml:id="formula_22">T q+1 =T q \ T D q .<label>(15)</label></formula><p>These three stages are repeated until all N domains have been exhausted. Then similarly, as in CGCT, the final model is fine-tuned withŜ Q . When CGCT is trained using the DCL strategy we refer to the model as D-CGCT. We would like to point that the DCL can also be realized with a single classifier model (see Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Experimental Details</head><p>Datasets.</p><p>We conduct experiments on five standard UDA benchmarks: Digits-five <ref type="bibr" target="#b53">[54]</ref>, Office-31 <ref type="bibr" target="#b42">[43]</ref>, PACS <ref type="bibr" target="#b22">[23]</ref>, Office-Home <ref type="bibr" target="#b51">[52]</ref> and the very large scale Do-mainNet <ref type="bibr" target="#b35">[36]</ref>   <ref type="table" target="#tab_7">Table 2</ref>. Dataset details for multi-target domain adaptation.</p><p>Evaluation protocol. We use the classification accuracy to evaluate the performance. The classification accuracy is computed for every possible combination of one source domain and the rest of the target domains. The performance for a given direction, i.e., source→rest, is given by averaging the accuracy on all the target domains, where source signifies the source domain and rest indicates all the unlabeled domains except the source. Importantly, in all our experiments we always report the final classification accuracy obtained with the G mlp because the G gcn always requires a mini-batch at inference, an assumption which is easily violated when deployed in the real world.</p><p>Implementation details. To be fairly comparable with the state-of-the-art methods, we adopt the backbone feature extractor networks used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b36">37]</ref> for the corresponding datasets. We train the networks by using a Stochastic Gradient Descent (SGD) optimizer having an initial learning rate of 1e-3 and decay exponentially. More details about the network architecture and experimental set-up can be found in the Supp. Mat.</p><p>Hyperparameter selection. In our final model we used only a single set of hyperparameters, which are λ edge = 1, λ node = 0.3, λ adv = 1 and τ = 0.7. Following the standard protocol in <ref type="bibr" target="#b45">[46]</ref>, we used a held-out validation set of 1000 samples for the MNIST → rest direction to tune these hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>In this section we discuss the design choices of our proposed contributions and report the results of a thorough ablation study. Our ablation analysis highlights the importance of the graph co-teaching and the curriculum learning. We run the ablation experiments on Office-Home with ResNet-18 <ref type="bibr" target="#b16">[17]</ref> as backbone network and on Digits-five with a network adopted from AMEAN <ref type="bibr" target="#b8">[9]</ref>. We adopt the CDAN as a baseline for adaptation in Tab. <ref type="bibr" target="#b2">3</ref>   <ref type="table">Table 3</ref>. Ablation study of different co-teaching strategies on Office-Home. We reported the classification accuracy averaged across all the source → rest directions.</p><p>Graph co-teaching. The goal of this particular ablation study is to analyse why our proposed graph co-teaching is beneficial and the manner in which it should be realised in an adaptation framework. To this end, as shown in the Tab. 3, we design some baselines that can be distinguished in the manner in which the G mlp and the G gcn provide pseudo-labels to the each other (columns 3 to 5) and then compare it to our D-CGCT. In more details, the baseline models can be described as: i) M1: a baseline where the G mlp provides pseudo-labels to itself, f edge and f node after each curriculum step t q dcl ; ii) M2: a baseline similar to M1, except that the G gcn provides the pseudo-labels; iii) M3: another baseline which is similar to M1 but with an exception that the G gcn also provides pseudo-labels to f edge for the current target domain in an ongoing t q dcl step.  Unsurprisingly, M1 performs the worst of all the baselines because the pseudo-labels computed by the G mlp are less accurate due to G mlp not taking into account the feature aggregation from multiple domains. Contrarily, the baseline M2 performs better than the M1 due to the fact that M2 uses G gcn for pseudo-labeling, which are more accurate. This highlights the importance of feature aggregation in the MTDA setting. One other thing that separates D-CGCT from both M1 and M2 is the co-teaching, which is absent in the latter baselines. Since, the D-CGCT enables co-teaching, with the G mlp and the G gcn providing pseudolabels to each other, it does not overfit on the same "incorrect" pseudo-label, thereby achieving more robust predictions. Contrarily, M3 uses co-teaching and yet it fails to achieve comparable performance. We speculate that, since the f edge is also trained with the pseudo-labels obtained from the G gcn for the current target domain in a t q dcl step, it becomes susceptible to noise. Thus, in summary, the graph co-teaching is the most effective when the G gcn is exploited to provide pseudo-labels only after each curriculum step.</p><p>Curriculum learning. We also study the effect of domain-aware curriculum learning in isolation from coteaching. For that purpose, as shown in the Tab. 5, we start with the baseline model CDAN by treating all the target domains as one single domain. When the domain labels of the target are available, the baseline improves by 1.33%, indicating that the domain labels can indeed improve the performance of an adaptation model. To show the benefit of the DCL without co-teaching, we train the Base † + DCL, and it yields an average accuracy that is higher than the Base. † + PL counterpart. The advantage of using DCL is further amplified when coupled with the CGCT, where the D-CGCT  <ref type="table">Table 5</ref>. Ablation results of different baselines using ResNet-18 as backbone on Office-Home. Baseline: CDAN <ref type="bibr" target="#b26">[27]</ref> model that combines all the target domains into a single target domain. " †" indicates the baseline models that use the domain labels of the target. GCN ‡: the baseline model with the GCN as the single classification head. PL: using pseudo-labels. outperforms all other baselines, including the CGCT. Due to the gradual adaptation, the D-CGCT also leads to the better cluster formation than the CGCT, as shown by the t-SNE visualization in the <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>To demonstrate that the order of target domains selection in the DCL indeed makes a difference, we consider a reverse-domain curriculum learning where the hardest domain is selected first, followed by the less hard ones. To this end, we train two models: i) Baseline † +DCL; and ii) Baseline † +Rev-DCL and compare their performances in the <ref type="figure" target="#fig_2">Fig. 3</ref>. In both the datasets we observe the same phenomenon that the reverse-curriculum being detrimental. This once again re-establishes the importance of the proposed DCL in the MTDA setting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-The-Art</head><p>We compare our proposed method and its variants with several state-of-the-art methods that are designed exclusively for the MTDA as well as the STDA methods that can be extended and used in the MTDA setting. In the main paper we only report the results for the Office-31, Office-Home and DomainNet experiments. Due to lack of space we report the numbers for Digits-five and PACS in the Supp. Mat.</p><p>In Tab. 4 we report the numbers for Office-31 and Office-Home for single-target, target-combined and multi-target setting. The single-target setting denotes training singlesource to single-target models, the target-combined means treating all the target domains as one aggregated target, while the multi-target setting comprise of training a single model for single-source to multiple-targets. As can be observed, in all the settings our proposed CGCT and D-CGCT outperform all the state-of-the-art methods. Specifically, for the Office-31, our CGCT without using domain labels is already 2.4% better than the HGAN <ref type="bibr" target="#b55">[56]</ref>, which is a MTDA method exploiting domain labels for feature aggregation with a single GCN classifier besides pseudo-labeling. This highlights the importance of having a co-teaching strategy with two classifiers and curriculum learning for counteracting the impact of noisy pseudo-labels in the GCN framework. We also observed that incorporating domain informa-tion following the proposed DCL strategy improves the performance in the Office-Home, with the D-CGCT achieving 5.5% improvement over MT-MTDA <ref type="bibr" target="#b34">[35]</ref>, a MTDA method that also utilizes domain labels. Finally, as can be seen from the Tab. 6, the D-CGCT advances the state-of-the-art results for the very challenging DomainNet dataset by a non-trivial margin of 5.6%. This further verifies the effectiveness of our proposed methods for addressing the MTDA. Overcoming negative transfer. Careful inspection of the Tab. 4 tells us that the single-target DA methods always outperform the same STDA method when applied in the multi-target setting. For e.g., CDAN is 4.3% better in the single-target than in the multi-target setting. The drop in performance for the multi-target setting clearly hints at the fact that negative transfer <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b8">9]</ref> is quite prevalent in the MTDA, despite having access to more data. Contrarily, our proposed CGCT when applied to both the settings fares equally well for the Office-Home and outperforms the single-target counterpart by 0.6% for the Office-31. This once again shows that the design choices made in our CGCT and D-CGCT lead to learning more robust domain-invariant features and provide resilience against negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To address multi-target domain adaptation (MTDA), we proposed Curriculum Graph Co-Teaching (CGCT) that uses a graph convolutional network to perform robust feature aggregation across multiple domains, which is then trained with a co-teaching and curriculum learning strategy. To better exploit domain labels of the target we presented a Domain-aware curriculum (DCL) learning strategy that adapts easier target domains first and harder later, enabling a smoother feature alignment. Through extensive experiments we demonstrate that our proposed contributions handsomely outperform the state-of-the-art in the MTDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide more implementation details, discussion of the proposed CGCT, and additional experimental results. In details, we provide the pseudo-code algorithms of the proposed CGCT and D-CGCT in Sec. A. We highlight the key differences between CGCT and prior works in Sec. B. The details of datasets and implementation are provided in Sec. C and Sec. D, respectively. The additional experimental results are reported in Sec. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithms</head><p>In this section we provide the pseudo-code algorithms for the proposed CGCT (see Sec. 3.2 of the main paper) and D-CGCT (see Sec. 3.3 of the main paper) in the Alg. 1 and Alg. 2, respectively. Note that the adaptation stage in the Alg. 2 can be replaced by any desired single-target domain adaptation (STDA) method of choice, thereby, making the proposed DCL flexible to a wide variety of STDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>Here we highlight the keys differences between the CGCT and PGL <ref type="bibr" target="#b29">[30]</ref> as well as the dual classifier-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b43">44]</ref>. The PGL <ref type="bibr" target="#b29">[30]</ref> exploits the graph learning framework in an episodic fashion to obtain pseudo-labels for the unlabeled target samples, which are then used to bootstrap the model by training on the pseudo-labeled target data. While our proposed method is similar in spirit to the episodic training in <ref type="bibr" target="#b29">[30]</ref>, we do not solely rely on the GCN to obtain the pseudo-labels. We conjecture that due to the fully-connected nature of the graph and lack of target labels, the GCN will be prone to accumulate features of dissimilar neighbours, thereby, resulting in the erroneous label propagation. To address this peculiarity, we propose to resort to the co-teaching paradigm, where the G mlp is exploited to train the f edge network. As the two classifiers will capture different aspects of training <ref type="bibr" target="#b15">[16]</ref>, it will prevent the f edge to be trained with the same erroneous pseudo-labels as the f node . We validate this conjecture empirically, where a network with a single GCN classifier with pseudo-labels performs sub-optimally compared to CGCT (see Tab. 5 row 7 of the main paper). Finally, the dual classifier-based methods maintain two classifiers to identify and filter either harder target samples <ref type="bibr" target="#b43">[44]</ref> or noisy samples <ref type="bibr" target="#b15">[16]</ref>. Contrarily, we maintain G mlp and G gcn to provide feedback to each other by exploiting the key observation that each classifier learns different patterns during training. Furthermore, given the intrinsic design of the G gcn , we also do away with an extra adhoc loss of keeping the weights of two networks different.  </p><formula xml:id="formula_23">8 for k in (1 : K) do 9B q s ← (xs,i, ys,i) B i=1 ∼Ŝ q 10B q t ← (xt,i) B i=1 ∼ T 11ŷ ← softmax(G mlp (F (x))) 12ȳ ← softmax(Ggcn(F (x))) 13d ← sigmoid(D(F (x)))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>Digits-five <ref type="bibr" target="#b53">[54]</ref> is composed of five domains that are drawn from the: i) grayscale handwritten digits MNIST <ref type="bibr" target="#b21">[22]</ref> (mt); ii) a coloured version of mt, called as MNIST-M <ref type="bibr" target="#b11">[12]</ref> (mm); iii) USPS <ref type="bibr" target="#b10">[11]</ref> (up), which is a lower resolu-  </p><formula xml:id="formula_24">,i) B i=1 ∼Ŝ q 16B q t ← (xt,i) B i=1 ∼ T D q 17ŷ ← softmax(G mlp (F (x))) 18ȳ ← softmax(Ggcn(F (x))) 19d ← sigmoid(D(F (x)))</formula><formula xml:id="formula_25">D D q t ← D D q t ||{(xt,j, argmax c∈nc p(ȳt,j = c|xt,j))} Append 29 end 30 end 31Ŝ q+1 ←Ŝ q ∪ D D q t Pseudo-source 32T q+1 =T q \ T D q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="33">end</head><p>Step 3: Fine-tuning on pseudo-source dataset <ref type="bibr" target="#b33">34</ref>  tion, 16×16, of the handwritten digits mt; iv) a real-world dataset of digits called SVHN <ref type="bibr" target="#b33">[34]</ref> (sv); and v) a synthetically generated dataset Synthetic Digits <ref type="bibr" target="#b11">[12]</ref> (sy). Following the protocol of <ref type="bibr" target="#b8">[9]</ref>, we sub-sample 25,000 and 9,000 samples from the training and test sets of mt, mm, sv and sy and use as train and test sets, respectively. For the up domain we use all the 7,348 training and 1,860 and test samples, for our experiments. All the images are re-scaled to a 28×28 resolution.</p><p>Office31 <ref type="bibr" target="#b42">[43]</ref> is a standard visual DA dataset comprised of three domains: Amazon, DSLR and Webcam. The dataset consists of 31 distinct object categories with a total of 4,652 samples.</p><p>Office-Home <ref type="bibr" target="#b51">[52]</ref> is a relatively newer DA benchmark that is larger than Office31 and is composed of four different visual domains: Art, Clipart, Product and Real. It consists of 65 object categories and has 15,500 images in total.</p><p>PACS <ref type="bibr" target="#b22">[23]</ref> is another visual DA benchmark that also consists of four domains: Photo (P), Art Painting (A), Cartoon (C) and Sketch (S). This dataset is captured from 7 object categories and has 9,991 images in total.</p><p>DomainNet <ref type="bibr" target="#b35">[36]</ref> is the most challenging and very large scale DA benchmark, which has six different domains: Clipart (C), Infograph (I), Painting (P), Quickdraw (Q), Real (R) and Sketch (S). It has around 0.6 million images, including both train and test images, and has 345 different object categories. We use the official training and testing splits, as mentioned in <ref type="bibr" target="#b36">[37]</ref>, for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>General Setting. To be fairly comparable with the state-of-the-art methods, we adopted comparable backbone feature extractors in the corresponding experiments and datasets. For Digits-five, we have used a small convolutional network as the backbone feature extractor (see Tab. 7), which is adapted from <ref type="bibr" target="#b8">[9]</ref> and includes two conv layers and two fc layers. We trained the model using a Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 1e-3. For the rest of the datasets, we have adoptd ResNet <ref type="bibr" target="#b16">[17]</ref> based feature extractors. Specifically, for the ablation studies on Office-Home, we have used ResNet-18 as the backbone network. For the state-of-theart comparisons on Office31, PACS and Office-Home we have used ResNet-50. For the DomainNet, we have utilized ResNet-101 as used by the competitor methods. Similarly to the Digits-five, SGD optimizer is used with an initial learning rate of 1e-3 and is decayed exponentially. Each curriculum step consists of K = 10, 000 training iterations for all the datasets, except the DomainNet, where K = 50, 000 due to large size of the dataset. The final finetuning step is trained with K = 15, 000 iterations for all datasets.</p><p>GCN architecture. We have implemented f node net-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Ablations</head><p>To explain why the step-by-step adaptation in the proposed DCL better addresses the alleviation of the larger domain-shifts in the MTDA setting, we plot the classification accuracy with the D-CGCT in <ref type="figure">Fig. 4</ref>. As can be observed from the <ref type="figure">Fig. 4 (a)</ref>, for Photo → rest setting in the PACS, when the adaptation first begins with the Art as target, the performance of the model on the unseen Cartoon domain simultaneously improves in the first 10k iterations (or the 1 st curriculum step), despite the network not seeing any sample from the Cartoon domain. This phenomenon is even vividly noticeable in the second curriculum step, where the performance on the unseen Sketch largely increases when the Cartoon is selected for adaptation. This in other words means that the domain-shift between the source (Photo) and the farthest target (Sketch) has already been considerably reduced by the time the Sketch enters the adaptation stage (from 20k iterations on wards). Thus, we empirically demonstrate the prime reason behind the DCL achieving superior performance over other state-of-the-art MTDA methods. Similar observations can also be noticed for the Office-Home. We depict the Product → rest setting in the <ref type="figure">Fig. 4 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Comparison with the State-of-the-Art</head><p>In this section we compare with the state-of-the-art methods for the Digits-five and PACS. Since, the recent work of MTDA, HGAN <ref type="bibr" target="#b55">[56]</ref>, does not report results with all the domains available in the PACS and the DomainNet, we additionally report the results with those selected domains in this section for a fair comparison. In the Tab. 8, 9 and 10, we club the baselines into two distinct settings: target combined and multi-target. In the former setting, the domain labels of the targets are latent, and all the target do-   <ref type="table">Table 9</ref>. Comparison with the state-of-the-art methods on the PACS. All methods use the ResNet-50 as the backbone. "Target Combined" indicates methods are performed on one source to one combined target domain. "Multi-Target" indicates methods are performed on one source to multi-target setting. Our proposed models are highlighted in bold.  <ref type="table" target="#tab_6">Table 10</ref>. Comparison with the state-of-the-art methods on the DomainNet. All methods use the ResNet-101 as the backbone. "Target Combined" indicates methods are performed on one source to one combined target domain. "Multi-Target" indicates methods are performed on one source to multi-target setting. Our proposed models are highlighted in bold.</p><p>mains are combined into a single target domain. Whereas in the latter, each target domain is treated separately. For both the settings, we just train one single model for a given source → rest, as in HGAN <ref type="bibr" target="#b55">[56]</ref>.</p><p>In the Tab. 8, we report the state-of-the-art comparison on the Digits-five. For a fair comparison, we compare with the baselines reported in <ref type="bibr" target="#b8">[9]</ref> that use a backbone network similar to the one described in the Tab. 7. In both the target combined and multi-target settings, our proposed methods outperform all other baselines. For the PACS, reported in the Tab. 9, we notice that domain labels is very vital for mitigating multiple domain-shifts. For example, CDAN in the multi-target setting performs 9.8% better than its target combined counterpart. Similar trend can also be observed between our CGCT and D-CGCT, with the D-CGCT outperforming the former by a large margin. Finally, we reevaluate our methods on the 5 domains of the DomainNet, by leaving out the Quickdraw domain as in <ref type="bibr" target="#b55">[56]</ref>. Results are reported in the Tab. 10. We produce state-of-the-art performance in the DomainNet for both the settings by non-trivial margins. This further shows that our proposed feature aggregation and training strategy are much more effective than the HGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Visualization</head><p>In this section we visualize the features learned by our models and compare them with the baseline methods. The To better visualize the decision boundaries in the latent feature space, we select 10 classes, randomly from the Office-Home, and depict the t-SNE plots of the feature embeddings in the <ref type="figure">Fig. 6</ref>. It is can be seen that our models learn features that can be easily separated by a linear classifier, much easier than the CDAN models. In particular, the CDAN when using domain labels (see <ref type="figure">Fig. 6</ref> (b)) produces more overlapping classes than our D-CGCT (see <ref type="figure">Fig. 6 (d)</ref>). Thus, when the domain labels are leveraged with our DCL strategy, the model produces features that are more discriminative, thereby leading to an improved performance in the MTDA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The pipeline of the proposed framework: a) CGCT: Curriculum Graph Co-Teaching and b) DCL: Domain-aware curriculum learning. (a) In the CGCT, the MLP Classifier provides pseudo-labels (PL) ( arrow) for the target samples to guide the Edge Network to learn the Affinity Matrix, whereas the Node Classifier of the GCN provides PL (bold → arrow) to the MLP Classifier at the end of each curriculum step, realizing the co-teaching. (b) In the DCL, the target domains are selected for adaptation, one at a time per domain curriculum step t q dcl , with the "easier" domains selected first and then the "harder" ones. After PL are obtained, the pseudo-labeled target dataset is added to the Pseudo Source dataset, which is then used in the next adaptation step. concatenation function. The final f (L)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>t-SNE plots of the feature embeddings with Product → rest in Office-Home. Left: CGCT. Right: D-CGCT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the DCL with the reverse-domain curriculum model on Office-Home and Digits-Five. In the reverse-domain curriculum model the order of selection of target domains is exactly opposite to that of the DCL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>14 update ψ by min ψ λ adv adv 15 update 19 for 3 :</head><label>1415193</label><figDesc>θ, φ by min θ,φ mlp ce − λ adv adv 16 update θ, ϕ, ϕ by min θ,ϕ,ϕ λ edge edge bce xt,j ∈ T do 20 wj ← maxc∈n c p(ȳt,j = c|xt,j) 21 if wj &gt; τ then 22 D q t ← D q t ||{(xt,j, argmax c∈nc p(ȳt,j = c|xt,j))} Fine-tuning on pseudo-source dataset 27 for k in (1 : K ) do 28 (xs,i, ys,i) B i=1 ∼Ŝ Q 29 update θ, φ by min θ,φ mlp ce 30 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>20 update 25 for 27 if wj &gt; τ then 28</head><label>20252728</label><figDesc>ψ by min ψ λ adv adv 21 update θ, φ by min θ,φ mlp ce − λ adv adv 22 update θ, ϕ, ϕ by min θ,ϕ,ϕ λ edge edge bce xt,j ∈ T D q do 26 wj ← maxc∈n c p(ȳt,j = c|xt,j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Photo → rest in the PACS (a) Product → rest in the Office-Home Figure 4. The classification accuracy line plots with the D-CGCT using ResNet-50 as the backbone. At each indicated training iteration in the x-axis, a new target domain (shown in brackets) is selected for adaptation. Layer k size , C in , Cout, st, pad IN/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>t-SNE plots of the feature embeddings for the Product → rest of the Office-Home. All the models use ResNet-50 as backbone. Each colour indicates a different domain. t-SNE plots of the feature embeddings for the Product → rest of the Office-Home depicting only 10 randomly sampled classes. All the methods use ResNet-50 as backbone. Each colour indicates a different class while each shape represents a different domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>depicts the t-SNE plots of the feature embeddings computed by feature extractor network (ResNet-50) for the direction Product → rest of the Office-Home. The plots in the Fig. 5 (c) and (d) demonstrate that the proposed CGCT and D-CGCT result in well clustered and discriminative fea-tures compared to CDAN baselines (see Fig. 5 (a) and (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(0.6 million images). The statistics of the datasets are summarized in Tab. 2. More details on the datasets can be found in the Supp. Mat.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#domains #classes</cell><cell>#images</cell></row><row><cell>Digits-five</cell><cell>5</cell><cell>10</cell><cell>∼ 145K</cell></row><row><cell>PACS</cell><cell>4</cell><cell>7</cell><cell>9,991</cell></row><row><cell>Office-31</cell><cell>3</cell><cell>31</cell><cell>4,652</cell></row><row><cell>Office-Home</cell><cell>4</cell><cell>65</cell><cell>15,500</cell></row><row><cell>DomainNet</cell><cell>6</cell><cell>345</cell><cell>∼ 0.6M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and Tab. 5.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>M2</cell><cell cols="3">Ggcn Ggcn Ggcn</cell><cell>59.6</cell></row><row><cell>M3</cell><cell>self</cell><cell>G mlp , Ggcn</cell><cell cols="2">G mlp 58.2</cell></row><row><cell>D-CGCT (Ours)</cell><cell cols="3">Ggcn G mlp Ggcn</cell><cell>60.8</cell></row></table><note>Pseudo-labels from Model Co-teaching G mlp f edge f node Avg(%) M1 self G mlp G mlp 57.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Comparison with state-of-the-art methods on Office-31 and Office-Home. All methods use the ResNet-50 as the backbone. Single-Target indicates methods are performed on one source to one target setting. Target-Combined indicates methods are performed on one source to aggregated targets setting, while the Multi-Target indicates methods are performed on one source to multi-target setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>+GCN ‡ 50.19 49.09 46.52 60.76 51.64 Base. † +GCN ‡ + PL 54.52 57.60 53.20 65.49 57.70</figDesc><table><row><cell></cell><cell>Office-Home</cell></row><row><cell>Model</cell><cell>Art Clipart Product Real Avg(%)</cell></row><row><cell>Source train</cell><cell>51.45 43.93 42.41 54.50 48.07</cell></row><row><cell>Baseline</cell><cell>50.70 50.78 47.95 57.63 51.77</cell></row><row><cell>Base.  CGCT</cell><cell>60.81 60.00 54.13 62.62 59.39</cell></row><row><cell>D-CGCT</cell><cell>61.42 60.73 57.27 63.8 60.81</cell></row></table><note>† 52.08 53.21 48.62 58.49 53.10 Base.† +PL 54.61 56.13 50.25 61.04 55.51 Base.† + DCL 55.94 56.66 52.85 60.18 56.41 Base.†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>DomainNet Model Cli. Inf. Pai. Qui. Rea. Ske. Avg(%) Source train 25.6 16.8 25.8 9.2 20.6 22.3 20.1 SE [10] 21.3 8.5 14.5 13.8 16.0 19.7 15.6 MCD [44] 25.1 19.1 27.0 10.4 20.2 22.5 20.7 DADA [37] 26.1 20.0 26.5 12.9 20.7 22.8 21.5 CDAN [27] 31.6 27.1 31.8 12.5 33.2 35.8 28.7 MCC [19] 33.6 30.0 32.4 13.5 28.0 35.3 28.8 CDAN + DCL 35.1 31.4 37.0 20.5 35.4 41.0 33.4 CGCT 36.1 33.3 35.0 10.0 39.6 39.7 32.3 D-CGCT 37.0 32.2 37.3 19.3 39.8 40.8 34.4Comparison with the state-of-the-art methods on DomainNet. All methods use the ResNet-101 as the backbone. The classification accuracy are reported for each source→rest direction, with each source domain being indicated in the columns. All the reported numbers are evaluated on the multi-target setting.</figDesc><table><row><cell>Source (product) Target (art) Target (clipart) Target (real)</cell><cell>Source (product) Target (art) Target (clipart) Target (real)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Procedure of Curriculum Graph Co-Teaching (CGCT) require: number of target domains N , classes nc require: source dataset S; combined target dataset T require: hyper-parameters B, τ , K, K , λ edge , λ node , λ adv require: networks F , D, G mlp , f edge , f node with parameters θ, ψ, φ, ϕ, ϕ , respectively. The f edge and f node form the Ggcn. Step 1: Pre-training on the source dataset</figDesc><table><row><cell cols="4">1 while ce has not converged do</cell></row><row><cell>2</cell><cell>(xs,i, ys,i) B i=1 ∼ S</cell><cell></cell></row><row><cell>3</cell><cell cols="2">update θ, φ by min θ,φ</cell><cell>mlp ce</cell></row><row><cell cols="2">4 end</cell><cell></cell></row><row><cell cols="4">Step 2: Curriculum learning</cell></row><row><cell>5Ŝ</cell><cell>0 ← S</cell><cell></cell></row><row><cell cols="2">6 Q ← N</cell><cell cols="2">Total # curriculum steps</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Curriculum step</cell></row><row><cell></cell><cell cols="3">Stage 1: Adaptation stage</cell></row></table><note>7 for q in (0 : Q − 1) do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Algorithm 2 :</head><label>2</label><figDesc>Training Procedure of Domainaware Curriculum Graph Co-Teaching (D-CGCT) require: number of target domains N , classes nc require: source dataset S; target dataset T = {Tj} N j=1 require: hyper-parameters B, τ , K, K , λ edge , λ node , λ adv require: networks F , D, G mlp , f edge , f node with parameters θ, ψ, φ, ϕ, ϕ , respectively. The f edge and f node form the Ggcn.</figDesc><table><row><cell cols="4">Step 1: Pre-training on the source dataset</cell></row><row><cell cols="4">1 while ce has not converged do</cell></row><row><cell>2</cell><cell cols="2">(xs,i, ys,i) B i=1 ∼ S</cell></row><row><cell>3</cell><cell cols="2">update θ, φ by min θ,φ</cell><cell>mlp ce</cell></row><row><cell cols="2">4 end</cell><cell></cell></row><row><cell cols="4">Step 2: Curriculum learning</cell></row><row><cell>5Ŝ</cell><cell cols="3">0 ← S andT 0 ← {Tj} N j=1</cell></row><row><cell cols="2">6 Q ← N</cell><cell cols="2">Total # curriculum steps</cell></row><row><cell cols="3">7 for q in (0 : Q − 1) do</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Curriculum step</cell></row><row><cell>8</cell><cell cols="2">H ← {}</cell><cell>Empty list</cell></row><row><cell></cell><cell cols="3">Stage 1: Domain selection stage</cell></row><row><cell>9</cell><cell cols="2">for Tj inT q do</cell></row><row><cell>10</cell><cell cols="3">compute H(Tj) as in Eqn. 12</cell></row><row><cell>11</cell><cell cols="2">H ← H || H(Tj)</cell><cell>Append</cell></row><row><cell>12</cell><cell>end</cell><cell></cell></row><row><cell>13</cell><cell cols="2">D q ← argmin j H</cell><cell>Chosen domain</cell></row><row><cell></cell><cell cols="3">Stage 2: Adaptation stage</cell></row><row><cell>14</cell><cell cols="2">for k in (1 : K) do</cell></row><row><cell cols="2">15B</cell><cell>q</cell></row></table><note>s ← (xs,i, ys</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The network architecture for the baseline<ref type="bibr" target="#b26">[27]</ref> used in the Digits-five experiments. Kernel size (k size ); in channels (C in ); out channels (C out ); stride (st); and padding (pad). IN stands for instance normalization. The input image resolution is 28 × 28 × 3.work with 2 conv layers followed by a Batch Normalization (BN) layer and ReLU activation, except the final layer. The first layer takes as input image features concatenated with the context of the mini-batch, i.e., the aggregated features of other images in a mini-batch (based on the affinity matrix estimated by the f edge ). The second conv layer outputs the logits that are equal to the number of classes n c . We have used 1x1 convolution kernels in the f node . Similarly, we have implemented the f edge network with 3 conv layers and 1x1 kernels, where the first two layers are followed by the BN layers and ReLU activations, except the last. The third conv layer has a single channel as output, thus, representing the similarity scores between samples in a mini-batch.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Digits-fiveSetting Model mt → mm,sv,sy,up mm → mt,sv,sy,up sv → mm,mt,sy,up sy → mm,sv,mt,up up → mm,sv,sy,mt Avg (%)</figDesc><table><row><cell></cell><cell>Source only</cell><cell>26.9</cell><cell>56.0</cell><cell>67.2</cell><cell>73.8</cell><cell>36.9</cell><cell>52.2</cell></row><row><cell></cell><cell>ADDA [50]</cell><cell>43.7</cell><cell>55.9</cell><cell>40.4</cell><cell>66.1</cell><cell>34.8</cell><cell>48.2</cell></row><row><cell></cell><cell>DAN [28]</cell><cell>31.3</cell><cell>53.1</cell><cell>48.7</cell><cell>63.3</cell><cell>27.0</cell><cell>44.7</cell></row><row><cell>Target</cell><cell>GTA [45]</cell><cell>44.6</cell><cell>54.5</cell><cell>60.3</cell><cell>74.5</cell><cell>41.3</cell><cell>55.0</cell></row><row><cell>Combined</cell><cell>RevGrad [12]</cell><cell>52.4</cell><cell>64.0</cell><cell>65.3</cell><cell>66.6</cell><cell>44.3</cell><cell>58.5</cell></row><row><cell></cell><cell>AMEAN [9]</cell><cell>56.2</cell><cell>65.2</cell><cell>67.3</cell><cell>71.3</cell><cell>47.5</cell><cell>61.5</cell></row><row><cell></cell><cell>CDAN [27]</cell><cell>53.0</cell><cell>76.3</cell><cell>65.6</cell><cell>81.5</cell><cell>56.2</cell><cell>66.5</cell></row><row><cell></cell><cell>CGCT</cell><cell>54.3</cell><cell>85.5</cell><cell>83.8</cell><cell>87.8</cell><cell>52.4</cell><cell>72.8</cell></row><row><cell>Multi-Target</cell><cell>CDAN [27] CDAN + DCL D-CGCT</cell><cell>53.7 62.0 65.7</cell><cell>76.2 87.8 89.0</cell><cell>64.4 87.8 88.9</cell><cell>80.3 92.3 93.2</cell><cell>46.2 63.2 62.9</cell><cell>64.2 78.6 79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Comparison with the state-of-the-art methods on the Digits-five. "Target Combined" indicates methods are performed on one source to one combined target domain. "Multi-Target" indicates methods are performed on one source to multi-target setting. Our proposed models are highlighted in bold.</figDesc><table><row><cell>PACS</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ronan Collobert, and Jason Weston. Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CLT</title>
		<meeting>CLT</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Just dial: Domain alignment layers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woong-Gi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tackgeun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Re-weighted adversarial adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blending-target domain adaptation by adversarial metaadaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised multitarget domain adaptation: An information theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Behnam Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Konstantinos Bousmalis, and Vladimir Pavlovic</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJNN</title>
		<meeting>IJNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Curriculum based dropout discriminator for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR-WS</title>
		<meeting>ICLR-WS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Open compound domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progressive graph learning for open-set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gcan: Graph convolutional adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhu</forename><surname>Le Thanh Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atif</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Antoine</forename><surname>Bela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blais-Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV, 2021</title>
		<meeting>WACV, 2021</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Synthetic to real adaptation with generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using feature-whitening and consensus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Trigan: Image-to-image translation for multi-source domain adaptation. Machine vision and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using full-feature whitening and colouring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIAP</title>
		<meeting>ICIAP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transferable curriculum for weakly-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ser-Nam Lim, and Abhinav Shrivastava. Curriculum manager for source selection in multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network for unsupervised multiple-target domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Overcoming negative transfer: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongrui</forename><surname>Wu</surname></persName>
		</author>
		<idno>arXiv, 2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep transfer network: Unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">Xinnan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
