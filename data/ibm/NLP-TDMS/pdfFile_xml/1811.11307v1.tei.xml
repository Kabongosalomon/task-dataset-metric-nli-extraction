<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Speech Enhancement with the Wave-U-Net</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macartney</surname></persName>
							<email>craig.macartney@city.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
							<email>t.e.weyde@city.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Speech Enhancement with the Wave-U-Net</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the use of the Wave-U-Net architecture for speech enhancement, a model introduced by Stoller et al for the separation of music vocals and accompaniment. This end-to-end learning method for audio source separation operates directly in the time domain, permitting the integrated modelling of phase information and being able to take large temporal contexts into account. Our experiments show that the proposed method improves several metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the speech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a reduced number of hidden layers is sufficient for speech enhancement in comparison to the original system designed for singing voice separation in music. We see this initial result as an encouraging signal to further explore speech enhancement in the time-domain, both as an end in itself and as a pre-processing step to speech recognition systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Audio source separation refers to the problem of extracting one or more target sources while suppressing interfering sources and noise <ref type="bibr" target="#b17">[18]</ref>. Two related tasks are those of speech enhancement and singing voice separation, both of which involve extracting the human voice as a target source. The former involves attempting to improve speech intelligibility and quality when obscured by additive noise <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref>; whilst the latter's focus is on separating music vocals from accompaniment <ref type="bibr" target="#b11">[12]</ref>.</p><p>Most audio source separation methods operate not directly in the time-domain, but with timefrequency representations as input and output (front-end). Since 2017, the U-Net architecture on magnitude spectrograms has achieved new state of the art results in audio source separation for music <ref type="bibr" target="#b5">[6]</ref> and speech dereverbration <ref type="bibr" target="#b1">[2]</ref>. Also, neural network architectures operating in the time domain have recently been proposed for speech enhancement <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. These approaches have been combined in the Wave-U-Net <ref type="bibr" target="#b11">[12]</ref> and applied to singing voice separation. In this paper we apply the Wave-U-Net to speech enhancement and show that it produces results that are better than the current state of the art.</p><p>The remainder of this paper is structured as follows. In section 2, we briefly review related work from the literature. In section 3, we introduce briefly the Wave-U-Net architecture and its application to speech. Section 4 presents the experiments we conducted and their results including comparison to other methods. Section 5 concludes this article with a final summary and perspectives for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Source separation of audio has seen great improvement in recent years through deep learning models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. These methods, as well as more traditional ones, mostly operate in the time-frequency domain, from deep recurrent architectures predicting soft masks, such as <ref type="bibr" target="#b3">[4]</ref>, to convolutional encoder-decoder architectures like that of <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr">Recently</ref> Crop and concatenate <ref type="figure">Figure 1</ref>: The Wave-U-Net architecture following <ref type="bibr" target="#b11">[12]</ref>.</p><p>achieved new state of the art results in audio source separation for music <ref type="bibr" target="#b5">[6]</ref> and speech dereverbration <ref type="bibr" target="#b1">[2]</ref>.</p><p>Also recently, models operating in the time domain have been developed. The development of Wavenet <ref type="bibr" target="#b16">[17]</ref> inspired other developments, including <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. The SEGAN <ref type="bibr" target="#b8">[9]</ref> architecture was developed for the purpose of speech enhancement and denoising. It employs a neural network in the time-domain with an encoder and decoder pathway that successively halves and doubles the resolution of feature maps in each layer, respectively, and features skip connections between encoder and decoder layers. It offers state-of-the-art results on the Voice Bank (VCTK) dataset <ref type="bibr" target="#b13">[14]</ref>.</p><p>The Wavenet for Speech Denoising <ref type="bibr" target="#b10">[11]</ref>, another architecture to operate directly in the time domain, takes its inspiration from <ref type="bibr" target="#b16">[17]</ref>. It has a non-causal conditional input and a parallel output of samples for each prediction and is based on the repeated application of dilated convolutions with exponentially increasing dilation factors to factor in context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Wave-U-Net for Speech Enhancement</head><p>The Wave-U-Net architecture of <ref type="bibr" target="#b11">[12]</ref> combines elements of both of the abovementioned architectures with the U-Net. The overall architecture is a one-dimensional U-Net with down and upsampling blocks.</p><p>As per the spectrogram-based U-Net architectures (e.g. <ref type="bibr" target="#b5">[6]</ref>), the Wave-U-Net uses a series of downsampling and upsampling blocks to make its predictions, whilst at each level of the network, the time resolution is halved.</p><p>In applying the Wave-U-Net architecture to the application of speech enhancement, our objective is to separate a mixture waveform m ∈ [−1, 1] L×C into K source waveforms S 1 , ..., S K with S k ∈ [−1, 1] L×C for all k ∈ 1, ..., K, C as the number of audio channels and L as the numbers of audio samples. In our case of monaural speech enhancement we have K = 2 and C = 1.</p><p>In doing so, for K sources to be estimated, a 1D convolution, zero-padded before convolving, of filter size 1 with K · C filters is applied to convert the stack of features at each audio sample into a source prediction for each sample. This is followed by a tanh nonlinearity to obtain a source signal estimate with values in the interval (−1, 1). All convolutions except the final ones are followed by LeakyReLU non-linearities. We use the same VCTK dataset <ref type="bibr" target="#b13">[14]</ref> as the SEGAN <ref type="bibr" target="#b8">[9]</ref>, which is available publicly, encouraging comparisons with future speech enhancement methods.</p><p>The dataset includes clean and noisy audio data at 48kHz sampling frequency. However, like the SEGAN, we downsample to 16kHz for training and testing. The clean data are recordings of sentences, sourced from various text passages, uttered by 30 English-speakers, male and female, with various accents -28 intended for training and 2 reserved for testing <ref type="bibr" target="#b15">[16]</ref>. The noisy data were generated by mixing the clean data with various noise datasets, as per the instructions provided in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>With respect to the training set, 40 different noise conditions are considered <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. These are composed of 10 types of noise (2 of which are artificially-generated 1 and 8 sourced from the DEMAND database <ref type="bibr" target="#b12">[13]</ref>, each mixed with clean speech at one of 4 signal-to-noise ratios (SNR) (15, 10, 5, and 0 dB). In total, this yields 11, 572 training samples, with approximately 10 different sentences in each condition per training speaker.</p><p>Testing conditions are mismatched from those of the training. The speakers, noise types and SNRs are all different. The separate test set with 2 speakers, unseen during training, consists of a total of 20 different noise conditions: 5 types of noise sourced from the DEMAND database at one of 4 SNRs each (17.5, 12.5, 7.5, and 2.5 dB) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. This yields 824 test items, with approximately 20 different sentences in each condition per test speaker <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>As per <ref type="bibr" target="#b11">[12]</ref>, our baseline model trains on randomly-sampled audio excerpts, using the ADAM optimization algorithm, a learning rate of 0.0001, decay rates β1 = 0.9 and β2 = 0.999 and a batch size of 16. We specify an initial network layer size of 12, like in <ref type="bibr" target="#b11">[12]</ref>, although this is varied across experiments, as described in the Results section below. 16 extra filters per layer are also specified, with downsampling block filters of size 15 and upsampling block filters of size 5 like in <ref type="bibr" target="#b11">[12]</ref>. We train for 2,000 iterations with mean squared error (MSE) over all source output samples in a batch as loss and apply early stopping if there is no improvement on the validation set for 20 epochs. We use a fixed validation set of 10 randomly selected tracks. Then, the best model is fine-tuned with the batch size doubled and the learning rate lowered to 0.00001, again until 20 epochs have passed without improved validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>To evaluate and compare the quality of the enhanced speech yielded by the Wave-U-Net, we mirror the objective measures provided in <ref type="bibr" target="#b8">[9]</ref>. Each measurement compares the enhanced signal with the clean reference of each of the 824 test set items. They have been calculated using the implementation provided in <ref type="bibr" target="#b6">[7]</ref> 2 . The first metric is that of the Perceptual Evaluation of Speech Quality (PESQ)more specifically the wide-band version recommended in ITU-T P.862.2 (from -0.5 to 4.5) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Secondly, composite measures of metrics that aim to computationally approximate the Mean Opinion Score (MOS) that would be produced from human perceptual trials are computed <ref type="bibr" target="#b10">[11]</ref>. These are: CSIG, a prediction of the signal distortion attending only to the speech signal <ref type="bibr" target="#b2">[3]</ref> (from 1 to 5); CBAK, a prediction of the intrusiveness of background noise <ref type="bibr" target="#b2">[3]</ref> (from 1 to 5); and COVL, a prediction of the overall effect <ref type="bibr" target="#b2">[3]</ref> (from 1 to 5). Last is the Segmental Signal-to-Noise Ratio (SSNR) <ref type="bibr" target="#b9">[10]</ref> (from 0 to ∞). <ref type="table" target="#tab_1">Table 1</ref> shows the results of these metrics for comparison across different speech enhancement architectures versus the overall best-performing Wave-U-Net model, that of the 10-layer with finetuning applied. As a comparative reference, it also shows the results of these metrics when applied: directly to the noisy signals; to signals filtered using the Wiener method, based on a priori SNR estimation; and to the SEGAN-enhanced signal, as provided in <ref type="bibr" target="#b8">[9]</ref>. The results indicate that the Wave-U-Net is the most effective model for speech enhancement. <ref type="table" target="#tab_2">Table 2</ref> shows the performance differences between different variations of the Wave-U-Net, with different numbers of layers. No fine-tuning was performed to obtain the results shown here, which explains the difference between the 10-layer Wave-U-Net in <ref type="table" target="#tab_1">Tables 1 and 2</ref>. The results suggest that fine-tuning does not make a meaningful difference, except on the CSIG measurement, and that performance reaches a peak around the 10-and 9-layer models, smaller than the best performing equivalent model for music vocals source separation in <ref type="bibr" target="#b11">[12]</ref>. This is likely due to the size of the receptive field, where for speech the optimal size is probably smaller than for music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Summary</head><p>The Wave-U-Net combines the advantages of several of the most recent successful architectures for music and speech source separation and our results show that it is particularly effective at speech enhancement. The results improve over the state of the art by a good margin even without significant adaptation or parameter tuning. This indicates that there is great potential for this approach in speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Future work</head><p>In comparison to the SEGAN architecture, it is possible that the advantage stems from the upsampling that avoids aliasing, which should be further investigated. The results indicate that there is room for increasing effectiveness and efficiency by further adapting the model size and other parameters, e.g. filter sizes, to the task and expanding to multi-channel audio and multi-source-separation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Objective evaluation -comparing the mean results of the untreated noisy signal, the Wiener-, SEGAN-and Wave-U-Net-enhanced signals. Higher scores are better for all metrics.</figDesc><table><row><cell cols="5">Metric Noisy Wiener SEGAN Wave-U-Net</cell></row><row><cell>PESQ</cell><cell>1.97</cell><cell>2.22</cell><cell>2.16</cell><cell>2.40</cell></row><row><cell>CSIG</cell><cell>3.35</cell><cell>3.23</cell><cell>3.48</cell><cell>3.52</cell></row><row><cell>CBAK</cell><cell>2.44</cell><cell>2.68</cell><cell>2.94</cell><cell>3.24</cell></row><row><cell>COVL</cell><cell>2.63</cell><cell>2.67</cell><cell>2.80</cell><cell>2.96</cell></row><row><cell>SSNR</cell><cell>1.68</cell><cell>5.07</cell><cell>7.73</cell><cell>9.97</cell></row><row><cell>4 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Objective evaluation -mean results, comparing variations of the Wave-U-Net model with different numbers of layers, without fine-tuning applied.</figDesc><table><row><cell cols="6">Metric 12-layer 11-layer 10-layer 9-layer 8-layer</cell></row><row><cell>PESQ</cell><cell>2.40</cell><cell>2.38</cell><cell>2.41</cell><cell>2.41</cell><cell>2.39</cell></row><row><cell>CSIG</cell><cell>3.49</cell><cell>3.47</cell><cell>3.43</cell><cell>3.54</cell><cell>3.51</cell></row><row><cell>CBAK</cell><cell>3.23</cell><cell>3.22</cell><cell>3.24</cell><cell>3.23</cell><cell>3.18</cell></row><row><cell>COVL</cell><cell>2.95</cell><cell>2.92</cell><cell>2.92</cell><cell>2.97</cell><cell>2.95</cell></row><row><cell>SSNR</cell><cell>9.79</cell><cell>9.95</cell><cell>9.98</cell><cell>9.87</cell><cell>9.30</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">available here: http://data.cstr.ed.ac.uk/cvbotinh/SE/data/ [14] 2 available here: https://ecs.utdallas.edu/loizou/speech/software.htm</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monoaural Audio Source Separation Using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pritish Chandna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Janer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gómez</surname></persName>
		</author>
		<ptr target="http://mtg.upf.edu/node/3680" />
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Latent Variable Analysis and Signal Separation (LVA ICA2017)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Speech dereverberation using fully convolutional networks. CoRR, abs/1803.08243</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><forename type="middle">E</forename><surname>Chazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1803.08243" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of Objective Quality Measures for Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senior</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Member</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASL.2007.911054</idno>
		<ptr target="http://www.utdallas.edu/loizou/speech/noizeus/" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech and language processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">229</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Singing-Voice Separation from Monaural Recordings Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<ptr target="http://www.isle.illinois.edu/sst/pubs/2014/huang-ismir2014.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<ptr target="https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Society for Music Information Retrieval Conference</title>
		<meeting>the 18th International Society for Music Information Retrieval Conference<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23" />
			<biblScope unit="page" from="745" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech Enhancement: Theory and Practice</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press, Inc</publisher>
			<biblScope unit="page">9781466504219</biblScope>
		</imprint>
	</monogr>
	<note>2nd edition, 2013. ISBN 1466504218</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multichannel audio source separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Arie</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1652" to="1664" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1428</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2017-1428" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Objective measures of speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schuyler</forename><forename type="middle">R</forename><surname>Quackenbush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Thomas Pinkney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="volume">0136290566</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Wavenet for Speech Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1706.07162.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.03185" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise record-ings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.5281/zen</idno>
		<ptr target="https://hal.inria.fr/hal-00796707" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Noisy speech database for training speech enhancement algorithms and TTS models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<idno type="DOI">10.7488/ds/2117</idno>
		<ptr target="http://dx.doi.org/10.7488/ds/2117" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. School of Informatics. Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noise-robust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<idno type="DOI">10.21437/SSW.2016-24</idno>
		<ptr target="http://dx.doi.org/10.21437/SSW.2016-24" />
	</analytic>
	<monogr>
		<title level="m">9th ISCA Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-159</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2016-159" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://www.isca-speech.org/archive/SSW_2016/abstracts/ssw9_DS-4_van_den_Oord.html" />
	</analytic>
	<monogr>
		<title level="m">The 9th ISCA Speech Synthesis Workshop</title>
		<meeting><address><addrLine>Sunnyvale, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Audio Source Separation and Speech Enhancement</title>
		<idno type="DOI">http:/doi.wiley.com/10.1002/9781119279860</idno>
		<idno>9781119279860. doi: 10.1002/ 9781119279860</idno>
		<ptr target="http://doi.wiley.com/10.1002/9781119279860" />
		<editor>Emmanuel Vincent, Tuomas Virtanen, and Sharon Gannot</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>John Wiley &amp; Sons Ltd</publisher>
			<pubPlace>Chichester, UK, 9</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
