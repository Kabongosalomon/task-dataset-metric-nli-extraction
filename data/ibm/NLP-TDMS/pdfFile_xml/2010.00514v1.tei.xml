<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Referring Image Segmentation via Cross-Modal Progressive Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<addrLine>6 360</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Referring Image Segmentation via Cross-Modal Progressive Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances. Code is available at https://github.com/ spyflying/CMPC-Refseg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As deep models have made significant progresses in vision or language tasks <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b25">[26]</ref> <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b38">[39]</ref>, fields combining them <ref type="bibr" target="#b36">[37]</ref>[28] <ref type="bibr" target="#b49">[50]</ref> have drawn great attention of researchers. In this paper, we focus on the referring image * Equal contribution † Corresponding author "The man holding a white frisbee" The model first perceives all the entities described in the expression based on entity words and attribute words, e.g., "man" and "white frisbee" (orange masks and blue outline). (c) After finding out all the candidate entities that may match with input expression, relational word "holding" can be further exploited to highlight the entity involved with the relationship (green arrow) and suppress the others which are not involved. (d) Benefiting from the relation-aware reasoning process, the referred entity is found as the final prediction (purple mask). (Best viewed in color).</p><p>segmentation (RIS) problem whose goal is to segment the entities described by a natural language expression. Beyond traditional semantic segmentation, RIS is a more challenging problem since the expression can refer to objects or stuff belonging to any category in various language forms and contain diverse contents including entities, attributes and relationships. As a relatively new topic that is still far from being solved, this problem has a wide range of potential applications such as interactive image editing, language-based robot controlling, etc. Early works <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b29">[30]</ref>[34] <ref type="bibr" target="#b22">[23]</ref> tackle this problem using a straightforward concatenation-andconvolution scheme to fuse visual and linguistic features.</p><p>Later works <ref type="bibr" target="#b37">[38]</ref>[3] <ref type="bibr" target="#b43">[44]</ref> further utilize inter-modality attention or self-attention to learn only visual embeddings or visual-textual co-embeddings for context modeling. However, these methods still lack the ability of exploiting different types of informative words in the expression to accurately align visual and linguistic features, which is crucial to the comprehension of both expression and image.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a) and (b), if the referent, i.e., the entity referred to by the expression, is described by "The man holding a white frisbee", a reasonable solution is to tackle the referring problem in a progressive way which can be divided into two stages. First, the model is supposed to perceive all the entities described in the expression according to entity words and attribute words, e.g., "man" and "white frisbee". Second, as multiple entities of the same category may appear in one image, for example, the three men in <ref type="figure" target="#fig_0">Figure 1</ref> (b), the model needs to further reason relationships among entities to highlight the referent and suppress the others that are not matched with the relationship cue given in the expression. In <ref type="figure" target="#fig_0">Figure 1</ref> (c), the word "holding" which associates "man" with "white frisbee" powerfully guides the model to focus on the referent who holds a white frisbee rather than the other two men, which assists in making correct prediction in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>.</p><p>Based on the above motivation, we propose a Cross-Modal Progressive Comprehension (CMPC) module which progressively exploits different types of words in the expression to segment the referent in a graph-based structure. Concretely, our CMPC module consists of two stages. First, linguistic features of entity words and attribute words (e.g., "man" and "white frisbee") extracted from the expression are fused with visual features extracted from the image to form multimodal features where all the entities considered by the expression are perceived. Second, we construct a fully-connected spatial graph where each vertex corresponds to an image region and feature of each vertex contains multimodal information of the entity. Vertexes require appropriate edges to communicate with each other. Naive edges treating all the vertexes equally will introduce abundant information and fail to distinguish the referent from other candidates. Therefore, our CMPC module employs relational words (e.g., "holding") of the expression as a group of routers to build adaptive edges to connect spatial vertexes, i.e., entities, that are involved with the relationship described in the expression. Particularly, spatial vertexes (e.g., "man") that have strong responses to the relational words (e.g., "holding") will exchange information with others (e.g., "frisbee") that also correlate with the relational words. Meanwhile, spatial vertexes that have weak responses to the relational words will have less interaction with others. After relation-aware reasoning on the multimodal graph, feature of the referent can be highlighted while those of the irrelevant entities can be sup-pressed, which assists in generating accurate segmentation.</p><p>As multiple levels of features can complement each other <ref type="bibr" target="#b22">[23]</ref>[44] <ref type="bibr" target="#b2">[3]</ref>, we also propose a Text-Guided Feature Exchange (TGFE) module to exploit information of multimodal features refined by our CMPC module from different levels. For each level of multimodal features, our TGFE module utilizes linguistic features as guidance to select useful feature channels from other levels to realize information communication. After multiple rounds of communication, multi-level features are further fused by ConvLSTM <ref type="bibr" target="#b41">[42]</ref> to comprehensively integrate low-level visual details and highlevel semantics for precise mask prediction.</p><p>Our contributions are summarized as follows: (1) We propose a Cross-Modal Progressive Comprehension (CMPC) module which first perceives all the entities that are possibly referred by the expression, then utilizes relationship cues of the input expression to highlight the referent while suppressing other irrelevant ones, yielding discriminative feature representations for the referent. <ref type="bibr" target="#b1">(2)</ref> We also propose a Text-Guided Feature Exchange (TGFE) module to conduct adaptive information communication among multi-level features under the guidance of linguistic features, which further enhances feature representations for mask prediction. (3) Our method achieves new state-ofthe-art results on four referring segmentation benchmarks, demonstrating the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>Semantic segmentation has made a huge progress based on Fully Convolutional Networks (FCN) <ref type="bibr" target="#b31">[32]</ref>. FCN replaces fully-connected layers in original classification networks with convolution layers and becomes the standard architecture of the following segmentation methods. DeepLab <ref type="bibr" target="#b3">[4]</ref>[5] <ref type="bibr" target="#b5">[6]</ref> introduces atrous convolution with different atrous rates into FCN model to enlarge the receptive field of filters and aggregate multi-scale context. PSPNet <ref type="bibr" target="#b48">[49]</ref> utilizes pyramid pooling operations to extract multi-scale context as well. Recent works such as DANet <ref type="bibr" target="#b10">[11]</ref> and CFNet <ref type="bibr" target="#b46">[47]</ref> employ self-attention mechanism <ref type="bibr" target="#b39">[40]</ref> to capture long-range dependencies in deep networks and achieve notable performance. In this paper, we tackle the more generalized and challenging semantic segmentation problem whose semantic categories are specified by natural language referring expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Referring Expression Comprehension</head><p>The goal of referring expression comprehension is to localize the entities in the image which are matched with the description of a natural language expression. Many works conduct localization in bounding box level. Liao et al. <ref type="bibr" target="#b26">[27]</ref> performs cross-modality correlation filtering to match multimodal features in real time. Relationships between vision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Progressive Comprehension</head><p>Coordinate Feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, ,</head><p>The man holding a white frisbee and language modalities <ref type="bibr" target="#b15">[16]</ref>[43] are also modeled to match the expression with most related objects. Modular networks are explored in <ref type="bibr" target="#b44">[45]</ref> to decompose the referring expression into subject, location and relationship so that the matching score is more finely computed. Beyond bounding box, the referred object can also be localized more precisely with segmentation mask. Hu et al. <ref type="bibr" target="#b16">[17]</ref> first proposes the referring segmentation problem and generates the segmentation mask by directly concatenating and fusing multimodal features from CNN and LSTM <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, multimodal LSTM is employed to sequentially fuse visual and linguistic features in multiple time steps. Based on <ref type="bibr" target="#b29">[30]</ref>, dynamic filters <ref type="bibr" target="#b33">[34]</ref> for each word further enhance multimodal features. Fusing multilevel visual features is explored in <ref type="bibr" target="#b22">[23]</ref> to recurrently refine the local details of segmentation mask. As context information is critical to segmentation task, Shi et al. <ref type="bibr" target="#b37">[38]</ref> utilizes word attention to aggregate only visual context to enhance visual features. For multimodal context extraction, cross-modal self-attention is exploited in <ref type="bibr" target="#b43">[44]</ref> to capture long-range dependencies between each image region and each referring word. Visual-textual co-embedding is explored in <ref type="bibr" target="#b2">[3]</ref> to measure compatibility between referring expression and image. Adversarial learning <ref type="bibr" target="#b35">[36]</ref> and cycleconsistency <ref type="bibr" target="#b7">[8]</ref> between referring expression and its reconstructed caption are also investigated to boost the segmentation performance. In this paper, we propose to progressively highlight the referent via entity perception and relationaware reasoning for accurate referring segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph-Based Reasoning</head><p>It has been shown that graph-based models are effective for context reasoning in many tasks. Dense CRF <ref type="bibr" target="#b1">[2]</ref> is a widely used graph model for post-processing in image segmentation. Recently, Graph Convolution Networks (GCN) <ref type="bibr" target="#b1">[2]</ref> becomes popular for its superiority on semisupervised classification. Wang et al. <ref type="bibr" target="#b40">[41]</ref> construct a spatial-temporal graph using region proposals as vertexes and conduct context reasoning with GCN, which performs well on video recognition task. Chen et al. <ref type="bibr" target="#b6">[7]</ref> propose a global reasoning module which projects visual feature into an interactive space and conducts graph convolution for global context reasoning. The reasoned global context is projected back to the coordinate space to enhance original visual feature. There are several concurrent works <ref type="bibr" target="#b23">[24]</ref>[25] <ref type="bibr" target="#b47">[48]</ref> sharing the same idea of projection and graph reasoning with different implementation details. In this paper, we propose to regard image regions as vertexes to build a spatial graph where each vertex saves multimodal feature vector as its state. Information flow among vertexes is routed by relational words in the referring expression and implemented using graph convolution. After the graph reasoning, image regions can generate accurate and coherent responses to the referring expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an image and a natural language expression, the goal of our model is to segment the corresponding entity referred to by the expression, i.e., the referent. The overall architecture of our model is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. We first extract the visual features of the image with a CNN backbone and the linguistic features of the expression with a text encoder. A novel Cross-Modal Progressive Comprehension (CMPC) module is proposed to progressively highlight the referent and suppress the others via entity perception and subsequent relation-aware reasoning on spatial region graph. The proposed CMPC module is applied to multiple levels of visual features respectively and the corresponding outputs are fed into a Text-Guided Feature Exchange (TGFE) module to communicate information under the guidance of linguistic modality. After the communication, multi-level features are finally fused with ConvL-STM <ref type="bibr" target="#b41">[42]</ref> to make the prediction. We will elaborate each part of our method in the rest subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual and Linguistic Feature Extraction</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our model takes an image and an expression as inputs. The multi-level visual features are extracted with a CNN backbone and respectively fused with an 8-D spatial coordinate feature O ∈ R H×W ×8 using a 1 × 1 convolution following prior works <ref type="bibr" target="#b29">[30]</ref> <ref type="bibr" target="#b43">[44]</ref>. After the convolution, each level of visual features are transformed to the same size of R H×W ×Cv , with H, W and C v being the height, width and channel dimension of the visual features. The transformed visual features are denoted as {X 3 , X 4 , X 5 } corresponding to the output of the 3rd, 4th and 5th stages of CNN backbone (e.g., ResNet-101 <ref type="bibr" target="#b13">[14]</ref>). For ease of presentation, we denote a single level of visual features as X in Sec. 3.2. The linguistic features L = {l 1 , l 2 , ..., l T } is extracted with a language encoder (e.g., LSTM <ref type="bibr" target="#b14">[15]</ref>), where T is the length of expression and l i ∈ R C l (i ∈ {1, 2, ..., T }) denotes feature of the i-th word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Modal Progressive Comprehension</head><p>As many entities may exist in the image, it is natural to progressively narrow down the candidate set from all the entities to the actual referent. In this section, we propose a Cross-Modal Progressive Comprehension (CMPC) module which consists of two stages, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. The first stage is entity perception. We associate linguistic features of entity words and attribute words with the correlated visual features of spatial regions using bilinear fusion <ref type="bibr" target="#b0">[1]</ref> to obtain the multimodal features M ∈ R H×W ×Cm . All the candidate entities are perceived by the fusion. The second stage is relation-aware reasoning. A fully-connected multimodal graph is constructed over M with relational words serving as a group of routers to connect vertexes. Each vertex of the graph represents a spatial region on M . By reasoning among vertexes of the multimodal graph, the responses of the referent matched with the relationship cue are highlighted while those of non-referred ones are suppressed accordingly. Finally, the enhanced multimodal featuresM g are further fused with visual and linguistic features.</p><p>Entity Perception. Similar to <ref type="bibr" target="#b42">[43]</ref>, we classify the words into 4 types, including entity, attribute, relation and unnecessary word. A 4-D vector is predicted for each word to indicate the probability of it being the four types respectively. We denote the probability vector for word t as p t = [p ent t , p attr t , p rel t , p un t ] ∈ R 4 and calculate it as:</p><formula xml:id="formula_0">p t = sof tmax(W 2 σ(W 1 l t + b 1 ) + b 2 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">W 1 ∈ R Cn×C l , W 2 ∈ R 4×Cn , b 1 ∈ R Cn and b 2 ∈ R 4 are learnable parameters, σ(·) is sigmoid function, p ent t , p attr t , p rel t</formula><p>and p un t denote the probabilities of word t being the entity, attribute, relation and unnecessary word respectively. Then the global language context of entities q ∈ R C l could be calculated as a weighted combination of the all the words in the expression:</p><formula xml:id="formula_2">q = T t=1 (p ent t + p attr t )l t .<label>(2)</label></formula><p>Next, we adopt a simplified bilinear fusion strategy [1] to associate q with the visual feature of each spatial region:</p><formula xml:id="formula_3">M i = (qW 3i ) (XW 4i ),<label>(3)</label></formula><formula xml:id="formula_4">M = r i=1 M i<label>(4)</label></formula><p>where W 3i ∈ R C l ×Cm and W 4i ∈ R Cv×Cm are learnable parameters, r is a hyper-parameter and denotes element-wise product. By integrating both visual and linguistic context into the multimodal features, all the entities that might be referred to by the expression are perceived appropriately.</p><p>Relation-Aware Reasoning. To selectively highlight the referent, we construct a fully-connected graph over the mutimodal features M and conduct reasoning over the graph according to relational cues in the expression. Formally, the multimodal graph is defined as G = (V, E, M g , A) where V and E are the sets of vertexes and edges, M g = {m i } N i=1 ∈ R N ×Cm is the set of vertex features, A ∈ R N ×N is the adjacency matrix and N is number of vertexes.</p><p>Details of relation-aware reasoning is illustrated in the right part of <ref type="figure" target="#fig_3">Figure 3</ref>. As each location on M represents a spatial region on the original image, we regard each region as a vertex of the graph and the multimodal graph is composed of N = H × W vertexes in total. After the reshaping operation, a linear layer is applied to M to transform it into the features of vertexes M g . The edge weights depend on the affinities between vertexes and relational words in the referring expression. Features of relational words R = {r t } T t=1 ∈ R T ×C l are calculated as:</p><formula xml:id="formula_5">r t = p rel t l t , t = 1, 2, ..., T.<label>(5)</label></formula><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, adjacency matrix A is formulated as:</p><formula xml:id="formula_6">B = (M g W 5 )(RW 6 ) T ,<label>(6)</label></formula><formula xml:id="formula_7">B 1 = sof tmax(B),<label>(7)</label></formula><formula xml:id="formula_8">B 2 = sof tmax(B T ),<label>(8)</label></formula><formula xml:id="formula_9">A = B 1 B 2 ,<label>(9)</label></formula><p>where W 5 ∈ R Cm×C h and W 6 ∈ R C l ×C h are learnable parameters. B ∈ R N ×T is the affinity matrix between M g and R. We apply the softmax function along the second and first dimension of B to obtain B 1 ∈ R N ×T and B 2 ∈ R T ×N respectively. A is obtained by matrix product of B 1 and B 2 . Each element A ij of A represents the normalized magnitude of information flow from the spatial region i to the region j, which depends on their affinities with relational words in the expression. In this way, relational words of the expression can be leveraged as a group of routers to build adaptive edges connecting vertexes. After the construction of multimodal graph G, we apply graph convolution <ref type="bibr" target="#b20">[21]</ref> to it as follow:</p><formula xml:id="formula_10">M g = (A + I)M g W 7 ,<label>(10)</label></formula><p>where W 7 ∈ R Cm×Cm is a learnable weight matrix. I is identity matrix serving as a shortcut to ease optimization. The graph convolution reasons among vertexes, i.e., image regions, so that the referent is selectively highlighted according to the relationship cues while other irrelevant ones are suppressed, which assists in generating more discriminative feature representations for referring segmentation.</p><p>Afterwards, reshaping operation is applied to obtain the enhanced multimodal featuresM g ∈ R H×W ×Cm . To incorporate the textual information, we first combine features of all necessary words into a vector s ∈ R C l with the predefined probability vectors:</p><formula xml:id="formula_11">s = T t=0 (p ent t + p attr t + p rel t )l t .<label>(11)</label></formula><p>We repeat s for H × W times and concatenate it with X andM g along channel dimension following with a 1 × 1 convolution to get the output features Y ∈ R H×W ×Cm , which is equipped with multimodal context for the referent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text-Guided Feature Exchange</head><p>As previous works [23] <ref type="bibr" target="#b43">[44]</ref> show that multi-level semantics are essential to referring segmentation, we further introduce a Text-Guided Feature Exchange (TGFE) module to communicate information among multi-level features based on the visual and language context. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, the TGFE module takes Y 3 , Y 4 , Y 5 and word features [l 1 , l 2 , ..., l T ] as input. After n rounds of feature exchange, Y</p><formula xml:id="formula_12">(n) 3 , Y (n) 4 , Y (n) 5</formula><p>are produced as outputs.</p><formula xml:id="formula_13">To get Y (k) i , i ∈ {3, 4, 5}, k ≥ 1, we first extract a global vector g (k−1) i ∈ R Cm of Y (k−1) i by weighted global pool- ing: g (k−1) i = Λ (k−1) i Y (k−1) i ,<label>(12)</label></formula><p>where the weight matrix Λ (k−1) i ∈ R HW is derived from:</p><formula xml:id="formula_14">Λ (k−1) i = (sW 8 )(Y (k−1) i W 9 ) T ,<label>(13)</label></formula><p>where W 8 ∈ R C l ×C h and W 9 ∈ R Cm×C h are transforming matrices. Then a context vector c (k−1) i which contains multimodal context of Y (k−1) i is calculated by fusing s and g (k−1) i with a fully connected layer. We finally select information correlated with level i from features of other two levels to form the refined features of level i at round k:</p><formula xml:id="formula_15">Y (k) i =        Y (k−1) i + j∈{3,4,5}\{i} σ(c (k−1) i ) Y (k−1) j , k ≥ 1 Y i , k = 0 (14)</formula><p>where σ(·) denotes the sigmoid function. After n rounds of feature exchange, features of each level are mutually refined to fit the context referred to by the expression. We further fuse the output features Y   cluding UNC <ref type="bibr" target="#b45">[46]</ref>, UNC+ <ref type="bibr" target="#b45">[46]</ref>, G-Ref <ref type="bibr" target="#b32">[33]</ref> and ReferIt <ref type="bibr" target="#b18">[19]</ref>. UNC, UNC+ and G-Ref datasets are all collected based on MS-COCO <ref type="bibr" target="#b28">[29]</ref>. They contain 19, 994, 19, 992 and 26, 711 images with 142, 209, 141, 564 and 104, 560 referring expressions for over 50, 000 objects, respectively. UNC+ has no location words and G-Ref contains much longer sentences (average length of 8.4 words) than others (less than 4 words), making them more challenging than UNC dataset. ReferIt dataset is collected on IAPR TC-12 <ref type="bibr" target="#b8">[9]</ref> and contains 19, 894 images with 130, 525 expressions for 96, 654 objects (including stuff).</p><p>Implementation Details. We adopt DeepLab-101 [5] pretrained on PASCAL-VOC dataset <ref type="bibr" target="#b9">[10]</ref> as the CNN backbone following prior works <ref type="bibr" target="#b43">[44]</ref>[23] and use the output of Res3, Res4 and Res5 for multi-level feature fusion. Input images are resized to 320 × 320. Channel dimensions of features are set as C v = C l = C m = C h = 1000 and the cell size of ConvLSTM <ref type="bibr" target="#b41">[42]</ref> is set to 500. When comparing with other methods, the hyper-parameter r of bilinear fusion is set to 5 and the number of feature exchange rounds n is set to 3. GloVe word embeddings <ref type="bibr" target="#b34">[35]</ref> pretrained on Common Crawl 840B tokens are adopted following <ref type="bibr" target="#b2">[3]</ref>. Number of graph convolution layers is set to 2 on G-Ref dataset and 1 on others. The network is trained using Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with the initial learning rate of 2.5e −4 and weight decay of 5e −4 . Parameters of CNN backbone are fixed during training. The standard binary cross-entropy loss averaged over all pixels is leveraged for training. For fair comparison with prior works, Dense-CRF <ref type="bibr" target="#b21">[22]</ref> is adopted to refine the segmentation masks.</p><p>Evaluation Metrics. Following prior works <ref type="bibr" target="#b16">[17]</ref>[44] <ref type="bibr" target="#b2">[3]</ref>, overall Intersection-over-Union (Overall IoU) and Prec@X are adopted as metrics to evaluate our model. Overall IoU calculates total intersection regions over total union regions of all the test samples. Prec@X measures the percentage of predictions whose IoU are higher than the threshold X with X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-arts</head><p>To demonstrate the superiority of our method, we evaluate it on four referring segmentation benchmarks. Comparison results are presented in <ref type="table" target="#tab_0">Table 1</ref>. We follow prior works <ref type="bibr" target="#b43">[44]</ref>[3] to only report overall IoU due to the limit of pages. Full results are included in supplementary materials. As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, our method outperforms all the previous state-of-the-arts on four benchmarks with large margins. Comparing with STEP <ref type="bibr" target="#b2">[3]</ref> which densely fuses 5 levels of features for 25 times, our method exploits fewer levels of features and fusion times while consistently achieving 1.40%-2.82% performance gains on all the four datasets, demonstrating the effectiveness of our modules. In particular, our method yields 2.65% IoU boost against STEP on G-Ref val set, indicating that our method could better handle long sentences than those lack the ability of progressive comprehension. Besides, ReferIt is a challenging dataset and previous methods only have marginal improvements on it. For example, STEP and CMSA <ref type="bibr" target="#b43">[44]</ref> obtain only 0.33% and 0.17% improvements on ReferIt test set respectively, while our method enlarges the performance gain to 1.40%, which shows that our model can well generalize to multiple datasets with different characteristics. In addition, our method also outperforms MAttNet <ref type="bibr" target="#b44">[45]</ref> by a large margin in Overall IoU. Though MAttNet achieves higher precisions (e.g., 75.16% versus 71.72% in Prec@0.5 on UNC val set) than ours, it relies on Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> pretrained on noticeably more COCO <ref type="bibr" target="#b28">[29]</ref> images (110K) than ours pretrained on PASCAL-VOC <ref type="bibr" target="#b9">[10]</ref> images (10K). Therefore, it may not be completely fair to directly compare performances of MAttNet with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We perform ablation studies on UNC val set and G-Ref val set to testify the effectiveness of each proposed module.</p><p>Components of CMPC Module. We first explore the  <ref type="bibr" target="#b34">[35]</ref> to initialize the embedding layer, which is also adopted in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Results of rows 1 to 5 are all based on single-level features, i.e. Res5. Our baseline is implemented as simply concatenating the visual feature extracted with DeepLab-101 and linguistic feature extracted with an LSTM and making prediction on the fusion of them. As shown in row 2 of Table 2, including EP brings 1.70% IoU improvement over the baseline, indicating the perception of candidate entities are essential to the feature alignment between visual and linguistic modalities. In row 3, RAR alone brings 6.04% IoU improvement over baseline, which demonstrates that leveraging relational words as routers to reason among spatial regions could effectively highlight the referent in the image, thus boosting the performance notably. Combining EP with RAR, as shown in row 4, our CMPC module could achieve 55.38% IoU with single level features, outperforming baseline with a large margin of 8.02% IoU. This indicates that our model could accurately identity the referent by progressively comprehending the expression and image. Integrated with GloVe word embeddings, the IoU gain further achieves 8.64% with the aid of large-scale corpus. We further conduct ablation studies based on multi-level features in rows 6 to 11 of <ref type="table" target="#tab_1">Table 2</ref>. Row 6 is the multilevel version of row 1 using ConvLSTM to fuse multi-level features. The TGFE module in rows 7 to 11 is based on single round of feature exchange. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our model performs consistently with the single level version, which well proves the effectiveness of our CMPC module. TGFE module.  Number of Graph Convolution Layer. In <ref type="table">Table 4</ref>, we explore the number of graph convolution layers in CMPC module based on single-level features. n is the number of graph convolution layers in CMPC. Results on UNC val set show that more graph convolution layers leads to performance degradation. However, on G-Ref val set, 2 layers of graph convolution in CMPC achieves better performance than 1 layer while 3 layers decreasing the performance. As the average length of expressions in G-Ref <ref type="bibr">(8.4</ref> words) is much longer than that of UNC (&lt; 4 words), we suppose that stacking more graph convolution layers in CMPC can appropriately improve the reasoning effect for longer referring expressions. However, too many graph convolution layers may introduce noises and harm the performance.</p><p>Qualitative Results. We presents qualitative comparison between the multi-level baseline model and our full Expression: "girl on phone" Expression: "big green suitcase" Expression: "stander in darker pants" Expression: "left cup"  <ref type="table" target="#tab_1">Table 2</ref>). (c) Results predicted by our full model (row 11 in <ref type="table" target="#tab_1">Table 2</ref>). (d) Ground-truth.  model in <ref type="figure" target="#fig_5">Figure 4</ref>. From the top-left example we can observe that the baseline model fails to make clear judgement between the two girls, while our full model is able to distinguish the correct girl having relationship with the phone, indicating the effectiveness of our CMPC module. Similar result is shown in the top-right example of <ref type="figure" target="#fig_5">Figure 4</ref>. As illustrated in the bottom row of <ref type="figure" target="#fig_5">Figure 4</ref>, attributes and location relationship can also be well handled by our full model. Visualization of Affinity Maps. We visualize the affinity maps between multimodal feature and the first word in the expression in <ref type="figure" target="#fig_7">Figure 5</ref>. As shown in (b) and (c), our model is able to progressively produce more concentrated responses on the referent as the expression becomes more informative from only entity words to the full sentence. Interestingly, when we manually modify the expression to refer to other entities in the image, our model is still able to correctly comprehend the new expression and identify the referent. For example, in the third row of <ref type="figure" target="#fig_7">Figure 5</ref> </p><formula xml:id="formula_16">(a) (b) (c) (d) (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>To address the referring image segmentation problem, we propose a Cross-Modal Progressive Comprehension (CMPC) module which first perceives candidate entities considered by the expression using entity and attribute words, then conduct graph-based reasoning with the aid of relational words to further highlight the referent while suppressing others. We also propose a Text-Guided Feature Exchange (TGFE) module which exploits textual information to selectively integrate features from multiple levels to refine the mask prediction. Our model consistently outperforms previous state-of-the-art methods on four benchmarks, demonstrating its effectiveness. In the future, we plan to analyze the linguistic information more structurally and explore more compact graph formulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Interpretation of our progressive referring segmentation method. (a) Input referring expression and image. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our proposed method. Visual features and linguistic features are first progressively aligned by our Cross-Modal Progressive Comprehension (CMPC) module. Then multi-level multimodal features are fed into our Text-Guided Feature Exchange (TGFE) module for information communication across different levels. Finally, multi-level features are fused with ConvLSTM for final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our Cross-Modal Progressive Comprehension module which consists of two stages. First, visual features X are bilinearly fused with linguistic features q of entity words and attribute words for Entity Perception (EP) stage. Second, multimodal features M from EP stage are fed into Relation-Aware Reasoning (RAR) stage for feature enhancement. A multimodal fully-connected graph G is constructed with each vertex corresponds to an image region on M . The adjacency matrix of G is defined as the product of the matching degrees between vertexes and relational words in the expression. Graph convolution is utilized to reason among vertexes so that the referent could be highlighted during the interaction with correlated vertexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>] for harvesting the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of referring image segmentation. (a) Original image. (b) Results predicted by the multi-level baseline model (row 6 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of affinity maps between images and expressions in our model. (a) Original image. (b)(c) Affinity maps of only entity words and full expressions in the test samples. (d) Ground-truth. (e) Affinity maps of expressions manually modified by us.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(e), when the expression changes from "Donut at the bottom" to "Donut at the left", high response area shifts from bottom donut to the left donut according to the expression. It indicates that our model can adapt to new expressions flexibly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>45.69 45.57 29.86 30.48 29.50 34.52 58.73 DMN [34] 49.78 54.83 45.13 38.88 44.22 32.29 36.76 Comparison with state-of-the-art methods on four benchmark datasets using overall IoU as metric. "n/a" denotes MAttNet does not use the same split as other methods.</figDesc><table><row><cell>. Experiments</cell></row><row><cell>4.1. Experimental Setup</cell></row><row><cell>Datasets. We conduct extensive experiments on four</cell></row><row><cell>benchmark datasets for referring image segmentation in-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on UNC val set. *Row 6 is the multi-level version of row 1 using only ConvLSTM for fusion. EP and RAR indicate entity perception stage and relation-aware reasoning stage in our CMPC module respectively.</figDesc><table><row><cell></cell><cell cols="10">EP RAR TGFE GloVe Prec@0.5 Prec@0.6 Prec@0.7 Prec@0.8 Prec@0.9 Overall IoU</cell></row><row><cell>1 2 3 4 5</cell><cell>√ √ √</cell><cell>√ √ √</cell><cell></cell><cell>√</cell><cell>48.01 49.76 59.32 62.86 62.87</cell><cell>37.98 40.35 51.16 54.54 54.91</cell><cell>27.92 30.15 40.59 44.10 44.16</cell><cell>16.30 17.84 26.50 28.65 28.43</cell><cell>3.72 4.16 6.66 7.24 7.23</cell><cell>47.36 49.06 53.40 55.38 56.00</cell></row><row><cell>6* 7 8 9 10 11</cell><cell>√ √ √</cell><cell>√ √ √</cell><cell>√ √ √ √ √</cell><cell>√</cell><cell>63.12 67.63 68.39 69.37 71.04 71.27</cell><cell>54.56 59.80 60.92 62.28 64.02 64.44</cell><cell>44.20 49.72 50.70 52.66 54.25 55.03</cell><cell>28.75 34.45 35.24 36.89 38.45 39.28</cell><cell>8.51 10.62 11.13 11.27 11.99 12.89</cell><cell>56.38 58.81 59.05 59.62 60.72 61.19</cell></row><row><cell cols="7">effectiveness of each component of our proposed CMPC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">module and the experimental results are shown in Ta-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ble 2. EP and RAR denotes the entity perception stage and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">relation-aware reasoning stage in CMPC module respec-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tively. GloVe means using GloVe word embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>presents the ablation results of TGFE module. n is the number of feature exchange rounds. The experiments are based on multi-level features with CMPC module. It is shown that only one round of feature exchange in TGFE could improve the IoU from 59.85% to 60.72%. When we increase the rounds of feature exchange in TGFE, the IoU increases as well, which well proves the effectiveness of our TGFE module. We further evaluate TGFE module on baseline model and the comparing results are shown in row 6 and row 7 ofTable 2. TGFE with single round of feature exchange improves the IoU from 56.38% to 58.81%, indicating that our TGFE module can effectively utilize rich contexts in multi-level features.</figDesc><table><row><cell>CMPC only</cell><cell>+TGFE n = 1 n = 2 n = 3</cell></row><row><cell>59.85</cell><cell>60.72 61.07 61.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Overall IoUs of different numbers of feature exchange rounds in TGFE module on UNC val set. n denotes the number of feature exchange rounds. UNC val 49.06 55.38 51.57 50.70 G-Ref val 36.50 38.19 40.12 38.96 Experiments of graph convolution on UNC val set and G-Ref val set in terms of overall IoU. n denotes the number of graph convolution layers in our CMPC module. Experiments are all conducted on single level features.</figDesc><table><row><cell>Dataset</cell><cell>CMPC n = 0 n = 1 n = 2 n = 3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Referring expression object segmentation with caption-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The segmented and annotated iapr tc-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hugo Jair Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villaseñor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02037</idno>
		<title level="m">Adversarialnas: Adversarial neural architecture search for gans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Psgan: Pose and expression robust spatial-aware gan for customizable makeup transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1909.06956</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gps: Group people segmentation with detailed part inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefei</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A real-time cross-modality correlation filtering method for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07072</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-grained humancentric tracklet segmentation with single frame supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Referring image segmentation by generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene graph generation with hierarchical context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-modal relationship inference for grounding referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Latentgnn: Learning efficient non-local relations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reasoning visual dialogs with structural and partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
