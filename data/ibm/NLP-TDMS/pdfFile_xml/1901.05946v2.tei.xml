<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>ZÃ¼rich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<title level="a" type="main">Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="sk">
<div xmlns="http://www.tei-c.org/ns/1.0"> arXiv:1901.05946v2 [cs.CV]  </div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixellevel annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertaintyaware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The state of the art in semantic segmentation is rapidly improving in recent years. Despite the advance, most meth-ods are designed to operate at daytime, under favorable illumination conditions. However, many outdoor applications require robust vision systems that perform well at all times of day, under challenging lighting conditions, and in bad weather <ref type="bibr" target="#b21">[22]</ref>. Currently, the popular approach to solving perceptual tasks such as semantic segmentation is to train deep neural networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref> using large-scale human annotations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. This supervised scheme has achieved great success for daytime images, but it scales badly to adverse conditions. In this work, we focus on semantic segmentation at nighttime, both at the method level and the evaluation level.</p><p>At the method level, this work adapts semantic segmentation models from daytime to nighttime, without annotations in the latter domain. To this aim, we propose a new method called Guided Curriculum Model Adaptation (GCMA). The underpinnings of GCMA are threefold: power of time, power of place, and power of data. Time: environmental illumination changes continuously from daytime to nighttime. This enables adding intermediate domains between the two to smoothly transfer semantic knowledge. This idea is found to be effective in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>; we extend it by adding two more modules. Place: images taken over different time but with the same 6D camera pose share a large portion of content. The shared content can be used to guide the knowledge transfer process from a favorable condition (daytime) to an adverse condition (nighttime). We formalize this observation and propose a solution for large-scale application. Data: GCMA takes advantage of the powerful image translation techniques to stylize large-scale real annotated daytime datasets to darker target domains in order to perform standard supervised learning.</p><p>The adversity of nighttime poses further challenges for perceptual tasks compared to daytime. The extracted features become corrupted due to visual hazards <ref type="bibr" target="#b40">[41]</ref> such as underexposure, noise, and motion blur. The degradation of affected input regions is often so intense that they are rendered indiscernible, i.e. determining their semantic content is impossible even for humans. We term such regions as invalid for the task of semantic segmentation. A robust model should predict with high uncertainty on invalid regions while still being confident on valid (discernible) regions, and a sound evaluation framework should reward such behavior. The above requirement is particularly significant for safety-oriented applications such as autonomous cars, since having the vision system declare a prediction as invalid can help the downstream driving system avoid the fatal consequences of this prediction being false, e.g. when a pedestrian is missed.</p><p>To this end, we design a generic uncertainty-aware annotation and evaluation framework for semantic segmentation in adverse conditions which explicitly distinguishes invalid from valid regions of input images, and apply it to nighttime. On the annotation side, our novel protocol leverages privileged information in the form of daytime counterparts of the annotated nighttime scenes, which reveal a large portion of the content of invalid regions. This allows to reliably label invalid regions and to indeed include invalid regions in the evaluation, contrary to existing semantic segmentation benchmarks <ref type="bibr" target="#b5">[6]</ref> which completely exclude them from evaluation. Moreover, apart from the standard classlevel semantic annotation, each image is annotated with a mask which designates its invalid regions. On the evaluation side, we allow the invalid label in predictions and adopt from <ref type="bibr" target="#b39">[40]</ref> the principle that for invalid pixels with legitimate semantic labels, both these labels and the invalid label are considered correct predictions. However, this principle does not cover the case of valid regions. We address this by introducing the concept of false invalid predictions. This enables calculation of uncertainty-aware intersection-overunion (UIoU), a joint performance metric for valid and invalid regions which generalizes standard IoU, reducing to the latter when no invalid prediction exists. UIoU rewards predictions with confidence that is consistent to human annotators, i.e. with higher confidence on valid regions than invalid ones, meeting the aforementioned requirement.</p><p>Finally, we present Dark Zurich, a dataset of real images which contains corresponding images of the same driving scenes at daytime, twilight and nighttime. We use this dataset to feed real data to GCMA and to create a benchmark with 151 nighttime images for our uncertainty-aware evaluation. Our dataset and code are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision at Nighttime. Nighttime has attracted a lot of attention in the literature due to its ubiquitous nature. Several works pertain to human detection at nighttime, using FIR cameras <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>, visible light cameras <ref type="bibr" target="#b14">[15]</ref>, or a combination of both <ref type="bibr" target="#b3">[4]</ref>. In driving scenarios, a few methods have been proposed to detect cars <ref type="bibr" target="#b16">[17]</ref> and vehicles' 1 https://trace.ethz.ch/projects/adverse/GCMA_UIoU rear lights <ref type="bibr" target="#b29">[30]</ref>. Contrary to these domain-specific methods, previous work also includes both methods designed for robustness to illumination changes, by employing domaininvariant representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> or fusing information from complementary modalities and spectra <ref type="bibr" target="#b32">[33]</ref>, and datasets with adverse illumination <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref> for localization benchmarking. A recent work <ref type="bibr" target="#b7">[8]</ref> on semantic nighttime segmentation shows that images captured at twilight are helpful for supervision transfer from daytime to nighttime. Our work is partially inspired by <ref type="bibr" target="#b7">[8]</ref> and extends it by proposing a guided curriculum adaptation framework which learns jointly from stylized images and unlabeled real images of increasing darkness and exploits scene correspondences.</p><p>Domain Adaptation. Performance of semantic segmentation on daytime scenes has increased rapidly in recent years. As a consequence, attention is now turning to adaptation to adverse conditions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. A case in point are recent efforts to adapt clear-weather models to fog <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, by using both labeled synthetic images and unlabeled real images of increasing fog density. This work instead focuses on the nighttime domain, which poses very different andas we would claim-greater challenges than the foggy domain (e.g. artificial light sources casting very different illumination patterns at night). A major class of adaptation approaches, including <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>, involves adversarial confusion or feature alignment between domains. The general concept of curriculum learning has been applied to domain adaptation by ordering tasks <ref type="bibr" target="#b41">[42]</ref> or target-domain pixels <ref type="bibr" target="#b46">[47]</ref>, while we order domains. Crossdomain correspondences as guidance have only been used very recently in <ref type="bibr" target="#b17">[18]</ref>, which requires pixel-level matches, while we use more generic image-level correspondences.</p><p>Semantic Segmentation Evaluation. Semantic segmentation evaluation is commonly performed with the IoU metric <ref type="bibr" target="#b8">[9]</ref>. Cityscapes <ref type="bibr" target="#b5">[6]</ref> introduced an instance-level IoU (iIoU) to remove the large-instance bias, as well as mean average precision for the task of instance segmentation. The two tasks have recently been unified into panoptic segmentation <ref type="bibr" target="#b15">[16]</ref>, with a respective panoptic quality metric. The most closely related work to ours in this regard is Wild-Dash <ref type="bibr" target="#b39">[40]</ref>, which uses standard IoU together with a finegrained evaluation to measure the impact of visual hazards on performance. In contrast, we introduce UIoU, a new semantic segmentation metric that handles images with regions of uncertain semantic content and is suited for adverse conditions. Our uncertainty-aware evaluation is complementary to uncertainty-aware methods such as <ref type="bibr" target="#b13">[14]</ref> that explicitly incorporate uncertainty in their model formulation and aims to promote the development of such methods, as UIoU rewards models that accurately capture heteroscedastic aleatoric uncertainty <ref type="bibr" target="#b13">[14]</ref> in the input images through the different treatment of invalid and valid regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Guided Curriculum Model Adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>GCMA involves a source domain S, an ultimate target domain T , and an intermediate target domainá¹ª . In this work, S is daytime, T is nighttime, andá¹ª is twilight time with an intermediate level of darkness between S and T . GCMA adapts semantic segmentation models through this sequence of domains (S,á¹ª , T ), which is sorted in ascending order with respect to level of darkness. The approach proceeds progressively and adapts the model from one domain in the sequence to the next. The knowledge is transferred through the domain sequence via this gradual adaptation process. The transfer is performed using two coupled branches: 1) learning from labeled synthetic stylized images and 2) learning from real data without annotations, to jointly leverage the assets of both. Stylized images inherit the human annotations of their original counterparts but contain unrealistic artifacts, whereas real images have less reliable pseudo-labels but are characterized by artifactfree textures.</p><p>Let us use z â {1, 2, 3} as the index in (S,á¹ª , T ). Once the model for the current domain z is trained, its knowledge can be distilled on unlabeled real data from z, and then used, along with a new version of synthetic data from the next domain z + 1 to adapt the current model to z + 1.</p><p>Before diving into the details, we first define all datasets used. The inputs for GCMA consist of: 1) a labeled daytime set with M real images D 1</p><formula xml:id="formula_0">lr = {(I 1 m , Y 1 m )} M m=1 , e.g. Cityscapes [6], where Y 1 m (i, j) â C = {1, ..., C} is the ground-truth label of pixel (i, j) of I 1 m ; 2) an unlabeled day- time set of N 1 images D 1 ur = {I 1 n } N1 n=1 ; 3) an unlabeled twilight set of N 2 images D 2 ur = {I 2 n } N2 n=1 ; and 4) an un- labeled nighttime set of N 3 images D 3 ur = {I 3 n } N3 n=1 .</formula><p>In order to perform knowledge transfer with annotated data, D 1 lr is rendered in the style of D 2 ur and D 3 ur . We use Cycle-GAN <ref type="bibr" target="#b44">[45]</ref> to perform this style transfer, leading to two more sets:</p><formula xml:id="formula_1">D 2 ls = {(Äª 2 m , Y 1 m )} M m=1 and D 3 ls = {(Äª 3 m , Y 1 m )} M m=1 , whereÄª 2</formula><p>m andÄª 3 m are the stylized twilight and nighttime version of I 1 m respectively, and labels are copied. For z = 1, the semantic segmentation model Ï 1 is trained directly on D 1 lr . In order to perform knowledge transfer with unlabeled data, pseudo-labels for all three unlabeled real datasets need to be generated. The pseudo-labels for D 1 ur are generated using the model Ï 1 viaÅ¶ 1 n = Ï 1 (I 1 n ). For z &gt; 1, training Ï z and generatingÅ¶ z m is performed progressively as GCMA proceeds, as is detailed in Sec. 3.1.1. All six datasets are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Guided Curriculum Model Adaptation</head><p>Since the method proceeds in an iterative manner, we present the algorithmic details only for a single adaptation <ref type="table">Table 1</ref>. The training sets used in GCMA. I indicates an image and Y its label map;Äª is a synthetic image andÅ¶ a pseudo-label map. See the text for details.</p><formula xml:id="formula_2">Labeled Unlabeled Real Synthetic Real 1. Daytime {(I 1 m , Y 1 m )} M m=1 {(I 1 n ,Å¶ 1 n )} N 1 n=1 2. Twilight time {(Äª 2 m , Y 1 m )} M m=1 {(I 2 n ,Å¶ 2 n )} N 2 n=1 3. Nighttime {(Äª 3 m , Y 1 m )} M m=1 {(I 3 n ,Å¶ 3 n )} N 3 n=1 step from z â 1 to z.</formula><p>The presented algorithm is straightforward to generalize to multiple intermediate target domains. In order to adapt the semantic segmentation model Ï zâ1 from the previous domain z â 1 to the current domain z, we generate synthetic stylized data in domain z: D z ls . For real unlabeled images, since no human annotations are available, we rely on a strategy of self-learning or curriculum learning. Our motivating assumption is that objects are generally easier to recognize in lighter conditions, so the tasks are solved in ascending order with respect to the level of darkness and the easier, solved tasks are used to retrain the model to further solve the harder tasks. This is in line with the concept of curriculum learning <ref type="bibr" target="#b1">[2]</ref>. In particular, the model Ï zâ1 for domain z â 1 can be applied to the unlabeled real images of domain z â 1 to generate supervisory labels for training Ï z . Specifically, the dataset of real images with pseudo-labels for adaptation to domain z</p><formula xml:id="formula_3">is D zâ1 ur = {(I zâ1 n ,Å¶ zâ1 n )} Nzâ1 n=1 , whereÅ¶ zâ1 n</formula><p>denotes the predicted labels of image I zâ1 n . A simple way to get these labels is by directly feeding I zâ1 n to Ï zâ1 , similar to the approach of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> for the case of fog. This choice, however, suffers from accumulation of substantial errors in the prediction of Ï zâ1 into the subsequent training step if domain z â 1 is not the daytime domain. We instead propose a method to refine these errors by using guidance from the semantics of a daytime image I 1 n that corresponds to I zâ1 n , i.e. depicts roughly the same scene as I zâ1 n (the difference in the camera pose is small):</p><formula xml:id="formula_4">Y zâ1 n = G Ï zâ1 (I zâ1 n ), I zâ1 n , Ï 1 (I 1 Azâ1â1(n) ) ,<label>(1)</label></formula><p>where G is a guidance function which will be defined in Sec. 3.2 and z â 1 &gt; 1. A zâ1â1 (n) is the correspondence function giving the index of the daytime image that corresponds to I zâ1 n . Once we have the two training sets D zâ1 ur (with labels inferred through (1)) and D z ls , learning Ï z is performed by optimizing a loss function that involves both datasets:</p><formula xml:id="formula_5">min Ï z (I,Y ) âD z ls L(Ï z (I), Y ) + Âµ (I,Å¶ ) âD zâ1 ur L(Ï z (I),Å¶ ) ,<label>(2)</label></formula><p>where L(., .) is the cross entropy loss and Âµ is a hyperparameter balancing the contribution of the two datasets.</p><p>In order to leverage the place prior at large scale to improve predictions through the guided label refinement defined in (1), specific aligned datasets need to be compiled. With this aim, we collected the Dark Zurich dataset by driving several laps in disjoint areas of Zurich; each lap was driven multiple times during the same day, starting from daytime through twilight to nighttime. The recordings include GPS readings and are split into three sets: daytime, twilight and nighttime (cf. Sec. 5). Since different drives of the same lap correspond to the same route, the camera orientation at a certain point of the lap is similar across all drives. We implement the correspondence function A zâ1 that assigns to each image in domain z its daytime counterpart using a GPS-based nearest neighbor assignment. The method presented in Sec. 3.2 carefully handles the effects of misalignment and dynamic objects in paired images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Guided Segmentation Refinement</head><p>In the following presentation of our guided segmentation refinement for dark images using corresponding daytime images, we drop for brevity the subscript which was used to indicate this correspondence. The guidance function G which models our refinement approach and was introduced in a general form in (1) can be written more specifically as</p><formula xml:id="formula_6">G Ï z (I z ), I z , Ï 1 (I 1 ) = R Ï z (I z ), B(Ï 1 (I 1 ), I z ) ,<label>(3)</label></formula><p>i.e. as the composition of a cross bilateral filter B on the daytime predictions, which aligns them to the dark image, with a fusion function R, which adaptively combines the aligned daytime predictions with the initial dark image predictions to refine the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cross Bilateral Filter for Prediction Alignment</head><p>The correspondences between real images that are used in GCMA are not perfect, in the sense that they are not aligned at a pixel-accurate level. Therefore, to leverage the prediction for the daytime image I 1 as guidance for refining the respective prediction for the dark image I z , it is necessary to first align the former prediction to I z . To this end, we operate on soft predictions and define a cross bilateral filter on the initial soft prediction map S 1 = Ï 1 (I 1 ) which uses the color of the dark image I z as reference:</p><formula xml:id="formula_7">S 1 (p) = qâN (p) G Ïs ( q â p )G Ïr ( I z (q) â I z (p) )S 1 (q) qâN (p) G Ïs ( q â p )G Ïr ( I z (q) â I z (p) ) .<label>(4)</label></formula><p>In <ref type="formula" target="#formula_7">(4)</ref>, p and q denote pixel positions, N (p) is the neighborhood of p, G Ïs is the spatial-domain Gaussian kernel and G Ïr is the color-domain kernel. The definition of the filter implies that only pixels q with similar color to the examined pixel p in the dark image I z contribute to the output S 1 (p), which shifts salient edges in the initial daytime prediction to their correct position in the dark image. For the color-domain kernel, we use the CIELAB version of I z , as it is more appropriate for measuring color similarity <ref type="bibr" target="#b23">[24]</ref>. We set the spatial parameter Ï s to 80 to account for large misalignment, and Ï r to 10 following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Confidence-Adaptive Prediction Fusion</head><p>The final step in our refinement approach is to fuse the aligned predictionS 1 for I 1 with the initial prediction S z = Ï z (I z ) for I z in order to obtain the refined predic-tionÅ z , the hard version of which is subsequently used in training. We propose an adaptive fusion scheme, which uses the confidence associated with the two predictions at each pixel to weigh their contribution in the output and addresses disagreements due to dynamic content by properly adjusting the fusion weights. Let us denote the confidence of the aligned predictionS 1 for I 1 at pixel p by F 1 (p) = max câCS 1 c (p) and respectively the confidence of the initial prediction S z for I z by F z (p). Our confidenceadaptive fusion is then defined aÅ</p><formula xml:id="formula_8">S z = F z F z + Î±F 1 S z + Î±F 1 F z + Î±F 1S 1 ,<label>(5)</label></formula><p>where 0 &lt; Î± = Î±(p) â¤ 1 may vary and we have completely dropped the pixel argument p for brevity. In this way, we allow the daytime image prediction to have a greater effect on the output at regions of the dark image which were not easy for model Ï z to classify, while preserving the initial prediction S z at lighter regions of the dark image where S z is more reliable. Our fusion distinguishes between dynamic and static scene content by regulating Î±. In particular, Î± downweights S 1 to induce a preference towards S z when both predictions have high confidence. However, apart from imperfect alignment, the two scenes also differ due to dynamic content. Intuitively, the prediction of a dynamic object in the daytime image should be assigned an even lower weight in case the corresponding prediction in the dark image does not agree, since this object might only be present in the former scene. More formally, we denote the subset of C that includes dynamic classes by C d and define</p><formula xml:id="formula_9">Î±(p) = ï£± ï£´ ï£² ï£´ ï£³ Î± l , if c 1 = arg max câCS 1 c (p) â C d and S z c1 (p) â¤ Î· or c 2 = arg max câC S z c (p) â C d andS 1 c2 (p) â¤ Î·, Î± h otherwise.<label>(6)</label></formula><formula xml:id="formula_10">(a) Dark image I z (b) Daytime image I 1 (c) Initial prediction S z for I z (d)</formula><p>Our refined predictionÅ z for I z <ref type="figure">Figure 1</ref>. Example pair of corresponding images from Dark Zurich, initial prediction for the dark image and our refined prediction.</p><p>In our experiments, we manually tune Î± l = 0.3, Î± h = 0.6 and Î· = 0.2 on a couple of training images (no grid search). A result of our guided refinement is shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Uncertainty-Aware Evaluation</head><p>Images taken under adverse conditions such as nighttime contain invalid regions, i.e. regions with indiscernible semantic content. Invalid regions are closely related to the concept of negative test cases which was considered in <ref type="bibr" target="#b39">[40]</ref>. However, invalid regions constitute intra-image entities and can co-exist with valid regions in the same image, whereas a negative test case refers to an entire image that should be treated as invalid. We build upon the evaluation of <ref type="bibr" target="#b39">[40]</ref> for negative test cases and generalize it to be applied uniformly to all images in the evaluation set, whether they contain invalid regions or not. Our annotation and evaluation framework includes invalid regions in the set of evaluated pixels, but treats them differently from valid regions to account for the high uncertainty of their content. In the following, we elaborate on the generation of ground-truth annotations using privileged information through the day-night correspondences of our dataset and present our UIoU metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Annotation with Privileged Information</head><p>For each image I, the annotation process involves two steps: 1) creation of the ground-truth invalid mask J, and 2) creation of the ground-truth semantic labeling H.</p><p>For the semantic labels, we consider a predefined set C of C classes, which is equal to the set of Cityscapes <ref type="bibr" target="#b5">[6]</ref> evaluation classes (C = 19). The annotator is first presented only with I and is asked to mark the valid regions in it as the regions which she can unquestionably assign to one of the C classes or declare as not belonging to any of them. The result of this step is the invalid mask J, which is set to 0 at valid pixels and 1 at invalid pixels.</p><p>Secondly, the annotator is asked to mark the semantic labels of I, only that this time she also has access to an auxiliary image I . This latter image has been captured with roughly the same 6D camera pose as I but under more favorable conditions. In our dataset, I is captured at daytime whereas I is captured at nighttime. The large overlap of static scene content between the two images allows the annotator to label certain regions in H with a legitimate semantic label from C, even though the same regions have been annotated as invalid (and are kept as such) in J. This allows joint evaluation on valid and invalid regions, as it creates regions which can accept both the invalid label and the ground-truth label from C as correct predictions. Due to the imperfect match of the camera poses for I and I , the labeling of invalid regions in H is done conservatively, marking a coarse boundary which may leave unlabeled zones around the true semantic boundaries in I, so that no pixel is assigned a wrong label. The parts of I which remain indiscernible even after inspection of I are left unlabeled in H. These parts as well as instances of classes outside C are not considered during evaluation. We illustrate a visual example of our annotation inputs and outputs in <ref type="figure" target="#fig_0">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Uncertainty-Aware Predictions</head><p>The semantic segmentation prediction that is fed to our evaluation is expected to include pixels labeled as invalid. Instead of defining a separate, explicit invalid class, which would potentially require the creation of new training data to incorporate this class, we allow a more flexible approach for soft predictions with the original set of semantic classes by using a confidence threshold, which affords an evaluation curve for our UIoU metric by varying this threshold.</p><p>In particular, we assume that the evaluated method outputs an intermediate soft prediction S(p) at each pixel p as a probability distribution among the C classes, which is subsequently converted to a hard assignment by outputting the classH(p) = arg max câC {S c (p)} with the highest probability. In this case, SH (p) (p) â [1/C, 1] is the effective confidence associated with the prediction. This assumption is not very restrictive, as most recent semantic segmentation methods are based on CNNs with a softmax layer that outputs such soft predictions.</p><p>The final evaluated outputÄ¤ is computed based on a free parameter Î¸ â [1/C, 1] which acts as a confidence threshold by invalidating those pixels where the confidence of the prediction is lower than Î¸, i.e.Ä¤(p) =H(p) if SH (p) (p) â¥ Î¸ and invalid otherwise. Increasing Î¸ results in more pixels being predicted as invalid. This approach is motivated by the fact that ground-truth invalid regions are identified during annotation by the uncertainty of their se- mantic content, which implies that a model should ideally place lower confidence (equivalently higher uncertainty) in predictions on invalid regions than on valid ones, so that the former get invalidated for lower values of Î¸ than the latter. The formulation of our UIoU metric rewards this behavior as we shall see next. Note that our evaluation does not strictly require soft predictions, as UIoU can be normally computed for fixed, hard predictionsÄ¤.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">UIoU</head><p>We propose UIoU as a generalization of the standard IoU metric for evaluation of semantic segmentation predictions which may contain pixels labeled as invalid. UIoU reduces to standard IoU if no pixel is predicted to be invalid, e.g. when Î¸ = 1/C.</p><p>The calculation of UIoU for class c involves five sets of pixels, which are listed along with their symbols: true positives (TP), false positives (FP), false negatives (FN), true invalids (TI), and false invalids (FI). Based on the groundtruth invalid masks J, the ground-truth semantic labelings H and the predicted labelsÄ¤ for the set of evaluation images, these five sets are defined as follows: </p><formula xml:id="formula_11">TP = {p : H(p) =Ä¤(p) = c},<label>(7)</label></formula><p>UIoU for class c is then defined as</p><formula xml:id="formula_13">UIoU = |TP| + |TI| |TP| + |TI| + |FP| + |FN| + |FI| .<label>(12)</label></formula><p>Note that a true invalid prediction results in equal reward to predicting the correct semantic label of the pixel. Moreover, an invalid prediction does not come at no cost: it incurs the same penalty on valid pixels as predicting an incorrect label. When dealing with multiple classes, we modify our notation to UIoU (c) (similarly for the five sets of pixels related to class c), which we avoided in the previous definitions to reduce clutter. The overall semantic segmentation performance on the evaluation set is reported as the mean UIoU over all C classes. By varying the confidence threshold Î¸ and using the respective output, we obtain a parametric expression UIoU(Î¸). When Î¸ = 1/C, no pixel is predicted as invalid and thus UIoU(1/C) = IoU. We motivate the usage of UIoU instead of standard IoU in case the test set includes ground-truth invalid masks by showing in Th. 1 that UIoU is guaranteed to be larger than IoU for some Î¸ &gt; 1/C under the assumption that predictions on invalid regions are associated with lower confidence than those on valid regions, which lies in the heart of our evaluation framework. The proof is in Appendix A.</p><p>Theorem 1. Assume that there exist Î¸ 1 , Î¸ 2 such that Î¸ 1 &lt; Î¸ 2 , âp : J(p) = 1 â SH (p) (p) â¤ Î¸ 1 and J(p) = 0 â SH (p) (p) â¥ Î¸ 2 . If we additionally assume that âp â FN (c) (1/C) âª FP (c) (1/C) : J(p) = 1, then IoU (c) &lt; UIoU (c) (Î¸ 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Dark Zurich Dataset</head><p>Dark Zurich was recorded in Zurich using a 1080p Go-Pro Hero 5 camera, mounted on top of the front windshield of a car. The collection protocol with multiple drives of several laps to establish correspondences is detailed in Sec. 3.</p><p>We split Dark Zurich and reserve one lap for testing. The rest of the laps remain unlabeled and are used for training. They comprise 3041 daytime, 2920 twilight and 2416 nighttime images extracted at 1 fps, which are named Dark Zurich-{day, twilight, night} respectively and correspond to the three sets in the rightmost column of <ref type="table">Table 1</ref>. From the testing night lap, we extract one image every 50m or 20s, whichever comes first, and assign to it the corresponding daytime image to serve as the auxiliary image I in our annotation (cf. Sec. 4.1). We annotate 151 nighttime images with fine pixel-level Cityscapes labels and invalid masks following our protocol and name this set Dark Zurich-test. In total, 272.2M pixels have been annotated with semantic labels and 56.7M of these pixels are marked as invalid. We validate the quality of our annotations by having 20 images annotated twice by different subjects and measuring consistency. 93.5% of the labeled pixels are consistent in the semantic annotations and respectively 95% in the invalid masks. We compare to existing annotated nighttime sets in <ref type="table" target="#tab_0">Table 2</ref>, noting that most large-scale sets for road scene parsing, such as Cityscapes <ref type="bibr" target="#b5">[6]</ref> and Mapillary Vistas <ref type="bibr" target="#b22">[23]</ref>, contain few or no nighttime scenes. Nighttime Driving <ref type="bibr" target="#b7">[8]</ref> and Raincouver <ref type="bibr" target="#b31">[32]</ref> only include coarse annotations. Dark Zurich-test contains ten times more nighttime images than WildDash <ref type="bibr" target="#b39">[40]</ref>-the only other dataset with reliable fine nighttime annotations. Detailed inspection showed that â¼70% of the 345 densely annotated nighttime images of BDD100K <ref type="bibr" target="#b38">[39]</ref> contain severe labeling errors which render them unsuitable for evaluation, especially in dark regions we treat as invalid (e.g. sky is often mislabeled as building). Our annotation protocol helps avoid such errors by properly defining invalid regions and using daytime images to aid annotation, and Dark Zurich-test is an initial high-quality benchmark to promote our uncertainty-aware evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Our architecture of choice for implementing GCMA is RefineNet <ref type="bibr" target="#b19">[20]</ref>. We use the publicly available RefineNet-res101-Cityscapes model, trained on Cityscapes, as the baseline model to be adapted to nighttime. Throughout our experiments, we train this model with a constant learning rate of 5 Ã 10 â5 on mini-batches of size 1.</p><p>Comparison to Other Adaptation Methods. Our first experiment compares GCMA to state-of-the-art approaches for adaptation of semantic segmentation models to nighttime. To obtain the synthetic labeled datasets for GCMA, we stylize Cityscapes to twilight using a CycleGAN model that is trained to translate Cityscapes to Dark Zurichtwilight (respectively to nighttime with Dark Zurich-night). The real training datasets for GCMA are Dark Zurich-day, instantiating D 1 ur , and Dark Zurich-twilight, instantiating D 2 ur . Each adaptation step comprises 30k SGD iterations and uses Âµ = 1. For the second step, we apply our guided refinement to the labels of Dark Zurich-twilight that are predicted by model Ï 2 fine-tuned in the first step, using the correspondences of Dark Zurich-twilight to Dark Zurich-day.</p><p>We evaluate GCMA on Dark Zurich-test against the state-of-the-art adaptation approaches AdaptSegNet <ref type="bibr" target="#b30">[31]</ref> and DMAda <ref type="bibr" target="#b7">[8]</ref> and report standard IoU performance in <ref type="table" target="#tab_1">Table 3</ref>, including invalid pixels which are assigned a le-gitimate semantic label in the evaluation. We have trained AdaptSegNet to adapt from Cityscapes to Dark Zurichnight. For fair comparison, we also report the performance of the respective baseline Cityscapes models for each method. RefineNet is the common baseline of GCMA and DMAda. GCMA significantly outperforms the other methods for most classes and achieves a substantial 10% improvement in the overall mIoU score against the secondbest method. The improvement with GCMA is pronounced for classes which usually appear dark at nighttime, such as sky, vegetation, terrain and person, indicating that our method successfully handles large domain shifts from its source daytime domain. These findings are supported by visually assessing the predictions of the compared methods, as in the examples of <ref type="figure" target="#fig_2">Fig. 3</ref>. We repeat the above comparison on Nighttime Driving <ref type="bibr" target="#b7">[8]</ref> in <ref type="table" target="#tab_2">Table 4</ref> and show that GCMA generalizes very well to different datasets.</p><p>Ablation Study for GCMA. We measure the individual effect of the main components of GCMA in <ref type="table" target="#tab_3">Table 5</ref> by evaluating its ablated versions on Dark Zurich-test. Direct adaptation to nighttime in a single step using only Cityscapes images stylized as nighttime with CycleGAN is a strong baseline, due to the reliable ground-truth labels that accompany the stylized Cityscapes, its high diversity and the limited artifacts of CycleGAN-based translation. Adding our real images to the training algorithm and applying our twostage curriculum significantly improves upon this baseline. Finally, our guided segmentation refinement in the second step of GCMA brings an additional 2.6% benefit, as it corrects a lot of errors in the pseudo-labels of the real twilight images, which helps compute more reliable gradients from the corrected loss during the subsequent training.</p><p>Comparisons with UIoU. In <ref type="figure">Fig. 4</ref>, we use our novel UIoU metric to evaluate GCMA against DMAda and our baseline RefineNet model on Dark Zurich-test for varying confidence threshold Î¸ and plot the resulting mean UIoU(Î¸) curves. Note that standard mean IoU can be read out from the leftmost point of each curve. First, our expectation based on Th. 1 is confirmed for all methods, i.e. maximum UIoU values over the range of Î¸ are larger than IoU by ca. 3%. This implies that on Dark Zurich-test, these models generally have lower confidence on invalid regions than valid ones. Second, the comparative performance of the methods is the same across all values of Î¸ -GCMA substantially outperforms the other two-which shows that UIoU is generally consistent with standard IoU and is a suitable substitute of the latter in adverse settings where declaring the input as invalid is relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have introduced GCMA, a method to gradually adapt semantic segmentation models from day-    of GCMA, which substantially improves upon competing state-of-the-art methods. Finally, evaluation on our benchmark with UIoU shows that invalidating predictions is useful when the input includes ambiguous content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>Proof. For brevity in the proof, we drop the class superscript (c) which is used in the statement of the theorem. Firstly, we draw an association between pixel sets related to the standard IoU = UIoU(1/C) and their counterparts for UIoU defined in (7)- <ref type="bibr" target="#b10">(11)</ref>. In particular, the following holds true:</p><formula xml:id="formula_14">|TP(1/C)| + |FN(1/C)| = |TP(Î¸)| + |FN(Î¸)| + |TI(Î¸)| + |FI(Î¸)|, âÎ¸ â [1/C, 1].<label>(13)</label></formula><p>The first assumption of Th. 1 implies that FI(Î¸ 1 ) = â, because âÎ¸ &lt; Î¸ 2 (including Î¸ 1 ) there exists no false invalid pixel for the examined class. Thus, applying (13) for Î¸ = Î¸ 1 leads to</p><formula xml:id="formula_15">|TP(1/C)| = |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FN(Î¸ 1 )| â |FN(1/C)|.<label>(14)</label></formula><p>Secondly, we plug the proposition of the first assumption of the theorem into the proposition of the second assumption to obtain</p><formula xml:id="formula_16">(FN(1/C) âª FP(1/C)) \ (FN(Î¸ 1 ) âª FP(Î¸ 1 )) = â. (15)</formula><p>We further elaborate on <ref type="bibr" target="#b14">(15)</ref>  (16) Both terms on the left-hand side of (16) are nonnegative based on our previous observations, while at the same time <ref type="bibr" target="#b15">(16)</ref> implies that at least one of the two is strictly positive. To complete the proof, we distinguish between the two corresponding cases.</p><p>In the first case, the first term in <ref type="formula" target="#formula_4">(16)</ref> is strictly positive, so <ref type="bibr" target="#b13">(14)</ref> implies</p><formula xml:id="formula_17">|TP(1/C)| &lt; |TP(Î¸ 1 )| + |TI(Î¸ 1 )|.<label>(17)</label></formula><p>We establish the inequality we are after by writing</p><formula xml:id="formula_18">IoU = = |TP(1/C)| |TP(1/C)| + |FN(1/C)| + |FP(1/C)| = |TP(1/C)| |TP(Î¸ 1 )| + |FN(Î¸ 1 )| + |TI(Î¸ 1 )| + |FI(Î¸ 1 )| + |FP(1/C)| â¤ |TP(1/C)| |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FP(Î¸ 1 )| + |FN(Î¸ 1 )| + |FI(Î¸ 1 )| &lt;</formula><p>|TP(Î¸ 1 )| + |TI(Î¸ 1 )| |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FP(Î¸ 1 )| + |FN(Î¸ 1 )| + |FI(Î¸ 1 )| = UIoU(Î¸ 1 ),</p><p>where we have used the definition of IoU in the second line, <ref type="bibr" target="#b12">(13)</ref> in the third line, FP(Î¸ 1 ) â FP(1/C) in the fourth line, <ref type="bibr" target="#b16">(17)</ref> in the fifth line, and the definition of UIoU that has been introduced in (12) in the last line.</p><p>In the second case, the second term in (16) is strictly positive, which implies that |FP(1/C)| &gt; |FP(Î¸ 1 )|.</p><p>Besides, applying the nonnegativity of the first term in <ref type="bibr" target="#b15">(16)</ref> to <ref type="bibr" target="#b13">(14)</ref> leads to |TP(1/C)| â¤ |TP(Î¸ 1 )| + |TI(Î¸ 1 )|.</p><p>Similarly to the first case, we establish the inequality we are after by writing IoU = = |TP(1/C)| |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FP(1/C)| + |FN(Î¸ 1 )| + |FI(Î¸ 1 )| &lt; |TP(1/C)| |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FP(Î¸ 1 )| + |FN(Î¸ 1 )| + |FI(Î¸ 1 )| â¤ |TP(Î¸ 1 )| + |TI(Î¸ 1 )| |TP(Î¸ 1 )| + |TI(Î¸ 1 )| + |FP(Î¸ 1 )| + |FN(Î¸ 1 )| + |FI(Î¸ 1 )| = UIoU(Î¸ 1 ),</p><p>where we have used the definition of IoU as well as <ref type="formula" target="#formula_4">(13)</ref> in the second line, <ref type="bibr" target="#b18">(19)</ref> in the third line, <ref type="bibr" target="#b19">(20)</ref> in the fourth line, and the definition of UIoU in the last line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Results</head><p>In <ref type="figure">Fig. 5</ref>, we compare our GCMA approach against AdaptSegNet <ref type="bibr" target="#b30">[31]</ref> and DMAda <ref type="bibr" target="#b7">[8]</ref> on additional images from Dark Zurich-test, further demonstrating the superiority of GCMA. For these images, we also present our annotations for invalid masks and semantic labels, which show that a significant portion of ground-truth invalid regions is indeed assigned a reliable semantic label through our annotation protocol and can thus be included in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Configuration of Training Sets for GCMA</head><p>In <ref type="figure">Fig. 6, we</ref> show examples from the six training sets we introduced in Sec. 3.1, which are used for implementing GCMA. Cityscapes is used to instantiate the labeled sets, while Dark Zurich is used for the unlabeled sets.</p><p>More examples of Cityscapes images stylized to nighttime using a CycleGAN model <ref type="bibr" target="#b44">[45]</ref> that is trained to translate Cityscapes to Dark Zurich-night are presented in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Selection for Prediction Fusion</head><p>For our confidence-adaptive prediction fusion, we demonstrate the benefit of selecting Î± l &lt; Î± h &lt; 1-the rationale of which is exposed in Sec. 3.2.2-through a visual example in <ref type="figure">Fig. 8</ref>. i <ref type="figure">Figure 5</ref>. Examples of our annotations and qualitative semantic segmentation results on Dark Zurich-test. From top to bottom row: nighttime image, invalid mask annotation overlaid on the image (valid pixels are colored green), semantic annotation, AdaptSegNet <ref type="bibr" target="#b30">[31]</ref>, DMAda <ref type="bibr" target="#b7">[8]</ref>, and GCMA (ours).  <ref type="figure">Figure 8</ref>. Dark image I z from Dark Zurich and our refined predictionsÅ z for the region indicated by the red box for different values of the parameters involved in the proposed confidence-adaptive prediction fusion. When Î± l = Î± h , reducing Î± h to a value lower than 1, e.g. (b)â(c), reduces false positives and/or false negatives both for static and dynamic classes, e.g. pole, sidewalk, road and car. When Î± h &lt; 1, reducing Î± l to a value lower than Î± h , e.g. (c)â(d), improves accuracy on pixels that are assigned to a dynamic class in either prediction, e.g. car, because of the formulation of (6). Best viewed with zoom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Input image I (b) Auxiliary image I (c) GT invalid mask J (d) GT semantic labeling H Example input images from Dark Zurich-test and output annotations with our protocol. Valid pixels in J are marked green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FP</head><label></label><figDesc>= {p : H(p) = c andÄ¤(p) = c},(8)FN = {p : H(p) = c andÄ¤(p) / â {c, invalid}},(9)TI = {p : H(p) = c andÄ¤(p) = invalid and J(p) = 1}, (10) FI = {p : H(p) = c andÄ¤(p) = invalid and J(p) = 0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative semantic segmentation results on Dark Zurich-test. "AdaptSegNet" adapts from Cityscapes to Dark Zurich-night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>by observing that FN(1/C) â© FP(1/C) = â, FN(Î¸ 1 ) â FN(1/C) and FP(Î¸ 1 ) â FP(1/C) to arrive at (|FN(1/C)| â |FN(Î¸ 1 )|) + (|FP(1/C)| â |FP(Î¸ 1 )|) &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a) D 1 Figure 6 .Figure 7 .</head><label>167</label><figDesc>lr : Cityscapes (b) D 1 ur : Dark Zurich-day (c) D 2 ls : Cityscapes-twilight style (d) D 2 ur : Dark Zurich-twilight (e) D 3 ls : Cityscapes-nighttime style (f) D 3 ur : Dark Zurich-night Sample images from the training sets used in GCMA. ii Top row: Examples of images from Cityscapes (D 1 lr in GCMA), bottom row: corresponding images from Cityscapes-nighttime style (D 3 ls in GCMA). (a) Dark image I z (b) Î± l = Î± h = 1 (c) Î± l = Î± h = 0.6 (d) Î± l = 0.3, Î± h = 0.6, Î· = 0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Dark Zurich against related datasets with nighttime semantic annotations. "Night annot.": annotated nighttime images, "Invalid": can invalid regions get legitimate labels?</figDesc><table><row><cell>Dataset</cell><cell cols="4">Night annot. Classes Reliable GT Fine GT Invalid</cell></row><row><cell>WildDash [40]</cell><cell>13</cell><cell>19</cell><cell></cell><cell>Ã</cell></row><row><cell>Raincouver [32]</cell><cell>95</cell><cell>3</cell><cell>Ã</cell><cell>Ã</cell></row><row><cell>BDD100K [39]</cell><cell>345</cell><cell>19</cell><cell>Ã</cell><cell>Ã</cell></row><row><cell>Nighttime Driving [8]</cell><cell>50</cell><cell>19</cell><cell>Ã</cell><cell>Ã</cell></row><row><cell>Dark Zurich</cell><cell>151</cell><cell>19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison on Dark Zurich-test. AdaptSegNet-CityscapesâDZ-night denotes adaptation from Cityscapes to Dark Zurich-night. 29.1 48.6 21.3 14.3 34.3 36.8 29.9 49.4 13.8 0.4 43.3 50.2 69.4 18.4 0.0 27.6 34.9 11.9 32.1 Ours: GCMA 81.7 46.9 58.8 22.0 20.0 41.2 40.5 41.6 64.8 31.0 32.1 53.5 47.5 75.5 39.2 0.0 49.6 30.7 21.0 42.0</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>sidew.</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorc.</cell><cell>bicycle</cell><cell>mIoU</cell></row><row><cell>RefineNet [20]</cell><cell cols="20">68.8 23.2 46.8 20.8 12.6 29.8 30.4 26.9 43.1 14.3 0.3 36.9 49.7 63.6 6.8 0.2 24.0 33.6 9.3 28.5</cell></row><row><cell>AdaptSegNet-Cityscapes [31]</cell><cell cols="20">79.0 21.8 53.0 13.3 11.2 22.5 20.2 22.1 43.5 10.4 18.0 37.4 33.8 64.1 6.4 0.0 52.3 30.4 7.4 28.8</cell></row><row><cell cols="21">AdaptSegNet-CityscapesâDZ-night [31] 86.1 44.2 55.1 22.2 4.8 21.1 5.6 16.7 37.2 8.4 1.2 35.9 26.7 68.2 45.1 0.0 50.1 33.9 15.6 30.4</cell></row><row><cell>DMAda [8]</cell><cell>75.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison on Nighttime Driving<ref type="bibr" target="#b7">[8]</ref>. Read asTable 3.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell></row><row><cell>RefineNet [20]</cell><cell>31.5</cell></row><row><cell>AdaptSegNet-Cityscapes [31]</cell><cell>32.6</cell></row><row><cell>AdaptSegNet-CityscapesâDZ-night [31]</cell><cell>34.5</cell></row><row><cell>DMAda [8]</cell><cell>36.1</cell></row><row><cell>Ours: GCMA</cell><cell>45.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablations of GCMA on Dark Zurich-test, reporting mIoU. ] and GCMA on Dark Zurich-test. We evaluate mean UIoU across the entire range [1/C, 1] of confidence threshold Î¸. For each method, the point at which mean UIoU is maximized is marked black and labeled with this maximum mean UIoU value.</figDesc><table><row><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean UIoU</cell><cell></cell><cell>0.458</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.355</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.318</cell></row><row><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell cols="2">RefineNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DMAda</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>GCMA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Confidence threshold</cell><cell></cell><cell></cell></row><row><cell>Daytime baseline: RefineNet [20]</cell><cell>28.5%</cell><cell cols="10">Figure 4. Uncertainty-aware evaluation of RefineNet [20],</cell></row><row><cell cols="2">+direct CycleGAN adapt. (w/o real, w/o curriculum) 37.1%</cell><cell cols="2">DMAda [8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+GCMA w/o guided refinement</cell><cell>39.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+GCMA w/ guided refinement</cell><cell>42.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">time to nighttime with stylized data and unlabeled real data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">of increasing darkness, as well as UIoU, a novel evalua-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tion metric for semantic segmentation designed for images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with indiscernible content. We have also presented Dark</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zurich, a large-scale dataset of real scenes captured at mul-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tiple times of day with cross-time-of-day correspondences,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and annotated 151 nighttime scenes of it with a new proto-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">col which enables our evaluation. Detailed evaluation with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">standard IoU on real nighttime sets demonstrates the merit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is funded by Toyota Motor Europe via the research project TRACE-ZÃ¼rich. We thank Simon Hecker for his advice on decoding GoPro GPS data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Road detection based on illuminant invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="193" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking image sensors under adverse weather conditions for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bijelic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Night-time pedestrian detection by visual-infrared video fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Congress on Intelligent Control and Automation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ROAD: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<meeting><address><addrLine>1, 2, 6, 7, 8, i, ii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection and tracking at nighttime for driver-assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural network-based human detection in nighttime images using visible light camera sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayes saliency-based object proposal generator for nighttime traffic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="825" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cross-season correspondence dataset for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RefineNet: Multipath refinement networks with identity mappings for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The Oxford RobotCar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>BulÃ²</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast approximation of the bilateral filter using a signal processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="52" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised image transformation for outdoor semantic labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking 6DOF outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Looking at vehicles in the night: Detection and dynamics of rear lights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>2, 7, 8, i, ii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Raincouver scene parsing benchmark for self-driving in adverse weather and at night</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AdapNet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DCAN: Dual channelwise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Addressing appearance change in outdoor robotics with adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pedestrian detection and tracking with night vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">BDD100K: a diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">WildDash -creating hazardaware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Fernandez</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How good is my test data? Introducing safety analysis for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Herzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
