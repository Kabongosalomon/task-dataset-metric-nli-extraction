<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
							<email>yxma@cs.hku.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<email>zhuxinge123@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
							<email>sibozhang@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>yangruigang@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
							<email>wenping@cs.hku.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc. 1</orgName>
								<orgName type="institution" key="instit3">The University of Hong</orgName>
								<address>
									<addrLine>Kong 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 3</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland at College</orgName>
								<address>
									<addrLine>Park 4</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To safely and efficiently navigate in complex urban traffic, autonomous vehicles must make responsible predictions in relation to surrounding traffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and critical task is to explore the movement patterns of different traffic-agents and predict their future trajectories accurately to help the autonomous vehicle make reasonable navigation decision. To solve this problem, we propose a long short-term memory-based (LSTM-based) realtime traffic prediction algorithm, TrafficPredict. Our approach uses an instance layer to learn instances' movements and interactions and has a category layer to learn the similarities of instances belonging to the same type to refine the prediction. In order to evaluate its performance, we collected trajectory datasets in a large city consisting of varying conditions and traffic densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another. We evaluate the performance of TrafficPredict on our new dataset and highlight its higher accuracy for trajectory prediction by comparing with prior prediction methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Autonomous driving is a significant and difficult task that has the potential to impact peoples day-to-day lives. The goal is to make a vehicle perceive the environment and safely and efficiently navigate any traffic situation without human intervention. Some of the challenges arise in dense urban environments, where the traffic consists of different kinds of traffic-agents, including cars, bicycles, buses, pedestrians, etc.. These traffic-agents have different shapes, dynamics, and varying behaviors and can be regarded as an instance of a heterogeneous multi-agent system. To guarantee the safety of autonomous driving, the system should be able to analyze the motion patterns of other traffic-agents and predict their future trajectories so that the autonomous vehicle can make appropriate navigation decisions.</p><p>Driving in an urban environment is much more challenging than driving on a highway. Urban traffic is riddled with more uncertainties, complex road conditions, and diverse traffic-agents, especially on some cross-roads. Different traffic-agents have different motion patterns. At the same Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="figure">Figure 1</ref>: Heterogeneous urban traffic scenario: We demonstrate the improved trajectory prediction accuracy of our method over prior approaches <ref type="bibr">(top)</ref>. The green solid lines denote ground truth trajectories (GT), pink solid lines are for our method (TP) and dashed lines are the predicted trajectories for other methods (ED, SL, SA). We observe 20% improvement in accuracy using TP. Traffic corresponding point cloud captured by LiDAR of the acquisition car is shown on the left bottom. Original trajectories of trafficagents in the scenario are shown on the right bottom: blue for cars, green for bicycles, and red for pedestrians. time, traffic-agents behaviors are deeply affected by other traffic-agents. It is necessary to consider the interaction between the agent to improve the accuracy of trajectory prediction.</p><p>The problem of predicting trajectories for moving agents has been studied extensively. Some traditional algorithms are based on motion models like kinematic and dynamic models (Toledo-Moreo and Zamora-Izquierdo 2009), Bayesian filters <ref type="bibr" target="#b7">(Kalman 1960)</ref>, Gaussian Processes (Rasmussen and Williams 2006), etc. These methods do not take into account interactions between the traffic-agents and the environment, making it difficult to analyze complicated sce-narios or perform long-term predictions. With the success of LSTM networks in modeling non-linear temporal dependencies <ref type="bibr" target="#b9">(Ma et al. 2017)</ref> in sequence learning and generation tasks, more and more works have been using these networks to predict trajectories of human crowds <ref type="bibr" target="#b1">(Alahi et al. 2016</ref>) and vehicles trajectories <ref type="bibr" target="#b8">(Lee et al. 2017)</ref>. The common limitation of these works is the focus on predicting one type of group (only pedestrians or cars, for example). These methods may not work in heterogeneous traffic, where different vehicles and pedestrians coexist and interact with each other <ref type="bibr">(Chandra et al. 2018b)</ref>.</p><p>Main Results: For the task of trajectory prediction in heterogeneous traffic, we propose a novel LSTM-based algorithm, TrafficPredict. Given a sequence of trajectory data, we construct a 4D Graph, where two dimensions are for instances and their interactions, one dimension is for time series, and one dimension is for high-level categorization. In this graph, all the valid instances and categories of trafficagents are denoted as nodes, and all the relationships in spatial and temporal space is represented as edges. Sequential movement information and interaction information are stored and transferred by these nodes and edges. Our LSTM network architecture is constructed on the 4D Graph, which can be divided into two main layers: one is the instance layer and the other is the category layer. The former is designed to capture dynamic properties and and interactions between the traffic-agents at a micro level. The latter aims to conclude the behavior similarities of instances of the same category using a macroscopic view and guide the prediction for instances in turn. We also use a self attention mechanism in the category layer to capture the historical movement patterns and highlight the category difference. Our method is the first to integrate the trajectory prediction for different kinds of trafficagents in one unified framework.</p><p>To better expedite research progress on prediction and navigation in challenging scenarios for autonomous driving, we provide a new trajectory dataset for complex urban traffic with heterogeneous traffic-agents during rush hours. Scenario and data sample of our dataset is shown in <ref type="figure">Fig. 1</ref>. In practice, TrafficPredict takes about a fraction of a second on a single CPU core and exhibits 20% accuracy improvement over prior prediction schemes. The novel components of our work include:</p><p>• Propose a new approach for trajectory prediction in heterogeneous traffic.</p><p>• Collect a new trajectory dataset in urban traffic with much interaction between different categories of traffic-agents.</p><p>• Our method has smaller prediction error compared with other state-of-art approaches.</p><p>The rest of the paper is organized as follows. We give a brief overview of related prior work in Section 2. In Section 3, we define the problem and give details of our prediction algorithm. We introduce our new traffic dataset and show the performance of our methods in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Classical methods for trajectory prediction</p><p>The problem of trajectory prediction or path prediction has been extensively studied. There are many classical approaches, including Bayesian networks (Lefèvre et al. These methods focus on analyzing the inherent regularities of objects themselves based on their previous movements. They can be used in simple traffic scenarios in which there are few interactions among cars, but these methods may not work well when different kinds of vehicles and pedestrians appear at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavior modeling and interactions</head><p>There is considerable work on human behavior and interactions. The Social Force model <ref type="bibr" target="#b5">(Helbing and Molnar 1995)</ref> presents a pedestrian motion model with attractive and repulsive forces, which has been extended by <ref type="bibr">(Yamaguchi et al. 2011)</ref>. Some similar methods have also been proposed that use continuum dynamics <ref type="bibr">(Treuille et al. 2006</ref>), Gaussian processes <ref type="bibr" target="#b11">(Wang et al. 2008</ref><ref type="bibr">), etc. Bera et al. (2016</ref> combine an Ensemble Kalman Filter and human motion model to predict the trajectories for crowds. These methods are useful for analyzing motions of pedestrians in different scenarios, such as shopping malls, squares, and pedestrian streets. There are also some approaches to classify group emotions or identify driver behaviors <ref type="bibr" target="#b3">(Cheung et al. 2018)</ref>. To extend these methods to general traffic scenarios, <ref type="bibr" target="#b9">(Ma et al. 2018)</ref> predicts the trajectories of multiple traffic-agents by considering kinematic and dynamic constraints. However, this model assumes perfect sensing and shape and dynamics information for all of the traffic agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN networks for sequence prediction</head><p>In recent years, the concept of the deep neural network (DNN) has received a huge amount of attention due to its good performance in many areas <ref type="bibr" target="#b4">(Goodfellow et al. 2016)</ref>. Recurrent neural network (RNN) is one of the DNN architectures and is widely used for sequence generation in many domains, including speech recognition (Graves and Jaitly 2014), machine translation <ref type="bibr" target="#b3">(Chung et al. 2015)</ref>, and image captioning <ref type="bibr" target="#b10">(Vinyals et al. 2015)</ref>. Many methods based on long short-term Memory (LSTM), one variant of RNN, have been proposed for maneuver classification <ref type="bibr" target="#b8">(Khosroshahi 2017)</ref> and trajectory prediction (Altché and Fortelle 2017). Some methods <ref type="bibr" target="#b9">Park et al. 2018;</ref><ref type="bibr" target="#b8">Lee et al. 2017)</ref> produce the probabilistic information about the future locations of vehicles over an occupancy grid map or samples by making use of an encoder-decoder structure. However, these sampling-based methods suffer from inherent inaccuracies due to discretization limits. Another method (Deo and Trivedi 2018) presents a model that outputs the multimodal distribution and then generates trajectories. Nevertheless, most of these methods require clear road lanes frame n frame n+1 frame n frame n+1</p><p>Instance Layer Category Layer  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TrafficPredict</head><p>In this section, we present our novel algorithm to predict the trajectories of different traffic-agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>We assume each scene is preprocessed to get the categories and spatial coordinates of traffic-agents. At any time t, the feature of the ith traffic-agent A t i can be denoted as</p><formula xml:id="formula_0">f t i = (x t i , y t i , c t i )</formula><p>, where the first two items are coordinates in the x-axis and y-axis respectively, and the last item is the category of the traffic-agent. In our dataset, we currently take into account three types of traffic-agents, c i ∈ {1, 2, 3}, where 1 stands for pedestrians, 2 represents bicycles and 3 denotes cars. Our approach can be easily extended to take into account more agent types. Our task is to observe features of all the traffic-agents in the time interval [1 : T obs ] and then predict their discrete positions at [T obs + 1 : T pred ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D Graph Generation</head><p>In urban traffic scenarios where various traffic-agents are interacting with others, each instance has its own state in relation to the interaction with others at any time and they also have continuous information in time series. Considering traffic-agents as instance nodes and relationships as edges, we can construct a graph in the instance layer, shown in <ref type="figure" target="#fig_1">Fig.2 (b)</ref>. The edge between two instance nodes in one frame is called spatial edge <ref type="bibr" target="#b6">(Jain et al. 2016;</ref><ref type="bibr">Vemula et al. 2017)</ref>, which can transfer the interaction information between two traffic-agents in spatial space. The edge between the same instance in adjacent frames is the temporal edge, which is able to pass the historic information frame by frame in temporal space. The feature of the spatial edge</p><formula xml:id="formula_1">(A t i , A t j ) for A t i can be computed as f t ij = (x t ij , y t ij , c t ij ), where x t ij = x t j − x t i , y t ij = y t j − y t i stands for the rela- tive position from A t j to A t i , c t ij is an unique encoding for (A t i , A t j )</formula><p>. When traffic-agent A j considers the spatial edge, the spatial edge is represented as</p><formula xml:id="formula_2">(A t j , A t i ). The feature of the temporal edge (A t i , A t+1 i )</formula><p>is computed in the same way. It is normally observed that the same kind of trafficagents have similar behavior characteristics. For example, the pedestrians have not only similar velocities but also similar reactions to other nearby traffic-agents. These similarities will be directly reflected in their trajectories. We construct a super node C t u , u ∈ {1, 2, 3} for each kind of traffic-agent to learn the similarities of their trajectories and then utilize that super node to refine the prediction for instances. <ref type="figure" target="#fig_1">Fig.2</ref> (c) shows the graph in the category layer. All instances of the same type are integrated into one group and each group has an edge oriented toward the corresponding super node. After summarizing the motion similarities, the super node passes the guidance through an oriented edge to the group of instances. There are also temporal edges between the same super node in sequential frames. This category layer is specially designed for heterogeneous traffic and can make full use of the data to extract valuable information to improve the prediction results. This layer is very flexible and can be easily degenerated to situations when several categories disappear in some frames. Finally, we get the 4D Graph for a traffic sequence with two dimensions for traffic-agents and their interactions, one dimension for time series, and one dimension for high-level categories. By this 4D Graph, we construct an information network for the entire traffic. All the information can be delivered and utilized through the nodes and edges of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>Our TrafficPredict algorithm is based on the 4D Graph, which consists of two main layers: the instance layer and the category layer. Details are given below.</p><p>Instance Layer The instance layer aims to capture the movement pattern of instances in traffic. For each instance node A i , we have an LSTM, represented as L i . Because different kinds of traffic-agents have different dynamic properties and motion rules, only instances of the same type share the same parameters. There are three types of traffic-agents in our dataset: vehicles, bicycles, and pedestrians. Therefore, we have three different LSTMs for instance nodes. We also distribute LSTM L ij for each edge (A i , A j ) of the graph. All the spatial edges share the same parameters and all the temporal edges are classified into three types according to corresponding node type.</p><p>For edge LSTM L ij at any time t, we embed the feature f t ij into a fixed vector e t ij , which is used as the input to LSTM:</p><formula xml:id="formula_3">e t ij = φ(f t ij ; W e spa ),<label>(1)</label></formula><formula xml:id="formula_4">h t ij = LST M (h t−1 ij ; e t ij ; W r spa ),<label>(2)</label></formula><p>where φ(·) is an embedding function, h t ij is the hidden state also the output of LSTM L ij , and W e spa are the embedding weights, and W r spa are LSTM cell weights, which contains the movement pattern of the instance itself. LSTMs for temporal edges L ii are defined in a similar way with parameters W e tem and W r tem . Each instance node may connect with several other instance nodes via spatial edges. However, each of the other instances has different impacts on the node's behavior. We use a soft attention mechanism (Vemula et al. 2017) to distribute various weights for all the spatial edges of one instance node where W i and W ij are embedding weights, Dot(·) is the dot product, and m √ de is a scaling factor (Vaswani et al. 2017). The final weights are ratios of w(h t ij ) to the sum. The output vector H t i is computed as a weighted sum of h t ij . H t i stands for the influence exhibited on an instance's trajectory by surrounding traffic-agents and h t ii denotes the information passing by temporal edges. We concatenate them and embed the result into a fixed vector a t i . The node features f t i and a t i can finally concatenate with each other to feed the instance LSTM L i .</p><formula xml:id="formula_5">w(h t ij ) = sof tmax( m √ d e Dot(W i h t ii , W ij h t ij )),<label>(3)</label></formula><formula xml:id="formula_6">e t i = φ(f t i ; W e ins ),<label>(4)</label></formula><formula xml:id="formula_7">a t i = φ(concat(h t ii , H t i ); W a ins ),<label>(5)</label></formula><formula xml:id="formula_8">h1 t i = LST M (h2 t−1 i ; concat(e t i , a t i ); W r ins ),<label>(6)</label></formula><p>where W e ins and W a ins are the embedding weights, W r ins is the LSTM cell weight for the instance node, h1 t i is the first hidden state of the instance LSTM. h2 t−1 i is the final hidden state of the instance LSTM in the last frame, which will be described in next section.</p><p>Category Layer Usually traffic-agents of the same category have similar dynamic properties, including the speed, acceleration, steering, etc., and similar reactions to other kinds of traffic-agents or the whole environment. If we can learn the movement patterns from the same category of instances, we can better predict trajectories for the entire instances. The category layer is based on the graph in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. There are four important components: the super node for a specified category, the directed edge from a group of instances to the super node, the directed edge from the super node to instances, and the temporal edges for super nodes.</p><p>Taking one super node with three instances as the example, the architecture in the category layer is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Assume there are n instances belonging to the same category in the current frame. We have already gotten the hidden state h1 and the cell state c from the instance LSTM, which are the input for the category layer. Because the cell state c contains the historical trajectory information of the instance, self-attention mechanism <ref type="bibr" target="#b9">(Vaswani et al. 2017</ref>) is used on c by softmax operation to explore the pattern of the internal sequence. At time t, the movement feature d for the mth instance in the category is captured as follows.</p><formula xml:id="formula_9">d t m = h1 t m sof tmax(c t m ),<label>(7)</label></formula><p>Then, we obtain the feature F t u for the corresponding super node C t u by computing the average of all the instances' movement feature of the category.</p><formula xml:id="formula_10">F t u = 1 n n m=1 d t m ,<label>(8)</label></formula><p>F t u captures valid trajectory information from instances and learn the internal movement law of the category. Equation <ref type="formula" target="#formula_9">(7)-(8)</ref> show the process of transferring information on the directed edge from a group of instances to the super node.</p><p>The feature F t uu of the temporal edge of super node is computed by F t u − F t−1 u . Take W e st as embedding weights and W r st as the LSTM cell weights. The LSTM of the temporal edge between the same super node in adjacent frames can be computed as follows.</p><formula xml:id="formula_11">e t uu = φ(F t uu ; W e st ),<label>(9)</label></formula><formula xml:id="formula_12">h t uu = LST M (h t−1 uu ; e t uu ; W r st ),<label>(10)</label></formula><p>Next, we integrate the information from the group of instances and the temporal edge as the input to the super node. We embed the feature F t u into fixed-length vectors and then concatenate with h t uu together. The hidden state h t u of super node can be gotten by follows.</p><formula xml:id="formula_13">e t u = φ(F t u ; W e sup ),<label>(11)</label></formula><formula xml:id="formula_14">h t u = LST M (h t−1 u ; concat(e t u ; h t uu ); W k sup ),<label>(12)</label></formula><p>Finally, we describe the process of transferring guidance on the directed edge from the super node to instances. For the mth instance in the group, the hidden state of the super node is concatenated with the first hidden state h1 t m and then embedded into a vector with the same length of h1 t m . The second hidden state h2 t m is the final output of the instance node.</p><formula xml:id="formula_15">h2 t m = φ(concat((h1 t m ; h t u ); W r s )),<label>(13)</label></formula><p>where W r s is the embedding weights. By the network of the category layer, we use the similarity inside the same type of instances to refine the prediction of trajectories for instances. Position estimation We assume the position of the trafficagent in next frame meets a bivariate Gaussian distribution as <ref type="bibr" target="#b1">(Alahi et al. 2016)</ref> with parameters including the mean</p><formula xml:id="formula_16">µ t i = (µ x , µ y ) t i , standard deviation σ t i = (σ x , σ y ) t i and cor- relation coefficient ρ t i .</formula><p>The corresponding position can be represented as follows.</p><formula xml:id="formula_17">(x t i , y t i ) ∼ N (µ t i , σ t i , ρ t i ),<label>(14)</label></formula><p>The second hidden state of traffic-agents at any time is used to to predict these parameters by linear projection.</p><formula xml:id="formula_18">[µ t i , σ t i , ρ t i ] = φ(h2 t−1 i ; W f ),<label>(15)</label></formula><p>The loss function is defined by the negative log-Likelihood L i .</p><formula xml:id="formula_19">L i (W spa , W tem , W ins , W st , W sup , W s , W f ) = − T pred t=T obs +1 log(P (x t i , y t i |µ t i , σ t i , ρ t i )),<label>(16)</label></formula><p>We train the model by minimizing the loss for all the trajectories in the training dataset. We jointly back-propagated through instance nodes, super nodes and spatial and temporal edges to update all the parameters to minimize the loss at each time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>We use Apollo acquisition car (BaiduApollo 2018) to collect traffic data, including camera-based images and LiDARbased point clouds, and generate trajectories by detection and tracking.</p><p>Our new dataset is a large-scale dataset for urban streets, which focuses on trajectories of heterogeneous traffic-agents for planning, prediction and simulation tasks. Our acquisition car runs in urban areas in rush hours in those scenarios shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The data is generated from a variety of sensors, including LiDAR (Velodyne HDL-64E S3), radar (Continental ARS408-21), camera, high definition maps and <ref type="table">Table 2</ref>: The average displacement error and the final displacement error of the prior methods (ED, SL, SA) and variants of our method (TP) on our new dataset. For each evaluation metric, we show the values on pedestrians, bicycles, vehicles, and all the traffic-agents. We set the observation time as 2 seconds and the prediction time as 3 seconds for these measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Methods </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics and Baselines</head><p>We use the following metrics <ref type="bibr" target="#b9">(Pellegrini et al. 2009;</ref><ref type="bibr">Vemula et al. 2017)</ref> to measure the performance of algorithms used for predicting the trajectories of traffic-agents.</p><p>1. Average displacement error: The mean Euclidean distance over all the predicted positions and real positions during the prediction time. 2. Final displacement error: The mean Euclidean distance between the final predicted positions and the corresponding true locations.</p><p>We compare our approach with these methods below: • RNN ED (ED): An RNN encoder-decoder model, which is widely used in motion and trajectory prediction for vehicles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In our evaluation benchmarks, the dimension of hidden state of spatial and temporal edge cell is set to 128 and that of node cell is 64 (for both instance layer and category layer). We also apply the fixed input dimension of 64 and attention layer of 64. During training, Adam (Kingma and Ba 2014) optimization is applied with β 1 =0.9 and β 2 =0.999. Learning rate is scheduled as 0.001 and a staircase weight decay is applied. The model is trained on a single Tesla K40 GPU with a batch size of 8. For the training stability, we clip the gradients with the range -10 to 10. During the computation of predicted trajectories, we observe trajectories of 2 seconds and predict the future trajectories in next 3 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>The performance of all the prior methods and our algorithm on heterogeneous traffic datasets is shown in <ref type="table">Table.</ref> 2. We compute the average displacement error and the final displacement error for all the instances and we also count the error for pedestrians, bicycles and vehicles, respectively. The social attention (SA) model considers the spatial relations of instances and has smaller error than RNN ED and Social LSTM. Our method without category layer (TP-NoCL) not only considers the interactions between instances but also distinguishes between instances by using different LSTMs. Its error is similar to SA. By adding the category layer without self attention, the prediction results of TP-NoSA are more accurate in terms of both metrics. The accuracy improvement becomes is more evident after we use the selfattention mechanism in the design of category layer. Our algorithm, TrafficPredict, performs better in terms of all the metrics with about 20% improvement of accuracy. It means the category layer has learned the inbuilt movement patterns for traffic-agents of the same type and provides good guidance for prediction. The combination of the instance layer and the category layer makes our algorithm more applicable in heterogeneous traffic conditions. We illustrate some prediction results on corresponding 2D images in <ref type="figure">Fig. 5</ref>. The scenario in the image captured by the front-facing camera does not show the entire scenario. However, it is more intrinsic to project the trajectory results on the image. In most heterogeneous traffic scenarios, our algorithm computes a reasonably accurate predicted trajectory <ref type="figure">Figure 5</ref>: Illustration of our TrafficPredict (TP) method on camera-based images. There are six scenarios with different road conditions and traffic situations. We only show the trajectories of several instances in each scenario. The ground truth (GT) is drawn in green and the prediction results of other methods (ED,SL,SA) are shown with different dashed lines. The prediction trajectories of our TP algorithm (pink lines) are the closest to ground truth in most of the cases.  <ref type="figure">Figure 6</ref>: Illustration of some prediction results by our method. The ground truth of trajectories of vehicles, bicycles and pedestrians are drawn by blue, green and red respectively. Predicted locations are all represented by yellow stars. For each instance, first five discrete points are observed positions, but there are some overlaps in the illustration of pedestrian trajectories.</p><p>and is close to the ground truth. If we have prior trajectories over a longer duration, the prediction accuracy increases. When traffic-agents are moving on straight lanes, it is easy to predict their trajectories because almost all the trafficagents are moving in straight direction. It is more challenging to provide accurate prediction in cross roads, as the agents are turning. <ref type="figure">Fig. 5</ref> shows 2D experimental results of two sequences in cross areas. There are some overlaps on trajectories. In these scenarios, there are many curves with high curvature because of left turn. Our algorithm can compute accurate predicted trajectories in these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have presented a novel LSTM-based algorithm, TrafficPredict, for predicting trajectories for heterogeneous traffic-agents in urban environment. We use a in-stance layer to capture the trajectories and interactions for instances and use a category layer to summarize the similarities of movement pattern of instances belong to the same type and guide the prediction algorithm. All the information in spatial and temporal space can be leveraged and transferred in our designed 4D Graph. Our method outperforms previous state-of-the-art approaches in improving the accuracy of trajectory prediction on our new collected dataset for heterogeneous traffic. We have evaluated our algorithm in traffic datasets corresponding to urban dense scenarios and observe good accuracy. Our algorithm is realtime and makes no assumption about the traffic conditions or the number of agents.</p><p>Our approach has some limitations. Its accuracy varies based on traffic conditions and the duration of past trajectories. In the future, we will consider more constraints, like the lane direction, the traffic signals and traffic rules, etc. to further improve the accuracy of trajectory prediction. Furthermore, we would like to evaluate the performance in more dense scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2011), Monte Carlo Simulation (Danielsson et al. 2007), Hidden Markov Models (HMM) (Firl et al. 2012), Kalman Filters (Kalman 1960), linear and non-linear Gaussian Process regression models (Rasmussen and Williams 2006), etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our 4D Graph for a traffic sequence. (a) Icons for instances and categories are shown on the left table. (b) The instance layer of the 4D Graph with spatial edges as solid lines and temporal edges as dashed lines. (c) The category layer with temporal edges of super nodes drawn by dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the network for one super node in the category layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Scenarios used for data collection: (a) Normal lanes with various traffic-agents. (b) Crossroads with different traffic-agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>Social LSTM (SL): An LSTM-based network with social pooling of hidden states (Alahi et al. 2016). The model performs better than traditional methods, including the linear model, the Social force model, and Interacting Gaussian Processes. • Social Attention (SA): An attention-based S-RNN architecture (Vemula et al. 2017), which learn the relative influence in the crowd and predict pedestrian trajectories. • TrafficPredict-NoCL (TP-NoCL): The proposed method without the category layer. • TrafficPredict-NoSA (TP-NoSA): The proposed method without the self-attention mechanism of the category layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2D/3D object detection, and tracking. However, the total time of the dataset with tracklets is about 22 minutes. In addition, there are few intersection between vehicles, pedestrians and cyclists in KITTI, which makes it insufficient for exploring the motion patterns of traffic-agents in challenging traffic conditions. There are some pedestrian trajectory datasets like ETH<ref type="bibr" target="#b9">(Pellegrini et al. 2009</ref>), UCY (Lerner et al. 2007), etc., but such datasets only focus on human crowds without any vehicles.</figDesc><table /><note>and simple driving scenarios without other types of traffic- agents passing through. Based on images, (Chandra et al. 2018a) models the interactions between different traffic- agents by a LSTM-CNN hybrid network for trajectory pre- diction. Taking into account the human-human interactions, some approaches (Alahi et al. 2016; Gupta et al. 2018; Vemula et al. 2017) use LSTM for predicting trajectories of pedestrians in a crowd and they show good performance on public crowd datasets. However, these methods are also limited in terms of trajectory prediction in complex traffic scenarios where the interactions are among not only pedes- trians but also heterogeneous traffic-agents. Traffic datasets There are several datasets related to traffic scenes. Cityscapes (Cordts et al. 2016) contains 2D semantic, instance-wise, dense pixel annotations for 30 classes. Apol- loScape (Huang et al. 2018) is a large-scale comprehensive dataset of street views that contains higher scene complexi- ties, 2D/3D annotations and pose information, lane markings and video frames. However these two dataset do not provide trajectories information. The Simulation (NGSIM) dataset (Administration 2005) has trajectory data for cars, but the scene is limited to highways with similar simple road con- ditions. KITTI (Geiger et al. 2013) is a dataset for different computer vision tasks such as stereo, optical ow,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The acquisition time, total frames, total instances (count ID), average instances per frame, acquisition devices of NGSIM, KITTI (with tracklets) and our dataset.</figDesc><table><row><cell>Count</cell><cell></cell><cell cols="3">NGSIM KITTI Our</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell></row><row><cell>duration (min)</cell><cell></cell><cell>45</cell><cell>22</cell><cell>155</cell></row><row><cell>frames (×10 3 )</cell><cell></cell><cell>11.2</cell><cell>13.1</cell><cell>93.0</cell></row><row><cell></cell><cell cols="2">pedestrian 0</cell><cell>0.09</cell><cell>16.2</cell></row><row><cell>total (×10 3 )</cell><cell>bicycle</cell><cell>0</cell><cell>0.04</cell><cell>5.5</cell></row><row><cell></cell><cell>vehicle</cell><cell>2.91</cell><cell>0.93</cell><cell>60.1</cell></row><row><cell></cell><cell cols="2">pedestrian 0</cell><cell>1.3</cell><cell>1.6</cell></row><row><cell>average (1/f)</cell><cell>bicycle</cell><cell>0</cell><cell>0.24</cell><cell>1.9</cell></row><row><cell></cell><cell>vehicle</cell><cell>845</cell><cell>3.4</cell><cell>12.9</cell></row><row><cell></cell><cell>camera</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell></row><row><cell>device</cell><cell>lidar</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell></cell><cell>GPS</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Dinesh Manocha is supported in part by ARO Contract W911NF16-1-0085, and Intel. We appreciate all the people who offered help for collecting the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Federal Highway Administration. Us highway 101 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>F. Altché and A. De La Fortelle</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="359" />
		</imprint>
	</monogr>
	<note>ITSC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trajectory dataset for urban traffic</title>
		<ptr target="https://github.com/ApolloAuto/apollo.5" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>BaiduApollo 2018] BaiduApollo</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Traphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04767</idno>
		<idno>arXiv:1805.05499</idno>
		<ptr target="http://gamma.cs.unc.edu/HTI/.2" />
	</analytic>
	<monogr>
		<title level="m">Multi-modal trajectory prediction of surrounding vehicles with maneuver based lstms</title>
		<editor>Danielsson et al. 2007] S. Danielsson, L. Petersson, and A. Eidehall. Monte</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
		</imprint>
	</monogr>
	<note>IEEE. Geiger et al. 2013] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. IJRR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-toend speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
	<note>Graves and Jaitly 2014] A. Graves and N. Jaitly</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, number CONF</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">4282</biblScope>
		</imprint>
	</monogr>
	<note>CVPR Workshop</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network. ITSC, 2017. 2 [Kingma and Ba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosroshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">B</forename><surname>Learning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">N</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Classification and Prediction of Maneuvers of Surround Vehicles at Intersections using LSTMs</title>
		<editor>Lefèvre et al. 2011] S. Lefèvre, C. Laugier, and J.Ibañez-Guzmán</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
		<respStmt>
			<orgName>UC San Diego</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>CGF</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06338</idno>
		<idno>Pellegrini et al. 2009</idno>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<editor>Treuille et al. 2006] A. Treuille, S. Cooper, and Z. Popović. Continuum crowds. TOG</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>NIPS. Vemula et al. 2017] A. Vemula, K. Muelling, and J. Oh. Social attention: Modeling attention in human crowds. arXiv:1710.04689, 2017. 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Who are you with and where are you going? In CVPR</title>
		<editor>Yamaguchi et al. 2011] K. Yamaguchi, A.C. Berg, L.E. Ortiz, and T.L. Berg</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
