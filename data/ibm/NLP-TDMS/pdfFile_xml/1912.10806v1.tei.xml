<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuqing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stock price prediction is important for value investments in the stock market. In particular, short-term prediction that exploits financial news articles is promising in recent years. In this paper, we propose a novel deep neural network DP-LSTM for stock price prediction, which incorporates the news articles as hidden information and integrates difference news sources through the differential privacy mechanism. First, based on the autoregressive moving average model (ARMA), a sentiment-ARMA is formulated by taking into consideration the information of financial news articles in the model. Then, an LSTM-based deep neural network is designed, which consists of three components: LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM scheme can reduce prediction errors and increase the robustness. Extensive experiments on S&amp;P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32% improvement in mean MPA of prediction result, and (ii) for the prediction of the market index S&amp;P 500, we achieve up to 65.79% improvement in MSE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stock prediction is crucial for quantitative analysts and investment companies. Stocks' trends, however, are affected by a lot of factors such as interest rates, inflation rates and financial news <ref type="bibr" target="#b11">[12]</ref>. To predict stock prices accurately, one must use these variable information. In particular, in the banking industry and financial services, analysts' armies are dedicated to pouring over, analyzing, and attempting to quantify qualitative data from news. A large amount of stock trend information is extracted from the large amount of text and quantitative information that is involved in the analysis.</p><p>Investors may judge on the basis of technical analysis, such as charts of a company, market indices, and on textual information such as news blogs or newspapers. It is however difficult for investors to analyze and predict market trends according to all of these information <ref type="bibr" target="#b21">[22]</ref>. A lot of artificial intelligence approaches have been investigated to automatically predict those trends <ref type="bibr" target="#b2">[3]</ref>. For instance, investment simulation analysis with artificial markets or stock trend analysis with lexical cohesion based metric of financial news' sentiment polarity. Quantitative analysis today is heavily dependent on data. However, the majority of such data is unstructured text that comes from sources like financial news articles. The challenge is not only the amount of data that are involved, but also the kind of language that is used in them to express sentiments, which means emoticons. Sifting through huge volumes of this text data is difficult as well as time-consuming. It also requires a great deal of resources and expertise to analyze all of that <ref type="bibr" target="#b3">[4]</ref>.</p><p>To solve the above problem, in this paper we use sentiment analysis to extract information from textual information. Sentiment analysis is the automated process of understanding an opinion about a given subject from news articles <ref type="bibr" target="#b4">[5]</ref>. The analyzed data quantifies reactions or sentiments of the general public toward people, ideas or certain products and reveal the information's contextual We use valence aware dictionary and sentiment reasoner (VADER) to extract sentiment scores. VADER is a lexicon and rule-based sentiment analysis tool attuned to sentiments that are expressed in social media specifically <ref type="bibr" target="#b5">[6]</ref>. VADER has been found to be quite successful when dealing with NY Times editorials and social media texts. This is because VADER not only tells about the negativity score and positively but also tells us about how positive or negative a sentiment is.</p><p>However, news reports are not all objective. We may increase bias because of some non-objective reports, if we rely on the information that is extracted from the news for prediction fully. Therefore, in order to enhance the prediction model's robustness, we will adopt differential privacy (DP) method. DP is a system for sharing information about a dataset publicly by describing groups' patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result. For example, rather than simply reporting the sum, we can inject noise from a Laplace or gaussian distribution, producing a result that's not quite exact, that masks the contents of any given row.</p><p>In the last several years a promising approach to private data analysis has emerged, based on DP, which ensures that an analysis outcome is "roughly as likely" to occur independent of whether any individual opts in to, or to opts out of, the database. In consequence, any one individual's specific data can never greatly affect the results. General techniques for ensuring DP have now been proposed, and a lot of datamining tasks can be carried out in a DP method, frequently with very accurate results <ref type="bibr" target="#b20">[21]</ref>. We proposed a DP-LSTM neural network, which increase the accuracy of prediction and robustness of model at the same time.</p><p>The remainder of the paper is organized as follows. In Section 2, we introduce stock price model, the sentiment analysis and differential privacy method. In Section 3, we develop the different privacyinspired LSTM (DP-LSTM) deep neural network and present the training details. Prediction results are provided in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>In this section, we first introduce the background of the stock price model, which is based on the autoregressive moving average (ARMA) model. Then, we present the sentiment analysis details of the financial news and introduce how to use them to improve prediction performance. At last, we introduce the differential privacy framework and the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ARMA Model</head><p>The ARMA model, which is one of the most widely used linear models in time series prediction <ref type="bibr" target="#b16">[17]</ref>, where the future value is assumed as a linear combination of the past errors and past values. ARMA is used to set the stock midterm prediction problem up. Let X A t be the variable based on ARMA at time t, then we have</p><formula xml:id="formula_0">X A t = f 1 ({X t−i } p i=1 ) = µ + p i=1 φ i X t−i − q i=1 ψ j t−j + t ,<label>(1)</label></formula><p>where X t−i denotes the past value at time t − i; t denotes the random error at time t; φ i and ψ j are the coefficients; µ is a constant; p and q are integers that are often referred to as autoregressive and moving average polynomials, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentiment Analysis</head><p>Another variable highly related to stock price is the textual information from news, whose changes may be a precursor to price changes. In our paper, news refers to a news article's title on a given trading day. It has been used to infer whether an event had informational content and whether investors' interpretations of the information were positive, negative or neutral. We hence use sentiment analysis to identify and extract opinions within a given text. Sentiment analysis aims at gauging the attitude, sentiments, evaluations and emotions of a speaker or writer based on subjectivity's computational treatment in a text <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b19">[20]</ref>.  . We divide the news based on their compound score. For both positive news and negative news, we count all the words and rank them to create the wordcloud. The larger the word, more frequently it has appeared in the source. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the sentiment analysis results obtained from financial news titles that were based on VADER. VADER uses a combination of a sentiment lexicon which are generally labelled according to their semantic orientation as either negative or positive. VADER has been found to be quite successful when dealing with news reviews. It is fully open-sourced under the MIT License. The result of VADER represent as sentiment scores, which include the positive, negative and neutral scores represent the proportion of text that falls in these categories. This means all these three scores should add up to 1. Besides, the Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). <ref type="figure" target="#fig_1">Figure 2</ref> shows the positive and negative wordcloud, which is an intuitive analysis of the number of words in the news titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentiment-ARMA Model and Loss Function</head><p>To take the sentiment analysis results of the financial news into account, we introduce the sentiment-ARMA model as followŝ</p><formula xml:id="formula_1">X t = αX A t + λS A t + c = αX A t + λf 2 ({S t−i } p i=1 Sentiment ) + c,<label>(2)</label></formula><p>where α and λ are weighting factors; c is a constant; and f 2 (·) is similar to f 1 (·) in the ARMA model <ref type="bibr" target="#b0">(1)</ref> and is used to describe the prediction problem.</p><p>In this paper, the LSTM neural network is used to predict the stock price, the input data is the previous stock price and the sentiment analysis results. Hence, the sentiment based LSTM neural network </p><formula xml:id="formula_2">L = min p+T t=p+1 X t −X t 2 2 ,<label>(3)</label></formula><p>where T denotes the number of prediction time slots, i.e., t = 1, ..., p are the observations (training input data), t = p + 1, ..., p + T are the predicts (training output data); andX t is given in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overview of LSTM</head><formula xml:id="formula_3">Denote X train t = {X t−i , S t−i } p i=1</formula><p>as the training input data. <ref type="figure" target="#fig_2">Figure 3</ref> shows the LSTM's structure network, which comprises one or more hidden layers, an output layer and an input layer <ref type="bibr" target="#b15">[16]</ref>. LSTM networks' main advantage is that the hidden layer comprises memory cells. Each memory cell recurrently has a core self-connected linear unit called " Constant Error Carousel (CEC)" <ref type="bibr" target="#b12">[13]</ref>, which provides short-term memory storage and has three gates:</p><p>• Input gate, which controls the information from a new input to the memory cell, is given by</p><formula xml:id="formula_4">i t = σ(W i × [h t−1 , X train t ] + b i ),<label>(4)</label></formula><formula xml:id="formula_5">c t = tanh(W c × [h t−1 , X train t ] + b c ),<label>(5)</label></formula><p>where h t−1 is the hidden state at the time step t − 1; i t is the output of the input gate layer at the time step t;ĉ t is the candidate value to be added to the output at the time step t; b i and b c are biases of the input gate layer and the candidate value computation, respectively; W i and W c are weights of the input gate and the candidate value computation, respectively; and σ(x) = 1/(1 + e −x ) is the pointwise nonlinear activation function. • Forget gate, which controls the limit up to which a value is saved in the memory, is given by</p><formula xml:id="formula_6">f t = σ(W f × [h t−1 , X train t ] + b f ),<label>(6)</label></formula><p>where f t is the forget state at the time step t, W f is the weight of the forget gate; and b f is the bias of the forget gate. • Output gate, which controls the information output from the memory cell, is given by</p><formula xml:id="formula_7">c t = f t × c t−1 + i t ×ĉ t ,<label>(7)</label></formula><formula xml:id="formula_8">o t = σ(W o × [h t−1 , X train t ] + b o ),<label>(8)</label></formula><formula xml:id="formula_9">h t = o t × tanh(c t ),<label>(9)</label></formula><p>where new cell states c t are calculated based on the results of the previous two steps; o t is the output at the time step t; W o is the weight of the output gate; and b o is the bias of the output gate <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Definition of Differential Privacy</head><p>Differential privacy is one of privacy's most popular definitions today, which is a system for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset. It intuitively requires that the mechanism that outputs information about an underlying dataset is robust to one sample's any change, thus protecting privacy. A mechanism f is a random function that takes a dataset N as input, and outputs a random variable f (N ). For example, suppose N is a news articles dataset, then the function that outputs compound score of articles in N plus noise from the standard normal distribution is a mechanism <ref type="bibr" target="#b6">[7]</ref>.</p><p>Although differential privacy was originally developed to facilitate secure analysis over sensitive data, it can also enhance the robustness of the data. Note that finance data, especially news data and stock data, is unstable with a lot of noise, with a more robust data the accuracy of prediction will be improved. Since we predict stock price by fusing news come from different sources, which might include fake news. Involving differential privacy in the training to improve the robustness of the finance news is meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training DP-LSTM Neural Network</head><p>It is known that it is risky to predict stocks by considering news factors, because news can't guarantee full notarization and objectivity, many times extreme news will have a big impact on prediction models. To solve this problem, we consider entering the idea of the differential privacy when training. In this section, our DP-LSTM deep neural network training strategy is presented. The input data consists of three components: stock price, sentiment analysis compound score and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preprocessing and Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data Preprocessing</head><p>The data for this project are two parts, the first part is the historical S&amp;P 500 component stocks, which are downloaded from the Yahoo Finance. We use the data over the period of from 12/07/2017 to 06/01/2018. The second part is the news article from financial domain are collected with the same time period as stock data. Since our paper illustrates the relationship between the sentiment of the news articles and stocks' price. Hence, only news article from financial domain are collected. The data is mainly taken from Webhose archived data, which consists of 306242 news articles present in JSON format, dating from December 2017 up to end of June 2018. The former 85% of the dataset is used as the training data and the remainder 15% is used as the testing data. The News publishers for this data are CNBC.com, Reuters.com, WSJ.com, Fortune.com. The Wall Street Journal is one of the largest newspapers in the United States, which coverage of breaking news and current headlines from the US and around the world include top stories, photos, videos, detailed analysis and in-depth thoughts; CNBC primarily carries business day coverage of U.S. and international financial markets, which following the end of the business day and on non-trading days; Fortune is an American multinational business magazine; Reuters is an international news organization. We preprocess the raw article body and use NLTK sentiment package alence Aware Dictionary and Sentiment Reasoner (VADER) to extract sentiment scores.</p><p>The stocks with missing data are deleted, and the dataset we used eventually contains 451 stocks and 4 news resources (CNBC.com, Reuters.com, WSJ.comFortune.com.). Each stock records the adjust close price and news compound scores of 121 trading days.  A rolling window with size 10 is used to separate data, that is, We predict the stock price of the next trading day based on historical data from the previous 10 days, hence resulting in a point-by-point prediction <ref type="bibr" target="#b14">[15]</ref>. In particular, the training window is initialized with all real training data. Then we shift the window and add the next real point to the last point of training window to predict the next point and so forth. Then, according to the length of the window, the training data is divided into 92 sets of training input data (each set length 10) and training output data (each set length 1). The testing data is divided into input and output data of 9 windows (see <ref type="figure" target="#fig_3">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Normalization</head><p>To detect stock price pattern, it is necessary to normalize the stock price data. Since the LSTM neural network requires the stock patterns during training, we use "min-max" normalization method to reform dataset, which keeps the pattern of the data <ref type="bibr" target="#b10">[11]</ref>, as follow:</p><formula xml:id="formula_10">X n t = X t − min(X t ) max(X t ) − min(X t ) ,<label>(10)</label></formula><p>where X n t denotes the data after normalization. Accordingly, de-normalization is required at the end of the prediction process to get the original price, which is given bŷ</p><formula xml:id="formula_11">X t =X n t [max(X t ) − min(X t )] + min(X t ),<label>(11)</label></formula><p>whereX n t denotes the predicted data andX t denotes the predicted data after de-normalization. Note that compound score is not normalized, since the compound score range from -1 to 1, which means all the compound score data has the same scale, so it is not require the normalization processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adding Noise</head><p>We consider the differential privacy as a method to improve the robustness of the LSTM predictions <ref type="bibr" target="#b7">[8]</ref>. We explore the interplay between machine learning and differential privacy, and found that differential privacy has several properties that make it particularly useful in application such as robustness to extract textual information <ref type="bibr" target="#b8">[9]</ref>. The robustness of textual information means that accuracy is guaranteed to be unaffected by certain false information <ref type="bibr" target="#b9">[10]</ref>.</p><p>The input data of the model has 5 dimensions, which are the stock price and four compound scores as (X t , S t 1 , S t 2 , S t 3 , S t 4 ), t = 1, ..., T , where X t represents the stock price and S t i , i = 1, ..., 4 respectively denote the mean compound score calculated from WSJ, CNBC, Fortune and Reuters. According to the process of differential privacy, we add Gaussian noise with different variances to the news according to the variance of the news, i.e., the news compound score after adding noise is given by</p><formula xml:id="formula_12">S t i = S t i + N (0, λvar(S i )), i = 1, ..., 4,<label>(12)</label></formula><p>where var(·) is the variance operator, λ is a weighting factor and N (·) denotes the random Gaussian process with zero mean and variance λvar(S i ).</p><p>We used python to crawl the news from the four sources of each trading day, perform sentiment analysis on the title of the news, and get the compound score. After splitting the data into training sets and test sets, we separately add noise to each of four news sources of the training set, then, for n-th stock, four sets of noise-added data (X n t , S t</p><formula xml:id="formula_13">1 , S t 2 , S t 3 , S t 4 ), (X n t , S t 1 , S t 2 , S t 3 , S t 4 ), (X n t , S t 1 , S t 2 , S t 3 , S t 4 ), (X n t , S t 1 , S t 2 , S t 3 , S t 4 )</formula><p>are combined into a new training data through a rolling window. The stock price is then combined with the new compound score training data as input data for our DP-LSTM neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Setting</head><p>The LSTM model in <ref type="figure" target="#fig_2">figure 3</ref> has six layers, followed by an LSTM layer, a dropout layer, an LSTM layer, an LSTM layer, a dropout layer, a dense layer, respectively. The dropout layers (with dropout rate 0.2) prevent the network from overfitting. The dense layer is used to reshape the output. Since a network will be difficult to train if it contains a large number of LSTM layers <ref type="bibr" target="#b15">[16]</ref>, we use three LSTM layers here.</p><p>In each LSTM layer, the loss function is the mean square error (MSE), which is the sum of the squared distances between our target variable and the predicted value. In addition, the ADAM <ref type="bibr" target="#b16">[17]</ref> is used as optimizer, since it is straightforward to implement, computationally efficient and well suited for problems with large data set and parameters.</p><p>There are many methods and algorithms to implement sentiment analysis systems. In this paper, we use rule-based systems that perform sentiment analysis based on a set of manually crafted rules. Usually, rule-based approaches define a set of rules in some kind of scripting language that identify subjectivity, polarity, or the subject of an opinion. We use VADER, a simple rule-based model for general sentiment analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Evaluation</head><p>In this section, we validate our DP-LSTM based on the S&amp;P 500 stocks. We calculate the mean prediction accuracy (MPA) to evaluate the proposed methods, which is defined as</p><formula xml:id="formula_14">MPA t = 1 − 1 L L =1 |X t, −X t, | X t, ,<label>(13)</label></formula><p>where X t, is the real stock price of the -th stock on the t-th day, L is the number of stocks andX t, is the corresponding prediction result. <ref type="figure" target="#fig_4">Figure 5</ref> plots the average score for all news on the same day over the period. The compound score is fluctuating between -0.3 and 0.15, indicating an overall neutral to slightly negative sentiment. The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).   <ref type="table">Table 1</ref>, we give the mean MPA results for the prediction prices, which shows the accuracy performance of DP-LSTM is 0.32% higer than the LSTM with news. The result means the DP framework can make the prediction result more accuracy and robustness.</p><p>Note that the results are obtained by running many trials, since we train stocks separately and predict each price individually due to the different patterns and scales of stock prices. This in total adds up to  <ref type="table">Table 1</ref> is the average of these 451 runs. Furthermore, we provide results for 9 duration over a period in <ref type="figure" target="#fig_5">Figure 6</ref>. The performance of our DP-LSTM is always better than the LSTM with news. Based on the sentiment-ARMA model and adding noise for training, the proposed DP-LSTM is more robust. The investment risk based on this prediction results is reduced. In <ref type="figure" target="#fig_7">Figure 7</ref>, we can see the prediction results of DP-LSTM with is closer to the real S&amp;P 500 index price line than other methods. The two lines (prediction results of LSTM with news and LSTM without news) almost coincide in <ref type="figure" target="#fig_7">Figure 7</ref>. We can tell the subtle differences from the <ref type="table" target="#tab_3">Table 2</ref>, that DP-LSTM is far ahead, and LSTM with news is slightly better than LSTM without news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we integrated the deep neural network with the famous NLP models (VADER) to identify and extract opinions within a given text, combining the stock adjust close price and compound score to reduce the investment risk. We first proposed a sentiment-ARMA model to represent the stock price, which incorporates influential variables (price and news) based on the ARMA model. Then, a DP-LSTM deep neural network was proposed to predict stock price according to the sentiment-ARMA model, which combines the LSTM, compound score of news articles and differential privacy method. News are not all objective. If we rely on the information extracted from the news for prediction fully, we may increase bias because of some non-objective reports. Therefore, the DP-LSTM enhance robustness of the prediction model. Experiment results based on the S&amp;P 500 stocks show that the proposed DP-LSTM network can predict the stock price accurately with robust performance, especially for S&amp;P 500 index that reflects the general trend of the market. S&amp;P 500 prediction results show that the differential privacy method can significantly improve the robustness and accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>NLTK processing. For preprocessing, each news title will be tokenized into individual words. Then applying SentimentIntensityAnalyzer from NLTK vadar to calculate the polarity score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Positive wordcloud (left) and negative wordcloud (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>LSTM procedure (named sentiment-LSTM) is aimed to minimize the following loss function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Schematic diagram of rolling window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>NLTK result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Mean prediction accuracies of the DP-LSTM and vanilla LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>shows the MPAs of the proposed DP-LSTM and vanilla LSTM for comparison. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Prediction result of LSTM based on price.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:1912.10806v1 [q-fin.ST] 20 Dec 2019 polarity. Sentiment analysis allows us to understand if newspapers are talking positively or negatively about the financial market, get key insights about the stock's future trend market.</figDesc><table /><note>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>S&amp;P 500 predicted results. 451 runs. The results shown in</figDesc><table><row><cell>Metrics</cell><cell cols="2">LSTM without news LSTM with news</cell><cell>DP-LSTM</cell></row><row><cell>MSE</cell><cell>580.9226827</cell><cell>536.6306251</cell><cell>198.7500672</cell></row><row><cell>Accuracy</cell><cell>0.99263803</cell><cell>0.99292492</cell><cell>0.99582651</cell></row><row><cell>Mean error percent</cell><cell>0.00736197</cell><cell>0.00707508</cell><cell>0.00417349</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Risk management via anomaly circumvent: mnemonic deep learning for midterm stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd KDD Workshop on Anomaly Detection in Finance (Anchorage &apos;19)</title>
		<meeting>2nd KDD Workshop on Anomaly Detection in Finance (Anchorage &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating a piece-wise linear representation method and a neural network model for stock trading points prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="80" to="92" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for stock prediction using numerical and textual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Akita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Does summarization help stock prediction? A news impact analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="26" to="34" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for event-driven stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Differential privacy and machine learning: a survey and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Zhanglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7584</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A general approach to adding differential privacy to iterative training procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06210</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lecuyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03471</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bat-neural network multi-agent system (BN-NMAS) for stock price prediction: Case study of DAX stock price</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Hafezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Shahrabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Hadavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="196" to="210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating a piecewise linear representation method and a neural network model for stock trading points prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">-</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hao</forename><surname>Chin-Yuan Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="80" to="92" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short term memory networks for anomaly detection in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Malhotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Presses universitaires de Louvain</title>
		<meeting>Presses universitaires de Louvain</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haşim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Ep</forename><surname>Box</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Affective computing and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimistic bull or pessimistic bear: adaptive deep reinforcement learning for stock portfolio allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
