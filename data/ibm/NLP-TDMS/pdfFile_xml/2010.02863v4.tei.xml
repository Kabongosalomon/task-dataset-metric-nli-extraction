<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIRECTIONAL GRAPH NETWORKS ANISOTROPIC AGGREGATION IN GRAPH NEURAL NETWORKS VIA DIRECTIONAL VECTOR FIELDS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
							<email>dominique@valencediscovery.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Létourneau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Valence Discovery Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Valence Discovery Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>MILA Montreal</addrLine>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DIRECTIONAL GRAPH NETWORKS ANISOTROPIC AGGREGATION IN GRAPH NEURAL NETWORKS VIA DIRECTIONAL VECTOR FIELDS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The lack of anisotropic kernels in graph neural networks (GNNs) strongly limits their expressiveness, contributing to well-known issues such as over-smoothing. To overcome this limitation, we propose the first globally consistent anisotropic kernels for GNNs, allowing for graph convolutions that are defined according to topologicaly-derived directional flows. First, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. Then, we propose the use of the Laplacian eigenvectors as such vector field. We show that the method generalizes CNNs on an n-dimensional grid and is provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman 1-WL test. We evaluate our method on different standard benchmarks and see a relative error reduction of 8% on the CIFAR10 graph dataset and 11% to 32% on the molecular ZINC dataset, and a relative increase in precision of 1.6% on the MolPCBA dataset. An important outcome of this work is that it enables graph networks to embed directions in an unsupervised way, thus allowing a better representation of the anisotropic features in different physical or biological problems. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most important distinctions between convolutional neural networks (CNNs) and graph neural networks (GNNs) is that CNNs allow for any convolutional kernel, while most GNN methods are limited to symmetric kernels (also called isotropic kernels) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>. There are some implementations of asymmetric kernels using gated mechanisms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>, motif attention <ref type="bibr" target="#b37">[38]</ref>, edge features <ref type="bibr" target="#b13">[14]</ref>, port numbering <ref type="bibr" target="#b40">[41]</ref> or the 3D structure of molecules <ref type="bibr" target="#b25">[26]</ref>.</p><p>However, to the best of our knowledge, there are currently no methods that allow asymmetric graph kernels that are dependent on the full graph structure or directional flows. They either depend on local structures or local features. This is in opposition to images, which exhibit canonical directions: the horizontal and vertical axes. The absence of an analogous concept in graphs makes it difficult to define directional message passing and to produce an analogue of the directional frequency filters (or Gabor filters) widely present in image processing <ref type="bibr" target="#b36">[37]</ref>. In fact, there is numerous evidence that directional filtering is fundamental image processing <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>We propose a novel idea for GNNs: use vector fields in the graph to define directions for the propagation of information. An overview of this framework is presented in figure 1. Using this approach, the usual message-passing structure of a GNN is projected onto globally-defined directions so that the contribution of each neighbouring node n v is weighted by its alignment with the vector fields at the receiving node n u . This enables our method to propagate information via directional derivatives or smoothing of the features.</p><p>In order to define globally consistent directional fields over general graphs, we propose to use the gradients of the low-frequency eigenvectors φ k of the graph Laplacian, since they are known to capture key information about the global</p><formula xml:id="formula_0">= ∇ = = ∇ = ⋮ = concat ⋮ = MLP Graph</formula><p>The a-directional adjacency matrix is given as an input. We then compute the Laplacian matrix .</p><p>: number of nodes : number of edges</p><p>The eigenvectors of are computed and sorted such that has the lowest nonzero eigenvalue and has the -th lowest.</p><p>We compute the -first eigenvectors with a complexity of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(e) (b) (c) (d) (f) (g) (a)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-computed steps</head><p>A graph with the node features is given. is the feature matrix of the graph at the 0th GNN layer, of size × .</p><p>The aggregation matrices , ,…, are taken from the precomputed steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph neural network steps +</head><p>The gradient of is a function of the edges (a matrix) such that ∇ = − if the nodes , are connected, or ∇ = 0 otherwise.</p><p>If the graph has a known direction, it can be encoded as field .</p><p>Each row , : of the field is normalized by it's norm. ,: = ,:</p><p>.:</p><formula xml:id="formula_1">+ •</formula><p>is the directional smoothing matrix. =</p><p>• is the directional derivative matrix.</p><p>,: = ,: − diag :, ,:</p><p>The aggregation matrices , ,…, are used to aggregate the features via the matrix prodict . For we take the absolute value due to the sign ambiguity of .</p><p>is the columnconcatenation of all directional and a-directional aggregations.</p><p>The complexity is , or if the aggregations are parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This is the only step with learned parameters.</head><p>Based on the GCN method, each aggregation is followed by a multi layer perceptron (MLP) on all the features.</p><p>The MLP is applied on the columns of , thus we have a complexity of .</p><p>• has columns</p><p>• has 2 + 1 columns  structure of graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, these eigenvectors can be used to define optimal partitions of the nodes in a graph, to give a natural ordering <ref type="bibr" target="#b31">[32]</ref>, and to find the dominant directions of the graph diffusion process <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>. Further, we show that they generalize the horizontal and vertical directional flows in a grid (see <ref type="figure" target="#fig_1">figure 2</ref>), allowing them to guide the aggregation and mimic the asymmetric and directional kernels present in computer vision. In fact, we demonstrate mathematically that our work generalizes CNNs, by reproducing all convolutional kernels of radius R in an n-dimensional grid, while also bringing the powerful data augmentation capabilities of reflection, rotation or distortion of the directions. Additionally, we also prove that our directional graph networks (DGNs) are more discriminative than standard GNNs in regards to the Weisfeiler-Lehman 1-WL test, confirming an increase of expressiveness.</p><formula xml:id="formula_2">• has columns ,<label>, , , , , , , , , , , → +</label></formula><p>We further show that our DGN model theoretically and empirically allows for efficient message passing across distant communities, which counteracts the well-known problem of over-smoothing in GNNs. Alternative methods reduce the impact of over-smoothing by using skip connections <ref type="bibr" target="#b32">[33]</ref>, global pooling <ref type="bibr" target="#b0">[1]</ref>, or randomly dropping edges during training time <ref type="bibr" target="#b38">[39]</ref>, but without solving the underlying problem.</p><p>Our method distinguishes itself from other spectral GNNs since the literature usually uses the low frequencies to estimate local Fourier transforms in the graph <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref>. Instead, we do not try to approximate the Fourier transform, but only to define a directional flow at each node and guide the aggregation.</p><p>We tested our method on 5 standard datasets from <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>, using two types of architectures, and either using or ignoring edge features. In all cases, we observed state-of-the-art results from the proposed DGN, with relative improvements of 8% on CIFAR10, 11-32% on ZINC, 0.8% on MolHIV and 1.6% on MolPCBA. Most of the improvement is attributed to the directional derivative aggregator, highlighting our method's ability of capturing directional high-frequency signals in graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theoretical development 2.1 Intuitive overview</head><p>One of the biggest limitations of current GNN methods compared to CNNs is the inability to do message passing in a specific direction such as the horizontal one in a grid graph. In fact, it is difficult to define directions or coordinates based solely on the shape of the graph.</p><p>The lack of directions strongly limits the discriminative abilities of GNNs to understand local structures and simple feature transformations. Most GNNs are invariant to the permutation of the neighbours' features, so the nodes' received signal is not influenced by swapping the features of two neighbours. Therefore, several layers in a deep network will be employed to understand these simple changes instead of being used for higher level features, leading to problematic phenomena such as a over-squashing <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the first part of the theoretical development, we develop the mathematical theory for general vector fields F . Intuitively, defining a vector field over a graph corresponds to assigning a scalar weight to edges corresponding to the magnitude of the flow in that direction. Note that F has the same shape as the adjacency matrix and the same zero entries. As an example a left-to-right flow in a grid corresponds to a matrix with positive values over all left-to-right edges, negative over the right-to-left edges and 0 on the vertical edges.</p><p>In the second part, we set F to be the gradient of the low-frequency eigenvectors of the Laplacian. Using this directional field, we show that the expressiveness of GNNs can be improved, while providing an intuitive directional flows over a variety of graphs (see <ref type="figure" target="#fig_1">figure 2</ref>). For example, we prove that in grid-shaped graphs some of these eigenvectors correspond to the horizontal and vertical flows. Again, we observe in the Minnesota map that the first 3 non-constant eigenvectors produce logical directions, namely South/North, suburb/city, and West/East.</p><p>Another important contribution-also noted in figure 2-is the ability to define any kind of directional flow based on prior knowledge of the problem. Hence, instead of relying on eigenvectors to find directions in a map, we can simply use the cardinal directions or the rush-hour traffic flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview of the theoretical contributions</head><p>Vector fields in a graph. Using directions in a graph is novel and not intuitive, so our first step is to define a simple nomenclature where we use a vector field to define a directional flow at each node.</p><p>Directional smoothing and derivatives. To make use of vector fields over graphs, we define aggregation matrices that can either smooth the signal (low pass filter) or compute its derivative (high pass filter) according to the directions specified by the vector field.</p><p>Gradient of the Laplacian eigenvectors. We show that using the gradient of the low-frequency eigenvectors of the graph Laplacian generates interpretable vector fields that counteract the over-smoothing problem.</p><p>Generalization of CNNs. We demonstrate that, when applied to a grid graph, the eigenvector-based directional aggregation generalizes convolutional neural networks.</p><p>Comparison to the Weisfeiler-Lehman (WL) test. We prove that the proposed DGN is more expressive than the 1-WL test, and thus more expressive than ordinary GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Vector fields in a graph</head><p>This section presents the ideas of differential geometry applied to graphs, with the goal of finding proper definitions of scalar products, gradients and directional derivatives. For reference see for example <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Let G = (V, E) be a graph with V the set of vertices and E ⊂ V × V the set of edges. The graph is undirected meaning that (i, j) ∈ E iff (j, i) ∈ E. Define the vector spaces L 2 (V ) and L 2 (E) as the set of maps V → R and E → R with x, y ∈ L 2 (V ) and F , H ∈ L 2 (E) and scalar products</p><formula xml:id="formula_3">x, y L 2 (V ) : = i∈V x i y i F , H L 2 (E) : = (i,j)∈E F (i,j) H (i,j)<label>(1)</label></formula><p>Think of E as the "tangent space" to V and of L 2 (E) as the set of "vector fields" on the space V with each row F i,: representing a vector at the i-th node, and the element F i,j being the component of the vector going from node i to j through edge e ij . Note that with n the number of nodes in G, any x ∈ L 2 (V ) can be represented as an n coordinates vector and F ∈ L 2 (E) can be represented as an n × n matrix.</p><p>Define the pointwise scalar product as the map L 2 (E) × L 2 (E) → L 2 (V ) taking 2 vector fields and returning their inner product at each point of V , at the node i is defined by equation 2.</p><formula xml:id="formula_4">F , H i := j:(i,j)∈E F i,j H i,j<label>(2)</label></formula><p>In equation 3, we define the gradient ∇ as a mapping L 2 (V ) → L 2 (E) and the divergence div as a mapping L 2 (E) → L 2 (V ), thus leading to an analogue of the directional derivative in equation 4.</p><formula xml:id="formula_5">(∇x) (i,j) := x(j) − x(i) (div F ) i := j:(i,j)∈E F (i,j)<label>(3)</label></formula><p>Definition 1. The directional derivative of the function x on the graph G in the direction of the vector fieldF where each vector is of unit-norm is</p><formula xml:id="formula_6">DF x(i) := ∇x,F i = j:(i,j)∈E (x(j) − x(i))F i,j<label>(4)</label></formula><p>|F | will denote the absolute value of F and ||F i,: || L p the L p -norm of the i-th row of F . We also define the forward/backward directions as the positive/negative parts of the field F ± .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Directional smoothing and derivatives</head><p>Next, we show how the vector field F is used to guide the graph aggregation by projecting the incoming messages. Specifically, we define the weighted aggregation matrices B av and B dx that allow to compute the directional smoothing and directional derivative of the node features, as presented visually in <ref type="figure" target="#fig_0">figure 1</ref></p><formula xml:id="formula_7">-d.</formula><p>The directional average matrix B av is the weighted aggregation matrix such that all weights are positives and all rows have an L 1 -norm equal to 1, as shown in equation 5 and theorem 2.1, with a proof in the appendix C.1.</p><formula xml:id="formula_8">B av (F ) i,: = |F i,: | ||F i,: || L 1 +<label>(5)</label></formula><p>The variable is an arbitrarily small positive number used to avoid floating-point errors. The L 1 -norm denominator is a local row-wise normalization. The aggregator works by assigning a large weight to the elements in the forward or backward direction of the field, while assigning a small weight to the other elements, with a total weight of 1. The directional derivative matrix B dx is defined in (6) and theorem 2.2, with the proof in appendix C.2. Again, the denominator is a local row-wise normalization but can be replaced by a global normalization. diag(a) is a square, diagonal matrix with diagonal entries given by a. The aggregator works by subtracting the projected forward message by the backward message (similar to a center derivative), with an additional diagonal term to balance both directions. x These aggregators are directional, interpretable and complementary, making them ideal choices for GNNs. We discuss the choice of aggregators in more details in appendix A, while also providing alternative aggregation matrices such as the center-balanced smoothing, the forward-copy, the phantom zero-padding, and the hardening of the aggregators using softmax/argmax on the field. We further provide a visual interpretation of the B av and B dx aggregators in <ref type="figure" target="#fig_4">figure 3</ref>. Interestingly, we also note in appendix A.1 that B av and B dx yield respectively the mean and Laplacian aggregations when F is a vector field such that all entries are constant F ij = ±C.  </p><formula xml:id="formula_9">B dx (F ) i,: =F i,: − diag jF :,j i,: F i,: = F i,: ||F i,: || L 1 + (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Gradient of the Laplacian eigenvectors as interpretable vector fields</head><p>In this section we give theoretical support for the choice of gradients of the eigenfunctions of the Laplacian as sensible vectors along which to do directional message passing since they are interpretable and allow to reduce the oversmoothing. This section gives a theoretical ground to the intuitive directions presented in <ref type="figure" target="#fig_1">figure 2</ref>, and is the motivation behind steps (b-c) in <ref type="figure" target="#fig_0">figure 1</ref>.</p><p>As usual the combinatorial, degree-normalized and symmetric normalized Laplacian are defined as</p><formula xml:id="formula_10">L = D − A, L norm = D −1 L, L sym = D − 1 2 LD − 1 2<label>(7)</label></formula><p>The eigenvectors of these matrices are known to capture many essential properties of graphs, making them a natural foundation for directional message passing. For example, the Laplacian eigenvectors corresponding to the smallest eigenvalues (i.e., the low frequency eigenvectors) effectively capture the community structure of a graph, and these eigenvectors also play the role of Fourier modes in graph signal processing <ref type="bibr" target="#b17">[18]</ref>. Indeed, the Laplacian eigenvectors hold such rich information about graph structure that their study is the focus of the mathematical subfield of spectral graph theory <ref type="bibr" target="#b7">[8]</ref>.</p><p>In order to illustrate the utility of these eigenvectors in the context of GNNs, we show that the low-frequency eigenvectors provide a natural direction that allows us to pass messages between distant nodes in a graph. In particular, we show in theorem 2.3 (proved in appendix C.3) that by passing information in the direction of φ 1 , the eigenvector associated to the lowest non-trivial frequency of L norm , DGNs can efficiently share information between distant nodes of the graph by reducing the diffusion distance between them. This idea is reflected in <ref type="figure" target="#fig_1">figure 2</ref>, where we see that the eigenvectors of the Laplacian give directions that correspond to a natural notion of distance on real-world graphs.</p><p>In the next paragraphs, we will prove that following the gradient of the eigenvectors allows to effectively reduce the heat-kernel distance between pairs of nodes.</p><p>Consider the transition matrix W = D −1 A. Its entries can be used to define a random walk with probability to move from node x to node y equal to p 1 (x, y) = 1 dx if x and y are neighbors and 0 if not. Notice that the probability to transition from x to y in k steps is given by the x, y entry of the matrix W k . This matrix is also called the discrete heat kernel p k (x, y) = (W k ) x,y . Given a Markov processX k defined by the transition matrices W k , j = 1, ..., k, we can define a continuous time random walk on the same graph in the following way. Let N t be a mean 1 Poisson random variable, the continuous time random variable is defined by X t :=X Nt with transition probability q t (x, y) = P (X t = y|x 0 = x).</p><p>In <ref type="bibr" target="#b2">[3]</ref>, the following identity is shown</p><formula xml:id="formula_11">q t (x, y) = ∞ n=0 e −t t k k! p k (x, y)</formula><p>Or in matrix form q t = e t(W −I) = e −tLnorm . This transition probability is also called the continuous time heat kernel because it satisfies the continuous time heat equation on graphs d dt q t = −L norm q t . In <ref type="bibr" target="#b8">[9]</ref> the following distance is defined Definition 2 (Diffusion distance). The diffusion distance at time t between the nodes x, y is</p><formula xml:id="formula_12">d t (x, y) := z∈V q t (x, z) − q t (y, z) 2 1 2 (8)</formula><p>The diffusion distance is small when there is high probability that two random walks starting at x and y meet at time t. The diffusion distance is used as a model of how the data at a node x influences a node y in a GNN. The symmetrisation of the heat kernel in the diffusion distance and the use of continuous time are slight departure from the actual process of information diffusion in a GNN but allow us to describe the important phenomenons with much simpler statements. Definition 3 (Gradient step). Suppose the two neighboring nodes x and z are such that φ(z) − φ(x) is maximal among the neighbors of x, then we will say z is obtained from x by taking a step in the direction of the gradient ∇φ. Theorem 2.3 (Gradient steps reduce diffusion distance). Let x, y be nodes such that φ 1 (x) &lt; φ 1 (y). Let x be the node obtained from x by taking one step in the direction of ∇φ 1 , then there is a constant C such that for C ≤ t we have</p><formula xml:id="formula_13">d t (x , y) &lt; d t (x, y).</formula><p>With the reduction in distance being proportional to e −λ1 .</p><p>From this theorem, we see that moving from node x to node x by following the gradient of the eigenvector φ 1 is guaranteed to reduce the heat kernel distance with a destination node y. While the theorem always holds for φ 1 , it should be true for higher frequency eigenvectors if the graph has added structure for example if it is an approximation of a surface or a higher dimensional manifold.</p><p>In the context of GNNs, Theorem 2.3 also has implications for the well-known problems of over-smoothing and over-squashing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. In most GNN models, node representations become over-smoothed after several rounds of message passing, as the representations tend to reach a mean-field equilibrium equivalent to the stationary distribution of a random walk <ref type="bibr" target="#b17">[18]</ref>. Researchers have also highlighted the related issue of over-squashing, which reflects the inability for GNNs to propagate informative signals between distant nodes in a graph <ref type="bibr" target="#b0">[1]</ref>.</p><p>Both these problems are related to the fact that the influence of one node's input on the final representation of another node in a GNN is correlated with the diffusion distance between the nodes <ref type="bibr" target="#b45">[46]</ref>. Theorem 2.3 highlights how the DGN approach can alleviate these issues. In particular, the Laplacian eigenfunctions reveal directions that can counteract over-smoothing and over-squashing by allowing efficient propagation of information between distant nodes instead of following a diffusion process.</p><p>Finally it is interesting to note that by selecting different eigenvectors as basis of directions, our method further aligns with a theorem that multiple independent aggregators are needed to distinguish neighbourhoods of nodes with continuous features <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Choosing a basis of the Laplacian eigenspace</head><p>When using eigenvectors of the Laplacian φ i to define directions in a graph, we need to keep in mind that there is never a single eigenvector associated to an eigenvalue, but a whole eigenspace. If an eigenvalue has multiplicity of k, the associated eigenspace has dimension k and any collection of k orthogonal vectors could be chosen as basis of that space and as vectors for the definitions of the aggregation matrices B defined in the previous sections.</p><p>Disconnected graphs. When a graph is disconnected, then the eigenfunctions will simply be the combination of the eigenfunctions of each connected components. Hence, one must consider φ i as the i-th eigenvector of each component when taken separately.</p><p>Normalizing the eigenvectors. For an eigenvalue of multiplicity 1, there are always two unit norm eigenvectors of opposite sign, which poses a problem during the directional aggregation. We can make a choice of sign and later take the absolute value (i.e. B av in equation <ref type="bibr" target="#b4">5</ref>). An alternative that applies to multiplicities higher than 1 is to take samples of orthonormal bases of the eigenspace and use each choice to augment the training (see section 2.10).</p><p>Multiplicities greater than 1. Although multiplicities higher than one do happen for low-frequencies (square grids have a multiplicity 2 for λ 1 ) this is not common in "real-world graphs" since it suggests symmetries in the graph which are uncommon. Furthermore, we found no λ 1 multiplicity greater than 1 in the ZINC and PATTERN datasets. We further discuss these rare cases and how to deal with them in appendix B.4.</p><p>Orthogonal directions. Although all φ are orthogonal, their gradients, used to define directions, are not always locally orthogonal (e.g. there are many horizontal flows in the grid). This concern is left to be addressed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Generalization of the convolution on a grid</head><p>In this section we show that our method generalizes CNNs by allowing to define any radius-R convolutional kernels in grid-shaped graphs. The radius-R kernel at node u is a convolutional kernel that takes the weighted sum of all nodes v at a distance d(u, v) ≤ R.</p><p>Consider the lattice graph Γ of size N 1 × N 2 × ... × N n where each vertices are connected to their direct non-diagonal neighbour. We know from Lemma C.1 that, for each dimension, there is an eigenvector that is only a function of this specific dimension. For example, the lowest frequency eigenvector φ 1 always flows in the direction of the longest length. Hence, the Laplacian eigenvectors of the grid can play a role analogous to the axes in Euclidean space, as shown in figure 2.</p><p>With this knowledge, we show in theorem 2.4 (proven in C.6), that we can generalize all convolutional kernels in an n-dimensional grid. This is a strong result since it demonstrates that our DGN framework generalizes CNNs when applied on a grid, thus closing the gap between GNNs and the highly successful CNNs on image tasks. Note that when the size of a given dimension is an integer multiple of another direction, e.g. N = M or N = 3M , then you will find a multiplicity of 2 for the m − th eigenvector. Hence, the eigenvector used to define the direction is not unique. This does not void theorem 2.4 since the eigenvectors flowing in the horizontal/vertical directions are still valid choices. </p><formula xml:id="formula_14">CNN equivalent on image × , with &gt; ; % ≠ 0 Graph aggregation 1 1 = 2 1 1 -1 = 2 1 1 1 = 2 1 -1 = 2 1 2 + 3 4 + 5 4 − 5 2 − 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Extending the radius of the aggregation kernel</head><p>Having aggregation kernels for neighbours of distance 2 or 3 is important to improve the expressiveness of GNNs, their ability to understand patterns, and to reduce the number of layers required. However, the lack of directions in GNNs strongly limits the radius of the kernels since, given a graph of regular degree d, a mean/sum aggregation at a radius-R will result in a heavy over-squashing of O(d R ) messages. Using the directional fields, we can enumerate different paths, thus assigning a different weight for different R-distant neighbours. This method, proposed in appendix A.7, avoids the over-squashing. (Empirical results on this extension are left for future work.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Comparison with Weisfeiler-Lehman (WL) test</head><p>We also compare the expressiveness of the Directional Graph Networks with the classical WL graph isomorphism test which is often used to classify the expressivity of graph neural networks <ref type="bibr" target="#b44">[45]</ref>. In theorem 2.5 (proven in appendix C. <ref type="bibr" target="#b6">7)</ref> we show that DGNs are capable of distinguishing pairs of graphs that the 1-WL test (and so ordinary GNNs) cannot differentiate. Theorem 2.5 (Comparison with 1-WL test). DGNs using the mean aggregator, any directional aggregator of the first Laplacian eigenvector and injective degree-scalers are strictly more powerful than the 1-WL test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Data augmentation</head><p>Another theoretical result is that the directions in the graph allow to replicate some of the most common data augmentation techniques used in computer vision, namely reflection, rotation and distortion. The main difference is that, instead of modifying the image (such as a 5 • rotation), the proposed transformation is applied on the vector field defining the aggregation kernel (thus rotating the kernel by −5 • without changing the image). This offers the advantage of avoiding to pre-process the data since the augmentation is done directly on the kernel at each iteration of the training.</p><p>The simplest augmentation is the vector field flipping, which is done changing the sign of the field F , as stated in definition 4. This changes the sign of B dx , but leaves B av unchanged. Definition 4 (Reflection of the vector field). For a vector field F , the reflected field is −F .</p><p>Let F 1 , F 2 be vector fields in a graph, withF 1 andF 2 being the field normalized such that each row has a unitary L 2 -norm. Define the angle vector α by (F 1 ) i,: , (F 2 ) i,: = cos(α i ). The vector fieldF ⊥ 2 is the normalized component ofF 2 perpendicular toF 1 . The equation below definesF ⊥ 2 . The next equation defines the angle</p><formula xml:id="formula_15">(F ⊥ 2 ) i,: = (F 2 − F 1 ,F 2 F 1 ) i,: ||(F 2 − F 1 ,F 2 F 1 ) i,: || Notice that we then have the decomposition (F 2 ) i,: = cos(α i )(F 1 ) i,: + sin(α i )(F ⊥ 2 ) i,:</formula><p>. Definition 5 (Rotation of the vector fields). ForF 1 andF 2 non-colinear vector fields with each vector of unitary length, their rotation by the angle θ in the plane formed by</p><formula xml:id="formula_16">{F 1 ,F 2 } iŝ F θ 1 =F 1 diag(cos θ) +F ⊥ 2 diag(sin θ) F θ 2 =F 1 diag(cos(θ + α)) +F ⊥ 2 diag(sin(θ + α))<label>(9)</label></formula><p>Finally, the following augmentation has a similar effect to a wave distortion applied on images. Definition 6 (Random distortion of the vector field). For vector field F and anti-symmetric random noise matrix R, its randomly distorted field is F = F + R • A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>We implemented the models using the DGL and PyTorch libraries and we provide the code at the address https://github.com/Saro00/DGN. We test our method on standard benchmarks from <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>, namely ZINC, CIFAR10, PATTERN, MolHIV and MolPCBA with more details on the datasets and how we enforce a fair comparison in appendix B.1.</p><p>For the empirical experiments we inserted our proposed aggregation method in two different type of message passing architectures used in the literature: a simple convolutional architecture similar to the one present in GCN (equation 10a) <ref type="bibr" target="#b24">[25]</ref> and a more complex and general one typical of MPNNs (10b) <ref type="bibr" target="#b13">[14]</ref> with or without edge features e ji . The time complexity of our approach is O(Em), which is identical to PNA <ref type="bibr" target="#b9">[10]</ref>, where E is the number of edges and m the number of aggregators, with an additional O(Ek) to pre-compute the k-first eigenvectors, as explained in the appendix B.2.</p><formula xml:id="formula_17">X (t+1) i = U (j,i)∈E X (t) j (10a) X (t+1) i = U X (t) i , (j,i)∈E M X (t) i , X (t) j , e ji optional<label>(10b)</label></formula><p>Here, is an operator which concatenates the results of multiple aggregators, X is the node features, M is a linear transformation and U a multiple layer perceptron (MLP). This simple architecture of equation 10a is observed visually in steps (f-g) of figure 1.</p><p>We further use degree scalers S(d, α) defined below to scale the aggregation results according to each node's degree, as proposed by the PNA model <ref type="bibr" target="#b9">[10]</ref>. Here, d is the degree of a given node, δ is the average node degree in the training set, and α is a parameter set to −1 for degree-attenuation and 1 for degree amplification. Note that each degree scaler is applied to the result of each aggregator, and the results are concatenated.</p><formula xml:id="formula_18">S(d, α) = log(d + 1) δ α , δ = 1 |train| i ∈ train log(d i + 1)<label>(11)</label></formula><p>We tested the directional aggregators across the datasets using the gradient of the first k eigenvectors ∇φ 1,...,k as the underlying vector fields. Here, k is a hyperparameter, usually 1 or 2, but could be bigger for high-dimensional graphs.</p><p>To deal with the arbitrary sign of the eigenvectors, we take the absolute value of the result of equation 6, making it invariant to a reflection of the field. In case of a disconnected graph, φ i is the i-th eigenvector of each connected component. Despite the numerous aggregators proposed in appendix A, only B dx and B av are tested empirically.</p><p>The metrics used to measure the performance of a model depend are enforced for each dataset and provided by <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>. In particular, we use the mean absolute error (MAE), the accuracy (acc), the area under the receiver operating curve (ROC-AUC), and the average precision (AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Directional aggregation</head><p>Using the benchmarks introduced in section 3, we present in <ref type="figure">figure 5</ref> a fair comparison of various aggregation strategies using the same parameter budget and hyperparameters. We see a consistent boost in the performance for simple, complex and complex with edges models using directional aggregators compared to the mean-aggregator baseline.</p><p>With our theoretical analysis in mind, we expected to perform well on PATTERN since the flow of the first eigenvectors are meaningful directions in a stochastic block model (i.e., these eigenvectors tend to correlate with community membership). The results match our expectations, outperforming all the previous models.</p><p>In particular, we see a significant improvement in the molecular datasets (ZINC, MolHIV and MolPCBA) when using the directional aggregators, especially for the derivative aggregation B 1 dx (noted dx 1 in <ref type="figure">figure 5</ref>). We believe this is due to the capacity to efficiently move messages across opposite parts of the molecule and to better understand the role of atom pairs. We further believe that the derivative aggregator is better able to capture high-frequency directional signals, similarly to the Gabor filters in computer vision.</p><p>Further, the thesis that DGNs can bridge the gap between CNNs and GNNs is supported by the clear improvements on CIFAR10 over the baselines.</p><p>In the work by <ref type="bibr" target="#b11">[12]</ref>, they proposed the use of positional encoding of the eigenvectors. However, our experiments with the positional encoding of the first 2 non-trivial eigenvectors, noted pos 1 , pos 2 in <ref type="figure">figure 5</ref>, showed no clear improvement on most datasets. In fact, Dwivedi et al. noted that many eigenvectors and high network depths are required for improvements, yet we outperform their results with fewer parameters, less depth, and only 1-2 eigenvectors, further motivating their use as directional flows instead of positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to the literature</head><p>In order to compare our model with the literature, we fine-tuned it on the various datasets and we report its performance in figure 6. We observe that DGN provides significant improvement across all benchmarks, highlighting the importance of anisotropic kernels that are dependant on the graph topology.</p><p>Note that the results in <ref type="figure">Figure 6</ref> are better those in <ref type="figure">Figure 5</ref> since the latter uses a more exhaustive parameter search, and uses the min/max aggregators proposed in PNA <ref type="bibr" target="#b9">[10]</ref> alongside the directional aggregators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Preliminary results of data augmentation</head><p>To evaluate the effectiveness of the proposed augmentation, we trained the models on a reduced version of the CIFAR10 dataset. The results in <ref type="figure" target="#fig_7">figure 7</ref> show clearly a higher expressive power of the dx aggregator, enabling it to fit well  <ref type="figure">Figure 5</ref>: Test set results using a parameter budget of ∼ 100k with the same hyperparameters as <ref type="bibr" target="#b9">[10]</ref>, except MolPCBA with a budget of ∼ 7M . The low-frequency Laplacian eigenvectors are used to define the directions, except for CIFAR10 that uses the coordinates of the image. For brevity, we denote dx i and av i as the directional derivative B i dx and smoothing B i av aggregators of the i-th direction. We also denote pos i as the i-th eigenvector used as positional encoding for the mean aggregator.  <ref type="figure">Figure 6</ref>: Fine-tuned results of the DGN model against models from <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>: GCN <ref type="bibr" target="#b24">[25]</ref>, GraphSage <ref type="bibr" target="#b16">[17]</ref>, GIN <ref type="bibr" target="#b44">[45]</ref>, GAT <ref type="bibr" target="#b42">[43]</ref>, MoNet <ref type="bibr" target="#b34">[35]</ref>, GatedGCN <ref type="bibr" target="#b3">[4]</ref> and PNA <ref type="bibr" target="#b9">[10]</ref>. All the models use ∼ 100k parameters, except those with * who use 300k to 6.5M . the training data. For a small dataset, this comes at the cost of overfitting and a reduced test-set performance, but we observe that randomly rotating or distorting the kernels counteracts the overfitting and improves the generalization.</p><p>As expected, the performance decreases when the rotation or distortion is too high since the augmented graph changes too much. In computer vision images similar to CIFAR10 are usually rotated by less than 30 • <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b35">36]</ref>. Further, due to the constant number of parameters across models, less parameters are attributed to the mean aggregation in the directional models, thus it cannot fit well the data when the rotation/distortion is too strong since the directions are less informative. We expect large models to perform better at high angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The proposed DGN method allows to address many problems of GNNs, including the lack of anisotropy, the low expressiveness, the over-smoothing and over-squashing. For the first time in graph networks, we generalize the directional properties of CNNs and their data augmentation capabilities. Based on the intuitive idea that the lowfrequency eigenvectors of the graph Laplacian gives an interpretable directional flow, we backed our work by a set of strong theoretical results showing that these eigenvectors are important in connecting nodes that are far away and improving the expressiveness in regards to the WL-test.</p><p>The work being also supported by strong empirical results, we believe it will give rise to a new family of directional GNNs. In fact, we introduce in the appendix different avenues for future work, including the hardening of the aggregators A.4, the introduction of a zero-padding at the boundaries A.6, the implementation of radius-R kernels A.7, and the full study of directional data augmentation. Future methods could also improve the choice of multiple directions beyond the selection of the k-lowest frequencies.</p><p>0°2°5°10°20°45°R otation angle  Broader Impact. This work will extend the usability of graph networks to all problems with engineering and physically defined directions, thus making GNN a new laboratory for signal processing, physics, material science and molecular and cell biology. In fact, the anisotropy present in a wide variety of systems could be expressed as vector fields (spinor, tensor) compatible with the DGN framework, without the need of eigenvectors. One example is magnetic anisotropicity in metals, alloys and organic molecules that is dependant on the relative orientation to the magnetic field. Other examples are the response of materials to high electromagnetic fields; all kind of field propagation in crystals lattices (vibrations, heat, shear and frictional force, young modulus, light refraction, birefringence); multi-body or liquid motion; magnons and solitons in different media, fracture propagation, traffic modelling; developmental biology and embryology, and design of novel materials and constrained structures. Finally applications based on neural operators for ODE/PDE may benefit as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix -Choices of directional aggregators</head><p>This appendix helps understand the choice of B av and B dx in section 2.4 and presents different directional aggregators that can be used as an alternative to the ones proposed.</p><p>A simple alternative to the directional smoothing and directional derivative operator is to simply take the forward/backward values according to the underlying positive/negative parts of the field F , since it can effectively replicate them. However, there are many advantage of using B av,dx . First, one can decide to use either of them and still have an interpretable aggregation with half the parameters. Then, we also notice that B av,dx regularize the parameter by forcing the network to take both forward and backward neighbours into account at each time, and avoids one of the neighbours becoming too important. Lastly, they are robust to a change of sign of the eigenvectors since B av is sign invariant and B dx will only change the sign of the results, which is not the case for forward/backward aggregations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Retrieving the mean and Laplacian aggregations</head><p>It is interesting to note that we can recover simple aggregators from the aggregation matrices B av (F ) and B dx (F ).</p><p>Let F be a vector field such that all edges are equally weighted F ij = ±C for all edges (i, j). Then, the aggregator B av is equivalent to a mean aggregation:</p><formula xml:id="formula_19">B av (F )x = D −1 Ax</formula><p>Under the condition F ij = C, the differential aggregator is equivalent to a Laplacian operator L normalized using the degree</p><formula xml:id="formula_20">D B dx (CA)x = D −1 (A − D)x = −D −1 Lx</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Global field normalization</head><p>The proposed aggregators are defined with a row-wise normalized field</p><formula xml:id="formula_21">F i,: = F i,: ||F i,: || L P</formula><p>meaning that all the vectors are of unit-norm and the aggregation/message passing is done only according to the direction of the vectors, not their amplitude. However, it is also possible to do a global normalization of the field F by taking a matrix-norm instead of a vector-norm. Doing so will modulate the aggregation by the amplitude of the field at each node. One needs to be careful since a global normalization might be very sensitive to the number of nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Center-balanced aggregators</head><p>A problem arises in the aggregators B dx and B av proposed in equations 5 and 6 when there is an imbalance between the positive and negative terms of F ± . In that case, one of the directions overtakes the other in terms of associated weights.</p><p>An alternative is also to normalize the forward and backward directions separately, to avoid having either the backward or forward direction dominating the message.</p><formula xml:id="formula_22">B av−center (F ) i,: = F + i,: + F − i,: ||F + i,j + F − i,j || L1 , F ± i,: = |F ± i,: | ||F ± i,: || L 1 +<label>(12)</label></formula><p>The same idea can be applied to the derivative aggregator equation 13 where the positive and negative parts of the field F ± are normalized separately to allow to project both the forward and backward messages into a vector field of unit-norm. F + is the out-going field at each node and is used for the forward direction, while F − is the in-going field used for the backward direction. By averaging the forward and backward derivatives, the proposed matrix B dx-center represents the centered derivative matrix.</p><formula xml:id="formula_23">B dx-center (F ) i,: = F i,: − diag   j F :,j   i,: , F i,: = 1 2      F + i,: ||F + i,: || L 1 + forward field + F − i,: ||F − i,: || L 1 + backward field     <label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hardening the aggregators</head><p>The aggregation matrices that we proposed, mainly B dx and B av depend on a smooth vector field F . At any given node, the aggregation will take a weighted sum of the neighbours in relation to the direction of F . Hence, if the field F v at a node v is diagonal in the sense that it gives a non-zero weight to many neighbours, then the aggregator will compute a weighted average of the neighbours.</p><p>Although there are clearly good reasons to have this weighted-average behaviour, it is not necessarily desired in every problem. For example, if we want to move a single node across the graph, this behaviour will smooth the node at every step. Instead, we propose below to soften and harden the aggregations by forcing the field into making a decision on the direction it takes.</p><p>Soft hardening the aggregation is possible by using a softmax with a temperature T on each row to obtain the field F softhard .</p><formula xml:id="formula_24">(F softhard ) i,: = sign(F i,: )softmax(T |F i,: |)<label>(14)</label></formula><p>Hardening the aggregation is possible by using an infinite temperature, which changes the softmax functions into argmax. In this specific case, the node with the highest component of the field will be copied, while all other nodes will be ignored.</p><formula xml:id="formula_25">(F hard ) i,: = sign(F i,: )argmax(|F i,: |)<label>(15)</label></formula><p>An alternative to the aggregators above is to take the softmin/argmin of the negative part and the softmax/argmax of the positive part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Forward and backward copy</head><p>The aggregation matrices B av and B dx have the nice property that if the field is flipped (change of sign), the aggregation gives the same result, except for the sign of B dx . However, there are cases where we want to propagate information in the forward direction of the field, without smoothing it with the backward direction. In this case, we can define the strictly forward and strictly backward fields below, and use them directly with the aggregation matrices.</p><formula xml:id="formula_26">F forward = F + , F backward = F −<label>(16)</label></formula><p>Further, we can use the hardened fields in order to define a forward copy and backward copy, which will simply copy the node in the direction of the highest field component.</p><formula xml:id="formula_27">F forward copy = F + hard , F backward copy = F − hard<label>(17)</label></formula><p>A.6 Phantom zero-padding Some recent work in computer vision has shown the importance of zero-padding to improve CNNs by allowing the network to understand it's position relative to the border <ref type="bibr" target="#b21">[22]</ref>. In contrast, using boundary conditions or reflection padding makes the network completely blind to positional information. In this section, we show that we can mimic the zero-padding in the direction of the field F for both aggregation matrices B av and B dx .</p><p>Starting with the B av matrix, in the case of a missing neighbour in the forward/backward direction, the matrix will compensate by adding more weights to the other direction, due to the denominator which performs a normalization. Instead, we would need the matrix to consider both directions separately so that a missing direction would result in zero padding. Hence, we define B av,0pad below, where either the F + or F − will be 0 on a boundary with strictly in-going/out-going field.</p><p>(B av,0pad ) i,: = 1 2</p><formula xml:id="formula_28">|F + i,: | ||F + i,: || L 1 + + |F − i,: | ||F − i,: || L 1 +<label>(18)</label></formula><p>Following the same argument, we define B dx,0pad below, where either the forward or backward term is ignored. The diagonal term is also removed at the boundary so that the result is a center derivative equal to the subtraction of the forward term with the 0-term on the back (or vice-versa), instead of a forward derivative.</p><formula xml:id="formula_29">B dx−0pad (F ) i,: =        F + i,: if j F − i,j = 0 F − i,: if j F + i,j = 0 1 2 F + i,: + F − i,: − diag j F + :,j + F − :,j i,:</formula><p>, otherwise</p><formula xml:id="formula_30">F + i,: = F + i,: ||F + i,: || L 1 + F − i,: = F − i,: ||F − i,: || L 1 +<label>(19)</label></formula><p>A.7 Extending the radius of the aggregation kernel</p><p>We aim at providing a general radius-R kernel B R that assigns different weights to different subsets of nodes n u at a distance R from the center node n v .</p><p>First, we decompose the matrix B(F ) into positive and negative parts B ± (F ) representing the forward and backward steps aggregation in the field F .</p><formula xml:id="formula_31">B(F ) = B + (F ) − B − (F )<label>(20)</label></formula><formula xml:id="formula_32">Thus, defining B ± f b (F ) i,: = F ± i,:</formula><p>||Fi,:|| L p , we can find different aggregation matrices by using different combinations of walks of radius R. First demonstrated for a grid in theorem 2.4, we generalize it in equation 21 for any graph G.</p><p>Definition 7 (General radius R n-directional kernel). Let S n be the group of permutations over n elements with a set of directional fields F i .</p><formula xml:id="formula_33">B R := V ={v1,v2,...,vn}∈N n ||V || L 1 ≤R, −R≤vi≤R</formula><p>Any choice of walk V with at most R steps using all combinations of v1, v2, ..., vn σ∈Sn optional permutations</p><formula xml:id="formula_34">a V N j=1 (B sgn(v σ(j) ) f b (F σ(j) )) |v σ(j) |</formula><p>Aggregator following the steps V , permuted by Sn <ref type="bibr" target="#b20">(21)</ref> In this equation, n is the number of directional fields and R is the desired radius. V represents all the choices of walk {v 1 , v 2 , ..., v n } in the direction of the fields {F 1 , F 2 , ..., F n }. For example, V = {3, 1, 0, −2} has a radius R = 6, with 3 steps forward of F 1 , 1 step forward of F 2 , and 2 steps backward of F 4 . The sign of each B ± f b is dependant to the sign of v σ(j) , and the power |v σ(j) | is the number of aggregation steps in the directional field F σ(j) . The full equation is thus the combination of all possible choices of paths across the set of fields F i , with all possible permutations. Note that we are restricting the sum to v i having only a possible sign; although matrices don't commute, we avoid choosing different signs since it will likely self-intersect a lower radius walk. The permutations σ are required since, for example, the path up → left is different (in a general graph) than the path left → up.</p><p>This matrix B R has a total of R r=0 (2n) r = (2n) R+1 −1 2n−1 parameters, with a high redundancy since some permutations might be very similar, e.g. for a grid graph we have that up → left is identical to left → up. Hence, we can replace the permutation S n by a reverse ordering, meaning that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Arcsine of the eigenvectors</head><p>Since the eigenvectors φ i are equivalent to the Fourier basis and represent the waves in the graphs, then it is expected that they behave similarity to sine/cosine waves when the graph is similar to a grid. This is further highlighted by the proof that the eigenvectors of a grid are all sines/cosines in appendix C.4.</p><p>Hence, when we define the field F as F i = ∇φ i , we must realize that the gradient will be lower near the minima/maxima of the eigenvector, as it is the case with sine/cosine waves. In the paper, we cope with this problem by dividing by the norm of the field F L 1 in equations 5 and 6.</p><p>Another solution is to use the arcsine of the eigenvectors so that the function eigenvectors become similar to triangle functions and the gradient is almost uniform. However, since the arcsine function works only in the range [−1, 1], then we must first normalize the eigenvector by it's maximum, as given by equation <ref type="bibr" target="#b21">22</ref>.</p><formula xml:id="formula_35">F i asin = ∇ arcsin φ i max(|φ i |)<label>(22)</label></formula><p>B Appendix -Implementation details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Benchmarks and datasets</head><p>We use a variety of benchmarks proposed by <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref> to test the empirical performance of our proposed methods.</p><p>In particular, to have a wide variety of graphs and tasks we chose:</p><p>1. ZINC, a graph regression dataset from molecular chemistry. The task is to predict a score that is a subtraction of computed properties logP − SA, with logP being the computed octanol-water partition coefficient, and SA being the synthetic accessibility score <ref type="bibr" target="#b22">[23]</ref>. 2. CIFAR10, a graph classification dataset from computer vision <ref type="bibr" target="#b28">[29]</ref>. The task is to classify the images into 10 different classes, with a total of 5000 training image per class and 1000 test image per class. Each image has 32 × 32 pixels, but the pixels have been clustered into a graph of ∼ 100 super-pixels. Each super-pixel becomes a node in an almost grid-shaped graph, with 8 edges per node. The clustering uses the code from <ref type="bibr" target="#b26">[27]</ref>, and results in a different number of super-pixels per graph. 3. PATTERN, a node classification synthetic benchmark generated with Stochastic Block Models, which are widely used to model communities in social networks. The task is to classify the nodes into 2 communities and it tests the fundamental ability of recognizing specific predetermined subgraphs. 4. MolHIV, a graph classification benchmark from molecular chemistry. The task is to predict whether a molecule inhibits HIV virus replication or not. The molecules in the training, validation and test sets are divided using a scaffold splitting procedure that splits the molecules based on their two-dimensional structural frameworks. 5. MolPCBA, a graph classification benchmark from molecular chemistry. It consists of measured biological activities of small molecules generated by high-throughput screening. The dataset consists of a total of 437,929 molecules divided using a scaffold slitting procedure and a set of 128 properties to predict for each.</p><p>For the results in <ref type="figure">figure 5</ref>, our goal is to provide a fair comparison to demonstrate the capacity of our proposed aggregators. Therefore, we compare the various methods on both types of architectures using the same hyperparameters tuned in previous works <ref type="bibr" target="#b9">[10]</ref> for similar networks. The models vary exclusively in the aggregation method and the width of the architectures to keep a set parameter budget. Following the indication of the benchmarks' authors, we averaged the performances of the models on 4 runs with different initialization seeds for the benchmarks from <ref type="bibr" target="#b11">[12]</ref> (ZINC, PATTERN and CIFAR10) and 10 runs for the ones from <ref type="bibr" target="#b19">[20]</ref> (MolHIV and MolPCBA 2 ).</p><p>For the results in <ref type="figure">figure 6</ref>, we took the fine tuned results of other models from the corresponding public leaderboards by <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>. For the DGN results we fine tuned the model taking the lowest validation loss across runs with the following hyperparameters (you can also find the fine tuned commands in the documentation of the code repository): In CIFAR10 it is impossible to numerically compute a deterministic vector field with eigenvectors due to the multiplicity of λ 1 being greater than 1. This is caused by the symmetry of the square image, and is extremely rare in real-world graphs. Therefore, we used as underlying vector field the gradient of the coordinates of the image. Note that these directions are provided in the nodes' features in the dataset and available to all models, that they are co-linear to the eigenvectors of the grid as per lemma C.1, and that they mimic the inductive bias in CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation and computational complexity</head><p>Unlike several more expressive graph networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>, our method does not require a computational complexity superlinear with the size of the graph. The calculation of the first k eigenvectors during pretraining, done using Lanczos method <ref type="bibr" target="#b29">[30]</ref> and the sparse module of Scipy, has a time complexity of O(Ek) where E is the number of edges. During training the complexity is equivalent to a m-aggregator GNN O(Em) <ref type="bibr" target="#b9">[10]</ref> for the aggregation and O(N m) for the MLP.</p><p>To all the architectures we added residual connections <ref type="bibr" target="#b18">[19]</ref>, batch normalization <ref type="bibr" target="#b20">[21]</ref> and graph size normalization <ref type="bibr" target="#b11">[12]</ref>.</p><p>For some of the datasets with non-regular graphs, we combine the various aggregators with logarithmic degree-scalers as in <ref type="bibr" target="#b9">[10]</ref>.</p><p>An important thing to note is that, for dynamic graphs, the eigenvectors need to be re-computed dynamically with the changing edges. Fortunately, there are random walk based algorithms that can estimate φ 1 quickly, especially for small changes to the graph <ref type="bibr" target="#b10">[11]</ref>. In the current empirical results, we do not work with dynamic graphs.</p><p>To evaluate the difficulty of computing the eigenvectors on very large graphs, we decided to load the COLLAB dataset comprising of a single graph with 235k nodes and 2.35M edges <ref type="bibr" target="#b11">[12]</ref>. Computing it's first 6 eigenvectors using the scipy eigsh function with machine precision took 25.5 minutes on an Intel® Xeon® CPU @ 2.20GHz. This is acceptable, knowing that a general training time can take hours, and that the result can be cached and reused during debugging and hyper-parameter optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Running time</head><p>The precomputation of the first four eigenvectors for all the graphs in the datasets takes 38s for ZINC, 96s for PATTERN and 120s for MolHIV on CPU. <ref type="table" target="#tab_7">Table 1</ref> shows the average running time on GPU for all the various model from <ref type="figure">figure  5</ref>. On average, the epoch running time is 15% slower for the DGN compared to the mean aggregation, but a faster convergence for DGN means that the total training time is on average 2% faster for DGN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Eigenvector multiplicity</head><p>The possibility to define equivariant directions using the low-frequency Laplacian eigenvectors is subject to the uniqueness of those vectors. When the dimension of the eigenspaces associated with the lowest eigenvalues is 1, the eigenvectors are defined up to a constant factor. In section 2.5, we propose the use of unit vector normalization and an absolute value to eliminate the scale and sign ambiguity. When the dimension of those eigenspaces is greater than 1, it is not possible to define equivariant directions using the eigenvectors.</p><p>Fortunately, it is very rare for the Laplacian matrix to have repeated eigenvalues in real-world datasets. We validate this claim by looking at ZINC and PATTERN datasets where we found no graphs with repeated Fiedler vector and only one graph out of 26k with multiplicity of the second eigenvector greater than 1.</p><p>When facing a graph that presents repeated Laplacian eigenvalues, we propose to randomly shuffle, during training time, different eigenvectors randomly sampled in the eigenspace. This technique will act as a data augmentation of the graph during training time allowing the network to train with multiple directions at the same time. The operation y = B av x is the directional average of x, in the sense that y u is the mean of x v , weighted by the direction and amplitude of F .</p><p>Proof. This should be a simple proof, that if we want a weighted average of our neighbours, we simply need to multiply the weights by each neighbour, and divide by the sum of the weights. Of course, the weights should be positive. </p><formula xml:id="formula_36">− diag j F :,j x is   F x − diag   j F   x   i = j F i,j x(j) −   j F i,j   x(i) = j:(i,j)∈E (x(j) − x(i))F i,j = D F x(i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of theorem 2.3 (Gradient steps reduce diffusion distance)</head><p>Let x, y be nodes such that φ 1 (x) &lt; φ 1 (y). Let x be the node obtained from x by taking one step in the direction of ∇φ 1 , then there is a constant C such that for C ≤ t we have d t (x , y) &lt; d t (x, y).</p><p>With the reduction in distance being proportional to e −λ1 .</p><p>Recall that p k (x, y) = (D −1 A) k x,y is the discrete heat kernel at step k, q t (x, y) = k≥0 e −t t k k! p k (x, y) is the continuous heat kernel at time t. In <ref type="bibr" target="#b2">[3]</ref>, it is shown that the continuous heat kernel is computed by q t (x, y) = e −tLnorm . Following     &lt; t if we take t to be larger than the term on the left the inequality we get d t (x , y) &lt; d t (x, y).</p><p>The constant C in the statement is the constant on the left side of the inequality. It is also interesting to note that C is expected to be positive since the term λ 1 − λ 2 is negative and the argument of the log will most likely be &lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof for Lemma C.1 (Cosine eigenvectors)</head><p>Consider the lattice graph Γ of size N 1 × N 2 × ... × N n , that has vertices i=1,...,n {1, ..., N i } and the vertices (x i ) i=1,...,n and (y i ) i=1,...,n are connected by an edge iff |x i − y i | = 1 for one index i and 0 for all other indices. Note that there are no diagonal edges in the lattice. The eigenvector of the Laplacian of the grid L(Γ) are given by φ j .</p><p>Lemma C.1 (Cosine eigenvectors). The Laplacian of Γ has an eigenvalue 2 − 2 cos π Ni with the associated eigenvector φ j that depends only the variable in the i-th dimension and is constant in all others, with</p><formula xml:id="formula_37">φ j = 1 N1 ⊗ 1 N2 ⊗ ... ⊗ x 1,Ni ⊗ ... ⊗ 1 Nn , and x 1,Ni (j) = cos πj n − π 2n</formula><p>Proof. First, recall the well known result that the path graph on N vertices P N has eigenvalues λ k = 2 − 2 cos πk n with associated eigenvector x k with i-th coordinate</p><p>x k (i) = cos πki n + πk 2n</p><p>The Cartesian product of two graphs G = (V G , E G ) and</p><formula xml:id="formula_38">H = (V H , E H ) is defined as G × H = (V G×H , E G×H ) with V G×H = V G × V H and ((u 1 , u 2 ), ((v 1 , v 2 )) ∈ E G×H iff either u 1 = v 1 and (u 2 , v 2 ) ∈ E H or (u 1 , v 1 ) ∈ V G and u 2 = v 2 .</formula><p>It is shown in <ref type="bibr" target="#b12">[13]</ref> that if (µ i ) i=1,...,m and (λ j ) j=1,...,n are the eigenvalues of G and H respectively, then the eigenvalues of the Cartesian product graph G × H are µ i + λ j for all possible eigenvalues µ i and λ j . Also, the eigenvectors associated to the eigenvalue µ i + λ j are u i ⊗ v j with u i an eigenvector of the Laplacian of G associated to the eigenvalue µ i and v j an eigenvector of the Laplacian of H associated to the eigenvalue λ j .</p><p>Finally, noticing that a lattice of shape N 1 × N 2 × ... × N n is really the Cartesian product of path graphs of length N 1 up to N n , we conclude that there are eigenvalues 2 − 2 cos π Ni . Denoting by 1 Nj the vector in R Nj with only ones as coordinates, then the eigenvector associated to the eigenvalue 2 − 2 cos π Ni is</p><formula xml:id="formula_39">1 N1 ⊗ 1 N2 ⊗ ... ⊗ x 1,Ni ⊗ ... ⊗ 1 Nn</formula><p>where x 1,Ni is the eigenvector of the Laplacian of P Ni associated to its first non-zero eigenvalue. 2 − 2 cos π Ni .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Radius 1 convolution kernels in a grid</head><p>In this section we show any radius 1 convolution kernel can be obtained as a linear combination of the B dx (∇φ i ) and B av (∇φ i ) matrices for the right choice of Laplacian eigenvectors φ i . First we show this can be done for 1-d convolution kernels.</p><p>Theorem C.2. On a path graph, any 1D convolution kernel of size 3 k is a linear combination of the aggregators B av , B dx and the identity I.</p><p>Proof. Recall from the previous proof that the first non zero eigenvalue of the path graph P N has associated eigenvector φ 1 (i) = cos( πi N − π 2N ). Since this is a monotone decreasing function in i, the i-th row of ∇φ 1 will be (0, ..., 0, s i−1 , 0, −s i+1 , 0, ..., 0)</p><p>with s i−1 and s i+1 &gt; 0. We are trying to solve with s = (s i−1 , 0, −s i+1 ), which always has a solution because s i−1 , s i+1 &gt; 0.</p><p>Theorem C.3 (Generalization radius-1 convolutional kernel in a grid). Let Γ be the n-dimensional lattice as above and let φ j be the eigenvectors of the Laplacian of the lattice as in theorem C.1. Then any radius 1 kernel k on Γ is a linear combination of the aggregators B av (φ i ), B dx (φ i ) and I.</p><p>Proof. This is a direct consequence of C.2 obtained by adding n 1-dimensional kernels, with each kernel being in a different axis of the grid as per Lemma C.1. See <ref type="figure" target="#fig_6">figure 4</ref> for a visual example in 2D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Proof for theorem 2.4 (Generalization radius-R convolutional kernel in a lattice)</head><p>For an n-dimensional lattice, any convolutional kernel of radius R can be realized by a linear combination of directional aggregation matrices and their compositions.</p><p>Proof. For clarity, we first do the 2 dimensional case for a radius 2, then extended to the general case. Let k be the radius 2 kernel on a grid represented by the matrix a 5×5 =      0 0 a −2,0 0 0 0 a −1,−1 a −1,0 a −1,1 0 a 0,−2 a 0,−1 a 0,0 a 0,1 a 0,2 0 a 1,−1 a 1,0 a 1,1 0 0 0 a 2,0 0 0      since we supposed the N 1 × N 2 grid was such that N 1 &gt; N 2 , by theorem C.1, we have that φ 1 is depending only in the first variable x 1 and is monotone in x 1 . Recall from C.1 that</p><formula xml:id="formula_40">φ 1 (i) = cos πi N 1 + π 2N 1</formula><p>The vector N1 π ∇ arccos(φ 1 ) will be denoted by F 1 in the rest. Notice all entries of F 1 are 0 or ±1. Denote by F 2 the gradient vector N2 π ∇ arccos(φ k ) where φ k is the eigenvector given by theorem C.1 that is depending only in the second variable x 2 and is monotone in with F j = Nj π ∇ arccos φ j ,φ j the eigenvector with lowest eigenvalue only dependent on the j-th variable and given in theorem C.1 and is the matrix multiplication. V represents all the choices of walk {v 1 , v 2 , ..., v n } in the direction of the fields {F 1 , F 2 , ..., F n }. For example, V = {3, 1, 0, −2} has a radius R = 6, with 3 steps forward of F 1 , 1 step forward of F 2 , and 2 steps backward of F 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Proof for theorem 2.5 (Comparison with 1-WL test)</head><p>DGNs using the mean aggregator, any directional aggregator of the first Laplacian eigenvector and injective degreescalers are strictly more powerful than the 1-WL test.</p><p>Proof. We will show that (1) DGNs are at least as powerful as the 1-WL test and (2) there is a pair of graphs which are not distinguishable by the 1-WL test which DGNs can discriminate.</p><p>Since the DGNs include the mean aggregator combined with at least an injective degree-scaler, <ref type="bibr" target="#b9">[10]</ref> show that the resulting architecture is at least as powerful as the 1-WL test.</p><p>Then, to show that the DGNs are strictly more powerful than the 1-WL test it suffices to provide an example of a pair of graphs that DGNs can differentiate and 1-WL cannot. Such a pair of graphs is illustrated in <ref type="figure" target="#fig_13">figure 8</ref>.</p><p>The 1-WL test (as any MPNN with, for example, sum aggregator) will always have the same features for all the nodes labelled with a and for all the nodes labelled with b and, therefore, will classify the graphs as isomorphic. DGNs, via the directional smoothing or directional derivative aggregators based on the first eigenvector of the Laplacian matrix, will update the features of the a nodes differently in the two graphs (figure 8 presents also the aggregation functions) and will, therefore, be capable of distinguishing them. shows the node feature updates done at every layer. MPNN with mean/sum aggregators and the 1-WL test only use the updates in the first row and therefore cannot distinguish between the nodes in the two graphs. DGNs also use directional aggregators that, with the vector field given by the first eigenvector of the Laplacian matrix, provides different updates to the nodes in the two graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation matrix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the steps required to aggregate messages in the direction of the eigenvectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Possible directional flows in different types of graphs. The node coloring is a potential map and the edges represent the gradient of the potential with the arrows in the direction of the flow. The first 3 columns present the arcosine of the normalized eigenvectors (acosφ) as node coloring, and their gradients represented as edge intensity. The last column presents examples of inductive bias introduced in the choice of direction. (a) The eigenvectors 1 and 2 are the horizontal and vertical flows of the grid. (b) The eigenvectors 1 and 2 are the flow in the longest and second-longest directions. (c) The eigenvectors 1, 2 and 3 flow respectively in the South-North, suburbs to the city center and West-East directions. We ignore φ 0 since it is constant and has no direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 . 1 (</head><label>21</label><figDesc>Directional smoothing). The operation y = B av x is the directional average of x, in the sense that y u is the mean of x v , weighted by the direction and amplitude of F . With x v the features at the nodes v neighbouring u, and y u the directional smoothing at node u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 . 2 (</head><label>22</label><figDesc>Directional derivative). SupposeF have rows of unit L 1 norm. The operation y = B dx (F )x is the centered directional derivative of x in the direction of F , in the sense of equation 4, i.e. y = DF x = F − diag jF :,j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of how the directional aggregation works at a node n v , with the arrows representing the direction and intensity of the field F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Theorem 2.4 (Generalization radius-R convolutional kernel in a lattice). For an n-dimensional lattice, any convolutional kernel of radius R can be realized by a linear combination of directional aggregation matrices and their compositions. As an example, figure 4 shows how a linear combination of the first and m-th aggregators B(∇φ 1,m ) realize a kernel on an N × M grid, where m = N/M and N &gt; M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Realization of a radius-1 convolution using the proposed aggregators. I x is the input feature map, * the convolutional operator, I y the convolution result, and B i = B(∇φ i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Accuracy of the various models using data augmentation with a complex architecture of ∼ 100k parameters and trained on 10% of the CIFAR10 training set (4.5k images). An angle of x corresponds to a rotation of the kernel by a random angle sampled uniformly in (−x°, x°) using definition 5 with F 1,2 being the gradient of the horizontal/vertical coordinates. A noise of 100x% corresponds to a distortion of each eigenvector with a random noise uniformly sampled in (−x · m, x · m) where m is the average absolute value of the eigenvector's components. The mean baseline model is not affected by the augmentation since it does not use the underlining vector field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Nj</head><label></label><figDesc>B j = B N ...B 2 B 1 . Doing so does not perfectly generalize the radius-R kernel for all graphs, but it generalizes it on a grid and significantly reduces the number of parameters to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C</head><label></label><figDesc>Appendix -Mathematical proofs C.1 Proof for theorem 2.1 (Directional smoothing)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>C. 2</head><label>2</label><figDesc>Proof for theorem 2.2 (Directional derivative) SupposeF have rows of unit L 1 norm. The operation y = B dx (F )x is the centered directional derivative of x in the direction of F , in the sense of equation 4, i.e. y = DF x = F − diag jF :,j x Proof. Since F rows have unit L 1 norm,F = F . The i-th coordinate of the vector F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(</head><label></label><figDesc>aB av + bB dx + cId) i,: = (0, ..., 0, x, y, z, 0, ..., 0) with x, y, z, in positions i − 1, i and i + 1. This simplifies to solving a 1 s L 1 |s| + b 1 s L 2 s + c(0, 1, 0) = (x, y, z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 + π 2N 2</head><label>22</label><figDesc>x 1 and recall φ k (i) = cos πi N For a matrix B, let B ± the positive/negative parts of B, ie matrices with positive entries such that B = B + − B − . Let B r1 be a matrix representing the radius 1 kernel with weights The matrix B r1 can be obtained by theorem C.3. Then the radius 2 kernel k is defined by all the possible combinations of 2 positive/negative steps, plus the initial radius-1 kernel. B r2 = −2≤i,j≤2 |i|+|j|=2 a i,j (F sgn(i) 1 ) |i| (F sgn(j) 2 ) |j| Any combination of 2 steps + B r1 all possible single-steps with sgn the sign function sgn(i) = + if i ≥ 0 and − if i &lt; 0. The matrix B r2 then realises the kernel a 5×5 .We can further extend the above construction to N dimension grids and radius R kernels kV ={v1,v2,...,v N }∈N n ||V || L 1 ≤R −R≤vi≤RAny choice of walk V with at most R-stepsAggregator following the steps defined in V</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Illustration of an example pair of graphs which the 1-WL test cannot distinguish but DGNs can. The table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Direc�onal smoothing aggrega�on Direc�onal deriva�ve aggrega�on Graph features focused on the neighbourhood of</head><label></label><figDesc></figDesc><table><row><cell>, 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>, 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>, 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>: Node receiving the message</cell><cell>Weighted forward deriva�ve with 1</cell><cell>+</cell><cell>Weighted backward deriva�ve with 2 +</cell><cell>Weighted backward deriva�ve with 3</cell></row><row><cell>1,2,3 : Neighbouring node : Feature at node</cell><cell cols="4">Sum of the absolute weights</cell></row><row><cell>, : Direc�onal vector field between the node and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1. ZINC: weight decay ∈ {1 · 10 −5 , 10 −6 , 3 · 10 −7 }, aggregators ∈ {(mean, avg 1 ), (mean, dx 1 ), (mean, av 1 , dx 1 ), (mean, min, max, av 1 ), (mean, min, max, dx 1 )} av 1 , av 2 ), (mean, dx 1 , dx 2 ), (mean, dx 1 , dx 2 , av 1 , av 2 ), (mean, max, min, dx 1 , dx 2 ), (mean, max, min, av 1 , av 2)} 3. PATTERN: weight decay ∈ {0, 10 −8 }, architecture ∈ {simple, complex}, aggregators ∈ {(mean, av 1 ), (mean, dx 1 ), (mean, av 1 , dx 1 )} 4. MolHIV: aggregators ∈ {(mean, dx 1 ), (mean, av 1 ), (mean, dx 1 , av 1 ), (mean, max, dx 1 ), (mean, max, dx 1 , av 1 ), (mean, max, min, av 1 , dx 1 )}, dropout ∈ {0.1, 0.3, 0.5}, L ∈ {4, 6} 5. for MolPCBA, given we did not start from any previously tuned architecture, we performed a line search with the following hyperparameters: mix of aggregators ∈ {mean, max, min, sum, dx 1 , dx 2 , av 1 , av 2 }, dropout ∈ {0.1, 0.2, 0.3, 0.4}, L ∈ {4, 6, 8}, weight decay ∈ {10 −7 , 10 −6 , 3 · 10 −6 , 10 −5 , 3 · 10 −5 }, batch size ∈ {128.512.2048, 3072}, learning rate ∈ {10 −2 , 10 −3 , 5 · 10 −4 , 2 · 10 −4 }, learning rate patience ∈ {4, 6, 8}, learning rate reduce factor ∈ {0.5, 0.8}, architecture type ∈ {simple, complex, towers}, edge features dimension ∈ {0, 8, 16, 32}</figDesc><table><row><cell>2. CIFAR10:</cell><cell>weight decay ∈</cell><cell>{3 · 10 −6 }, dropout ∈</cell><cell>{0.1, 0.3}, aggregators ∈</cell></row><row><cell>{(mean,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>Average running time for the non-fine tuned models from figure 5. Each entry represents average time per epoch / average total training time.For the first four datasets, each of the models has a parameter budget ∼ 100k and was run on a Tesla T4 (15GB GPU). The avg increase row is the average of the relative running time of all rows compared to the mean row, with a negative value meaning a faster running time.</figDesc><table><row><cell></cell><cell></cell><cell>ZINC</cell><cell></cell><cell></cell><cell cols="2">PATTERN</cell></row><row><cell>Aggregators</cell><cell>Simple</cell><cell>Complex</cell><cell cols="2">Complex-E</cell><cell>Simple</cell><cell>Complex</cell></row><row><cell>mean</cell><cell cols="6">3.29s/1505s 3.58s/1584s 3.56s/1654s 153.1s/10154s 117.8s/9031s</cell></row><row><cell>mean dx 1</cell><cell cols="4">3.86s/1122s 3.77s/1278s 4.22s/1371s</cell><cell>144.9s/8109s</cell><cell>127.2s/8417s</cell></row><row><cell cols="5">mean dx 1 dx 2 4.23s/1360s 4.55s/1560s 4.63s/1680s</cell><cell>153.3s/8057s</cell><cell>167.9s/9326s</cell></row><row><cell>mean av 1</cell><cell cols="4">3.68s/1297s 3.84s/1398s 3.92s/1272s</cell><cell>128.0s/8680s</cell><cell>88.1s/7456s</cell></row><row><cell cols="5">mean av 1 av 2 3.95s/1432s 4.03s/1596s 4.07s/1721s</cell><cell cols="2">134.2s/8115s 170.4s/11114s</cell></row><row><cell cols="5">mean dx 1 av 1 3.89s/1079s 4.09s/1242s 4.58s/1510s</cell><cell>118.6s/6221s</cell><cell>144.2s/9112s</cell></row><row><cell>avg increase</cell><cell cols="4">+19%/-16% +13%/-11% +20%/-9%</cell><cell>-11%/-23%</cell><cell>+18%/+1%</cell></row><row><cell></cell><cell cols="2">CIFAR10</cell><cell></cell><cell>MolHIV</cell><cell cols="2">MolPCBA</cell></row><row><cell>Aggregators</cell><cell>Simple</cell><cell>Complex</cell><cell></cell><cell>Simple</cell><cell>Complex</cell><cell>Complex-E</cell></row><row><cell>mean</cell><cell cols="6">83.6s/10526s 78.7s/10900s 11.4s/2189s 279s/30128s 356s/38126s</cell></row><row><cell>mean dx 1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">12.6s/2348s 304s/34129s 461s/43419s</cell></row><row><cell>mean dx 1 dx 2</cell><cell>98.4s/8405s</cell><cell cols="5">100.9s/5191s 14.1s/2345s 314s/36581s 334s/38363s</cell></row><row><cell>mean av 1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">12.2s/2177s 297s/30316s 436s/54545s</cell></row><row><cell cols="7">mean av 1 av 2 117.1s/12834s 89.5s/14481s 13.9s/2150s 315s/42297s 333s/36641s</cell></row><row><cell>mean dx 1 av 1</cell><cell></cell><cell></cell><cell></cell><cell cols="3">14.0s/2070s 326s/37523s 461s/59109s</cell></row><row><cell>avg increase</cell><cell>+29%/+1%</cell><cell cols="2">+21%/-10%</cell><cell cols="3">+17%/+1% +12%/+20% +14%/+22%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For MolPCBA, due to the computational cost of running models in the large dataset and the relatively low variance, we only used 1 run for the results in figure 5, but 10 runs in those forfigure 6</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05205</idno>
		<title level="m">On the bottleneck of graph neural networks and its practical implications</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-dimensional directional wavelets and the scale-angle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murenzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="281" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Random Walks and Heat Kernels on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">T</forename><surname>Barlow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eigenvalues in Riemannian geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Chavel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrete green&apos;s functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series A</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="214" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">and Conference Board of the Mathematical Sciences. Spectral Graph Theory. CBMS Regional Conference Series. Conference Board of the mathematical sciences</title>
		<meeting><address><addrLine>U.S.</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>CBMS Conference on Recent Advances in Spectral Graph Theory, National Science Foundation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue: Diffusion Maps and Wavelets</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwaraj</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do</forename><surname>Young Eun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00283</idno>
		<title level="m">Fiedler vector approximation via interacting random walks</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Algebraic connectivity of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1973-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discrete calculus : applied analysis on graphs for computational science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polimeni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometrical structure of laplacian eigenfunctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Grebenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="601" to="667" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network using directional wavelets for low-dose x-ray CT reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhee</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="360" to="375" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs. ICLR2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4204" to="4214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Lanczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<publisher>United States Governm. Press Office Los</publisher>
			<pubPlace>Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<title level="m">Graph convolutional neural networks with complex rational spectral filters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Laplace-beltrami eigenfunctions towards an algorithm that &quot;understands&quot; geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Shape Modeling and Applications 2006 (SMI&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10943" to="10953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<title level="m">Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Comparing data augmentation strategies for deep image classification. Session 2: Deep Learning for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An overview of early vision in InceptionV1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<idno>e00024.002</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxing</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08270</idno>
		<title level="m">Graph convolutional neural networks via motif-based attention</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">DropEdge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The principal components analysis of a graph, and its relationships to spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luh</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="371" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07785</idno>
		<title level="m">Graph wavelet neural network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The finer directional wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
	<note>Proceedings. (ICASSP &apos;05</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
