<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning What and Where to Draw</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
							<email>reedscot@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<email>akata@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Mohan</surname></persName>
							<email>santoshm@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Tenka</surname></persName>
							<email>samtenka@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning What and Where to Draw</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 × 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text-and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating realistic images from informal descriptions would have a wide range of applications. Modern computer graphics can already generate remarkably realistic scenes, but it still requires the substantial effort of human designers and developers to bridge the gap between high-level concepts and the end product of pixel-level details. Fully automating this creative process is currently out of reach, but deep networks have shown a rapidly-improving ability for controllable image synthesis.</p><p>In order for the image-generating system to be useful, it should support high-level control over the contents of the scene to be generated. For example, a user might provide the category of image to be generated, e.g. "bird". In the more general case, the user could provide a textual description like "a yellow bird with a black head".</p><p>Compelling image synthesis with this level of control has already been demonstrated using convolutional Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[Goodfellow et al., 2014</ref><ref type="bibr" target="#b16">, Radford et al., 2016</ref>. Variational Autoencoders also show some promise for conditional image synthesis, in particular recurrent versions such as DRAW <ref type="bibr" target="#b7">[Gregor et al., 2015</ref><ref type="bibr" target="#b13">, Mansimov et al., 2016</ref>. However, current approaches have so far only used simple conditioning variables such as a class label or a non-localized caption <ref type="bibr" target="#b19">[Reed et al., 2016b]</ref>, and did not allow for controlling where objects appear in the scene.</p><p>To generate more realistic and complex scenes, image synthesis models can benefit from incorporating a notion of localizable objects. The same types of objects can appear in many locations in different scales, poses and configurations. This fact can be exploited by separating the questions of "what" and "where" to modify the image at each step of computation. In addition to parameter efficiency, this yields the benefit of more interpretable image samples, in the sense that we can track what the network was meant to depict at each location.</p><p>Beak Belly This bird is bright blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right leg</head><p>This bird is completely black. Head a man in an orange jacket, black pants and a black cap wearing sunglasses skiing For many image datasets, we have not only global annotations such as a class label but also localized annotations, such as bird part keypoints in Caltech-USCD birds (CUB) <ref type="bibr" target="#b25">[Wah et al., 2011]</ref> and human joint locations in the MPII Human Pose dataset (MHP) <ref type="bibr" target="#b1">[Andriluka et al., 2014]</ref>. For CUB, there are associated text captions, and for MHP we collected a new dataset of 3 captions per image.</p><p>Our proposed model learns to perform locationand content-controllable image synthesis on the above datasets. We demonstrate two ways to encode spatial constraints (though there could be many more). First, we show how to condition on the coarse location of a bird by incorporating spatial masking and cropping modules into a text-conditional GAN, implemented using spatial transformers. Second, we can condition on part locations of birds and humans in the form of a set of normalized (x,y) coordinates, e.g. beak@(0.23,0.15). In the second case, the generator and discriminator use a multiplicative gating mechanism to attend to the relevant part locations.</p><p>The main contributions are as follows: (1) a novel architecture for text-and location-controllable image synthesis, yielding more realistic and higher-resolution CUB samples, (2) a text-conditional object part completion model enabling a streamlined user interface for specifying part locations, and (3) exploratory results and a new dataset for pose-conditional text to human image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In addition to recognizing patterns within images, deep convolutional networks have shown remarkable capability to generate images. <ref type="bibr" target="#b4">Dosovitskiy et al. [2015]</ref> trained a deconvolutional network to generate 3D chair renderings conditioned on a set of graphics codes indicating shape, position and lighting. <ref type="bibr" target="#b26">Yang et al. [2015]</ref> followed with a recurrent convolutional encoder-decoder that learned to apply incremental 3D rotations to generate sequences of rotated chair and face images. <ref type="bibr" target="#b14">Oh et al. [2015]</ref> used a similar approach in order to predict action-conditional future frames of Atari games.  trained a network to generate images that solved visual analogy problems.</p><p>The above models were all deterministic (i.e. conventional feed-forward and recurrent neural networks), trained to learn one-to-one mappings from the latent space to pixel space. Other recent works take the approach of learning probabilistic models with variational autoencoders <ref type="bibr" target="#b9">[Kingma and</ref><ref type="bibr">Welling, 2014, Rezende et al., 2014]</ref>. <ref type="bibr" target="#b11">Kulkarni et al. [2015]</ref> developed a convolutional variational autoencoder in which the latent space was "disentangled" into separate blocks of units corresponding to graphics codes. <ref type="bibr" target="#b7">Gregor et al. [2015]</ref> created a recurrent variational autoencoder with attention mechanisms for reading and writing portions of the image canvas at each time step (DRAW).</p><p>In addition to VAE-based image generation models, simple and effective Generative Adversarial Networks <ref type="bibr" target="#b6">[Goodfellow et al., 2014]</ref> have been increasingly popular. In general, GAN image samples are notable for their relative sharpness compared to samples from the contemporary VAE models. Later, class-conditional GAN <ref type="bibr" target="#b3">[Denton et al., 2015]</ref> incorporated a Laplacian pyramid of residual images into the generator network to achieve a significant qualitative improvement. <ref type="bibr" target="#b16">Radford et al. [2016]</ref> proposed ways to stabilize deep convolutional GAN training and synthesize compelling images of faces and room interiors. Spatial Transformer Networks (STN) <ref type="bibr" target="#b8">[Jaderberg et al., 2015]</ref> have proven to be an effective visual attention mechanism, and have already been incorporated into the latest deep generative models. <ref type="bibr" target="#b5">Eslami et al. [2016]</ref> incorporate STNs into a form of recurrent VAE called Attend, Infer, Repeat (AIR), that uses an image-dependent number of inference steps, learning to generate simple multi-object 2D and 3D scenes. <ref type="bibr" target="#b21">Rezende et al. [2016]</ref> build STNs into a DRAW-like recurrent network with impressive sample complexity visual generalization properties. <ref type="bibr" target="#b12">Larochelle and Murray [2011]</ref> proposed the Neural Autoregressive Density Estimator (NADE) to tractably model distributions over image pixels as a product of conditionals. Recently proposed spatial grid-structured recurrent networks <ref type="bibr" target="#b23">[Theis and</ref><ref type="bibr">Bethge, 2015, van den Oord et al., 2016]</ref> have shown encouraging image synthesis results. We use GANs in our approach, but the same principle of separating "what" and "where" conditioning variables can be applied to these types of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Adversarial Networks</head><p>Generative adversarial networks (GANs) consist of a generator G and a discriminator D that compete in a two-player minimax game. The discriminator's objective is to correctly classify its inputs as either real or synthetic. The generator's objective is to synthesize images that the discriminator will classsify as real. D and G play the following game with value function V (D, G):</p><formula xml:id="formula_0">min G max D V (D, G) = E x∼p data (x) [log D(x)] + E x∼pz(z) [log(1 − D(G(z)))]</formula><p>where z is a noise vector drawn from e.g. a Gaussian or uniform distribution. <ref type="bibr" target="#b6">Goodfellow et al. [2014]</ref> showed that this minimax game has a global optimium precisely when p g = p data , and that when G and D have enough capacity, p g converges to p data .</p><p>To train a conditional GAN, one can simply provide both the generator and discriminator with the additional input c as in <ref type="bibr" target="#b3">[Denton et al., 2015</ref><ref type="bibr" target="#b16">, Radford et al., 2016</ref> yielding G(z, c) and D(x, c). For an input tuple (x, c) to be intepreted as "real", the image x must not only look realistic but also match its context c. In practice G is trained to maximize log D(G(z, c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structured joint embedding of visual descriptions and images</head><p>To encode visual content from text descriptions, we use a convolutional and recurrent text encoder to learn a correspondence function between images and text features, following the approach of <ref type="bibr" target="#b18">Reed et al. [2016a]</ref> (and closely related to <ref type="bibr" target="#b10">Kiros et al. [2014]</ref>). Sentence embeddings are learned by optimizing the following structured loss:</p><formula xml:id="formula_1">1 N N n=1 ∆(y n , f v (v n )) + ∆(y n , f t (t n ))<label>(1)</label></formula><p>where {(v n , t n , y n ), n = 1, ..., N } is the training data set, ∆ is the 0-1 loss, v n are the images, t n are the corresponding text descriptions, and y n are the class labels. f v and f t are defined as</p><formula xml:id="formula_2">f v (v) = arg max y∈Y E t∼T (y) [φ(v) T ϕ(t))], f t (t) = arg max y∈Y E v∼V(y) [φ(v) T ϕ(t))]<label>(2)</label></formula><p>where φ is the image encoder (e.g. a deep convolutional network), ϕ is the text encoder, T (y) is the set of text descriptions of class y and likewise V(y) for images. Intuitively, the text encoder learns to produce a higher compatibility score with images of the correspondong class compared to any other class, and vice-versa. To train the text encoder we minimize a surrogate loss related to Equation 1 (see <ref type="bibr" target="#b0">Akata et al. [2015]</ref> for details). We modify the approach of <ref type="bibr" target="#b18">Reed et al. [2016a]</ref> in a few ways: using a char-CNN-GRU <ref type="bibr" target="#b2">[Cho et al., 2014]</ref> instead of char-CNN-RNN, and estimating the expectations in Equation 2 using the average of 4 sampled captions per image instead of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Adversarial What-Where Networks (GAWWN)</head><p>In the following sections we describe the bounding-box-and keypoint-conditional GAWWN models. <ref type="figure">Figure 2</ref> shows a sketch of the model, which can be understood by starting from input noise z ∈ R Z and text embedding t ∈ R T (extracted from the caption by pre-trained 2 encoder ϕ(t)) and following the arrows. Below we walk through each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bounding-box-conditional text-to-image model</head><p>First, the text embedding (shown in green) is replicated spatially to form a M × M × T feature map, and then warped spatially to fit into the normalized bounding box coordinates. The feature map entries outside the box are all zeros. <ref type="bibr">3</ref> The diagram shows a single object, but in the case of multiple localized captions, these feature maps are averaged. Then, convolution and pooling operations are applied to reduce the spatial dimension back to 1 × 1. Intuitively, this feature vector encodes the coarse spatial structure in the image, and we concatenate this with the noise vector z. Figure 2: GAWWN with bounding box location control. In the next stage, the generator branches into local and global processing stages. The global pathway is just a series of stride-2 deconvolutions to increase spatial dimension from 1 × 1 to M × M . In the local pathway, upon reaching spatial dimension M × M , a masking operation is applied so that regions outside the object bounding box are set to 0. Finally, the local and global pathways are merged by depth concatenation. A final series of deconvolution layers are used to reach the final spatial dimension. In the final layer we apply a Tanh nonlinearity to constrain the outputs to [−1, 1].</p><p>In the discriminator, the text is similarly replicated spatially to form a M × M × T tensor. Meanwhile the image is processed in local and global pathways. In the local pathway, the image is fed through stride-2 convolutions down to the M × M spatial dimension, at which point it is depth-concatenated with the text embedding tensor. The resulting tensor is spatially cropped to within the bounding box coordinates, and further processed convolutionally until the spatial dimension is 1 × 1. The global pathway consists simply of convolutions down to a vector, with additive contribution of the orignal text embedding t. Finally, the local and global pathway output vectors are combined additively and fed into the final layer producing the scalar discriminator score. <ref type="figure" target="#fig_1">Figure 3</ref> shows the keypoint-conditional version of the GAWWN, described in detail below. The location keypoints are encoded into a M × M × K spatial feature map in which the channels correspond to the part; i.e. head in channel 1, left foot in channel 2, and so on. The keypoint tensor is fed into several stages of the network. First, it is fed through stride-2 convolutions to produce a vector that is concatenated with noise z and text embedding t. The resulting vector provides coarse information about content and part locations. Second, the keypoint tensor is flattened into a binary matrix with a 1 indicating presence of any part at a particular spatial location, then replicated depth-wise into a tensor of size M × M × H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keypoint-conditional text-to-image model</head><p>In the local and global pathways, the noise-text-keypoint vector is fed through deconvolutions to produce another M × M × H tensor. The local pathway activations are gated by pointwise multiplication with the keypoint tensor of the same size. Finally, the original M × M × K keypoint tensor is depth-concatenated with the local and global tensors, and processed with further deconvolutions to produce the final image. Again a Tanh nonlinearity is applied.</p><p>In the discriminator, the text embedding t is fed into two stages. First, it is combined additively with the global pathway that processes the image convolutionally producing a vector output. Second, it is spatially replicated to M × M and then depth-concatenated with another M × M feature map in the local pathway. This local tensor is then multiplicatively gated with the binary keypoint mask exactly as in the generator, and the resulting tensor is depth-concatenated with the M × M × T keypoints. The local pathway is fed into further stride-2 convolutions to produce a vector, which is then additively combined with the global pathway output vector, and then into the final layer producing the scalar discriminator score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conditional keypoint generation model</head><p>From a user-experience perspective, it is not optimal to require users to enter every single keypoint of the parts of the object they wish to be drawn (e.g. for birds our model would require 15). Therefore, it would be very useful to have access to all of the conditional distributions of unobserved keypoints given a subset of observed keypoints and the text description. A similar problem occurs in data imputation, e.g. filling in missing records or inpainting image occlusions. However, in our case we want to draw convincing samples rather than just fill in the most likely values.</p><p>Conditioned on e.g. only the position of a bird's beak, there could be several very different plausible poses that satisfy the constraint. Therefore, a simple approach such as training a sparse autoencoder over keypoints would not suffice. A DBM <ref type="bibr" target="#b22">[Salakhutdinov and Hinton, 2009]</ref> or variational autoencoder <ref type="bibr" target="#b20">[Rezende et al., 2014]</ref> could in theory work, but for simplicity we demonstrate the results achieved by applying the same generic GAN framework to this problem.</p><p>The basic idea is to use the assignment of each object part as observed (i.e. conditioning variable) or unobserved as a gating mechanism. Denote the keypoints for a single image as k i := {x i , y i , v i }, i = 1, ..., K, where x and y indicate the row and column position, respectively, and v is a bit set to 1 if the part is visible and 0 otherwise. If the part is not visible, x and y are also set to 0. Let k ∈ [0, 1] K×3 encode the keypoints into a matrix. Let the conditioning variables (e.g. a beak position specified by the user) be encoded into a vector of switch units s ∈ {0, 1} K , with the i-th entry set to 1 if the i-th part is a conditioning variable and 0 otherwise. We can formulate the generator network over keypoints G k , conditioned on text t and a subset of keypoints k, s, as follows:</p><formula xml:id="formula_3">G k (z, t, k, s) := s k + (1 − s) f (z, t, k)<label>(3)</label></formula><p>where denotes pointwise multiplication and f : R Z+T +3K → R 3K is an MLP. In practice we concatenated z, t and flattened k and chose f to be a 3-layer fully-connected network.</p><p>The discriminator D k learns to distinguish real keypoints and text (k real , t real ) from synthetic. In order for G k to capture all of the conditional distributions over keypoints, during training we randomly sample switch units s in each mini-batch. Since we would like to usually specify 1 or 2 keypoints, in our experiments we set the "on" probability to 0.1. That is, each of the 15 bird parts only had a 10% chance of acting as a conditioning variable for a given training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we describe our experiments on generating images from text descriptions on the Caltech-UCSD Birds (CUB) and MPII Human Pose (MHP) datasets.</p><p>CUB <ref type="bibr" target="#b25">[Wah et al., 2011]</ref> has 11,788 images of birds belonging to one of 200 different species. We also use the text dataset from <ref type="bibr" target="#b18">Reed et al. [2016a]</ref> including 10 single-sentence descriptions per bird image. Each image also includes the bird location via its bounding box, and keypoint (x,y) coordinates for each of 15 bird parts. Since not all parts are visible in each image, the keypoint data also provides an additional bit per part indicating whether the part can be seen.</p><p>MHP <ref type="bibr" target="#b1">Andriluka et al. [2014]</ref> has 25K images with 410 different common activities. For each image, we collected 3 single-sentence text descriptions using Mechanical Turk. We asked the workers to describe the most distinctive aspects of the person and the activity they are engaged in, e.g. "a man in a yellow shirt preparing to swing a golf club". Each image has potentially multiple sets of (x,y) keypoints for each of the 16 joints. During training we filtered out images with multiple people, and for the remaining 19K images we cropped the image to the person's bounding box.</p><p>We encoded the captions using a pre-trained char-CNN-GRU as described in <ref type="bibr" target="#b18">[Reed et al., 2016a]</ref>. During training, the 1024-dimensional text embedding for a given image was taken to be the average of four randomly-sampled caption encodings corresponding to that image. Sampling multiple captions per image provides further information required to draw the object. At test time one can average together any number of description embeddings, including a single caption.</p><p>For both CUB and MHP, we trained our GAWWN using the ADAM solver with batch size 16 and learning rate 0.0002 (See Alg. 1 in <ref type="bibr" target="#b19">[Reed et al., 2016b]</ref> for the conditional GAN training algorithm). The models were trained on all categories and we show samples on a set of held out captions. For the spatial transformer module, we used a Torch implementation provided by <ref type="bibr" target="#b15">Oquab [2016]</ref>. Our GAN implementation is loosely based on dcgan.torch 4 .</p><p>In experiments we analyze how accurately the GAWWN samples reflect the text and location constraints. First we control the location of the bird by interpolation via bounding boxes and keypoints. We consider both the case of (1) ground-truth keypoints from the data set, and (2) synthetic keypoints generated by our model, conditioned on the text. Case (2) is advantageous because it requires less effort from a hypothetical user (i.e. entering 15 keypoint locations). We then compare our CUB results to representative samples from the previous work. Finally, we show samples on textand pose-conditional generation of images of human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Controlling bird location via bounding boxes</head><p>We first demonstrate sampling from the text-conditional model while varying the bird location. Since location is specified via bounding box coordinates, we can also control the size and aspect ratio of the bird. This is shown in <ref type="figure">Figure 4</ref> by interpolating the bounding box coordinates while at the same time fixing the text and noise conditioning variables.  <ref type="figure">Figure 4</ref>: Controlling the bird's position using bounding box coordinates. and previously-unseen text. With the noise vector z fixed in every set of three frames, the background is usually similar but not perfectly invariant. Interestingly, as the bounding box coordinates are changed, the direction the bird faces does not change. This suggests that the model learns to use the the noise distribution to capture some aspects of the background and also non-controllable aspects of "where" such as direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Controlling individual part locations via keypoints</head><p>In this section we study the case of text-conditional image generation with keypoints fixed to the ground-truth. This can give a sense of the performance upper bound for the text to image pipeline, because synthetic keypoints can be no more realistic than the ground-truth. We take a real image and its keypoint annotations from the CUB dataset, and a held-out text description, and draw samples conditioned on this information. <ref type="figure">Figure 5</ref> shows several image samples that accurately reflect the text and keypoint constraints. More examples including success and failure are included in the supplement. We observe that the bird pose respects the keypoints and is invariant across the samples. The background and other small details, such as thickness of the tree branch or the background color palette do change with the noise.</p><p>The GAWWN model can also use keypoints to shrink, translate and stretch objects, as shown in <ref type="figure">Figure 6</ref>. We chose to specify beak and tail positions, because in most cases these define an approximate bounding box around the bird. This large black bird has a long neck and tail feathers.</p><p>This bird is mostly white with a thick black eyebrow, small and black beak and a long tail. This is a small yellowish green bird with a pointy black beak, black eyes and gray wings.</p><p>This pale pink bird has a black eyebrow and a black pointy beak, gray wings and yellow underparts.</p><p>This bird has a bright red crown and black wings and beak. This large white bird has an orange-tipped beak.</p><p>GT GT GT GT GT GT <ref type="figure">Figure 5</ref>: Bird generation conditioned on fixed groundtruth keypoints (overlaid in blue) and previously unseen text. Each sample uses a different random noise vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shrinking Translation Stretching</head><p>This bird has a black head, a long orange beak and yellow body</p><p>This large black bird has a pointy beak and black eyes This small blue bird has a short pointy beak and brown patches on its wings Caption GT <ref type="figure">Figure 6</ref>: Controlling the bird's position using keypoint coordinates. Here we only interpolated the beak and tail positions, and sampled the rest conditioned on these two.</p><p>Unlike in the case of bounding boxes, we can now control which way the bird is pointing; note that here all birds face left, whereas when we use bounding boxes ( <ref type="figure">Figure 4</ref>) the orientation is random. Elements of the scene, even outside of the controllable location, adjust in order to be coherent with the bird's position in each frame although in each set of three frames we use the same noise vector z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generating both bird keypoints and images from text alone</head><p>Although ground truth keypoint locations lead to visually plausible results as shown in the previous sections, the keypoints are costly to obtain. In <ref type="figure">Figure 7</ref>, we provide examples of accurate samples using generated keypoints. Compared to ground-truth keypoints, on average we did not observe degradation in quality. More examples for each regime are provided in the supplement.</p><p>This bird has a yellow head, black eyes, a gray pointy beak and orange lines on its breast.</p><p>This water bird has a long white neck, black body, yellow beak and black head.</p><p>This bird is large, completely black, with a long pointy beak and black eyes.</p><p>This bird is completely red with a red and cone-shaped beak, black face and a red nape.</p><p>This white bird has gray wings, red webbed feet and a long, curved and yellow beak.</p><p>This small bird has a blue and gray head, pointy beak and a white belly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>GT GT GT <ref type="figure">Figure 7</ref>: Keypoint-and text-conditional bird generation in which the keypoints are generated conditioned on unseen text. The small blue boxes indicate the generated keypoint locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to previous work</head><p>In this section we compare our results with previous text-to-image results on CUB. In <ref type="figure" target="#fig_2">Figure 8</ref> we show several representative examples that we cropped from the supplementary material of <ref type="bibr" target="#b19">[Reed et al., 2016b]</ref>. We compare against the actual ground-truth and several variants of GAWWN. We observe that the 64 × 64 samples from <ref type="bibr" target="#b19">[Reed et al., 2016b]</ref> mostly reflect the text description, but in some cases lack clearly defined parts such as a beak. When the keypoints are zeroed during training, our GAWWN architecture actually fails to generate any plausible images. This suggests that providing additional conditioning variables in the form of location constraints is helpful for learning to generate high-resolution images. Overall, the sharpest and most accurate results can be seen in the 128 × 128 samples from our GAWWN with real or synthetic keypoints (bottom two rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Beyond birds: generating images of humans</head><p>In this section we apply our model to generating images of humans conditioned on a description of their appearance and activity, and also on their approximate pose. This is a much more challenging A small sized bird that has tones of brown and dark red with a short stout bill   <ref type="bibr" target="#b19">Reed et al. [2016b]</ref> and also the groundtruth images. For the ground-truth row, the first entry corresonds directly to the caption, and the second two entries are sampled from the same species.</p><p>a woman in a yellow tank top is doing yoga.</p><p>the man wearing the red shirt and white pants play golf on the green grass a man in green shirt and white pants is swinging his golf club.</p><p>a man in a red sweater and grey pants swings a golf club with one hand. a woman in grey shirt is doing yoga.</p><p>a man in an orange jacket, black pants and a black cap wearing sunglasses skiing. a man is skiing and competing for the olympics on the slopes. a woman wearing goggles swimming through very murky water GT Samples GT Samples Caption Caption <ref type="figure">Figure 9</ref>: Generating humans. Both the keypoints and the image are generated from unseen text.</p><p>task than generating images of birds due to the larger variety of scenes and pose configurations that appear in MHP compared to CUB.</p><p>The human image samples shown in <ref type="figure">Figure 9</ref> tend to be much blurrier compared to the bird images, but in many cases bear a clear resemblance to the text query and the pose constraints. Simple captions involving skiing, golf and yoga tend to work, but complex descriptions and unusual poses (e.g. upside-down person on a trampoline) remain especially challenging. We also generate videos by (1) extracting pose keypoints from a pre-trained pose estimator from several YouTube clips, and (2) combining these keypoint trajectories with a text query, fixing the noise vector z over time and concatenating the samples (see supplement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work we showed how to generate images conditioned on both informal text descriptions and object locations. Locations can be accurately controlled by either bounding box or a set of part keypoints. On CUB, the addition of a location constraint allowed us to accurately generate compelling 128 × 128 images, whereas previous models could only generate 64 × 64. Furthermore, this location conditioning does not constrain us during test time, because we can also learn a text-conditional generative model of part locations, and simply generate them at test time.</p><p>An important lesson here is that decomposing the problem into easier subproblems can help generate realistic high-resolution images. In addition to making the overall text to image pipeline easier to train with a GAN, it also yields additional ways to control image synthesis. In future work, it may be promising to learn the object or part locations in an unsupervised or weakly supervised way. In addition, we show the first text-to-human image synthesis results, but performance on this task is clearly far from saturated and further architectural advances will be required to solve it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Text-to-image examples. Locations can be specified by keypoint or bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Text and keypoint-conditional GAWWN.. Keypoint grids are shown as 4 × 4 for clarity of presentation, but in our experiments we used 16 × 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of GAWWN to GAN-INT-CLS from</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Both φ and ϕ could be trained jointly with the GAN, but pre-training allows us to use the best available image features from higher resolution images (224 × 224) and speeds up GAN training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For details of how to apply this warping see equation 3 in<ref type="bibr" target="#b8">[Jaderberg et al., 2015]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/soumith/dcgan.torch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This small bird has a blue and gray head, pointy beak, black and white patterns on its wings and a white belly.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by NSF CAREER IIS-1453651, ONR N00014-13-1-0762, and a Sloan Research Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modules for spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Oquab</surname></persName>
		</author>
		<ptr target="github.com/qassemoquab/stnbhwd" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual analogy-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations for fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
