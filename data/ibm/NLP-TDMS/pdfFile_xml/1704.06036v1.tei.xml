<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end representation learning for Correlation Filter based tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end representation learning for Correlation Filter based tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training image: 255x255x3 Test image: 255x255x3 17x17x32 49x49x32 Correlation Filter Crop ★ 33x33x1 CNN CNN 49x49x32 <ref type="figure">Figure 1</ref>: Overview of the proposed network architecture, CFNet. It is an asymmetric Siamese network: after applying the same convolutional feature transform to both input images, the "training image" is used to learn a linear template, which is then applied to search the "test image" by cross-correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates. * Equal first authorship. is challenging. This problem emerges naturally in applications such as visual object tracking, where the goal is to re-detect an object over a video with the sole supervision of a bounding box at the beginning of the sequence. The main challenge is the lack of a-priori knowledge of the target object, which can be of any class.</p><p>The simplest approach is to disregard the lack of a-priori knowledge and adapt a pre-trained deep convolutional neural network (CNN) to the target, for example by using stochastic gradient descent (SGD), the workhorse of deep network optimization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. The extremely limited training data and large number of parameters make this a difficult learning problem. Furthermore, SGD is quite expensive for online adaptation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks are a powerful tool for learning image representations in computer vision applications. However, training deep networks online, in order to capture previously unseen object classes from one or few examples, A possible answer to these shortcomings is to have no online adaptation of the network. Recent works have focused on learning deep embeddings that can be used as universal object descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>. These methods use a Siamese CNN, trained offline to discriminate whether two image patches contain the same object or not. The idea is that a powerful embedding will allow the detection (and thus tracking) of objects via similarity, bypassing the online learning problem. However, using a fixed metric to compare appearance prevents the learning algorithm from exploiting any video-specific cues that could be helpful for discrimination.</p><p>An alternative strategy is to use instead an online learn-ing method such as the Correlation Filter (CF). The CF is an efficient algorithm that learns to discriminate an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. It has proved to be highly successful in object tracking (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame. It owes its speed to a Fourier domain formulation, which allows the ridge regression problem to be solved with only a few applications of the Fast Fourier Transform (FFT) and cheap element-wise operations. Such a solution is, by design, much more efficient than an iterative solver like SGD, and still allows the discriminator to be tailored to a specific video, contrary to the embedding methods.</p><p>The challenge, then, is to combine the online learning efficiency of the CF with the discriminative power of CNN features trained offline. This has been done in several works (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>), which have shown that CNNs and CFs are complementary and their combination results in improved performance.</p><p>However, in the aforementioned works, the CF is simply applied on top of pre-trained CNN features, without any deep integration of the two methods. End-to-end training of deep architectures is generally preferable to training individual components separately. The reason is that in this manner the free parameters in all components can co-adapt and cooperate to achieve a single objective. Thus it is natural to ask whether a CNN-CF combination can also be trained end-to-end with similar benefits.</p><p>The key step in achieving such integration is to interpret the CF as a differentiable CNN layer, so that errors can be propagated through the CF back to the CNN features. This is challenging, because the CF itself is the solution of a learning problem. Hence, this requires to differentiate the solution of a large linear system of equations. This paper provides a closed-form expression for the derivative of the Correlation Filter. Moreover, we demonstrate the practical utility of our approach in training CNN architectures endto-end.</p><p>We present an extensive investigation into the effect of incorporating the CF into the fully-convolutional Siamese framework of Bertinetto et al. <ref type="bibr" target="#b2">[3]</ref>. We find that the CF does not improve results for networks that are sufficiently deep. However, our method enables ultra-lightweight networks of a few thousand parameters to achieve state-of-theart performance on multiple benchmarks while running at high framerates.</p><p>Code and results are available online 1 . 1 www.robots.ox.ac.uk/˜luca/cfnet.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Since the seminal work of Bolme et al. <ref type="bibr" target="#b3">[4]</ref>, the Correlation Filter has enjoyed great popularity within the tracking community. Notable efforts have been devoted to its improvement, for example by mitigating the effect of periodic boundaries <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>, incorporating multi-resolution feature maps <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> and augmenting the objective with a more robust loss <ref type="bibr" target="#b26">[27]</ref>. For the sake of simplicity, in this work we adopt the basic formulation of the Correlation Filter.</p><p>Recently, several methods based on Siamese networks have been introduced <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>, raising interest in the tracking community for their simplicity and competitive performance. For our method, we prefer to build upon the fully-convolutional Siamese architecture <ref type="bibr" target="#b2">[3]</ref>, as it enforces the prior that the appearance similarity function should commute with translation.</p><p>At its core, the Correlation Filter layer that we introduce amounts to computing the solution to a regularized deconvolution problem, not to be confused with upsampling convolution layers that are sometimes referred to as "deconvolution layers" <ref type="bibr" target="#b20">[21]</ref>. Before it became apparent that algorithms such as SGD are sufficient for training deep networks, Zeiler et al. <ref type="bibr" target="#b34">[35]</ref> introduced a deep architecture in which each layer solves a convolutional sparse coding problem. In contrast, our problem has a closed-form solution since the Correlation Filter employs quadratic regularization rather than 1-norm regularization.</p><p>The idea of back-propagating gradients through the solution to an optimization problem during training has been previously investigated. Ionescu et al. <ref type="bibr" target="#b14">[15]</ref> and Murray <ref type="bibr" target="#b24">[25]</ref> have presented back-propagation forms for the SVD and Cholesky decomposition respectively, enabling gradient descent to be applied to a network that computes the solution to either a system of linear equations or an eigenvalue problem. Our work can be understood as an efficient backpropagation procedure through the solution to a system of linear equations, where the matrix has circulant structure.</p><p>When the solution to the optimization problem is obtained iteratively, an alternative is to treat the iterations as a Recurrent Neural Network, and to explicitly unroll a fixed number of iterations <ref type="bibr" target="#b36">[37]</ref>. Maclaurin et al. <ref type="bibr" target="#b23">[24]</ref> go further and back-propagate gradients through an entire SGD learning procedure, although this is computationally demanding and requires judicious bookkeeping. Gould et al. <ref type="bibr" target="#b10">[11]</ref> have recently considered differentiating the solution to general arg min problems without restricting themselves to iterative procedures. However, these methods are unnecessary in the case of the Correlation Filter, as it has a closed-form solution.</p><p>Back-propagating through a learning algorithm invites a comparison to meta-learning. Recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1]</ref> have proposed feed-forward architectures that can be interpreted as learning algorithms, enabling optimization by gradient descent. Rather than adopt an abstract definition of learning, this paper propagates gradients through a conventional learning problem that is already widely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We briefly introduce a framework for learning embeddings with Siamese networks (Section 3.1) and the use of such an embedding for object tracking (Section 3.2) before presenting the CFNet architecture (Section 3.3). We subsequently derive the expressions for evaluation and backpropagation of the main new ingredient in our networks, the Correlation Filter layer, which performs online learning in the forward pass (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully-convolutional Siamese networks</head><p>Our starting point is a network similar to that of <ref type="bibr" target="#b2">[3]</ref>, which we later modify in order to allow the model to be interpreted as a Correlation Filter tracker. The fullyconvolutional Siamese framework considers pairs (x , z ) comprising a training image x and a test image z 2 . The image x represents the object of interest (e.g. an image patch centered on the target object in the first video frame), while z is typically larger and represents the search area (e.g. the next video frame).</p><p>Both inputs are processed by a CNN f ρ with learnable parameters ρ. This yields two feature maps, which are then cross-correlated:</p><formula xml:id="formula_0">g ρ (x , z ) = f ρ (x ) f ρ (z ) .<label>(1)</label></formula><p>Eq. 1 amounts to performing an exhaustive search of the pattern x over the test image z . The goal is for the maximum value of the response map (left-hand side of eq. 1) to correspond to the target location. To achieve this goal, the network is trained offline with millions of random pairs (x i , z i ) taken from a collection of videos. Each example has a spatial map of labels c i with values in {−1, 1}, with the true object location belonging to the positive class and all others to the negative class. Training proceeds by minimizing an element-wise logistic loss over the training set:</p><formula xml:id="formula_1">arg min ρ i (g ρ (x i , z i ), c i ) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tracking algorithm</head><p>The network itself only provides a function to measure the similarity of two image patches. To apply this network to object tracking, it is necessary to combine this with a procedure that describes the logic of the tracker. Similar to <ref type="bibr" target="#b2">[3]</ref>, we employ a simplistic tracking algorithm to assess the utility of the similarity function.</p><p>Online tracking is performed by simply evaluating the network in forward-mode. The feature representation of the target object is compared to that of the search region, which is obtained in each new frame by extracting a window centred at the previously estimated position, with an area that is four times the size of the object. The new position of the object is taken to be the location with the highest score.</p><p>The original fully-convolutional Siamese network simply compared every frame to the initial appearance of the object. In contrast, we compute a new template in each frame and then combine this with the previous template in a moving average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Correlation Filter networks</head><p>We propose to modify the baseline Siamese network of eq. 1 with a Correlation Filter block between x and the cross-correlation operator. The resulting architecture is illustrated in <ref type="figure">Figure 1</ref>. This change can be formalized as:</p><formula xml:id="formula_2">h ρ,s,b (x , z ) = s ω (f ρ (x )) f ρ (z ) + b<label>(3)</label></formula><p>The CF block w = ω(x) computes a standard CF template w from the training feature map x = f ρ (x ) by solving a ridge regression problem in the Fourier domain <ref type="bibr" target="#b13">[14]</ref>. Its effect can be understood as crafting a discriminative template that is robust against translations. It is necessary to introduce scalar parameters s and b (scale and bias) to make the score range suitable for logistic regression. Offline training is then performed in the same way as for a Siamese network (Section 3.1), replacing g with h in eq. 2. We found that it was important to provide the Correlation Filter with a large region of context in the training image, which is consistent with the findings of Danelljan et al. <ref type="bibr" target="#b7">[8]</ref> and Kiani et al. <ref type="bibr" target="#b15">[16]</ref>. To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window <ref type="bibr" target="#b3">[4]</ref> and the final template is cropped <ref type="bibr" target="#b29">[30]</ref>.</p><p>Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3]</ref> with CNN features, as proposed in previous work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>. However, these earlier networks were not trained end-to-end. The novelty is to compute the derivative of the CF template with respect to its input so that a network incorporating a CF can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Correlation Filter</head><p>We now show how to back-propagate gradients through the Correlation Filter solution efficiently and in closed form via the Fourier domain.</p><p>Formulation. Given a scalar-valued image x ∈ R m×m , the Correlation Filter is the template w ∈ R m×m whose inner product with each circular shift of the image x * δ −u is as close as possible to a desired response y[u] <ref type="bibr" target="#b13">[14]</ref>, minimizing</p><formula xml:id="formula_3">u∈U ( x * δ −u , w − y[u]) 2 = w x − y 2 . (4) Here, U = {0, . . . , m − 1} 2 is the domain of the image, y ∈ R m×m is a signal whose u-th element is y[u], and δ τ is the translated Dirac delta function δ τ [t] = δ[t − τ ].</formula><p>In this section, we use * to denote circular convolution and to denote circular cross-correlation. Recall that convolution with the translated δ function is equivalent to transla-</p><formula xml:id="formula_4">tion (x * δ τ )[t] = x[t − τ mod m].</formula><p>Incorporating quadratic regularization to prevent overfitting, the problem is to find</p><formula xml:id="formula_5">arg min w 1 2n w x − y 2 + λ 2 w 2<label>(5)</label></formula><p>where n = |U| is the effective number of examples. The optimal template w must satisfy the system of equations (obtained via the Lagrangian dual, see Appendix C, supplementary material)</p><formula xml:id="formula_6">     k = 1 n (x x) + λδ k * α = 1 n y w = α x (6)</formula><p>where k can be interpreted as the signal that defines a circulant linear kernel matrix, and α is a signal comprised of the Lagrange multipliers of a constrained optimization problem that is equivalent to eq. 5. The solution to eq. 6 can be computed efficiently in the Fourier domain <ref type="bibr" target="#b13">[14]</ref>,</p><formula xml:id="formula_7">     k = 1 n ( x * • x) + λ1 α = 1 n k −1 • y w = α * • x (7a) (7b) (7c)</formula><p>where we use x = F x to denote the Discrete Fourier Transform of a variable, x * to denote the complex conjugate, • to denote element-wise multiplication and 1 to denote a signal of ones. The inverse of element-wise multiplication is element-wise scalar inversion. Notice that the operations in eq. 7 are more efficiently computed in the Fourier domain, since they involve element-wise operations instead of more expensive convolutions or matrix operators (eq. 6). Moreover, the inverse convolution problem (to find α such that k * α = 1 n y) is the solution to a diagonal system of equations in the Fourier domain (eq. 7b).</p><p>Back-propagation. We adopt the notation that if x ∈ X = R n is a variable in a computational graph that computes a final scalar loss ∈ R, then ∇ x ∈ X denotes the vector of partial derivatives (∇ x ) i = ∂ /∂x i . If y ∈ Y = R m is another variable in the graph, which is computed directly from x according to y = f (x), then the so-called back-propagation map for the function f is a lin-</p><formula xml:id="formula_8">ear map from ∇ y ∈ Y to ∇ x ∈ X .</formula><p>Appendix D gives a tutorial review of the mathematical background. In short, the back-propagation map is the linear map which is the adjoint of the differential. This property was used by Ionescu et al. <ref type="bibr" target="#b14">[15]</ref> to compute backpropagation maps using matrix differential calculus. While they used the matrix inner product X, Y = tr(X T Y ) to find the adjoint, we use Parseval's theorem, which states that the Fourier transform is unitary (except for a scale factor) and therefore preserves inner products x, y ∝ x, y .</p><p>To find the linear map for back-propagation through the Correlation Filter, we first take the differentials of the system of equations in eq. 6 that defines the template w</p><formula xml:id="formula_9">     dk = 1 n (dx x + x dx) dk * α + k * dα = 1</formula><p>n dy dw = dα x + α dx <ref type="bibr" target="#b7">(8)</ref> and then take the Fourier transform of each equation and rearrange to give the differential of each dependent variable in <ref type="figure" target="#fig_0">Figure 2</ref> as a linear function (in the Fourier domain) of the differentials of its input variables</p><formula xml:id="formula_10">       dk = 1 n ( dx * • x + x * • dx) dα = k −1 • 1 n dy − dk • α dw = dα * • x + α * • dx . (9a) (9b) (9c)</formula><p>Note that while these are complex equations, that is simply because they are the Fourier transforms of real equations. The derivatives themselves are all computed with respect to real variables.</p><p>The adjoints of these linear maps define the overall backpropagation map from ∇ w to ∇ x and ∇ y . We defer the derivation to Appendix B and present here the final result,</p><formula xml:id="formula_11">             ∇ α = x • ( ∇ w ) * ∇ y = 1 n k − * • ∇ α ∇ k = − k − * • α * • ∇ α ∇ x = α • ∇ w + 2 n x • Re{ ∇ k } .<label>(10)</label></formula><p>It is necessary to compute forward Fourier transforms at the start and inverse transforms at the end. The extension to multi-channel images is trivial and given in Appendix E (supplementary material).</p><p>As an interesting aside, we remark that, since we have the gradient of the loss with respect to the "desired" response y, it is actually possible to optimize for this parameter rather than specify it manually. However, in practice we did not find learning this parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The principal aim of our experiments is to investigate the effect of incorporating the Correlation Filter during training. We first compare against the symmetric Siamese architecture of Bertinetto et al. <ref type="bibr" target="#b2">[3]</ref>. We then compare the endto-end trained CFNet to a variant where the features are replaced with features that were trained for a different task. Finally, we demonstrate that our method achieves state-ofthe-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation criteria</head><p>Popular tracking benchmarks like VOT <ref type="bibr" target="#b16">[17]</ref> and OTB <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> have made all ground truth annotations available and do not enforce a validation/test split. However, in order to avoid overfitting to the test set in design choices and hyperparameter selection, we consider OTB-2013, OTB-50 and OTB-100 as our test set and 129 videos from VOT-2014, VOT-2016 and Temple-Color <ref type="bibr" target="#b19">[20]</ref> as our validation set, excluding any videos which were already assigned to the test set. We perform all of our tracking experiments in Sections 4.2, 4.3 and 4.4 on the validation set with the same set of "natural" hyperparameters, which are reasonable for all methods and not tuned for any particular method.</p><p>As in the OTB benchmark <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, we quantify the performance of the tracker on a sequence in terms of the average overlap (intersection over union) of the predicted and ground truth rectangles in all frames. The success rate of a tracker at a given threshold τ corresponds to the fraction of frames in which the overlap with the ground truth is at least τ . This is computed for a uniform range of 100 thresholds between 0 and 1, effectively constructing the cumulative distribution function. Trackers are compared using the area under this curve.</p><p>Mimicking the TRE (Temporal Robustness Evaluation) mode of OTB, we choose three equispaced points per sequence and run the tracker from each until the end. Differently from the OTB evaluation, when the target is lost (i.e. the overlap with the ground truth becomes zero) the tracker is terminated and an overlap of zero is reported for all remaining frames. Despite the large number of videos, we still find that the performance of similarity networks varies considerably as training progresses. To mitigate this effect, we average the final tracking results that are obtained using the parameters of the network at epochs 55, 60, . . . , 95, 100 (the final epoch) to reduce the variance. These ten results are used to estimate the standard deviation of the distribution of results, providing error bars for most figures in this section. While it would be preferable to train all networks to convergence multiple times with different random seeds, this would require significantly more resources. Our baseline diverges slightly from <ref type="bibr" target="#b2">[3]</ref> in two ways. Firstly, we reduce the total stride of the network from 8 to 4 (2 at conv1, 2 at pool1) to avoid training Correlation Filters with small feature maps. Secondly, we always restrict the final layer to 32 output channels in order to preserve the high speed of the method with larger feature maps. These changes did not have a negative effect on the tracking performance of SiamFC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to Siamese baseline</head><p>The results show that CFNet is significantly better than the baseline when shallow networks are used to compute features. Specifically, it brings a relative improvement of 31% and 13% for networks of depth one and two respectively. At depths three, four and five, the difference is much less meaningful. CFNet is relatively unaffected by the depth of the network, whereas the performance of the baseline increases steadily and significantly with depth. It seems that the ability of the Correlation Filter to adapt the distance  metric to the content of the training image is less important given a sufficiently expressive embedding function. The CF layer can be understood to encode prior knowledge of the test-time procedure. This prior may become redundant or even overly restrictive when enough model capacity and data are available. We believe this explains the saturation of CFNet performance when more than two convolutional layers are used. <ref type="figure" target="#fig_3">Figure 4</ref> additionally shows that updating the template is always helpful, for both Baseline and CFNet architectures, at any depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature transfer experiment</head><p>The motivation for this work was the hypothesis that incorporating the CF during training will result in features that are better suited to tracking with a CF. We now compare our end-to-end trained CFNet to variants that use features from alternative sources: Baseline+CF and ImageNet+CF. The results are presented in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>To obtain the curve Baseline+CF we trained a baseline Siamese network of the desired depth and then combined those features with a CF during tracking. Results show that taking the CF into account during offline training is critical at depth one and two. However, it seems redundant when more convolutional layers are added, since using features from the Baseline in conjunction with the CF achieves similar performance. The ImageNet+CF variant employs features taken from a network trained to solve the ImageNet classification challenge <ref type="bibr" target="#b27">[28]</ref>. The results show that these features, which are often the first choice for combining CFs with CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, are significantly worse than those learned by CFNet and the Baseline experiment. The particularly poor performance of these features at deeper layers is somewhat unsurprising, since these layers are expected to have greater invariance to position when trained for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Importance of adaptation</head><p>For a multi-channel CF, each channel p of the template w can be obtained as w p = α x p , where α is itself a function of the exemplar x (Appendix C, supplementary material). To verify the importance of the online adaptation that solv-  <ref type="table">Table 1</ref>: Perfomance as overlap (IoU) and precision produced by the OTB toolkit for the OTB-2013, OTB-50 and OTB-100 datasets. The first and second best results are highlighted in each column. For details refer to Section 4.5.</p><p>ing a ridge regression problem at test time should provide, we propose a "constant" version of the Correlation Filter (CFNet-const) where the vector of Lagrange multipliers α is instead a parameter of the network that is learned offline and remains fixed at test time. <ref type="figure" target="#fig_5">Figure 6</ref> compares CFNet to its constant variant. CFNet is consistently better, demonstrating that in order to improve over the baseline Siamese network it is paramount to backpropagate through the solution to the inverse convolution problem that defines the Lagrange multipliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the state-of-the-art</head><p>We use the OTB-2013/50/100 benchmarks to confirm that our results are on par with the state-of-theart. All numbers in this section are obtained using the OTB toolkit <ref type="bibr" target="#b32">[33]</ref>. We report the results for the three best instantiations of CFNet from <ref type="figure" target="#fig_4">Figure 5</ref> (CFNet-conv2, CFNet-conv5, Baseline+CF-conv3), the best variant of the baseline (Baseline-conv5) and the most promising singlelayer network (CFNet-conv1). We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s <ref type="bibr" target="#b2">[3]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and LCT <ref type="bibr" target="#b22">[23]</ref>. We also include the recent SAMF <ref type="bibr" target="#b18">[19]</ref> and DSST <ref type="bibr" target="#b5">[6]</ref> for reference.</p><p>For the evaluation of this section, we use a different set of tracking hyperparameters per architecture, chosen to maximize the performance on the validation set after a random search of 300 iterations. More details are provided in the supplementary material. For the few greyscale sequences present in OTB, we re-train each architecture using exclusively greyscale images.</p><p>Both overlap (IoU) and precision scores <ref type="bibr" target="#b33">[34]</ref> are reported for OPE (one pass) and TRE (temporal robustness) evaluations. For OPE, the tracker is simply run once on each sequence, from the start to the end. For TRE, the tracker is instead started from twenty different starting points, and run until the end from each. We observed that this ensures more robust and reliable results compared to OPE.  Similarly to the analysis on the validation set, CFNet-conv2 is among the top performers and its accuracy rivals that of Baseline-conv5, which possesses approximately 30× as many parameters. In general, our best proposed CFNet variants are superior (albeit modestly) to the state-of-theart. In order to focus on the impact of our contribution, we decided to avoid including orthogonal improvements which can often be found in the tracking literature (e.g. bounding box regression <ref type="bibr" target="#b25">[26]</ref>, ensembling of multiple cues <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>, optical flow <ref type="bibr" target="#b28">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Speed and practical benefits</head><p>The previous sections have demonstrated that there is a clear benefit to integrating Correlation Filters into Siamese networks when the feature extraction network is relatively shallow. Shallow networks are practically advantageous in that they require fewer operations and less memory to evaluate and store. To understand the trade-off, <ref type="figure" target="#fig_7">Figure 7</ref> reports the speed and accuracy of both CFNet and the baseline for varying network depth <ref type="bibr" target="#b2">3</ref> .</p><p>This plot suggests that the two-layer CFNet could be the most interesting variant for practitioners requiring an accurate tracking algorithm that operates at high framerates. It runs at 75 frames per second and has less than 4% of the parameters of the five-layer baseline, requiring only 600kB to store. This may be of particular interest for embedded devices with limited memory. In contrast, methods like Deep-SRDCF <ref type="bibr" target="#b6">[7]</ref> and C-COT <ref type="bibr" target="#b8">[9]</ref>, which use out-of-the-box deep features for the Correlation Filter, run orders of magnitude slower. Even the one-layer CFNet remains competitive despite having less than 1% of the parameters of the five-layer baseline and requiring under 100kB to store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work proposes the Correlation Filter network, an asymmetric architecture that back-propagates gradients through an online learning algorithm to optimize the underlying feature representation. This is made feasible by establishing an efficient back-propagation map for the solution to a system of circulant equations.</p><p>Our empirical investigation reveals that, for a sufficiently deep Siamese network, adding a Correlation Filter layer does not significantly improve the tracking accuracy. We believe this is testament to the power of deep learning given sufficient training data. However, incorporating the Correlation Filter into a similarity network during training does enable shallow networks to rival their slower, deeper counterparts.</p><p>Future research may include extensions to account for adaptation over time, and back-propagating gradients through learning problems for related tasks such as one-shot learning and domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>We follow the procedure of <ref type="bibr" target="#b2">[3]</ref> to minimize the loss (equation 2) through SGD, with the Xavier-improved parameters initialization and using mini-batches of size 8. We use all the 3862 training videos of ImageNet Video <ref type="bibr" target="#b27">[28]</ref>, containing more than 1 million annotated frames, with multiple objects per frame. Training is conducted for 100 epochs, each sampling approximately 12 pairs (x i , z i ) from each video, randomly extracted so that they are at most 100 frames apart.</p><p>During tracking, a spatial cosine window is multiplied with the score map to penalize large displacements. Track- <ref type="bibr" target="#b2">3</ref> The speed was measured using a 4.0GHz Intel i7 CPU and an NVIDIA Titan X GPU. ing in scale space is achieved by evaluating the network at the scale of the previous object and at one adjacent scale on either side, with a geometric step of 1.04. Updating the scale is discouraged by multiplying the responses of the scaled object by 0.97. To avoid abrupt transitions of object size, scale is updated using a rolling average with learning rate 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Back-propagation for the Correlation Filter</head><p>As described in Appendix D (supplementary material), the back-propagation map is the adjoint of the linear maps that is the differential. These linear maps for the Correlation Filter are presented in eq. 9. We are free to obtain these adjoint maps in the Fourier domain since Parseval's theorem provides the preservation of inner products. Let J 1 denote the map dx → dk in eq. 9a. Hence manipulation of the inner product</p><formula xml:id="formula_12">F dk, F J 1 (dx) = dk, 1 n ( dx * • x + x * • dx) = 1 n dx, dk * • x + dk • x, dx = dx, 2 n Re{ dk} • x<label>(11)</label></formula><p>gives the back-propagation map</p><formula xml:id="formula_13">∇ x = 2 n x • Re{ ∇ k } .<label>(12)</label></formula><p>Similarly, for the linear map dk, dy → dα in eq. 9b,</p><formula xml:id="formula_14">F dα, F J 2 (dk, dy) = dα, k −1 [ 1 n dy − dk • α] = 1 n k − * • dα, dy + − k − * • α * • dα, dk ,<label>(13)</label></formula><p>the back-propagation maps are</p><formula xml:id="formula_15">∇ y = 1 n k − * • ∇ α (14) ∇ k = − k − * • α * • ∇ α ,<label>(15)</label></formula><p>and for the linear map dx, dα → dw in eq. 9c,</p><formula xml:id="formula_16">F dw, F J 3 (dx, dα) = dw, dα * • x + α * • dx = dα, dw * • x + dw • α, dx ,<label>(16)</label></formula><p>the back-propagation maps are</p><formula xml:id="formula_17">∇ α = x • ( ∇ w ) * ,<label>(17)</label></formula><formula xml:id="formula_18">∇ x = α • ∇ w .<label>(18)</label></formula><p>The two expressions for ∇ x above are combined to give the back-propagation map for the entire Correlation Filter block in eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Correlation Filter formulation C.1. Kernel linear regression</head><p>First, consider the general linear regression problem of learning the weight vector w that best maps each of n example input vectors x i ∈ R d to their target y i ∈ R. The squared error can be expressed</p><formula xml:id="formula_19">1 2n n i=1 (x T i w − y i ) 2 = 1 2n X T w − y 2<label>(19)</label></formula><p>where X ∈ R d×n is a matrix whose columns are the example vectors and y ∈ R n is a vector of the targets. Incorporating regularization, the problem is arg min</p><formula xml:id="formula_20">w 1 2n X T w − y 2 + λ 2 w 2 .<label>(20)</label></formula><p>Kernel linear regression can be developed by writing this as a constrained optimization problem arg min</p><formula xml:id="formula_21">w,r 1 2n r 2 + λ 2 w 2 subject to r = X T w − y<label>(21)</label></formula><p>and then finding a saddle point of the Lagrangian</p><formula xml:id="formula_22">L(w, r, υ) = 1 2n r 2 + λ 2 w 2 +υ T (r−X T w+y) . (22)</formula><p>The final solution can be obtained from the dual variable</p><formula xml:id="formula_23">w = 1 λ Xυ<label>(23)</label></formula><p>and the solution to the dual problem is</p><formula xml:id="formula_24">υ = λ n K −1 y<label>(24)</label></formula><p>where K = 1 n X T X + λI is the regularized kernel matrix. It is standard to introduce a scaled dual variable α = 1 λ v that defines w as a weighted combination of examples</p><formula xml:id="formula_25">w = Xα = n i=1 α i x i with α = 1 n K −1 y .<label>(25)</label></formula><p>The kernel matrix is n × n and therefore the dual solution is more efficient than the primal solution, which requires inversion of a d × d matrix, when the number of features d exceeds the number of examples n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Single-channel Correlation Filter</head><p>Given a scalar-valued example signal x with domain U and corresponding target signal y, the Correlation Filter w is the scalar-valued signal arg min</p><formula xml:id="formula_26">w 1 2n w x − y 2 + λ 2 w 2<label>(26)</label></formula><p>where signals are treated as vectors in R U and the circular cross-correlation of two signals w x is defined</p><formula xml:id="formula_27">(w x)[u] = t∈U w[t]x[u + t mod m] ∀u ∈ U . (27)</formula><p>The solution from the previous section can then be used by defining X to be the matrix in R U ×U such that X T w = w x. It follows that the kernel matrix K belongs to R U ×U and the dual variable α is a signal in R U .</p><p>The key to the correlation filter is that the circulant structure of X enables the solution to be computed efficiently in the Fourier domain. The matrix X has elements X[u, t] = x[u + t mod m]. Since the matrix X is symmetric, the template w is obtained as cross-correlation</p><formula xml:id="formula_28">w = Xα = α x .<label>(28)</label></formula><p>The linear map defined by the kernel matrix K is equivalent to convolution with a signal k</p><formula xml:id="formula_29">Kz = k * z ∀z<label>(29)</label></formula><p>which is defined k = 1 n x x + λδ, since</p><formula xml:id="formula_30">∀z : F X T Xz = F ((z x) x) = z • x * • x = F (z * (x x)) .<label>(30)</label></formula><p>Therefore the solution is defined by the equations</p><formula xml:id="formula_31">     k = 1 n x x + λδ k * α = 1 n y w = α x<label>(31)</label></formula><p>and the template can be computed efficiently in the Fourier domain</p><formula xml:id="formula_32">     k = 1 n x * • x + λ1 α = 1 n k −1 • y w = α * • x .<label>(32)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Multi-channel Correlation Filter</head><p>There is little advantage to the dual solution when training a single-channel Correlation Filter from the circular shifts of a single base example. However, the dual formulation is much more efficient in the multi-channel case <ref type="bibr" target="#b13">[14]</ref>.</p><p>For signals with k channels, each multi-channel signal is a collection of scalar-valued signals x = (x 1 , . . . , x k ), and the data term becomes</p><formula xml:id="formula_33">p w p x p − y 2 = p X T p w p − y 2<label>(33)</label></formula><p>and each channel of the template is obtained from the dual variables w p = X p α = α x p</p><p>The solution to the dual problem is still α = 1 n K −1 y, however the kernel matrix is now given <ref type="bibr" target="#b34">35)</ref> and the linear map defined by this matrix is equivalent to convolution with the signal</p><formula xml:id="formula_35">K = 1 n p X T p X p + λI<label>(</label></formula><formula xml:id="formula_36">k = 1 n p x p x p + λδ .<label>(36)</label></formula><p>Therefore the solution is defined by the equations</p><formula xml:id="formula_37">     k = 1 n p x p x p + λδ k * α = 1</formula><p>n y w p = α x p ∀p <ref type="bibr" target="#b36">(37)</ref> and the template can be computed efficiently in the Fourier domain</p><formula xml:id="formula_38">       k = 1 n p x * p • x p + λ1 α = 1 n k −1 • y w p = α * • x p ∀p .<label>(38)</label></formula><p>It is critical that the computation scales only linearly with the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adjoint of the differential</head><p>Consider a computational graph that computes a scalar loss ∈ R. Within this network, consider an intermediate function that computes y = f (x) where x ∈ X = R m and y ∈ Y = R n . Back-propagation computes the gradient with respect to the input ∇ x ∈ X from the gradient with respect to the output ∇ y ∈ Y.</p><p>The derivative ∂f (x)/∂x is a matrix in R n×m whose ijth element is the partial derivative ∂f i (x)/∂x j . This matrix relates the gradients according to</p><formula xml:id="formula_39">(∇ x ) T = ∂ ∂x = ∂ ∂y ∂y ∂x = (∇ y ) T ∂f (x) ∂x<label>(39)</label></formula><p>From this it is evident that the back-propagation map is the linear map which is the adjoint of that defined by the derivative. That is, if the derivative defines the linear map</p><formula xml:id="formula_40">J(u) = ∂f (x) ∂x u<label>(40)</label></formula><p>then the back-propagation map is the unique linear map J * that satisfies</p><formula xml:id="formula_41">J * (v), u = v, J(u) ∀u ∈ X , v ∈ Y<label>(41)</label></formula><p>and the gradient with respect to the input is obtained ∇ x = J * (∇ y ). This is the core of reverse-mode differentiation <ref type="bibr" target="#b11">[12]</ref>. An alternative way to obtain the linear map defined by the derivative is to use differential calculus. Whereas the derivative represents this linear map as a matrix with respect to the standard bases, the differential represents the linear map as an expression df (x; dx). This is valuable for working with variables that possess more interesting structure than simple vectors. This technique has previously been used for matrix structured back-propagation <ref type="bibr" target="#b14">[15]</ref>. In this paper, we use it for circulant structured back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Back-propagation for multi-channel case</head><p>The differentials of the equations that define the multichannel CF in eq. 37 are</p><formula xml:id="formula_42">     dk = 1 n p (dx p x p + x p dx p ) dk * α + k * dα = 1 n dy dw p = dα x p + α dx p ∀p ,<label>(42)</label></formula><p>and taking the Fourier transforms of these equations gives</p><formula xml:id="formula_43">       dk = 1 n p dx * p • x p + x * p • dx p dα = k −1 • 1 n dy − dk • α dw p = dα * • x p + α * • dx p ∀p .<label>(43)</label></formula><p>Now, to find the adjoint of the map dx → dk, we rearrange the inner product</p><formula xml:id="formula_44">F dk, F J 1 (dx) = dk, 1 n p dx * p • x p + x * p • dx p = 1 n p dx p , dk * • x p + dk • x p , dx p = p dx p , 2 n Re{ dk} • x p<label>(44)</label></formula><p>to give the back-propagation map</p><formula xml:id="formula_45">∇ xp = 2 n x p • Re{ ∇ k } ∀p .<label>(45)</label></formula><p>The linear map dk, dy → dα is identical to the singlechannel case. To find the adjoint of the map dx, dα → dw, we examine the inner-product</p><formula xml:id="formula_46">dw, J 3 (dx, dα) = p dw p , dα * • x p + α * • dx p = dα, p dw * p • x p + p dw p • α, dx p ,<label>(46)</label></formula><p>giving the back-propagation maps</p><formula xml:id="formula_47">∇ α = p x p • ( ∇ wp ) * ,<label>(47)</label></formula><formula xml:id="formula_48">∇ xp = α • ∇ wp ∀p .<label>(48)</label></formula><p>Finally, combining these results gives the procedure for back-propagation in the multi-channel case Again, it is important that the computation scales only linearly with the number of channels.</p><formula xml:id="formula_49">             ∇ α = p x p • ( ∇ wp ) * ∇ y = 1 n k − * • ∇ α ∇ k = − k − * • α * • ∇ α ∇ xp = α • ∇ wp + 2 n x p • Re{ ∇ k } ∀p .<label>(49)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Hyperparameter optimization</head><p>The hyperparameters that define the simplistic tracking algorithm have a significant impact on the tracking accuracy. These include parameters such as the penalty for changes in scale and position and the learning rate of the template average. Choosing hyperparameters is a difficult optimization problem: we cannot use gradient descent because the function is highly discontinuous, and each function evaluation is expensive because it involves running a tracker on every sequence from multiple starting points.</p><p>For the experiments of the main paper, where we sought to make a fair comparison of different architectures, we therefore used a natural choice of hyperparameters that were not optimized for any particular architecture. Ideally, we would use the optimal hyperparameters for each variant, except it would have been computationally prohibitive to perform this optimization for every point in every graph in the main paper (multiple times for the points with error bars).</p><p>To achieve results that are competitive with the state-ofthe-art, however, it is necessary to optimize the parameters of the tracking algorithm (on a held-out validation set).</p><p>To find optimal hyperparameters, we use random search with a uniform distribution on a reasonable range for each parameter. Specifically, we sample 300 random vectors of hyperparameters and run the evaluation described in Section 4.1 on the 129 videos of our validation set. Each method is then evaluated once on the test sets (OTB-2013, OTB-50 and OTB-100) using the hyperparameter vector which gave the best results on the validation set (specified in <ref type="table" target="#tab_2">Table 2</ref>). We emphasize that, even though the ground-truth labels are available for the videos in the benchmarks, we do not choose hyperparameters to optimize the results on the benchmarks, as this would not give a meaningful estimate of the generalization ability of the method.</p><p>Note that this random search is performed after training and is only used to choose parameters for the online tracking algorithm. The same network is used for all random samples. The training epoch with the best tracking results on the validation set (with natural tracking parameters) is chosen. <ref type="figure" target="#fig_8">Figure 8</ref> shows, for each method, the empirical distribution of results (in terms of average overlap) that is induced by the distribution of tracking parameters in random search.            <ref type="figure" target="#fig_3">Figure 14</ref>: OTB-100 precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Detailed results on the OTB benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success plots of TRE for OTB-100</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Internal computational graph for the Correlation Filter. The boxes denote functions, which are defined in eq. 7, and the circles denote variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Tracker accuracy for different network depths, on the 129 videos of the validation set. Error bars indicate two standard deviations. Refer to section 4.2 for more details. All figures best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 3</head><label>3</label><figDesc>and 4 compare the accuracy of both methods on the validation set for networks of varying depth. The feature extraction network of depth n is terminated after the n-th linear layer, including the following ReLU but not the following pooling layer (if any).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Success rates of rectangle overlap for individual trackers on the validation set. Solid and dotted lines represent methods that update the template with a running average learning rate of 0.01 and 0, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy of a Correlation Filter tracker when using features obtained via different methods. Error bars indicate two standard deviations. Refer to Section 4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of CFNet to a "constant" variant of the architecture, in which the Lagrange multipliers do not depend on the image (section 4.4). Error bars indicate two standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Tracker accuracy versus speed for CFNet and Siamese baseline. Labels indicate network depth. CFNet enables better accuracy to be obtained at higher speeds using shallower networks. Error bars indicate two standard deviations. Refer to section 4.6 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Empirical distribution of the average overlap for the hyperparameter search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figures</head><label></label><figDesc>Figures 9 to 14 show the curves produced by the OTB toolkit 4 for OTB-2013/50/100, of which we presented a summary in the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>OTB-2013 success rate. OPE for OTB-2013 LCT (2015) [86.2] Baseline+CF-conv3 (ours) [82.2] SiamFC-3s (2016) [81.0] CFNet-conv2 (ours) [80.7] Baseline-conv5 (ours) [80.6] CFNet-conv5 (ours) [80.3] Staple (2016) [79.3] CFNet-conv1 (ours) [77.6] DSST (2014) [74.0] OTB-2013 precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>OTB-50 success rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>OTB-50 precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>OTB-100 success rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>9 to 14 show the curves produced by the OTB toolkit 4 for OTB-2013/50/100, of which we presented a summary in the main paper.</figDesc><table><row><cell></cell><cell cols="3">avg. overlap best overlap scale step</cell><cell>scale penalty</cell><cell>scale l.r.</cell><cell>win. weight</cell><cell>template l.r.</cell></row><row><cell>CFNet-conv1</cell><cell>44.8</cell><cell>46.5</cell><cell>1.0355</cell><cell>0.9825</cell><cell>0.700</cell><cell>0.2375</cell><cell>0.0058</cell></row><row><cell>CFNet-conv2</cell><cell>47.8</cell><cell>49.5</cell><cell>1.0575</cell><cell>0.9780</cell><cell>0.520</cell><cell>0.2625</cell><cell>0.0050</cell></row><row><cell>Baseline+CF-conv3</cell><cell>47.7</cell><cell>49.9</cell><cell>1.0340</cell><cell>0.9820</cell><cell>0.660</cell><cell>0.2700</cell><cell>0.0080</cell></row><row><cell>CFNet-conv5</cell><cell>46.9</cell><cell>48.5</cell><cell>1.0310</cell><cell>0.9815</cell><cell>0.525</cell><cell>0.2000</cell><cell>0.0110</cell></row><row><cell>Baseline-conv5</cell><cell>47.8</cell><cell>49.2</cell><cell>1.0470</cell><cell>0.9825</cell><cell>0.680</cell><cell>0.1750</cell><cell>0.0102</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average and best overlap scores over 300 random sets of hyperparameters. Values of hyperparameters associated to the best performance are also reported. These parameters describe: the geometric step to use in scale search, the multiplicative penalty to apply for changing scale, the learning rate for updating the scale, the weight of an additive cosine window that penalizes translation, and the learning rate for the template average.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Success plots of TRE for OTB-2013</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Success plots of OPE for OTB-2013</cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Success rate</cell><cell>40 50 60</cell><cell></cell><cell cols="5">Baseline-conv5 (ours) [64.0] CFNet-conv2 (ours) [64.0] Baseline+CF-conv3 (ours) [63.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success rate</cell><cell>50 60 40</cell><cell></cell><cell></cell><cell cols="4">Baseline-conv5 (ours) [61.8] LCT (2015) [61.2] CFNet-conv2 (ours) [61.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell cols="4">CFNet-conv5 (ours) [62.6] SiamFC-3s (2016) [61.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell cols="5">CFNet-conv5 (ours) [61.1] Baseline+CF-conv3 (ours) [61.0]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Staple (2016) [61.7]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SiamFC-3s (2016) [60.7]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell cols="3">LCT (2015) [59.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Staple (2016) [60.0]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CFNet-conv1 (ours) [58.6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CFNet-conv1 (ours) [57.8]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSST (2014) [56.6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSST (2014) [55.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Success plots of TRE for OTB-50Success plots of OPE for OTB-50</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline+CF-conv3 (ours) [57.4] CFNet-conv5 (ours) [56.6] CFNet-conv2 (ours) [56.5] Baseline-conv5 (ours) [56.1] SiamFC-3s (2016) [55.5]</cell><cell>Success rate</cell><cell>40 50 30</cell><cell></cell><cell></cell><cell cols="4">CFNet-conv5 (ours) [53.9] Baseline+CF-conv3 (ours) [53.8] CFNet-conv2 (ours) [53.0] Baseline-conv5 (ours) [51.7] SiamFC-3s (2016) [51.6]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Staple (2016) [54.1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Staple (2016) [50.9]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAMF (2014) [51.4]</cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell cols="3">LCT (2015) [49.2]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CFNet-conv1 (ours) [51.0]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CFNet-conv1 (ours) [48.8]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LCT (2015) [49.5]</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SAMF (2014) [46.2]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DSST (2014) [48.4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DSST (2014) [45.2]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Precision plots of TRE for OTB-50</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Precision plots of OPE for OTB-50</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline+CF-conv3 (ours) [76.7] CFNet-conv5 (ours) [75.9] CFNet-conv2 (ours) [75.3]</cell><cell>Precision</cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CFNet-conv5 (ours) [73.2] Baseline+CF-conv3 (ours) [72.3] CFNet-conv2 (ours) [70.2]</cell></row><row><cell>SiamFC-3s (2016) [75.2]</cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SiamFC-3s (2016) [69.2]</cell></row><row><cell>Baseline-conv5 (ours) [74.2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LCT (2015) [69.1]</cell></row><row><cell>Staple (2016) [72.6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Baseline-conv5 (ours) [68.3]</cell></row><row><cell>SAMF (2014) [70.9]</cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Staple (2016) [68.1]</cell></row><row><cell>CFNet-conv1 (ours) [67.9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CFNet-conv1 (ours) [65.3]</cell></row><row><cell>LCT (2015) [67.4] DSST (2014) [64.1]</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SAMF (2014) [63.9] DSST (2014) [60.4]</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Location error threshold</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Precision plots of TRE for OTB-100Precision plots of OPE for OTB-100</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline+CF-conv3 (ours) [79.8] Baseline-conv5 (ours) [79.7] SiamFC-3s (2016) [79.5] CFNet-conv2 (ours) [79.1] Staple (2016) [78.9] CFNet-conv5 (ours) [78.8] SAMF (2014) [77.6]</cell><cell>Precision</cell><cell>40 50 30 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Staple (2016) [78.4] Baseline+CF-conv3 (ours) [77.7] CFNet-conv5 (ours) [77.7] SiamFC-3s (2016) [77.0] Baseline-conv5 (ours) [76.9] LCT (2015) [76.2] CFNet-conv2 (ours) [74.8] SAMF (2014) [74.6]</cell></row><row><cell>LCT (2015) [74.5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet-conv1 (ours) [71.3]</cell></row><row><cell>CFNet-conv1 (ours) [72.6]</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DSST (2014) [68.0]</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Location error threshold</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that this differs from<ref type="bibr" target="#b2">[3]</ref>, in which the target object and search area were instead denoted z and x respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The precision plots in this version of the paper are slightly different to those in the version submitted to CVPR. Whereas in the CVPR version, we adopted the "area under curve" precision metric, here we have used the standard precision metric with a single threshold of 20 pixels. This has little effect on the ordering of the trackers and all observations remained valid.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was supported by Apical Ltd., EPSRC grant Seebibyte EP/M013774/1 and ERC grants ERC-2012-AdG 321162-HELIOS, HELIOS-DFR00200, "Integrated and Detailed Image Understanding" (EP/L024683/1) and ERC 677195-IDIU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional Siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Once for all: A two-flow convolutional neural network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07507</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-aliasing correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Image and Signal Processing and Analysis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating derivatives: Principles and techniques of algorithmic differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correlation filters with limited boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Visual Object Tracking VOT2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese CNN for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradientbased hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07527</idno>
		<title level="m">Differentiation of the Cholesky decomposition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum margin correlation filter: A new approach for localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="631" to="643" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning detectors quickly with stationary statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferring rich feature hierarchies for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04587</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep learning of appearance models for online object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02568</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
