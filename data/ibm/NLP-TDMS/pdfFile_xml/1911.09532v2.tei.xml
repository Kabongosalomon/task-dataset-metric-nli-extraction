<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Cluster Ranking Model for Full Anaphora Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-22">22 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
							<email>juntao.yu@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
							<email>m.poesio@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Cluster Ranking Model for Full Anaphora Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-22">22 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anaphora Resolution</term>
					<term>Coreference</term>
					<term>Cluster ranking model</term>
					<term>Non-referring detection</term>
					<term>Deep Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anaphora resolution (coreference) systems designed for the CONLL 2012 dataset typically cannot handle key aspects of the full anaphora resolution task such as the identification of singletons and of certain types of non-referring expressions (e.g., expletives), as these aspects are not annotated in that corpus. However, the recently released CRAC 2018 Shared Task and Phrase Detectives (PD) datasets can now be used for that purpose. In this paper, we introduce an architecture to simultaneously identify non-referring expressions (including expletives, predicative NPs, and other types) and build coreference chains, including singletons. Our cluster-ranking system uses an attention mechanism to determine the relative importance of the mentions in the same cluster. Additional classifiers are used to identify singletons and non-referring markables. Our contributions are as follows. First of all, we report the first result on the CRAC data using system mentions; our result is 5.8% better than the shared task baseline system, which used gold mentions. Our system also outperforms the best-reported system on PD by up to 5.3%. Second, we demonstrate that the availability of singleton clusters and non-referring expressions can lead to substantially improved performance on non-singleton clusters as well. Third, we show that despite our model not being designed specifically for the CONLL data, it achieves a very competitive result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anaphora resolution is the task of identifying and resolving nominal anaphoric reference to discourse entities <ref type="bibr" target="#b31">(Poesio et al., 2016b)</ref>. <ref type="bibr">1</ref> It is an important aspect of natural language processing and has a substantial impact on downstream applications such as summarization <ref type="bibr" target="#b38">(Steinberger et al., 2007;</ref><ref type="bibr" target="#b39">Steinberger et al., 2016)</ref>. Since the CONLL 2012 shared task <ref type="bibr" target="#b35">(Pradhan et al., 2012)</ref>, the ONTONOTES corpus has been the dominant resource in research on identity anaphora resolution (coreference) <ref type="bibr" target="#b13">(Fernandes et al., 2014;</ref><ref type="bibr" target="#b4">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b26">Martschat and Strube, 2015;</ref><ref type="bibr" target="#b7">Clark and Manning, 2015;</ref><ref type="bibr" target="#b8">Clark and Manning, 2016a;</ref><ref type="bibr" target="#b9">Clark and Manning, 2016b;</ref><ref type="bibr" target="#b22">Lee et al., 2017;</ref><ref type="bibr" target="#b23">Lee et al., 2018;</ref><ref type="bibr" target="#b18">Kantor and Globerson, 2019;</ref><ref type="bibr" target="#b17">Joshi et al., 2019b;</ref><ref type="bibr" target="#b16">Joshi et al., 2019a)</ref>. But ONTONOTES has a number of limitations.</p><p>An often mentioned limitation is that singletons are not annotated <ref type="bibr" target="#b10">(De Marneffe et al., 2015;</ref><ref type="bibr" target="#b6">Chen et al., 2018)</ref>.</p><p>A less discussed, but still crucial, limitation is that although some types of non-referring expressions are marked in ONTONOTES, in particular predicative ones (a policeman in John is a policeman), other types are not, such as expletives, meaning that in It rained, It is not considered a markable. As a consequence, systems optimized for ONTONOTES are only evaluated on non-singleton coreference chains; their performance at identifying singletons, and distinguishing them from expletives, is not evaluated. But the decision to interpret it as referring or nonreferring <ref type="bibr" target="#b42">(Uryupina et al., 2016;</ref><ref type="bibr" target="#b44">Versley et al., 2008;</ref><ref type="bibr" target="#b3">Bergsma et al., 2008;</ref><ref type="bibr" target="#b2">Bergsma and Yarowsky, 2011;</ref><ref type="bibr" target="#b15">Hardmeier et al., 2015)</ref> is a key aspect of pronoun interpretation-for instance, for the purposes of machine translation <ref type="bibr" target="#b14">(Guillou and Hardmeier, 2016</ref>)-so systems trained on ONTONOTES have had to adopt a variety of workarounds. These limitation of ONTONOTES have however been corrected in a number of corpora, including ANCORA for Spanish <ref type="bibr" target="#b40">(Taulé et al., 2008)</ref>, TUBA-D/Z for German <ref type="bibr">(Telljohann et al., )</ref>, and, for English, ARRAU <ref type="bibr" target="#b43">(Uryupina et al., 2019)</ref>, which was used as dataset for the CRAC 2018 shared task <ref type="bibr" target="#b32">(Poesio et al., 2018)</ref>, and Phrase Detectives (PD) . The first contribution of this paper is the development of a system able to perform both coreference resolution and identification of non-referring markables and singletons, using the CRAC 2018 shared task and PD datasets. On CRAC, our model achieves a CONLL score of 77.9% on coreference chains, and an F1 score of 76.3% on nonreferring expressions identification. This is, to the best of our knowledge, the first modern result on the CRAC data using system mentions. Our CONLL score is even 5.8% higher than the baseline result on this dataset, 72.1% obtained by <ref type="bibr" target="#b32">(Poesio et al., 2018)</ref> using gold mentions. On PD, our model outperforms the best-performing system by up to 5.3%. Our second contribution is a novel and competitive cluster ranking architecture for anaphora resolution 2 . Current coreference models can be classified either as mention pair models <ref type="bibr" target="#b37">(Soon et al., 2001)</ref>, in which connections are established between mentions, or entity mention models, in which mentions are directly linked to entities / coreference chains <ref type="bibr" target="#b25">(Luo et al., 2004;</ref><ref type="bibr" target="#b36">Rahman and Ng, 2011)</ref>. The mention pair models are simpler in concept and easier to implement, so many SoTA systems are exclusively based on mention ranking <ref type="bibr" target="#b45">(Wiseman et al., 2015;</ref><ref type="bibr" target="#b8">Clark and Manning, 2016a;</ref><ref type="bibr" target="#b22">Lee et al., 2017)</ref>. But it has long been known that entity-level information is important for coreference <ref type="bibr" target="#b25">(Luo et al., 2004;</ref><ref type="bibr" target="#b31">Poesio et al., 2016b)</ref> so many systems attempted to explore features beyond those of mention pairs <ref type="bibr" target="#b4">(Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b7">Clark and Manning, 2015;</ref><ref type="bibr" target="#b9">Clark and Manning, 2016b;</ref><ref type="bibr" target="#b23">Lee et al., 2018;</ref><ref type="bibr" target="#b18">Kantor and Globerson, 2019;</ref><ref type="bibr" target="#b17">Joshi et al., 2019b;</ref><ref type="bibr" target="#b16">Joshi et al., 2019a)</ref>. However, those systems are usually much more complex than their mention ranking counterpart, since entity features are introduced in addition to their mention ranking part. Consider the <ref type="bibr" target="#b23">Lee et al. (2018)</ref> system, for instance: the full system has 9.6 million trainable parameters in total, which is double the number of the mention ranking part of the system (4.8M parameters). In this work, we demonstrate that it is possible to achieve SoTA results by cluster ranking alone, i.e. by linking mentions directly to the entities. As a result, our model is less complex than the existing entity-level models <ref type="bibr" target="#b18">Kantor and Globerson, 2019)</ref> using similar mention representations. Our model uses only 4.8M trainable parameters without increasing the complexity of a mention ranking model. Furthermore, our model is fast to train; we show that a cluster ranking model can be significantly sped up by training on oracle clusters 3 .</p><p>The key intuitions behind the proposed approach are (i) that cluster representations are crucial to the success of a cluster ranking system, and (ii) that a key property of these representations is that they should capture the fact that mentions in a cluster are not equally important. In particular, it is well-known that the mentions introducing an entity are generally more informative (e.g., the president of ACME, John Smith) whereas subsequent mentions tend to employ reduced forms (e.g., Mr. Smith, he) <ref type="bibr" target="#b0">(Ariel, 1990)</ref>. This motivates the use of cluster representations capable of preserving the greater importance of earlier mentions. Our approach captures this mention importance by using attention scores for the mentions in a cluster and combining the mention representations according to their attention scores. We then investigate the effect of the cluster histories by including all the history of the clusters as candidate assignments to the mentions. The resulting system, besides achieving the new SoTA on the CRAC dataset (whether including and excluding non-referring expressions and singletons), achieves CONLL scores equivalent to the current SoTA system not fine-tuned on BERT <ref type="bibr" target="#b18">(Kantor and Globerson, 2019)</ref> on CONLL data as well (in which non-referring expressions and singletons are not annotated). Our third and final contribution is the finding that training our system on annotations of singleton mentions and non-referring expressions enhance its performance on nonsingleton coreference chains. By evaluating our system on the CRAC data we show that gains of up to 1.4 percentage points on non-singleton coreference chains can be achieved by training the model with additional singleton mentions and non-referring expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System architecture</head><p>Anaphora resolution is the task of identifying the referring mentions in a text and assigning those mentions to disjoint clusters such that mentions in the same cluster refer to the same entity. The first subtask of anaphora resolution is mention detection, i.e., extracting candidate mentions from the document. Until 3 The oracle clusters are created from system mention using gold cluster information.</p><p>Algorithm 1: Cluster ranking algorithm.</p><formula xml:id="formula_0">Input: (N * i , sm(i), sǫ(i), β(i)) λT i=1</formula><p>Output: C λT 1 m = 0; C0 = {}; sc 0 = {}; 2 for i : 1..λT do 3 TMP ← sǫ(i);</p><formula xml:id="formula_1">4 for j : 1..m do 5 TMP ← sm(i) + sc(j) + smc(i, j) 6 end 7 b = arg max TMP; 8 if b == ǫ then 9 Ci = Ci−1 ∪ {N * i }; 10 sc i = sc i−1 ∪ sm(i); 11 m = m + 1; 12 else 13 C b i = m∈C b i−1 ∪N i a b i−1 (m) ·N * m ; 14 sc i (b) = m∈C b i−1 ∪N i a b i−1 (m) · sm(m); 15 end 16 end</formula><p>recently, most coreference systems selected mentions prior to coreference resolution via heuristic methods often based on parse trees <ref type="bibr" target="#b4">(Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b7">Clark and Manning, 2015;</ref><ref type="bibr" target="#b8">Clark and Manning, 2016a;</ref><ref type="bibr" target="#b9">Clark and Manning, 2016b;</ref><ref type="bibr" target="#b45">Wiseman et al., 2015;</ref><ref type="bibr" target="#b46">Wiseman et al., 2016)</ref>. <ref type="bibr" target="#b22">Lee et al. (2017)</ref> introduced a neural network approach for joint mention detection and coreference resolution, obtaining the best performing system at the time. The system was further extended by <ref type="bibr" target="#b23">Lee et al. (2018)</ref>, <ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref>, <ref type="bibr" target="#b17">Joshi et al. (2019b)</ref> and <ref type="bibr" target="#b16">Joshi et al. (2019a)</ref>, the current SoTA on the CONLL data set. Our model is also a joint system that predicts mentions and assigns them to the clusters jointly. For a given document D with T tokens, we define all possible spans in D as N I i=1 where I = T (T +1) 2 , s i , e i are the start and the end indices of N i where 1 ≤ i ≤ I. The task for a joint system is to partition all the spans (N ) into a sequence of clusters (C m ) M m=1 such that every mention in a specific cluster C m refers to the same entity. Let C i be the partially completed clusters up to span N i . The set of possible assignments for N i is defined as all the clusters up to the previous span (C i−1 ) and a special label ǫ. The ǫ is used for three situations: a span is not a mention, or is a non-referring expression, or is the first mention of a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mention Representation</head><p>We use a mention representation based on those in <ref type="bibr" target="#b18">Kantor and Globerson, 2019)</ref>. Our system represents a candidate span with the outputs of a BiL-STM, encoding the sentences in a document from both directions to obtain a representation for each token in the sentence. The BiLSTM takes as input the concatenated embeddings ((x t ) T t=1 ) of both word and character levels. For word embeddings, GloVe <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> and BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> embeddings are used. Character embeddings are learned by a convolution neural networks (CNN) during training. The tokens are represented by concatenated outputs from the forward and the back-ward LSTMs. The token representations (x * t ) T t=1 are used together with head representations (h * i ) to represent candidate spans (N * i ). The h * i of a span is obtained by applying an attention over its token representations ({x * si , ..., x * ei }), where s i and e i are the indices of the start and the end of the span respectively. Formally, we compute h * i , N * i as follows:</p><formula xml:id="formula_2">αt = FFNNα([x * t , φ(t)]) ai,t = exp(αt) e i k=s i exp(α k ) h * i = e i t=s i ai,t · xt N * i = [x * s i , x * e i , h * i , φ(i)]</formula><p>where φ(t), φ(i) are the cluster position and span width feature embeddings respectively. To make the task computationally tractable, our model only considers the spans up to a maximum length of l, i.e. e i − s i &lt; l, (s i , e i ) ∈ N . Further pruning is applied before feeding the candidate mentions to the coreference resolver. The top ranked λT spans are selected from lT candidate spans (λ &lt; l) by a scoring function s m . where:</p><formula xml:id="formula_3">sm(i) = FFNNm(N * i )</formula><p>The top λT selected spans are required not to be partially overlap, i.e. there is no such cases that s i &lt; s j ≤ e i &lt; e j or s j &lt; s i ≤ e j &lt; e i . The nested spans are not affected by this constrains since they are not partially overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Cluster Ranking Model</head><p>Let (N i ) λT i=1 denote the top ranked λT candidate mentions selected by the mention detector after pruning. The model builds the clusters (C m ) M m=1 by visitingN i in text order and assigning them a cluster in the case i = ǫ, or creating a new cluster if i = ǫ. Let C i be the partial clusters consisting of up to i th mentions, and c i the cluster assigned toN i . The task of our cluster ranking model is to outputĈ that maximises the score of the final clusters:</p><formula xml:id="formula_4">C = arg max c 1 ,...,c λT λT i=1 s(i, ci)</formula><p>where s(i, j) 4 is a scoring function between a mention N i and a set of possible assignments j ∈ {ǫ, C m i−1 }:</p><formula xml:id="formula_5">s(i, j) = sǫ(i) j = ǫ sm(i) + sc(j) + smc(i, j) j = ǫ</formula><p>and s ǫ (i) is the probability thatN i does not belongs to any of the previous clusters C m i−1 . To use a scoring function for ǫ instead of a constant 0 (used by <ref type="bibr" target="#b23">Lee et al. (2018)</ref>) gives us the flexibility to extend the function for handing more detailed types of ǫ, such as non-referring. s m (i) is the mention score that has been used to rank the candidate mentions. s c (j) is the cluster score computed from the mention scores that belongs to the cluster. s mc (i, j) is a pairwise score between i th mentionN i and j th partial cluster of C j i−1 . To implement the cluster ranking model we use an attention function a(m) <ref type="bibr" target="#b1">(Bahdanau et al., 2014)</ref> to assign an importance to each of the mentions. We compute the cluster score s c (j) and the cluster representation (C j * i−1 ) (for computing s mc (i, j)), by mention scores/representations and with consideration of mention importance. More precisely, we compute the scores as follows:</p><formula xml:id="formula_6">sǫ(i) = FFNNǫ(N * i ) sm(i) = FFNNm(N * i ) β(i) = FFNN β ([N * i , φ(i β )]) a j i−1 (m) = exp(β(m)) k∈C j i−1 exp(β(k)) sc i−1 (j) = m∈C j i−1 a j i−1 (m) · sm(m) C j * i−1 = m∈C j i−1 a j i−1 (m) ·N * m F * (i,j) = [N * i , C j * i−1 ,N * i • C j * i−1 , φ(i,ĵ), φ(j)] smc(i, j) = FFNNmc(F * (i,j) )</formula><p>Both s c (j) and C j * i−1 are updated each time a cluster is expanded. φ(i β ) is the position embeddings that indicates the position of a mention in the cluster. φ(i,ĵ) is a small set of features between theN i and the newest mentionNĵ of the cluster. We used the same features as <ref type="bibr" target="#b23">Lee et al. (2018)</ref>: these include genre, speaker (boolean, same or not) and distance (between i andĵ) features. φ(j) is cluster size, a common entity-level feature <ref type="bibr" target="#b4">(Björkelund and Kuhn, 2014)</ref>. The size is assigned into buckets according to its value. We use the buckets of <ref type="bibr" target="#b4">Björkelund and Kuhn (2014)</ref>, assigning the values in 8 buckets <ref type="bibr">([1,2,3,4,5-7,8-11,12-19,20+]</ref>). The pseudo-code of our model is shown in Algorithm 1. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cluster History</head><p>One of the advantages of the mention ranking model is that the correct cluster can be built by attaching the active mention to any of the antecedents in the correct cluster. This reduces the complexity of the task as there are multiple correct links. By contrast, in a standard cluster ranking model, only one correct cluster can be chosen. In order to make multiple links possible in our cluster ranking system, we extended our model by including all cluster histories (CH); this maximises the chance of choosing the correct clusters. (We make sure a mention is always attached to the latest version of the cluster by including an additional pointer linking every cluster history to the latest version of the cluster.) This makes the model slightly more similar to a mention ranking model; however, there is still a fundamental difference, as we use cluster representations instead of mention representations. We replace the line 13 and 14 of Algorithm 1 to get the model that includes cluster histories:</p><formula xml:id="formula_7">b = LATEST(b) Ci = Ci−1 ∪ m∈C b i−1 ∪N i a b i−1 (m) ·N * m sc i = sc i−1 ∪ m∈C b i−1 ∪N i a b i−1 (m) · sm(m) m = m + 1</formula><p>where LATEST(b) is a function to find the latest version of the cluster b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Identifying Non-Referring Expressions</head><p>To add non-referring expressions identification, we extend ǫ into multiple classes: NO for non-mention, NR for nonreferring and DN for discourse new, including singletons</p><formula xml:id="formula_8">sǫ(i) = sno(i) NO snr(i) + sm(i) NR s dn (i) + sm(i) DN</formula><p>Several non-referring types are annotated in the ARRAU corpus: in addition to expletives, there are also predicative NPs (e.g., a policeman in John is a policeman), nonreferring quantifiers (e.g.,nobody in I see nobody here ) <ref type="bibr" target="#b19">(Karttunen, 1976)</ref>, idioms (e.g., her hand in He asked her for her hand), etc. As we will see, the basic NR classifier can be extended to do a fine-grained classification of nonreferring expressions. By distinguishing 'non-mentionhood' from nonanaphoricity the system naturally resolves singletons (i.e. the clusters with a size of one). Non-referring expressions are usually filtered before building the coreference chains, e.g. in MARS <ref type="bibr" target="#b27">(Mitkov et al., 2002)</ref>; we will call this PREFILTERING approach. In the PREFILTERING approach, the system removes the markables identified as non-referring expressions from further processing once they have been identified. To be more specific, we replace line 8 of algorithm 1 with:</p><formula xml:id="formula_9">if b == NO or b == NR then Ci = Ci−1; sc i = sc i−1 ; m = m; else if b == DN then</formula><p>The PREFILTERING approach is aggressive, which might have a negative effect on results if referring expressions have been filtered incorrectly. We also tried therefore a second approach: only do prefiltering when the non-referring expressions classifier has high confidence (when the classifier has a softmax score above a heuristic threshold t (0 ≤ t ≤ 1)). The softmax score is calculated between previous clusters and classes in ǫ (i.e. TMP in algorithm 1). If the score is below this threshold, non-referring expressions are identified after (postfiltering) forming the clusters (we call this HYBRID approach). During postfiltering, candidates that are classified as non-referring markables with lower confidence and are not part of clusters are included as additional non-referring markables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Learning</head><p>To train a cluster ranking model on system clusters is challenging, as we need to find a way to learn from the partially correct clusters. It is also slow, as the system processes one mention at a time, hence cannot benefit largely from parallel computing. The solution we adopted was training the model on oracle clusters. This is simpler and faster, since the clusters for one training document can be created before computing more heavy stuff, e.g. the cluster scores s c (j) and pairwise scores s mc (i, j). More precisely, we create the oracle clusters during the training using gold cluster ids; system mentions belonging to the same gold clusters are grouped. This is much faster than training the model on the system mentions directly, since training on the system mentions requires computing scores for each mention separately. In a preliminary experiment, we discovered that by training on oracle clusters we obtain not only a better CONLL score, but also a fivefold speedup compared with the model trained on the system mentions directly. <ref type="bibr">6</ref> As a loss function, we optimize on the marginal loglikelihood of all the clusters that contain mentions from the same gold cluster <ref type="bibr">GOLD(i)</ref> </p><formula xml:id="formula_10">ofN i . Formally, logN i=1 ĉ∈C i−1 ∩GOLD(i) P (ĉ)</formula><p>In case C i−1 does not contain any mention from GOLD(i) orN i does not belongs to a gold cluster, we set GOLD(i) = {ǫ}. For our model to have more than one class in ǫ, the GOLD(i) is set to the relevant classes (NO,NR or DN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data and Hyperparameters</head><p>For full anaphora resolution, our primary evaluation dataset was the CRAC 2018 corpus <ref type="bibr" target="#b32">(Poesio et al., 2018)</ref>. In addition, we evaluated our model on the PD corpus, also containing expletives. Finally, we evaluated our model on the CONLL 2012 English corpora <ref type="bibr" target="#b35">(Pradhan et al., 2012)</ref> to compare its performance with the SoTA on the CONLL task. The CRAC Task 1 dataset is based on the RST portion of the ARRAU corpus <ref type="bibr" target="#b43">(Uryupina et al., 2019)</ref>. The annotation scheme specifies the annotation of referring expressions  (including singletons) and non-referring expressions; split antecedent plurals, generic references, and discourse deixis are annotated, as well as bridging references. The RST portion of ARRAU consists of news texts (1/3 of the PENN Treebank), with 228,000 tokens and 72,000 mentions. PD is a constantly growing corpus collected using the annotation game Phrase Detectives . The corpus was annotated by players and then aggregated by an aggregating method to create a silver standard corpus. Both singletons and non-referring markables are annotated. We used the latest release of the corpus, consisting of 542 documents, 408,000 tokens and 108,000 mentions. 7 . The CONLL datasets are the standard datasets for coreference. The English CONLL corpus consists of 3493 documents for a total of 1.6M tokens and 194,000 mentions. We use the official CONLL 2012 scorer to score our predictions when evaluating without singletons and nonreferring markables, and the official CRAC 2018 scorer <ref type="bibr" target="#b32">(Poesio et al., 2018)</ref> to evaluate other cases. The CRAC 2018 Extended Scorer is an extension of the CONLL 2012 official scorer developed by Nafise Moosavi that can handle singletons and non-referring markables. The Extended Scorer is identical to the CONLL scorer when evaluating without singletons and non-referring markables, but also reports P, R and F1 values for non-referring markables when those are considered. Following standard practice, we report recall, precision, and F1 scores for MUC, B 3 and CEAF φ4 and the average F1 score of those three metrics. Besides, we report the F1 score for non-referring when needed.</p><p>For our experiments, we use the same maximum span width (l = 30), number spans per tokens (λ = 0.4) and most of the network parameters as <ref type="bibr" target="#b23">Lee et al. (2018)</ref> and <ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref>. The details are in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on the CRAC data set</head><p>We first compared the two proposed approaches for using non-referring expressions, PREFILTERING and HYBRID.   For our HYBRID model, we set the threshold (t) to 0.5 after tuning on the development set. <ref type="table" target="#tab_3">Table 2</ref> shows the results of our models on the CRAC test set. As expected, the HYBRID model, using a less greedy pruning, achieved better F1 scores on all three coreference metrics. In terms of the non-referring scores (see <ref type="table" target="#tab_5">Table 3</ref>), the PREFILTER-ING approach has better recall and F1 score, while the HY-BRID approach has better precision. We hypothesize this is mainly because the PREFILTERING approach generates more non-referring expressions due to its greedy pruningi.e., the PREFILTERING approach keeps all the candidate non-referring markables once they are identified-while the HYBRID approach favours the coreference clusters for nonreferring markables fall below the threshold. The HYBRID approach has a better overall performance according to our weighed F1 scores (0.85 * COREF F1 + 0.15 * NR F1) The weights are determined by the proportion of the referring and non-referring markables in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-grained Non-referring</head><p>We further extended the basic NR classifier to recognise the more fine-grained classification of non-referring expressions annotated in the CRAC dataset by configuring our HY-BRID model to learn from the fine-grained types (FINE NR   F1) and achieves 76 -80% F1 score on predicates and coordinations, but has a lower F1 score of around 65% on recognising non-referring quantifiers and idioms. We also compared this model with the other models to dealing with non-referring expressions by collapsing the classifications it produces <ref type="table" target="#tab_5">(Table 3</ref>). As we can see from that Table, although the task is harder, using the fine-grained types for training results in slightly better performance on identifying non-referring markables in general than models trained on a single NR class. In term of the performance on coreference chains, the FINE NR approach achieved the same score as the PREFILTERING approach and slightly lower than the HYBRID approach (see <ref type="table" target="#tab_3">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training without Singletons and Non-referring</head><p>Finally, we trained our model without singletons and nonreferring expressions (NO NR) to assess their effects on non-singleton clusters (i.e. the standard CONLL setting). Since here we evaluate in a singleton excluded setting, we report for our models trained with singletons and nonreferring expressions the standard CONLL scores with singletons and non-referring markables excluded. As shown in <ref type="table" target="#tab_3">Table 2</ref>, all three models trained with additional singleton and non-referring markables achieved better CONLL scores when compared with the newly trained model. The system achieves substantial gains of up to 1.4 percentage points (HYBRID) by training with the additional singletons and non-referring expressions. This suggests that the availability of singletons and non-referring markables can help the decisions made for non-singleton clusters.</p><p>State-of-the-art Comparison Since the CRAC corpus was released recently, the only published results are those by the baseline system <ref type="bibr" target="#b21">(Lee et al., 2013)</ref> on the shared task <ref type="bibr" target="#b32">(Poesio et al., 2018</ref>). Our best system (HYBRID) outperforms this baseline by large margins (5.8% and 13.4% when evaluated with or without singletons respectively) (see <ref type="table">Ta-</ref>ble 2) even though that system was evaluated on gold mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the PD data set</head><p>We then test our best system on the PD corpus 8 . We compare our system with the results by <ref type="bibr" target="#b34">Poesio et al. (2019)</ref> (Table 5). Our system is 3% better when evaluated with singletons included and outperforms their system by 5.3% when evaluated without the singletons. In addition, our system achieved an F1 of 56.7% on non-referring expressions and this is 2.1% better than their result (54.6%). Overall, our system achieved the new SoTA on the PD data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on the CONLL data set</head><p>Finally, we tested our models on the CONLL data to assess the performance of our system on the standard data set. Table 6 compares our results with those of the top-performing systems on CONLL at the present time. We report precision, recall and F1 scores for all three major metrics (MUC, B 3 and CEAF φ4 ) and mainly focus on the average CONLL F1 scores presented in the last column. As showed in <ref type="table" target="#tab_9">Table 6</ref>, our model achieved a CONLL score of 76.4%, which is only 0.2% lower than the best-reported result at present, achieved by <ref type="bibr" target="#b18">(Kantor and Globerson, 2019</ref>) that use a similar mention representations as our system. Although the systems by <ref type="bibr" target="#b17">Joshi et al. (2019b)</ref> and <ref type="bibr" target="#b16">Joshi et al. (2019a)</ref> have better results than the <ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref> system, it is not directly comparable with our system, as their systems are fine-tuned on BERT. Such systems need to be trained on GPUs with 32GB memory, which are not available to our group. By contrast, our system was trained with a GTX 1080Ti GPU with an 11GB memory.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>We further analyze our model on the CONLL data to give a more detailed study on different aspects of our model. (We use the standard CONLL data instead of the CRAC data because the CONLL corpus is larger than the CRAC corpus and is widely used. As a result, the analysis on CONLL data might also be beneficial for other researchers focusing on CONLL only.) Mention Importance We first assess our hypothesis that our attention scores can capture mention importance-i.e., the finding from the linguistic and psychological literature on anaphora that the initial mentions of an entity tend to include more information, whereas the following mentions are generally reduced. <ref type="table" target="#tab_11">Table 7</ref> shows an analysis of the attention scores that supports this hypothesis. We computed the average attention scores for mentions in a cluster in order of mention. Clusters that have different size are analysed separately, as scores from different-sized clusters are not directly comparable. As we can see from the <ref type="table">Table,</ref> after analysis the attention scores assigned to the mentions at different positions in the cluster, we find that the attention scores assigned to the first mention in a cluster are always higher than others, which is in line with linguistic findings that mentions introducing an entity are more informative. This suggests that our attention model does capture something like mention importance. Why Cluster Ranking? The reason why we use a cluster ranking approach instead of mention ranking is not only because it is linguistically more appealing, but also due to several practical restrictions of the mention ranking models. First of all, the current SoTA mention-ranking systems tend to be hybrids, using entity-level features alongside mention-pair features. Thus, such models are usually more complex than pure mention ranking models, and substantially increase the number of trainable parameters. Take <ref type="bibr" target="#b23">Lee et al. (2018)</ref> system as an example. The mention rank-ing part of the system contains 4.8M parameters, but the full system has double the number of parameters (9.6M) to access entity-level features. Our system, on the other hand, links the mentions directly to the entity and uses only 4.8M parameters, which is much simpler than such hybrid models. Second, we hope that using a cluster ranking model will allow us to explore rich cluster level features and advanced search algorithms (e.g. beam search) in future work.</p><p>The Effect of Oracle Clusters on Training Time Training cluster ranking systems using system clusters is timeconsuming: Our model trained on system clusters takes 80 hours to train for 200K steps, which is much more than the 48 hours training time of the <ref type="bibr" target="#b23">Lee et al. (2018)</ref> system (400K steps). The main reason the cluster ranking system is slower than its mention ranking counterpart is that the cluster ranking model processes one mention at a time, hence does not benefit from parallelization. To solve this problem, we trained the system on oracle clusters instead. The oracle clusters are created by using the system mentions with the gold cluster ids. By doing so all the clusters can be created before resolving the mentions into the entities. As a result, the training (200K steps) can be finished in as little as 16 hours, which is 5x faster than training the model on system clusters, and 3x faster than training the mention ranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>We removed different parts of our model to show the importance of the individual part of our system (see <ref type="table" target="#tab_12">Table 8</ref>).</p><p>Position Embeddings We first removed the position embeddings, used in the self-attention to determine the relative importance of the mentions in the cluster. By removing the position embeddings, the relative importance of a mention becomes independent of its position in the cluster. As a result, the performance of the model drops by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Width Embeddings</head><p>We then removed the cluster width embeddings from our features. The cluster width embedding is a feature used in computing the pairwise scores, which allows mentions to known the size of individual candidate clusters. (Cluster size can be used as an indicator of cluster salience, as the larger the size, the more frequently an entity is mentioned, having therefore a higher salience.) The cluster width feature contributes 0.4% towards our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster History</head><p>We trained a model that keeps exactly one cluster per entity, and the history clusters are excluded from the candidate lists. This removing of history clusters reduces the chance of linking the mentions to the correct entity; as a consequence, the performance drops by 1 percentage point.</p><p>Oracle Clusters Finally, we trained a model using the system clusters directly instead of the oracle clusters. As we mentioned in the previous section, training on the system clusters is more time consuming than training on the oracle clusters. And replacing these clusters suggests that training on the oracle clusters is not only faster, but also results in better performance (0.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Pure Mention Ranking Models Most recent coreference systems are highly reliant on mention ranking, which is effective and generally faster to train compared with the cluster ranking system. Systems based only on the mention ranking model include <ref type="bibr" target="#b45">(Wiseman et al., 2015;</ref><ref type="bibr" target="#b9">Clark and Manning, 2016b;</ref><ref type="bibr" target="#b22">Lee et al., 2017)</ref>. <ref type="bibr" target="#b45">Wiseman et al. (2015)</ref> introduced a neural network based approach to solve the task in a nonlinear way.</p><p>In their system, the heuristic features commonly used in linear models are transformed by a tanh function to be used as the mention representations. <ref type="bibr" target="#b9">Clark and Manning (2016b)</ref> integrated reinforcement learning to let the model optimize directly on the B 3 scores. <ref type="bibr" target="#b22">Lee et al. (2017)</ref> first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a BiLSTM. Models using Entity Level Features Researchers have been aware of the importance of entity level information at least since <ref type="bibr" target="#b25">Luo et al. (2004)</ref>, and many systems trying to exploit cluster based features have been proposed since.</p><p>Among neural network models, <ref type="bibr" target="#b4">Björkelund and Kuhn (2014)</ref> built a latent tree system that explores non-local features through beam search. The global feature-aided model showed clear gains when compared with the model based only on pairwise features. <ref type="bibr" target="#b7">Clark and Manning (2015)</ref> introduced a entity-centric coreference system by manipulating the scores of a mention pair model.</p><p>The system first runs a mention pair model on the document and then uses an agglomerative clustering algorithm to build the clusters in an easy-first fashion.</p><p>This system was later extended by <ref type="bibr" target="#b9">Clark and Manning (2016b)</ref> to make it run on neural networks. <ref type="bibr" target="#b46">Wiseman et al. (2016)</ref> add to the <ref type="bibr" target="#b45">Wiseman et al. (2015)</ref> system an LSTM to encode the partial clusters. The outputs of the LSTM are used as additional features for the mention ranking model. <ref type="bibr" target="#b23">Lee et al. (2018)</ref> is an extended version of <ref type="bibr" target="#b22">Lee et al. (2017)</ref> mainly enhanced by using ELMo embeddings <ref type="bibr" target="#b29">(Peters et al., 2018)</ref>, but the use of second-order inference enabled the system explore partial entity level features and further improved the system by 0.4 percentage points. Later the model was further improved by <ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref>  Cluster Ranking Models To the best of our knowledge, our system is the only recent system that does not rely on a mention ranking model. However, there are a number of early studies that laid a solid foundation for the cluster ranking models (see <ref type="bibr" target="#b30">(Poesio et al., 2016a</ref>) for a survey).</p><p>The best known 'modern' examples are the systems proposed by <ref type="bibr" target="#b25">Luo et al. (2004)</ref> and by <ref type="bibr" target="#b36">Rahman and Ng (2011)</ref>, but this approach was the dominant model for anaphora resolution at least until the paper by <ref type="bibr" target="#b37">Soon et al. (2001)</ref>, as it directly implements the linguistically and psychologically motivated view that anaphora resolution involves the creation of a discourse model articulated around discourse entities <ref type="bibr" target="#b19">(Karttunen, 1976)</ref>. The entity mention model of <ref type="bibr" target="#b25">Luo et al. (2004)</ref> introduced the notion that a training instance consists of a mention and an active cluster, and therefore allowed for cluster-level features encoding information about multiple entities in the cluster. <ref type="bibr" target="#b25">Luo et al. (2004)</ref> also proposed a clustering algorithm in which the clustering options are encoded in a Bell tree that also specifies the coreference decisions resulting in a cluster-an idea related to our idea of cluster history. <ref type="bibr" target="#b36">Rahman and Ng (2011)</ref> introduced the term 'cluster ranking' and greatly developed the approach, e.g., by introducing a rich set of cluster-level features. Their model was the first cluster-ranking model to significantly outperform mention pair models. Singletons and Non-referring Expressions Again, to the best of our knowledge, ours is the only modern neural network-based, full coreference system that attempts to output singletons and non-referring markables. The Stanford Deterministic Coreference Resolver <ref type="bibr" target="#b21">(Lee et al., 2013)</ref> uses a number of filters to exclude expletives as well as quasi-referring mentions such as percentages (e.g., 9%) and measure NPs (e.g., a liter of milk) and its extension proposed by De <ref type="bibr" target="#b10">Marneffe et al. (2015)</ref> includes more fiters to exclude singletons, but these aspects of the system are not evaluated. The best-known systems also attempting to annotate non-referring markables date back to the pre-ONTONOTES era. The pronoun resolution algorithm proposed by <ref type="bibr" target="#b20">Lappin and Leass (1994)</ref> includes a series of hand-crafted heuristics to detect expletives. The statistical classifier proposed by <ref type="bibr" target="#b12">Evans (2001)</ref> classifies pronouns in several categories which, apart from nominal anaphoric, include cataphoric, pleonastic, and clause-anaphoric. <ref type="bibr" target="#b44">Versley et al. (2008)</ref> used the BBN pronoun corpus to confirm the hypothesis that tree kernels would be well-suited to identify expletive pronouns. <ref type="bibr" target="#b5">Boyd et al. (2005)</ref> develop a set of hand-crafted heuristics to identify non-referring nominals in the sense of <ref type="bibr" target="#b19">Karttunen (1976)</ref>. The systems developed by Bergsma and colleagues to identify pronominal it with a classifier using a combination of lexical features and web counts <ref type="bibr" target="#b3">(Bergsma et al., 2008;</ref><ref type="bibr" target="#b2">Bergsma and Yarowsky, 2011)</ref>. A lot of work on identifying expletives was carried out in the context of the DiscoMT evaluation campaigns, but this work was typically only focused on disambiguating pronoun it <ref type="bibr" target="#b24">(Loáiciga et al., 2017)</ref>. For more discussion of these and other systems, see <ref type="bibr" target="#b42">(Uryupina et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we presented the first neural network based system for full coreference resolution also covering singletons and non-referring markables. Our system uses an attention mechanism to form the cluster representations using mention importance scores from the mentions belonging to the cluster. By training the system on oracle clus-ters we show that a cluster ranking system can be trained 5x faster, and faster than a mention-ranking system with a similar architecture. Evaluation on the CRAC corpus shows that our system is 5.8% better than the only existing comparable system, the Shared Task baseline system that used the gold mentions. The evaluation on PD shows the same trend. Further evaluation on the CONLL corpus shows our system achieves on that corpus, for the subtask in which singleton and non-referring expression detection are excluded, a performance equivalent to that of the SoTA <ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref> system. We also demonstrated that a large improvement on non-singleton coreference chains can be made by training the system with additional singletons and non-referring expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This research was supported in part by the DALI project, ERC Grant 695662.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>who use BERT embeddings (Devlin et al., 2019) instead of ELMo embeddings. At this stage, both BERT and ELMo embeddings are used in a pretrained fashion. Recently, Joshi et al. (2019b) fine-tunes the BERT model for coreference task, result in again a small improvement. Later, Joshi et al. (2019a) introduces a BERT model (SpanBERT) specifically trained for the tasks that involves spans, by using the SpanBERT, the system achieved a substantial gain of 2.7% when compared with the Joshi et al. (2019b) model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters for our models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>79.0 77.2 75.9 80.7 78.2 75.2 77.3 76.2 77.2 HYBRID 77.9 78.5 78.2 77.4 80.3 78.8 75.4 78.1 76.8 77.9 FINE NR 76.7 77.3 77.0 76.8 79.7 78.2 74.9 78.0 76.4 77.2</figDesc><table><row><cell></cell><cell>Models</cell><cell>MUC P</cell><cell>R</cell><cell>F1</cell><cell>B 3 P</cell><cell>R</cell><cell>F1</cell><cell>CEAF φ 4 P R</cell><cell>F1</cell><cell>Avg. F1</cell></row><row><cell>Singletons included</cell><cell cols="10">PREFILTERING 75.5 Lee et al. (2013)* 72.1 58.9 64.8 77.5 77.1 77.3 64.2 88.1 74.3 72.1</cell></row><row><cell></cell><cell>PREFILTERING</cell><cell cols="9">75.5 79.0 77.2 67.0 73.0 69.9 67.1 65.1 66.1 71.1</cell></row><row><cell>Singletons excluded</cell><cell>HYBRID FINE NR NO NR</cell><cell cols="9">77.9 78.5 78.2 69.2 71.8 70.4 69.5 63.8 66.5 71.7 76.7 77.3 77.0 68.0 70.7 69.3 66.6 64.2 65.4 70.6 76.7 77.0 76.8 68.7 69.7 69.2 66.1 63.8 64.9 70.3</cell></row><row><cell></cell><cell cols="10">Lee et al. (2013)* 72.3 58.9 64.9 67.9 48.5 56.5 54.2 53.0 53.6 58.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The comparison between our models and the SoTA system on the CRAC test set. * indicates systems evaluated on the gold mentions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The scores for non-referring expressions of our models on the CRAC test set.</figDesc><table><row><cell>NR types</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Expletive</cell><cell cols="3">93.8 100.0 96.8</cell></row><row><cell>Predicate</cell><cell cols="2">77.6 75.2</cell><cell>76.4</cell></row><row><cell>Quantifier</cell><cell cols="2">65.0 64.7</cell><cell>64.9</cell></row><row><cell cols="3">Coordination 77.5 82.0</cell><cell>79.7</cell></row><row><cell>Idiom</cell><cell cols="2">77.0 55.9</cell><cell>64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The scores of our models on the fine-grained nonreferring types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The comparison between our models and the SoTA system on the PD test set.</figDesc><table><row><cell>Models</cell><cell>MUC P</cell><cell>R</cell><cell>F1</cell><cell>B 3 P</cell><cell>R</cell><cell>F1</cell><cell>CEAF φ 4 P R</cell><cell>F1</cell><cell>Avg. F1</cell></row><row><cell>Context Clark and Manning (2016a)</cell><cell cols="9">79.2 70.4 74.6 69.9 58.0 63.4 63.5 55.5 59.2 65.7</cell></row><row><cell>Independent Lee et al. (2017)</cell><cell cols="9">78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 67.2</cell></row><row><cell>Embeddings Zhang et al. (2018)</cell><cell cols="9">79.4 73.8 76.5 69.0 62.3 65.5 64.9 58.3 61.4 67.8</cell></row><row><cell>Pre-trained Lee et al. (2018)</cell><cell cols="9">81.4 79.5 80.4 72.2 69.5 70.8 68.2 67.1 67.6 73.0</cell></row><row><cell cols="10">Contextual Kantor and Globerson (2019) 82.6 84.1 83.4 73.3 76.2 74.7 72.4 71.1 71.8 76.6</cell></row><row><cell>Embeddings Our model</cell><cell cols="9">82.7 83.3 83.0 73.8 75.6 74.7 72.2 71.0 71.6 76.4</cell></row><row><cell>Fine-tuned Joshi et al. (2019b)</cell><cell cols="9">84.7 82.4 83.5 76.5 74.0 75.3 74.1 69.8 71.9 76.9</cell></row><row><cell>on BERT Joshi et al. (2019a)</cell><cell cols="9">85.8 84.8 85.3 78.3 77.9 78.1 76.4 74.2 75.3 79.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison between our models and the top performing systems on the CONLL test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The average mention importance attention scores in the CONLL development set, grouped by mentions position and cluster size in the final clusters.</figDesc><table><row><cell></cell><cell cols="2">Avg. F1 ∆</cell></row><row><cell>Our model</cell><cell>76.9</cell><cell></cell></row><row><cell>-Position emb</cell><cell>76.2</cell><cell>0.7</cell></row><row><cell>-Width emb</cell><cell>76.5</cell><cell>0.4</cell></row><row><cell cols="2">-Cluster history 75.9</cell><cell>1.0</cell></row><row><cell>-Oracle cluster</cell><cell>76.3</cell><cell>0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The comparison between our best model and different ablated models on CONLL development set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Some NLP researchers use the term anaphora resolution to refer to pronominal anaphoric reference only, but we use the term in the traditional linguistic and psycholinguistic sense (see<ref type="bibr" target="#b31">(Poesio et al., 2016b)</ref> for full discussion).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We follow<ref type="bibr" target="#b23">Lee et al. (2018)</ref> and use i to indicate the anaphor and j for the antecedent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We do not use coarse-to-fine pruning or higher-order inference, unlike<ref type="bibr" target="#b23">Lee et al. (2018)</ref> and<ref type="bibr" target="#b18">Kantor and Globerson (2019)</ref>. We found coarse-to-fine pruning does not improve our model when compared with simpler distance pruning. As for higherorder inference, our system already has access to the entity-level information by default, hence it is not necessary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We train both approaches on the CONLL data for 200K steps on a GTX 1080Ti GPU. It takes 16 and 80 hours to train a model on oracle and system mentions respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8"><ref type="bibr" target="#b34">Poesio et al. (2019)</ref> uses an early version of our system.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Accessing Noun-Phrase Antecedents. Croom Helm Linguistics Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ariel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nada: A robust system for non-referential pronoun detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Processing and Applications</title>
		<editor>Iris Hendrickx, et al.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional identification of non-referential pronouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifying non-referential it: a machine learning approach incorporating linguistically motivated patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gegg-Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Feature Selection for Machine Learning in NLP</title>
		<meeting>the ACL Workshop on Feature Selection for Machine Learning in NLP<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preco: A large-scale dataset in preschool vocabulary for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entity-level distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling the lifespan of discourse entities with application to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="445" to="475" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Applying machine learning toward an automatic classification of it. Literary and linguistic computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent trees for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="801" to="835" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protest: A test suite for evaluating pronouns in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hardmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<editor>Nicoletta Calzolari</editor>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France, may</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>et al., editors. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pronoun-focused mt and cross-lingual pronoun prediction: Findings of the 2015 discomt shared task on pronoun translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Discourse in Machine Translation</title>
		<meeting>the Second Workshop on Discourse in Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coreference resolution with entity equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="673" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discourse referents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karttunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics 7 -Notes from the Linguistic Underground</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="363" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An algorithm for pronominal anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lappin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Leass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="562" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deterministic coreference resolution based on entity-centric, precision-ranked rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="916" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Higherorder coreference resolution with coarse-to-fine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What is it? disambiguating the different readings of the pronoun it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loáiciga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hardmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A mention-synchronous coreference resolution algorithm based on the bell tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roukos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL</title>
		<meeting>of the ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new, fully automatic version of Mitkovs knowledge-poor pronoun resolution method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="168" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early approaches to anaphora resolution: Theoretically inspired and heuristic-based</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stuckardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vieira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Resolution: Algorithms, Resources and Applications</title>
		<editor>M. Poesio, et al.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Anaphora Resolution: Algorithms, Resources and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stuckardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Versley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grishina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Roesiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Simonjetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zinsmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anaphora resolution with the arrau corpus</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAACL Worskhop on Computational Models of Reference, Anaphora and Coreference (CRAC)</title>
		<meeting>of the NAACL Worskhop on Computational Models of Reference, Anaphora and Coreference (CRAC)<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A crowdsourced corpus of multiple judgments and disagreement on anaphoric interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kruschwitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1778" to="1789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Sixteenth Conference on Computational Natural Language Learning<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Narrowing the modeling gap: a cluster-ranking approach to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="469" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="De" to=" cember" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Two uses of anaphora resolution in summarization. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabadjov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jezek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1663" to="1680" />
		</imprint>
	</monogr>
	<note>Special issue on Summarization</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coreference applications to summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabadjov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Resolution: Algorithms, Resources and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ancora: Multilevel annotated corpora for catalan and spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Telljohann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zinsmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<title level="m">Stylebook for the tübingen treebank of written german</title>
		<imprint/>
	</monogr>
	<note>tüba-d/z</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting non-reference and non-anaphoricity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabadjov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Resolution: Algorithms, Resources, and Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="369" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Annotating a broad range of anaphoric phenomena, in a variety of genres: the ARRAU corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bristot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cavicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Delogu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coreference systems based on kernels methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK, August. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
