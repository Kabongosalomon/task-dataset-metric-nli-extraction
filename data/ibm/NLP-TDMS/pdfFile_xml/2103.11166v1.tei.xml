<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Subsampling for Generating High-Quality Images from Conditional Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-23">March 23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Ding</surname></persName>
							<email>xin.ding@stat</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Wang</surname></persName>
							<email>yongweiw@ece</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Subsampling for Generating High-Quality Images from Conditional Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-23">March 23, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subsampling unconditional generative adversarial networks (GANs) to improve the overall image quality has been studied recently. However, these methods often require high training costs (e.g., storage space, parameter tuning) and may be inefficient or even inapplicable for subsampling conditional GANs, such as class-conditional GANs and continuous conditional GANs (CcGANs), when the condition has many distinct values. In this paper, we propose an efficient method called conditional density ratio estimation in feature space with conditional Softplus loss (cDRE-F-cSP). With cDRE-F-cSP, we estimate an image's conditional density ratio based on a novel conditional Softplus (cSP) loss in the feature space learned by a specially designed ResNet-34 or sparse autoencoder. We then derive the error bound of a conditional density ratio model trained with the proposed cSP loss. Finally, we propose a rejection sampling scheme, termed cDRE-F-cSP+RS, which can subsample both class-conditional GANs and CcGANs efficiently. An extra filtering scheme is also developed for CcGANs to increase the label consistency. Experiments on CIFAR-10 and Tiny-ImageNet datasets show that cDRE-F-cSP+RS can substantially improve the Intra-FID and FID scores of BigGAN. Experiments on RC-49 and UTKFace datasets demonstrate that cDRE-F-cSP+RS also improves Intra-FID, Diversity, and Label Score of CcGANs. Moreover, to show the high efficiency of cDRE-F-cSP+RS, we * equal contribution compare it with the state-of-the-art unconditional subsampling method (i.e., DRE-F-SP+RS). With comparable or even better performance, cDRE-F-cSP+RS only requires about 10% and 1.7% of the training costs spent respectively on CIFAR-10 and UTKFace by DRE-F-SP+RS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr">[10]</ref> are popular generative models for image synthesis. Mathematically, they aim to estimate the marginal distribution of images. As an extension and an essential family of GANs, conditional GANs (cGANs) <ref type="bibr">[22]</ref> are able to estimate the image distribution given some conditions. These conditions are usually categorical variables such as class labels. cGANs with class labels are also known as class-conditional GANs <ref type="bibr" target="#b14">[27,</ref><ref type="bibr" target="#b12">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">32]</ref>. Recently, <ref type="bibr">[7,</ref><ref type="bibr">6]</ref> propose a new conditional GAN framework, continuous conditional GANs (Cc-GANs), which takes continuous, scalar variables (termed as regression labels) such as steering angles as conditions. Recent advances in unconditional GANs [14, 15] and conditional GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">7,</ref><ref type="bibr">6]</ref> generally enable these models to generate high-quality images. Nevertheless, low-quality images still appear frequently even with such advanced GAN models during image generation. We would like to clarify that image quality discussed in this paper refers to not only the visual quality but also <ref type="bibr" target="#b0">(1)</ref> image diversity for unconditional GANs; (2) intra-label diversity and label consistency (the consistency of generated images with respect to the conditioning label) <ref type="bibr">[7,</ref><ref type="bibr" target="#b4">5]</ref> for conditional GANs.</p><p>Improving the sampling strategy of a trained unconditional GAN to filter out low-quality samples attracted increasing attention recently [2, <ref type="bibr" target="#b18">31,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b8">21,</ref><ref type="bibr" target="#b3">4]</ref>. These post hoc, subsampling methods for GANs can improve the sample quality without changing the network architecture or training algorithms. To accomplish such a subsampling, discriminator rejection sampling (DRS) [2] accepts or rejects a fake image by rejection sampling (RS). Metropolis-Hastings GAN (MH-GAN) <ref type="bibr" target="#b18">[31]</ref> applies the Metropolis-Hastings (MH) algorithm to sample from a trained GAN. Denote the true marginal image distribution and the fake marginal image distribution by p r (x) and p g (x) respectively. The success of DRS and MH-GAN requires an accurate density ratio estimation (DRE) of r * (x) := p r (x)/p g (x). However, since the DRE step in both DRS and MH-GAN relies on the assumption of optimality of the discriminator, these two methods may not perform well if the discriminator is far from optimal.</p><p>[8] improves DRS and MH-GAN by proposing a new DRE method, termed density ratio estimation in the feature space with Softplus loss (DRE-F-SP), which does not require an optimal discriminator. By incorporating DRE-F-SP into RS, MH, and sampling importance resampling (SIR), <ref type="bibr">[8]</ref> introduces three density ratio based subsampling methods for GANs (i.e., DRE-F-SP+RS, DRE-F-SP+MH, and DRE-F-SP+SIR respectively). In addition to these density ratio based subsampling methods [2, <ref type="bibr" target="#b18">31,</ref><ref type="bibr">8]</ref>, <ref type="bibr" target="#b8">[21]</ref> refines generated images by using information from a trained discriminator. <ref type="bibr" target="#b3">[4]</ref> proposes to generate images by sampling from an energy-based model defined in the latent space of the generator in a GAN. The above methods have been demonstrated effective in subsampling different unconditional GANs (e.g., DCGAN <ref type="bibr" target="#b15">[28]</ref>, Wasserstein GAN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">11]</ref>, and MMD-GAN <ref type="bibr" target="#b7">[20]</ref>) trained on multiple datasets. These methods, however, are not designed for the conditional image synthesis setting.</p><p>A rejection sampling scheme is proposed by <ref type="bibr" target="#b13">[26]</ref> to subsample the auxiliary classifier GAN (ACGAN) <ref type="bibr" target="#b14">[27]</ref> (a class-conditional GAN) based on the gap in log-densities that measures the discrepancy between the true image distribution and the fake image distribution on given samples. Empirical studies show that this scheme can improve the performance of ACGAN in the class-conditional image synthesis. However, this method is not applicable to some class-conditional GANs (e.g., BigGAN <ref type="bibr" target="#b2">[3]</ref>) and it is not designed for CcGANs. Moreover, <ref type="bibr" target="#b13">[26]</ref> focuses on a semisupervised setting (most of the training data are unlabeled) while the supervised setting has not been well-studied yet.</p><p>Another promising approach to subsample classconditional GANs is applying DRE-F-SP+RS, DRE-F-SP+MH, or DRE-F-SP+SIR [8] within each image class. This approach is practical only if there are a small number of image classes. Conversely, if there exist many classes, it may be infeasible in practice. For example, if we apply this approach to subsample a BigGAN trained on Tiny-ImageNet <ref type="bibr" target="#b6">[19]</ref> with 200 classes, we need to fit 200 density ratio models separately, which is often time-consuming and computationally expensive. Moreover, since these 200 density ratio models may not share the same optimal training setups and it is generally impractical to tune the hyper-parameters for each model carefully, it is likely that some density ratio models are not well-trained. Besides the high training costs, this approach may be invalid in subsampling CcGANs because the feature extraction in DRE-F-SP is inapplicable to images with regression labels only. Even if we find a suitable feature extraction mechanism, this approach is still impractical unless there are very few distinct regression labels.</p><p>To address such challenges and fill the research gap in subsampling conditional GANs, we propose a conditional density ratio based subsampling method. The proposed method is applicable to both class-conditional GANs and CcGANs. Moreover, we only need to fit one density ratio model in subsampling such conditional GANs. Our contributions can be summarized as follows:</p><p>• In Section 3.1, we propose cDRE-F-cSP, which is able to estimate an image's density ratio conditional on a class/regression label. We first introduce a new feature extraction method for images with regression labels, since the feature extraction mechanism in DRE-F-SP [8] is not applicable to CcGANs <ref type="bibr">[7]</ref>. Then, we propose a novel conditional Softplus (cSP) loss, which enables us to estimate density ratios conditional on different labels by fitting only one density ratio model. This density ratio model takes as input both the high-level features and the class/regression label of an image and outputs the density ratio conditional on the given label.</p><p>• We derive in Section 3.2 the error bound of the pro-posed conditional density ratio model trained with the novel cSP loss.</p><p>• In Section 3.3, we propose a rejection sampling scheme based on the estimated conditional density ratios, termed cDRE-F-cSP+RS, to subsample cGANs. An extra filtering scheme is proposed in Section 3.4 for CcGANs to increase the label consistency.</p><p>• In Section 4, experiments on CIFAR-10, Tiny-ImageNet, RC-49 and UTKFace demonstrate the effectiveness of the proposed cDRE-F-cSP+RS in subsampling BigGAN and CcGANs in terms of multiple metrics. Even with much less training cost, the proposed method performs comparable to (or even better than) the state-of-the-art subsampling method.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative adversarial networks</head><p>A GAN model [10] includes two components: a generator G(z) and a discriminator D(x). The generator G(z) takes as the input random noise drawn from N (0, I) and outputs a fake image x g , which follows the fake marginal image distribution p g (x). The discriminator takes as the input a fake image x g or a real image x r and outputs the probability that this image comes from the true marginal image distribution p r (x). The discriminator is trained to distinguish between real and fake images while the generator is trained to fool the discriminator. This adversarial training of D(·) and G(·) aims to make p g (x) as close as possible to p r (x). cGANs <ref type="bibr">[22]</ref> extend GANs [10] into the conditional image synthesis setting, where a condition y is fed into both G(z, y) and D(x, y). Similar to unconditional GANs, cGANs approximate the true conditional image distribution p r (x|y) by the fake conditional image distribution p g (x|y). The conditional y is often a categorical variable such as a class label and cGANs with class labels as conditions are also known as class-conditional GANs. Classconditional GANs have been widely studied <ref type="bibr" target="#b14">[27,</ref><ref type="bibr" target="#b12">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">32]</ref>. State-of-the-art class conditional GANs such as BigGAN <ref type="bibr" target="#b2">[3]</ref> can generate photo-realistic images for a given class. However, GANs conditional on regression labels have been rarely studied due to two problems. First, very few (even zero) real images exist for some regression labels. Second, since regression labels are continuous and infinitely many, they cannot be embedded by one-hot encoding like class labels. To solve these two problems, <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> propose the Cc-GAN framework, which introduces novel empirical cGAN losses and label input mechanisms. The novel empirical cGAN losses, consisting of the hard vicinal discriminator loss (HVDL), the soft vicinal discriminator loss (SVDL), and a new generator loss, are developed to solve the first problem. The second problem is solved by a naive label input (NLI) mechanism and an improved label input (ILI) mechanism. The effectiveness of CcGAN has been demonstrated on diverse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subsampling GANs by DRE-F-SP+RS, DRE-F-SP+MH, or DRE-F-SP+SIR</head><p>[8] proposes a subsampling framework for unconditional GANs to replace DRS [2] and MH-GAN <ref type="bibr" target="#b18">[31]</ref>. This framework consists of two components: a density ratio estimation method termed DRE-F-SP and a density ratio based sampler. DRE-F-SP aims to estimate the density ratio function r * (x) := p r (x)/p g (x) based on N r real images x r 1 , x r 2 , . . . , x r N r ∼ p r (x) and N g fake images x g 1 , x g 2 , . . . , x g N g ∼ p g (x). Based on the estimated density ratios, to push p g to p r , a density ratio based sampler such as RS, MH, or SIR is used to sample from the trained GAN model. The framework with three different samplers results in three subsampling methods which are denoted respectively by DRE-F-SP+RS, DRE-F-SP+MH, and DRE-F-SP+SIR.</p><p>As the key component of the subsampling framework, DRE-F-SP [8] first trains a specially designed ResNet-34 [12] on a set of real images with class labels under the cross-entropy loss. The network architecture of this ResNet-34 is adjusted to ensure that the dimension of one hidden map h equals that of the input image x. Thus, this ResNet34 defines a mapping of an image x to a high-level feature h, i.e., h = φ(x), where φ is assumed invertible and the absolute value of the Jacobian determinant ∂h/∂x is assumed positive. Denote q r (h) and q g (h) respectively as the marginal distributions of real and fake features, <ref type="bibr">[8]</ref> shows that</p><formula xml:id="formula_0">ψ * (h) := q r (h) q g (h) = q r (h) · ∂h/∂x q g (h) · ∂h/∂x = p r (x) p g (x)</formula><p>= r * (x), <ref type="bibr" target="#b0">(1)</ref> which implies that the density ratio of an image x equals to the density ratio of the corresponding high-level feature h. Then, instead of estimating r(x) directly, DRE-F-SP models the true density ratio function ψ * (h) in the feature space by a 5-layer multilayer perceptron (MLP) denoted by ψ(h). Theoretically analysis in <ref type="bibr">[8]</ref> shows that DRE-F-SP enables us to model the true density ratio function by a small neural network (e.g., 5-layer MLP), which is able to increase the generalization performance of the density ratio model. DRE-F-SP also proposes to train this MLP by the Softplus (SP) loss, i.e.,</p><formula xml:id="formula_1">L u (ψ) =E h∼qg(h) [σ(ψ(h))ψ(h) − η(ψ(h))] − E h∼qr(h) [σ(ψ(h))] ,<label>(2)</label></formula><p>where σ(t) = e t /(1 + e t ) and η(t) = ln(1 + e t ). The empirical approximation of Eq.</p><p>(2) is</p><formula xml:id="formula_2">L u (ψ) = 1 N g N g i=1 [σ(ψ(h g i ))ψ(h g i ) − η(ψ(h g i ))] − 1 N r N r i=1 σ(ψ(h r i )),<label>(3)</label></formula><p>where h r i and h g i are high-level features extracted by φ from x r i and x g i respectively. In practice, DRE-F-SP minimizes the penalized Softplus loss:</p><formula xml:id="formula_3">min ψ L u (ψ) + λ Q u (ψ)<label>(4)</label></formula><p>where λ controls the penalty strength and</p><formula xml:id="formula_4">Q u (ψ) = 1 N g N g i=1 ψ(h g i ) − 1 2 .<label>(5)</label></formula><p>The penalty term Q u (ψ) encourages the average density ratios of fake images to be close to 1 to prevent ψ from overfitting the training data. Empirical study in <ref type="bibr">[8]</ref> shows that DRE-F-SP often performs well if λ ∈ [0, 0.1]. Thus, we fix λ = 10 −3 when implementing DRE-F-SP in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As discussed in Section 1, due to high training costs and the incomplete feature extraction mechanism (only suitable for images with class labels), the unconditional subsampling approach [8] (i.e., DRE-F-SP+RS, DRE-F-SP+MH, DRE-F-SP+SIR) may be impractical for subsampling cGANs. Moreover, the only existing conditional subsampling method <ref type="bibr" target="#b13">[26]</ref> is designed for ACGAN <ref type="bibr" target="#b14">[27]</ref> and cannot be applied to other cGANs. Motivated by these issues, in this section, we propose a general and efficient subsampling method, which is suitable for both class-conditional GANs and CcGANs no matter the number of distinct class/regression labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional density ratio estimation in feature space with conditional Softplus loss</head><p>In this section, we introduce cDRE-F-cSP, a novel conditional density ratio estimation (cDRE) method. Assume we have N r real image-label pairs (x r 1 , y r 1 ), . . . , (x r N r , y r N r ) and N g fake image-label pairs (x g 1 , y g 1 ), . . . , (x g N g , y g N g ). Based on these samples, cDRE-F-cSP aims to estimate r * (x|y) := p r (x|y)/p g (x|y). Similar to DRE-F-SP, we conduct cDRE in a feature space learned by a pre-trained neural network φ. For class-conditional GANs, we use the specially designed ResNet-34 (also used by DRE-F-SP) to extract high-level features. For CcGANs, since regression datasets (e.g., Cell-200 [6]) may not have class labels, we train a specially designed sparse autoencoder (AE) to extract features whose architecture is visualized in <ref type="figure" target="#fig_0">Fig. 1</ref>. The encoder with ReLU [9] as the final layer is treated as φ to extract sparse high-level features from images. The bottleneck dimension of the sparse autoencoder equals the dimension of the flattened input image. The decoding process is trained to not only reconstruct the input image but also predict the regression label of the input image. The training loss of this sparse AE is the summation of three components: (1) the mean square error (MSE) between the input image and the reconstructed image;</p><p>(2) the MSE between the true regression label and the predicted regression label; (3) the product of a positive constant λ and the mean of all elements in h, where λ controls the sparsity and is set as 10 −3 in our experiment.</p><p>The sparsity regularizer and the extra branch to predict regression labels are both used to avoid overfitting.</p><p>Next, we propose a formulation of the true conditional Since the encoder has ReLU as the last layer, the feature vector h is sparse. This autoencoder also has an extra branch to predict the regression label of x.</p><p>density ratio function in the feature space as follows</p><formula xml:id="formula_5">ψ * (h|y) := q r (h|y) q g (h|y) = q r (h|y) · ∂h/∂x q g (h|y) · ∂h/∂x = p r (x|y) p g (x|y) = r * (x|y),<label>(6)</label></formula><p>where q r (h|y) and q g (h|y) are respectively the real and fake condition distributions of high-level features. Based on Eq. (6) and the pre-trained neural network φ(x), we model the conditional density ratio function ψ * (h|y) in the feature space by a 5-layer MLP denoted by ψ(h|y) with both h and its label y as input. To train ψ(h|y), we propose the conditional Softplus (cSP) loss as follows:</p><formula xml:id="formula_6">L c (ψ) =E (h,y)∼qg(h,y) [σ(ψ(h|y)) − η(ψ(h|y))] − E (h,y)∼qr(h,y) [σ(ψ(h|y))] ,<label>(7)</label></formula><p>where q g (h, y) = q g (h|y)p(y), q r (h, y) = q r (h|y)p(y), and p(y) is the distribution of labels. The empirical approximation to Eq. <ref type="formula" target="#formula_6">(7)</ref> is</p><formula xml:id="formula_7">L c (ψ) = 1 N g Ng i=1 [σ(ψ(h g i |y g i ))ψ(h g i |y g i ) − η(ψ(h g i |y g i ))] − 1 N r Nr i=1 σ(ψ(h r i |y r i )).<label>(8)</label></formula><p>Similar to DRE-F-SP, to prevent ψ(h|y) from overfitting the training data, a natural constraint applied to ψ(h|y) is</p><formula xml:id="formula_8">E h∼qg(h|y) [ψ(h|y)] = 1.<label>(9)</label></formula><p>If Eq. (9) holds, then</p><formula xml:id="formula_9">E (h,y)∼qg(h,y) [ψ(h|y)] = 1.<label>(10)</label></formula><p>An empirical approximation to Eq. (10) is</p><formula xml:id="formula_10">1 N g N g i=1 ψ(h g i |y g i ) = 1.<label>(11)</label></formula><p>Therefore, in practice, we minimize the penalized version of Eq. (8) as follows:</p><formula xml:id="formula_11">min ψ L c (ψ) + λ Q c (ψ) ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_12">Q c (ψ) = 1 N g N g i=1 ψ(h g i |y g i ) − 1 2 .<label>(13)</label></formula><p>All experiments in this paper use λ = 10 −3 . An algorithm shown in Alg. 1 is used to implement cDRE-F-cSP in practice.</p><p>Algorithm 1: The optimization algorithm for the density ratio model training in cDRE-F-cSP.</p><p>Data: N r real image-label pairs {(x r 1 , y r 1 ), · · · , (x r N r , y r N r )}, a generator G, a pre-trained ResNet-34 or encoder φ(x) for feature extraction, a 5-layer MLP ψ(h|y) and a preset hyperparameter λ. Result: a trained conditional density ratio model</p><formula xml:id="formula_13">r(x|y) = ψ(φ(x)|y) = ψ(h|y). 1 Initialize ψ; 2 for k = 1 to K do 3 Sample a mini-batch of m real image-label pairs {(x r (1) , y r (1) ), · · · , (x r (m) , y r (m) )} from {(x r 1 , y r 1 ), · · · , (x r N r , y r N r )}; 4 Sample a mini-batch of m fake image-label pairs {(x g (1) , y g (1) ), · · · , (x g (m) , y g (m) )} from G; 5</formula><p>Update ψ via SGD or a variant with the gradient of Eq.(12),</p><p>i.e., Lc(ψ) + λ Qc(ψ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">end</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Error bound</head><p>In this section, we derive the error bound of a density ratio model ψ(h|y) trained with the Softplus loss L c (ψ). For simplicity, we ignore the penalty term in this analysis.</p><p>Firstly, we introduce some notations. Let Ψ = {ψ : h → ψ(h|y)} denote the hypothesis space of the density ratio model ψ(h|y). We also defineψ andψ as follows:ψ = arg min ψ∈Ψ L c (ψ) andψ = arg min ψ∈Ψ L c (ψ). Please note that the hypothesis space Ψ may not cover the true density ratio function ψ * . Therefore, L c (ψ) − L c (ψ * ) ≥ 0. Denote by α all learnable parameters of ψ and assume α is in a parameter space A. Denote σ(ψ(h|y))ψ(h|y) − η(ψ(h|y)) by g(h|y; α). LetR qr(h,y),N r (Ψ) denote the empirical Rademacher complexity of Ψ, which is defined based on independent feature-label pairs {(h r 1 , y r 1 ), . . . , (h r N r , y r N r )} from q r (h, y). Then, we derive the error bound of the conditional density ratio estimateψ under the theoretical loss as follows:</p><formula xml:id="formula_14">Theorem 1. If (i) N g is large enough, (ii) A is com- pact, (iii) ∀g(h|y; α) is continuous at α, (iv) ∀g(h|y; α), ∃ a function g u (h|y) that does not depend on α, s.t. |g(h|y; α)| ≤ g u (h|y), and (iv) E (h,y)∼qg(h,y) g u (h|y) &lt; ∞, then ∀δ ∈ (0, 1) and ∀δ ∈ (0, δ] with probability at least 1 − δ, L c (ψ) − L c (ψ * ) ≤ 1 N g +R qr(h,y),N r (Ψ) + 2 4 N r log 2 δ + L c (ψ) − L c (ψ * ).<label>(14)</label></formula><p>Proof. The proof is in Supp. S.2.</p><p>Remark 1.R qr(h,y),N r (Ψ) on the right of Eq. <ref type="formula" target="#formula_3">(14)</ref> implies that we should not use an overly complicated density ratio model. It supports our proposed cDRE-F-cSP because we just need a small neural network (e.g., a shallow MLP) to model the density ratio function in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">cDRE-F-cSP+RS: A cDRE-based rejection sampling scheme for conditional GANs</head><p>Based on the cDRE method proposed in Section 3.1, we develop a rejection sampling scheme, termed cDRE-F-cSP+RS, to subsample conditional GANs. The workflow can be summarized in <ref type="figure" target="#fig_1">Fig. 2</ref> and Alg. 2. This rejection sampling scheme is conducted for each distinct label y of interest. For example, on CIFAR-10 [17], we train only one density ratio model ψ(h|y), based on which we repeat the rejection sampling scheme 10 times for 10 classes respectively. Algorithm 2: Subsampling fake images with label y by cDRE-F-cSP+RS.</p><p>Data: a generator G, a trained ResNet-34 or encoder φ(x) for feature extraction, a trained conditional density ratio model ψ(h|y). Result: images = {N filtered fake images with label y} 1 Generate N burn-in fake images from G conditional on label y.; 2 Estimate the density ratios of these N fake images conditional on y by evaluating ψ(φ(x)|y); 3 M ← max{N estimated density ratios};</p><formula xml:id="formula_15">4 images ← ∅; 5 while |images| &lt; N do 6</formula><p>x ← get a fake image with label y from G; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A filtering scheme for subsampling Cc-GANs</head><p>To solve the problem of insufficient data, CcGANs use images with labels in the vicinity of y to estimate p r (x|y).</p><p>When the CcGAN training is complete, the generator can generate infinite images given a regression label y (termed the assigned label). However, the true labels of these generated images may be inconsistent with the assigned label, and such label inconsistency is very likely to happen in the CcGAN sampling because of the vicinal training technique. Consequently, evaluating the density ratio model ψ(φ(x)|y) on images generated from CcGANs with the assigned label y may be problematic because the true density ratio function r * (x|y) may be ill-defined on these images. We propose a filtering scheme in both the density ratio model training and the subsampling process to deal with this issue. To be specific, we first train a regressionoriented CNN (we use VGG-11 <ref type="bibr">[30]</ref> in our experiments) on the same dataset used for the CcGAN training. This CNN is used to predict labels of fake images generated from CcGANs, and these predicted labels are treated as the true labels of the generated images. Then, before evaluating L c (ψ) + λ Q c (ψ) in the density ratio model training (Alg. 1), we predict the labels of the m fake images and filter out those images if the mean absolute error (MAE) between their assigned labels and predicted labels is larger than a threshold τ . Finally, before conducting rejection sampling in the subsampling process ( <ref type="figure" target="#fig_1">Fig. 2</ref> and Alg. 2), we filter out fake images with MAE between assigned labels and true labels larger than the threshold τ . A rule of thumb to select the filtering threshold τ is shown in Alg. 3. Besides making valid the evaluation of ψ(φ(x)|y) on images generated from CcGANs, empirical study in Section 4.3 and 4.4 shows that this filtering scheme can effectively increase the label consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, the objective is to experimentally demonstrate the effectiveness of the proposed subsampling scheme for both class-conditional GANs and CcGANs.</p><p>We conduct experiments on four image datasets. In subsampling class-conditional GANs, we utilize the CIFAR-10 [17] and Tiny-ImageNet <ref type="bibr" target="#b6">[19]</ref> datasets. In subsampling CcGANs, we experiment on RC-49 [7, 6] and UTKFace Algorithm 3: A rule of thumb to select the filtering threshold τ .</p><p>1 Sample N f images per label from the trained CcGAN without subsampling; 2 Predict the labels of these fake images by the pre-trained VGG-11; <ref type="bibr" target="#b2">3</ref> Compute MAE between the predicted labels and assigned labels of the generated images; 4 Sort these MAEs from smallest to largest and the 0.8 quantile of these MAEs is set as the filtering threshold τ .</p><p>[33] datasets.</p><p>In the class-conditional GAN setting, the quality of fake images is evaluated by two metrics: Fréchet inception distance (FID) <ref type="bibr">[13]</ref> and Intra-FID <ref type="bibr" target="#b12">[25]</ref>. Intra-FID is an overall image quality metric, which computes the FID separately for each class and reports the average FID score. A lower Intra-FID/FID index indicates a better quality of sampled fake images, or vice versa. In the CcGAN setting, we follow CcGAN [7] and adopt four image quality assessment metrics: (i) Intra-FID <ref type="bibr" target="#b12">[25]</ref> is an overall image quality metric; (ii) Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b10">[23]</ref> evaluates the visual quality of fake images. Please note that visual quality is only one aspect of image quality; (iii) Diversity measures the diversity of fake images; and (iv) Label Score (LS) evaluates label consistency. We prefer small Intra-FID, FID, NIQE, and Label Score but large Diversity. Please refer to Supp. S.3.3 and S.5.3 for the details of these metrics. Many example fake images generated in each experiment are also shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR-10</head><p>We first evaluate the effectiveness of the proposed method in subsampling BigGAN <ref type="bibr" target="#b2">[3]</ref> in the conditional image synthesis setting on the CIFAR-10 dataset <ref type="bibr">[17]</ref>. Experimental setup: CIFAR-10 consists of 60,000 (32 × 32) RGB images uniformly from 10 classes. The overall number of training samples is 50,000 (5000 for each class), and the remaining 10,000 samples (1000 for each class) are for test. The class-conditional GAN models employ the BigGAN architecture <ref type="bibr" target="#b2">[3]</ref>. To investigate the effectiveness of the proposed method on different GAN models, we train the BigGAN under three different settings by varying the number of training samples. Thus we obtain three BigGAN models, denoted respectively as BC-50k, BC-20k, and BC-10k. Specifically, the BC-50k model is obtained by training BigGAN using all 50,000 samples from the CIFAR-10 training set. The other two models are obtained using randomly chosen subsets of the training set with 20,000 (BC-20k) and 10,000 (BC-10k) samples, respectively.</p><p>Next, we implement the proposed cDRE-F-cSP+RS for each setting. Firstly, we train a customized ResNet-34 classifier on the training set for feature extraction. Then, by employing the proposed loss in Eq. (12), a 5-layer MLP is trained as the conditional density ratio model in the feature space learned by the ResNet-34. As a reference, we also implement DRE-F-SP+RS [8] in each setting (denoted by 10-DRE-F-SP+RS), where ten unconditional density ratio models (ten 5-layer MLPs) are carefully trained for the ten image classes separately. Thus, the proposed method needs only 10% of the training costs (e.g., storage space and parameter tuning) spent by 10-DRE-F-SP+RS. Please refer to Section S.5 for the detailed setups.</p><p>Quantitative results: We quantitatively compare the image quality of fake images subsampled using no subsampling, 10-DRE-F-SP+RS, and cDRE-F-cSP+RS. With each sampling method, we draw 50,000 fake images (5000 per class) from each of the three GAN models. <ref type="table" target="#tab_0">Table 1</ref> compares the results and shows that samples from either 10-DRE-F-SP+RS or cDRE-F-cSP+RS subsampling methods significantly outperform no subsampling. Moreover, the performance gain tends to increase when GAN models become less well-trained after applying either of the two subsampling schemes. Finally, compared with 10-DRE-F-SP+RS, the proposed method achieves comparable performances, however, it requires only 10% of the computational costs to train the density ratio models. Please note that it is impractical to adopt DRE-F-SP+RS [8] and other unconditional subsampling methods [2, <ref type="bibr" target="#b18">31,</ref><ref type="bibr" target="#b8">21,</ref><ref type="bibr" target="#b3">4]</ref> if the number of classes is large (e.g., 200 classes on Tiny-ImageNet as shown in Section S.3).</p><p>We also show in <ref type="figure" target="#fig_4">Fig. 3</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tiny-ImageNet</head><p>This experiment further demonstrates the effectiveness of the proposed method in subsampling class-conditional GANs on the Tiny-ImageNet dataset <ref type="bibr" target="#b6">[19]</ref>. Quantitative results: <ref type="table" target="#tab_2">Table 2</ref> shows image quality comparisons of fake images sampled using no-subsampling and the proposed methods on Tiny-ImageNet. The proposed method enhances image quality for both metrics, particularly the FID score.  <ref type="figure" target="#fig_0">Fig. 1</ref>). The trained autoencoder is utilized as the feature extractor. We then train a 5-layer MLP to estimate conditional density ratios. In subsampling CcGANs, we also adopt the proposed filtering scheme in Alg. 3. Detailed experimental setups can be found in Supp S.5. Quantitative results: <ref type="table" target="#tab_3">Table 3</ref> reports quantitative evaluations of subsampled fake images from the no-subsampling and cDRE-F-cSP+RS on RC-49. Clearly, cDRE-F-cSP+RS consistently outperforms no-subsampling in all three settings in terms of the overall image quality mea-sured by Intra-FID. More specifically, both subsampling methods produce fake images with comparable visual quality as indicated by NIQE. The proposed subsampling method, however, generates fake images with improved diversity and label consistency on RC-49. Moreover, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, we evaluate the diversity/label consistency of both methods at each angle. The proposed method consistently improves over no-subsampling, indicating its effectiveness in generating more diverse and label consistent fake images uniformly over all angles in RC-49. Please refer to Supp. S.5.4 for visual comparison examples.</p><p>The influence of the MAE quantile in Alg. 3 on the performance of cDRE-F-cSP+RS is shown in <ref type="table" target="#tab_4">Table 4</ref>. We can see a smaller MAE quantile often leads to a larger Intra-FID, a higher Diversity, a smaller Label Score, but an unchanged NIQE, which implies there is a trade-off between the image diversity and label consistency and Intra-FID is more sensitive to Diversity. This table confirms 0.8 is a good choice for the MAE quantile, trading slightly less image diversity for better label consistency.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">UTKFace</head><p>This experiment evaluates the performance of subsampling methods on UTKFace <ref type="bibr" target="#b20">[33]</ref>  <ref type="table" target="#tab_6">Table 5</ref>, we show performance comparisons of the no-subsampling method, 60-DRE-F-SP+RS, and cDRE-F-cSP+RS. In terms of the overall image quality comparison, both the proposed method and 60-DRE-F-SP+RS outperform the no-subsampling method measured by Intra-FID. Compared with 60-DRE-F-SP+RS, the proposed method shows a comparable overall performance. The three methods generate fake images with comparable visual quality. While 60-DRE-F-SP+RS has slightly worse diversity than no-subsampling, the proposed cDRE-F-cSP+RS shows substantial improvement in image diversity and label score over both no-subsampling and 60-DRE-F-SP+RS. Some example images for this experiments are also shown in Supp. S.6.4. Proof. Following [8], we decompose L c (ψ) − L c (ψ * ) as follows:</p><formula xml:id="formula_16">L c (ψ) − L c (ψ * ) =L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ * ) (Since L c (ψ) − L c (ψ) ≤ 0) ≤L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ * ) ≤2 sup ψ∈Ψ L c (ψ) − L c (ψ) + L c (ψ) − L c (ψ * ). (S.15)</formula><p>The second term in Eq. (S.15) is a constant which implies an inevitable error. The first term can be bounded as follows:</p><formula xml:id="formula_17">sup ψ∈Ψ L c (ψ) − L c (ψ) ≤ sup ψ∈Ψ E (h,y)∼qg(h,y) [σ(ψ(h|y))ψ(h|y) − η(ψ(h|y))] − 1 N g N g i=1 [σ(ψ(h g i |y g i ))ψ(h g i |y g i ) − η(ψ(h g i |y g i ))] + E (h,y)∼qr(h,y) [σ(ψ(h|y))] − 1 N r N r i=1</formula><p>σ(ψ(h r i |y r i )) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(S.16)</head><p>Because of assumptions (i)-(iv), based on the uniform law of large number <ref type="bibr" target="#b16">[29]</ref>, for ∀ &gt; 0,</p><formula xml:id="formula_18">lim N g →∞ P sup ψ∈Ψ E (h,y)∼qg(h,y) [σ(ψ(h|y))ψ(h|y) − η(ψ(h|y))] − 1 N g N g i=1</formula><p>[σ(ψ(h g i |y g i ))ψ(h g i |y g i ) − η(ψ(h g i |y g i ))] = 0.</p><p>Based on this limit, we can derive an upper bound of the first term of Eq. (S.16) as follows. Since we can generate infinite fake images from a trained cGAN, N g is large enough. Let = 1/2N g , ∀δ 1 ∈ (0, 1) with probability at </p><formula xml:id="formula_19">least 1 − δ 1 , sup ψ∈Ψ E (h,y)∼qg(h,y) [σ(ψ(h|y))ψ(h|y) − η(ψ(h|y))] − 1 N g N g i=1 [σ(ψ(h g i |y g i ))ψ(h g i |y g i ) − η(ψ(h g i |y g i ))] ≤ 1 2N g .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3 More Details of Experiments on CIFAR-10</head><p>In this section, we introduce the training and test setups for our experiments on CIFAR-10. We also show some example fake images in Section S.3.4. fc→ 2048, GN (8 groups), ReLU, Dropout(p = 0.5) fc→ 1024, GN (8 groups), ReLU, Dropout(p = 0.5) fc→ 512, GN (8 groups), ReLU, Dropout(p = 0.5) fc→ 256, GN (8 groups), ReLU, Dropout(p = 0.5) fc→ 128, GN (8 groups), ReLU, Dropout(p = 0.5) fc→ 1, ReLU fc→ 2048, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 1024, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 512, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 256, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 128, GN (4 groups), ReLU, Dropout(p = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.1 Network architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3.4 Visual results</head><p>In this section, we show the example fake images generated from each subsampling method for some image classes (e.g., car and horse). E.g., in Figs. S.3.5 and S.3.6, we show 100 examples images for each method for the "car" class and the "horse" class in the BC-10k setting. We also show 100 real images as reference in the visualized image samples. In Figs. S.3.5, we observe numerous images (marked in red boxes) from the no-subsampling method, which we can hardly recognize as cars. By contrast, there appear much less such low-quality examples after subsampling by 10-DRE-F-SP+RS and the proposed cDRE-F-cSP+RS methods. We have similar observations in <ref type="figure" target="#fig_4">Fig. S.3</ref>.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4 More Details of Experiments on Tiny-ImageNet</head><p>In this section, we introduce the detailed training and testing setups of our experiments on Tiny-ImageNet. Some example fake images are shown in Section S.4.4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.1 Network architectures</head><p>Similar to the CIFAR-10 experiment, a specially designed ResNet-34 is trained to extract high-level features from images. This ResNet-34 is almost identical to the one used in the CIFAR-10 experiment. The MLP-5 with structure shown in <ref type="table" target="#tab_8">Table S</ref>.4.8 is used as the conditional density ratio model to implement cDRE-F-cSP on the Tiny-ImageNet dataset. fc→ 2048, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 1024, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 512, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 256, GN (4 groups), ReLU, Dropout(p = 0.5) fc→ 128, GN (4 groups), ReLU, Dropout(p = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.3 Testing setups</head><p>In the testing phase, we generate one million images (5000 per class) from the no-sbusampling and the proposed methods, respectively. Similar to CIFAR-10 experiments, we adopt FID and Intra-FID as two evaluation metrics. Both metrics are also computed based on the feature map with dimension 2048 of the last average pooling layer of a pretrained Inception V3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4.4 Visual results</head><p>In this section, we visualize some example fake images generated from two sampling methods for some image classes (e.g., goldfish and tower) in Figs. S.4.7 and S.4.8. In <ref type="figure" target="#fig_5">Fig. S.4</ref>.7, as marked by red boxes, such "goldfish" images from the no-subsampling method are of very low quality. Also, we can observe some replicated samples (a.k.a mode collapse) for samples from the no-subsampling method. Even for images that look like "goldfish", such images appear to look "darker" and less colorful than real images. Compared to the no-subsampling method, the proposed method generates significantly improved image quality, i.e., our image samples are both more diverse and look much more natural those from the baseline method. In <ref type="figure" target="#fig_5">Fig. S.4</ref>.8, we can also observe fake image samples from the no-subsampling method generally look less appealing than those from the proposed method. Moreover, for some images (marked in red boxes) from the no-subsampling method, the tower is actually missing in those "tower" images. However, we can clearly see the tower in images that are sampled from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5 More Details of Experiments on RC-49</head><p>In this section, we introduce the detailed training and testing setups of our experiments on RC-49. We also show some example fake images in Section S.5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.1 Network architectures</head><p>For the feature extraction model, we adopt a specially designed sparse antoencoder with architecture shown in <ref type="table" target="#tab_8">Table S</ref>.5.9. We also employ an MLP-5 as the conditional density ratio model, which uses the same architecture as shown in <ref type="table" target="#tab_8">Table S</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.2 Training setups</head><p>The sparse autoencoder for feature extraction is trained for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decay every 50 epochs with a factor 0.1), weight decay 10 −4 , and batch size 128. Similarly, the VGG-11 network in the filtering scheme is trained for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decay at epoch 50 and 120 with factor 0.1), weight decay 10 −4 , and batch size 128. The CcGAN (SVDL+ILI) training setup is consistent with work [6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.3 Testing setups</head><p>In the testing phase, we generate 179,800 fake images (200 per angle) from the no-subsampling and the proposed cDRE-F-cSP+RS methods, respectively. This experiment adopts four popular evaluation metrics-(i) Intra-FID <ref type="bibr" target="#b12">[25]</ref> is an overall image quality metric; (ii) Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b10">[23]</ref> evaluates the visual quality of fake images. Please note again that the visual quality is only one aspect of image quality; (iii) Diversity measures the diversity of fake images; and (iv) Label Score (LS) evaluates label consistency. Quantitatively, we prefer smaller Intra-FID, FID, NIQE and Label Score indices but larger Diversity values.</p><p>Specifically, the four metrics are computed as follows. (i) For the Intra-FID index, at each of the 899 angles (0.1 • − 89.9 • ), we compute the FID [13] value between 49 real images and 200 fake images in terms of the bottleneck feature of the pre-trained autoencoder. The Intra-FID score is the average FID over all 899 evaluation angles.</p><p>(ii) For the NIQE index, firstly we fit an NIQE model with the 49 real rendered chair images at each of the 899 angles which gives 899 NIQE models. We then compute an average NIQE score for each evaluated angle using the NIQE model at that angle. Finally, we report the average of the 899 average NIQE scores over the 899 yaw angles. The block size and the sharpness threshold are set to 8 and 0.1 respectively in this experiments. We employ the built-in NIQE library in MATLAB. (iii) For the Diversity index, at each evaluation angle, firstly we use a pretrained classification-oriented ResNet-34 to predict the chair types (49 types in total) of these 200 fake images. Then, an entropy value can be computed based on the chair type predictions at this angle. Finally, the Diversity index is defined as the average of the entropies at all 899 angles. (iv) For the Label Score index, at each evaluation angle, firstly we ask a pretrained regression-oriented ResNet-34 to predict the yaw angles of all fake image samples and the predicted angles are then compared with the assigned angles. The Label Score value is defined as the average absolute distance between the predicted angles and assigned angles over all fake images, which is equivalent to the Mean Absolute Error (MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5.4 Visual results</head><p>In this section, we show the example fake images generated from each subsampling method in Figs. S.5.9. From such image examples, we observe that the no-subsampling method generates many image samples that suffer from the mode collapse issue. By contrast, we have largely alleviated this issue in the proposed cDRE-F-cSP+RS subsampling method. Therefore, we can safely conclude that our method generates more diverse fake images than the baseline sampling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6 More Details of Experiments on UTKFace</head><p>This section describes the details of training and testing setups of our experiments on the UTKFace dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.1 Network architectures</head><p>The network architectures used in this experiment is similar to those in the RC-49 experiment. Please refer to Section S.5 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.2 Training setups</head><p>Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity, and Label Score. We also train a sparse autoencoder (bottleneck dimension is 512), a classification-oriented ResNet-34, and a regression-oriented ResNet-34 on the UTKFace dataset. Please note that, the UTKFace dataset consists of face images from 5 races based on which we train the classification-oriented ResNet-34. The autoencoder and both two ResNets are trained for 200 epochs with a batch size 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.3 Testing setups</head><p>In the testing stage, we generate 60,000 fake images (1000 per age) from each subsampling method. Similar to experiments on RC-49, we adopt Intra-FID, NIQE, Diversity and Label Score as quantitative measures in Section S.5.3.</p><p>Analogue to experiments on RC-49, four metrics can be computed accordingly. Please note that the metric values are average over 60 ages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6.4 Visual results</head><p>We show the example fake images generated from each subsampling method in Figs. S.6.10. As marked in red boxes in images from the no-subsampling method, we observe that, occasionally some face images indeed have clearly wrong ages. For example, in the second row, two marked face images at an age 9 look clearly more mature than they should be. Such image samples give rise to the label inconsistency issue in CcGANs. By contrast, we can hardly observe such label inconsistency issue for images from the proposed method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the sparse autoencoder for feature extraction in subsampling CcGANs. The bottleneck dimension equals the size of the flattened input image x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The workflow of cDRE-F-cSP+RS has two modules: cDRE-F-cSP and rejection sampling. M in the acceptance probability p equals to max h {q r (h)/q g (h)}, which can be estimated by evaluating ψ(h|y) on some burn-in samples before subsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7</head><label></label><figDesc>ratio ← ψ(φ(x)|y); 8 M ← max{M, ratio}; 9 p ← ratio/M (i.e., the acceptance probability in RS); 10 u ← Uniform[0, 1]; 11 if u ≤ p then 12Append(x, images);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the FID comparison for each class. For each class, cDRE-F-cSP+RS shows clear superiority over the no-subsampling scheme, and it achieves comparable performance with 10-DRE-F-SP+RS. Please refer to Supp. S.3.4 for visual comparison examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR-10: FID versus class plots in sampling the BC-10k class-conditional GAN model. FID scores of cDRE-F-cSP+RS are smaller than those of no-subsampling and comparable to those of 10-DRE-F-SP+RS over all classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Line graphs of Diversity/Label Score versus regression labels in subsampling the SR-5 CcGAN model. The proposed subsampling method consistently outperforms the no-subsampling method across all regression labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>18 )</head><label>18</label><figDesc>(S.17) The second term of Eq. (S.16) can be bounded based on Lemma 1 and Theorem 2 (the Rademacher bound [18]) in [8] as follows: ∀δ 2 ∈ (0, 1) with probability at least 1 − δ 2 , E (h,y)∼qr(h,y) [σ(ψ(h|y))] Let δ = max{δ 1 , δ 2 } and δ = δ, based on Eq. (S.17) and (S.18), we can derive Eq. (14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S. 3 . 5 :</head><label>35</label><figDesc>Example CIFAR-10 images for the "car" class in the BC-10k setting.(c) 10-DRE-F-SP+RS (d) cDRE-F-cSP+RS Figure S.3.6: Example CIFAR-10 images for the "horse" class in the BC-10k setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S. 4 . 8 :</head><label>48</label><figDesc>Example Tiny-ImageNet images for the "tower" class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S. 5 . 9 :</head><label>59</label><figDesc>Example RC-49 images for angles from 4.5 degree to 85.5 degree (from top to bottom) in the SR-5 setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CIFAR-10 dataset: Quality of 50,000 fake images (5000 per class) sampled from different sampling methods on three trained GAN models. cDRE-F-cSP+RS only requires 10% training costs of 10-DRE-F-SP+RS.</figDesc><table><row><cell>Sampling methods</cell><cell cols="2">BC-50k Intra-FID</cell><cell>FID</cell><cell cols="2">BC-20k Intra-FID</cell><cell>FID</cell><cell>BC-10k Intra-FID</cell><cell>FID</cell></row><row><cell>no-subsampling</cell><cell>0.915</cell><cell cols="2">0.415</cell><cell>1.636</cell><cell cols="2">0.973</cell><cell>3.271</cell><cell>2.601</cell></row><row><cell>10-DRE-F-SP+RS</cell><cell>0.542</cell><cell cols="2">0.226</cell><cell>1.158</cell><cell cols="2">0.675</cell><cell>2.511</cell><cell>1.948</cell></row><row><cell>cDRE-F-cSP+RS</cell><cell>0.598</cell><cell cols="2">0.240</cell><cell>1.267</cell><cell cols="2">0.700</cell><cell>2.621</cell><cell>1.987</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>] on this dataset, we compare the proposed subsampling scheme with the no-subsampling method only. From each method, we sample one million (5000 per class) fake images for image quality evaluation. Please refer to Supp. S.4 for details and Supp. S.4.4 for visual comparison examples.</figDesc><table><row><cell>Experimental setup: The Tiny-ImageNet dataset [19]</cell></row><row><cell>contains 200 image classes for training. Each class has 500</cell></row><row><cell>images for training and 50 images for test. All images are</cell></row><row><cell>RGB images of size 64×64. In the Tiny-ImageNet dataset,</cell></row><row><cell>we adopt the BigGAN architecture and train the BigGAN</cell></row><row><cell>model on all training images. We also adopt respectively</cell></row><row><cell>a customized ResNet-34 and a 5-layer MLP for feature</cell></row><row><cell>extraction and conditional density ratio estimation.</cell></row><row><cell>Since it is impractical to train 200 unconditional density</cell></row><row><cell>ratio models [8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Tiny-ImageNet dataset: Quality of one million</cell></row><row><cell cols="3">fake images (5000 per class) sampled from two sampling</cell></row><row><cell cols="2">methods on a trained BigGAN model.</cell><cell></cell></row><row><cell cols="2">Sampling methods Intra-FID</cell><cell>FID</cell></row><row><cell>no-subsampling</cell><cell>30.226</cell><cell>5.012</cell></row><row><cell>cDRE-F-cSP+RS</cell><cell>29.361</cell><cell>2.005</cell></row><row><cell>4.3 RC-49</cell><cell></cell><cell></cell></row></table><note>This experiment is conducted on RC-49 to show that the proposed subsampling scheme also works in CcGANs. Experimental setup: RC-49 is a benchmark dataset for CcGANs [7, 6]. It is made by rendering 49 3-D chair models individually. Each chair model is rendered at 899 yaw angles from 0.1 • to 89.9 • with a stepsize of 0.1 • . This dataset contains 44,051 RGB images of size 64 × 64 with corresponding yaw angles as labels. We follow the official implementation of CcGAN in [6] which adopts the SNGAN architecture [24]. On RC-49, we train three CcGAN models with SVDL+ILI by varying the number of training samples (i.e., 25, 15, 5) for each label. Thus we obtain three CcGAN models as SR-25, SR-15, and SR-5, respectively. To perform subsampling, we firstly train on RC-49 a sparse autoencoder with an extra branch for label prediction (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>RC-49 dataset: Quality of 179,800 fake images (200 per angle) sampled from two sampling methods on three trained GAN models.</figDesc><table><row><cell>CcGAN models</cell><cell>Sampling methods</cell><cell cols="3">Intra-FID NIQE Diversity</cell><cell>Label Score</cell></row><row><cell>SR-25</cell><cell>no-subsampling cDRE-F-cSP+RS</cell><cell>0.386 0.307</cell><cell>1.759 1.755</cell><cell>2.952 3.081</cell><cell>1.930 1.545</cell></row><row><cell>SR-15</cell><cell>no-subsampling cDRE-F-cSP+RS</cell><cell>0.395 0.309</cell><cell>1.779 1.794</cell><cell>2.939 3.052</cell><cell>2.020 1.559</cell></row><row><cell>SR-5</cell><cell>no-subsampling cDRE-F-cSP+RS</cell><cell>0.465 0.357</cell><cell>1.860 1.836</cell><cell>2.778 2.971</cell><cell>2.515 1.930</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>RC-49 dataset: Influence of the MAE quantile in the filtering algorithm (Alg. 3) on the performance of cDRE-F-cSP+RS in subsampling SR-25.</figDesc><table><row><cell>MAE quantile</cell><cell cols="4">Intra-FID NIQE Diversity Label Score</cell></row><row><cell>1.0</cell><cell>0.292</cell><cell>1.763</cell><cell>3.110</cell><cell>1.996</cell></row><row><cell>0.9</cell><cell>0.308</cell><cell>1.755</cell><cell>3.089</cell><cell>1.724</cell></row><row><cell>0.8</cell><cell>0.307</cell><cell>1.755</cell><cell>3.081</cell><cell>1.545</cell></row><row><cell>0.7</cell><cell>0.353</cell><cell>1.756</cell><cell>3.035</cell><cell>1.371</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">UTKFace dataset: Quality of 60,000 fake images</cell></row><row><cell cols="5">(1000 per age) sampled from different sampling methods</cell></row><row><cell cols="5">on the SU-200 CcGAN model. cDRE-F-cSP+RS only</cell></row><row><cell cols="5">requires 1.7% training costs of 60-DRE-F-SP+RS.</cell></row><row><cell>Subsampling methods</cell><cell cols="3">Intra-FID NIQE Diversity</cell><cell>Label Score</cell></row><row><cell>no-subsampling</cell><cell>0.432</cell><cell>1.725</cell><cell>1.304</cell><cell>7.444</cell></row><row><cell>60-DRE-F-SP+RS</cell><cell>0.423</cell><cell>1.722</cell><cell>1.298</cell><cell>7.300</cell></row><row><cell>cDRE-F-cSP+RS</cell><cell>0.424</cell><cell>1.737</cell><cell>1.335</cell><cell>6.763</cell></row><row><cell cols="2">5 Conclusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">This work presents a novel conditional subsampling</cell></row><row><cell cols="5">scheme to improve the image quality of fake images from</cell></row><row><cell cols="5">conditional GANs. First, we propose novel conditional</cell></row><row><cell cols="5">extensions of density ratio estimation (cDRE) in the fea-</cell></row><row><cell cols="5">ture space and the Softplus loss function (cSP). Then, we</cell></row><row><cell cols="5">learn the conditional density ratio model through an MLP</cell></row><row><cell cols="5">network. Also, we derive the error bound of a conditional</cell></row><row><cell cols="5">density ratio model trained with the proposed cSP loss.</cell></row><row><cell cols="5">A novel filtering scheme is also proposed in subsampling</cell></row><row><cell cols="5">CcGANs to improve the label consistency. Finally, we vali-</cell></row><row><cell cols="5">date the effectiveness of the proposed subsampling scheme</cell></row><row><cell cols="5">with extensive experiments in sampling multiple condi-</cell></row><row><cell cols="5">tional GAN models on four image datasets with diverse</cell></row><row><cell>evaluation metrics.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S</head><label>S</label><figDesc></figDesc><table><row><cell>.3.7: The 5-layer MLP for cDRE in feature space</cell></row><row><cell>for CIFAR-10. The embedded class label is appended to</cell></row><row><cell>the extracted feature h.</cell></row><row><cell>Input: extracted feature h ∈ R 3072 and embedded class label y ∈ R 10 Concatenate [h, y] ∈ R 3082</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S</head><label>S</label><figDesc></figDesc><table><row><cell>.4.8: 5-layer MLP for cDRE in feature space for</cell></row><row><cell>Tiny-ImageNet. The embedded class label is appended to</cell></row><row><cell>the extracted feature h.</cell></row></table><note>Input: extracted feature h ∈ R 12,288 and embedded class label y ∈ R 200 Concatenate [h, y] ∈ R 12,488</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>.4.8 (i.e., encoder branch), Table S.5.10 (i.e., decoder branch), and Table S.5.11 (i.e., label prediction branch). We adopt the SNGAN architecture for CcGAN which is also used by [6].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S</head><label>S</label><figDesc>.5.11: The architecture of the label prediction branch in the sparse autoencoder for 64 × 64 images. Input: extracted sparse features h ∈ R 4×4×768</figDesc><table><row><cell>from Table S.5.9.</cell></row><row><cell>fc→ 2048, BN, ReLU</cell></row><row><cell>fc→ 1024, BN, ReLU</cell></row><row><cell>fc→ 512, BN, ReLU</cell></row><row><cell>fc→ 128, BN, ReLU</cell></row><row><cell>fc→ 1, ReLU</cell></row><row><cell>Output: the predicted labelŷ</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Real Images (b) no-subsampling</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Real Images (b) no-subsampling (c) cDRE-F-cSP+RS Figure S.4.7: Example Tiny-ImageNet images for the "goldfish" class.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Real Images(b) no-subsampling (c) cDRE-F-cSP+RS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminator rejection sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06060</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the evaluation of conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drozdzal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08175</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Concentration of measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<publisher>Link</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><forename type="middle">Poczos</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collaborative sampling in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining GOLD samples for conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Some useful asymptotic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxia</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Link</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metropolis-Hastings generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6345" to="6353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The models for the computation of Intra-FID, NIQE, Diversity, and Label Score are consistent with those used by</title>
		<ptr target="https://github.com/UBCDingXin/improved_CcGANformoredetails" />
		<imprint/>
	</monogr>
	<note>7, 6]. Please refer to the official implementation of CcGANs at</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The architecture of the encoder in the sparse autoencoder for extracting features from 64 × 64 RGB images. In convolutional (Conv) operations, ch denotes the number of channels, k/s/p denote kernel size, stride and number of padding</title>
		<idno>Table S.5.9</idno>
	</analytic>
	<monogr>
		<title level="m">respectively. Input: an RGB image x ∈ R 64×64×3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BN, ReLU Conv (ch → 128, k4/s2/p1), BN, ReLU Conv (ch → 128, k3/s1/p1), BN, ReLU Conv (ch → 256, k4/s2/p1), BN, ReLU Conv (ch → 256, k3/s1/p1), BN, ReLU Conv (ch → 512, k4/s2/p1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">k3/s1/p1)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>ch → 64, k4/s2/p1. ch → 768, k3/s1/p1), ReLU Output: extracted sparse features h ∈ R 4×4×768</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The architecture of the decoder in the sparse autoencoder for reconstructing 64 × 64 input images from extracted features. In transposed-convolutional (ConvT) operations, ch denotes the number of channels, k/s/p denote kernel size, stride and number of padding, respectively</title>
		<idno>Table S.5.10</idno>
		<imprint/>
	</monogr>
	<note>Input: extracted sparse features h ∈ R 4×4×768 from Table S.5.9</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BN, ReLU ConvT (ch → 256, k4/s2/p1), BN, ReLU ConvT (ch → 256, k3/s1/p1), BN, ReLU ConvT (ch → 128, k4/s2/p1), BN, ReLU ConvT (ch → 128, k3/s1/p1), BN, ReLU ConvT (ch → 64, k4/s2/p1), BN, ReLU ConvT (ch → 3, k3/s1/p1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Convt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Convt</surname></persName>
		</author>
		<idno>k3/s1/p1</idno>
		<ptr target="c)60-DRE-F-SP+RS(d)cDRE-F-cSP+RS" />
	</analytic>
	<monogr>
		<title level="m">Tanh Output: a reconstructed image x ∈ R 64×64×3</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>ch → 512, k4/s2/p1</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">10: Example UTKFace images for ages from 3 to 57</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Figure</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
