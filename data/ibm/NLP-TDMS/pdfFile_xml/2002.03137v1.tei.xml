<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symbiotic Attention with Privileged Information for Egocentric Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
							<email>xiaohan.wang-3@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>linchao.zhu@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Symbiotic Attention with Privileged Information for Egocentric Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Egocentric video recognition is a natural testbed for diverse interaction reasoning. Due to the large action vocabulary in egocentric video datasets, recent studies usually utilize a twobranch structure for action recognition, i.e., one branch for verb classification and the other branch for noun classification. However, correlation studies between the verb and the noun branches have been largely ignored. Besides, the two branches fail to exploit local features due to the absence of a position-aware attention mechanism. In this paper, we propose a novel Symbiotic Attention framework leveraging Privileged information (SAP) for egocentric video recognition. Finer position-aware object detection features can facilitate the understanding of actor's interaction with the object. We introduce these features in action recognition and regard them as privileged information. Our framework enables mutual communication among the verb branch, the noun branch, and the privileged information. This communication process not only injects local details into global features but also exploits implicit guidance about the spatio-temporal position of an on-going action. We introduce novel symbiotic attention (SA) to enable effective communication. It first normalizes the detection guided features on one branch to underline the action-relevant information from the other branch. SA adaptively enhances the interactions among the three sources. To further catalyze this communication, spatial relations are uncovered for the selection of most action-relevant information. It identifies the most valuable and discriminative feature for classification. We validate the effectiveness of our SAP quantitatively and qualitatively. Notably, it achieves the state-ofthe-art on two large-scale egocentric video datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We have witnessed a significant progress in tackling many computer vision problems, e.g., image classification <ref type="bibr" target="#b10">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b6">He et al. 2016;</ref><ref type="bibr" target="#b8">Huang et al. 2017</ref><ref type="bibr">), detection (Girshick et al. 2014</ref><ref type="bibr" target="#b14">Ren et al. 2015;</ref>, segmentation <ref type="bibr" target="#b11">(Long, Shelhamer, and Darrell 2015;</ref><ref type="bibr" target="#b1">Chen et al. 2017;</ref>. In video analysis, with the emerging of  <ref type="figure">Figure 1</ref>: Three sources of information are leveraged. The VerbNet extracts motion information from raw videos, while background noise possibly degrades the recognition of target action. The NounNet recognizes the object in the scene. However, distracting objects interfere with accurate noun classification. Local position-aware object detection features serves as privileged information to enhance the communication between two branches.</p><p>deep convolutional neural networks and large-scale datasets, the action recognition performance has been prominently boosted <ref type="bibr" target="#b15">(Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b21">Wang et al. 2016;</ref><ref type="bibr" target="#b23">Xie et al. 2018;</ref><ref type="bibr" target="#b22">Wu et al. 2019b)</ref>. In typical action recognition datasets <ref type="bibr" target="#b0">(Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b4">Goyal et al. 2017)</ref>, the video duration is usually less than 10 seconds. These trimmed videos contain a single action that dominates the whole clip. As the background environment is not distracting, it is often not necessary to identify the interacting object. However, in many real-world applications, e.g., a robot navigates in the mall, the surroundings are noisy with multiple actors and objects. Human action recognition in videos has evolved from classifying a single action in a clear background to understanding complex human-object interactions in a highly distracting environment. To enable the recognition of more complex videos, a challenging large-scale first-person dataset, i.e., EPIC-Kitchens <ref type="bibr" target="#b2">(Damen et al. 2018)</ref>, was recently introduced for egocentric daily human activities understanding. This dataset provides rich interactions, covering adequate objects and natural actions. Compared to thirdperson action recognition, it requires to distinguish the object that human is interacting with from various small distracting objects. The intense camera motion, occlusion, and first-person viewpoint make it even more challenging to recognize fine actions.</p><p>In EPIC-Kitchens, due to the large action vocabulary, The verb and the noun classifiers are usually trained separately. The verb branch focuses on classifying verbs, e.g., put, open, that the actor is performing. Large camera motion and subtle occurring action positions are the main obstacles for verb classification. The noun branch is to classify the object the actor is interacting with. As shown in <ref type="figure">Figure 1</ref>, distracting objects in oblique view decrease the prediction score of the interacting object. The predictions from the two branches are usually merged without interactions for action classification. <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>) utilized 3D convolution neural networks (CNN) for the standalone verb and noun classification. They leverage object detection features for longer context modeling. However, the long-term feature bank is aggregated via a simple max pooling or average pooling operation, while the more sophisticated non-local operator is found not very effective. <ref type="bibr" target="#b0">(Baradel et al. 2018)</ref> introduced object relation network for high-level object reasoning, where the relation modeling facilitates object recognition. However, due to the existence of distracting objects, the learned object relatedness without guided supervision might be not useful to identify the object that human is interacting with. These works ignore the mutual communication between the standalone branches. Instead, they only focus on contextual modeling and relation reasoning on a single branch. Even for a human, it can be difficult to recognize an action by only looking at objects while ignoring the actor's intention, or only understanding motion changes without the awareness of the interacting object. To better exploit the mutual benefits of the interactions among different sources, we make the following contributions.</p><p>First, privileged local features are dynamically integrated to encourage the learning of action-relevant representations. For verb classification, the negative effect of background noise can be suppressed when target object information is effectively leveraged. However, noun representation loses finer spatial information. It fails to provide detailed position information to be exploited for the attendance of an ongoing action. Position-aware object detection features offer detailed local understanding of the objects. These features can serve as privileged information to possibly reduce the object-irrelevant motions. In noun classification, the introduced privileged information enhances the significance of correlated objects. The feature of action-relevant objects will be reinforced, thanks to the finer object presentations from the object detector.</p><p>Second, we propose symbiotic attention to enable mutual interactions among the three sources. The privileged information is first merged with the global feature from one branch. It is then normalized by a gated channel attention mechanism, which reweights the merged feature. This normalization process underlines the action-relevant information from the feature in the other branch. After this, each spatial position has integrated all three sources, i.e., the two branches information, and the privileged information. We leverage a spatial relation module to further catalyze the communication between the verb and the noun features. Most action-relevant information is identified to generate a discriminative feature for final classification. This symbiotic attention mechanism dynamically integrates three sources of information towards better action recognition.</p><p>The effectiveness of our SAP framework is validated quantitatively and qualitatively. We achieve the state-ofthe-art performance on two large-scale egocentric video datasets, i.e., EPIC-Kitchens and EGTEA <ref type="bibr" target="#b10">(Li, Liu, and Rehg 2018)</ref>. Notably, we outperform the state-of-the-art <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>) by 2.7% on EPIC-Kitchens test unseen set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Deep Video Recognition</head><p>Deep learning methods have achieved promising performance on the video classification task. (Simonyan and Zisserman 2014) proposed to utilize both RGB frames and optical flow as the 2D CNN input to modeling appearance and motion, respectively. TSN <ref type="bibr" target="#b21">(Wang et al. 2016</ref>) extended the two-stream CNN by extracting features from multiple temporal segments.  proposed a 3D CNN to learn the spatial-temporal information. I3D initializes 3D CNN with the inflated weights of 2D CNN. <ref type="bibr" target="#b5">(Hara, Kataoka, and Satoh 2018)</ref> evaluated various 3D CNN architectures on a large-scale video dataset and demonstrated the effectiveness of 3D models. More recently, <ref type="bibr" target="#b23">(Xie et al. 2018)</ref> and  proposed to decompose the 3D kernel to spatial and temporal convolution. Moreover, Recurrent Neural Networks (RNNs) are effective architectures for temporal modeling and have been found useful for video classification in <ref type="bibr" target="#b0">(Abu-El-Haija et al. 2016;</ref><ref type="bibr" target="#b24">Zhu, Xu, and Yang 2017)</ref>. These deep models are designed for third-person video recognition. They are able to capture motion and scene information but are not sufficient to locate various small objects in egocentric videos accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Person Action Recognition</head><p>Compared to third-person video recognition, egocentric action recognition is more dependent on the modeling of the hand-object interaction. <ref type="bibr" target="#b4">(Fathi, Farhadi, and Rehg 2011)</ref> proposed to learn a hierarchical model which exploits the consistent appearance of objects, hands, and actions and refines the object prediction based on action context. <ref type="bibr" target="#b12">(Ma, Fan, and Kitani 2016)</ref> utilized a hand segmentation net to locate the object of interest. After that, the cropped regions and optical flow images are fed to a two-stream CNN to learn the action and object representation jointly. <ref type="bibr" target="#b0">(Baradel et al. 2018)</ref> proposed to perform object-level visual reasoning about spatio-temporal interactions in videos through the integration of object detection networks. More recently, <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>) combined Long-Term Feature Banks that contains object-centric detection features with 3D CNN to improve the accuracy of object recognition. The attention mechanism is efficient to locate the region of interest on the feature map. <ref type="bibr" target="#b16">(Sudhakaran, Escalera, and Lanz 2019)</ref> proposed a Long Short-Term Attention model to focus on features from relevant spatial parts. They extended LSTM with a recurrent attention component and an output pooling component to track the discriminative area smoothly across the video sequence. <ref type="bibr" target="#b10">(Li, Liu, and Rehg 2018)</ref> proposed to generate attention map of the hand-object interaction by the guide of the gaze information. <ref type="bibr" target="#b9">(Kazakos et al. 2019</ref>) developed a egocentric action recognition model using three modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-Object Interaction</head><p>Reasoning the interaction between human and objects is relevant to our task. Most methods in this field are based on detection models. For example, (Gkioxari et al. 2018) predicted a density map to locate the interacted object and calculated the action score, with a modified Faster RCNN architecture. <ref type="bibr" target="#b13">(Qi et al. 2018)</ref> proposed Graph Parsing Neural Networks that incorporates structural knowledge and deep object detection model. <ref type="bibr" target="#b3">(Fang et al. 2018</ref>) developed a pairwise body-part attention model which can learn to focus on crucial parts for human-object interaction (HOI) recognition. Besides, some works use human-object interactions to help recognize actions. <ref type="bibr" target="#b20">(Wang and Gupta 2018)</ref> proposed to represent videos as space-time region graphs, which models shape dynamics and relationships between actors and objects. ) developed an Actor-Centric Relation Network for spatio-temporal action localization. Most of these HOI techniques rely on the appearance of the actors, which is absent in egocentric videos. Instead of the use of the detection features of humans, we pay attention to the interactions between the motion and the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method Overview</head><p>In this section, we illustrate our network architecture for egocentric video recognition. We develop three base networks to extract features from the input video: (1) VerbNet is a 3D CNN and takes a video clip as input. It is designed to capture the motion information.</p><p>(2) NounNet has the same architecture as VerbNet. It is trained to produce a feature representing object appearance. (3) Object detection model takes sampled individual frames as input. We use Faster R-CNN as our detector and utilize RoIAlign operation to obtain the local object features as privileged information. The output features of the three base models are fed to the subsequent SAP module. We aim to enable effective communication among VerbNet, NounNet, and Privileged Information. The SAP module generates two feature vectors which can be used to predict verb class and noun class. The overall framework is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>For each input egocentric video X = {x 1 , ..., x t } with t frames, its verb and noun label is y v and y n , respectively. The action y = (y v , y n ) is a combination of the verb and noun. We use two individual 3D CNNs as the backbones in our framework, with one for the verb feature extraction and the other for the noun feature extraction. The extracted verb feature f v ∈ R C contains the motion information, where C is the dimension of the extracted feature. Differently, the noun feature f n ∈ R C contains the global appearance information.</p><p>To enhance the global representation through the communication between two branches, we use a pre-trained detection model to provide detailed information of objects in the video. Considering the efficiency, for each video, we only use M sampled frames for detection inference. The output of the RoIAlign layer of the detection model is regarded as the feature for each detected object. To save memory usage and reduce the noisy information, we only keep top-K object features according to their confidence scores for each sampled frame. Thus, we have the auxiliary object feature matrix f o ∈ R N ×C , which contains N = M × K object features of the video. The verb feature f v , noun feature f n , and object feature matrix f o are interacted with each other to produce more discriminative features for action recognition with the following SAP module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbiotic Attention with Privileged Information</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, SAP includes three stages. First, the privileged information is integrated into the global feature from one branch. Second, the fused object-centric features are recalibrated by the other branch utilizing cross stream gating mechanism. After that, the normalized feature matrix is attended by the other branch to aggregate the most actionrelevant information within an action-attended relation module. Considering the symmetry of SAP, we only formulize the noun branch as an example in the following section.</p><p>Privileged Information Integration. The separated verb branch and noun branch produce two feature vectors f v and f n by global average pooling. The local information in these features is indistinct. We aim to leverage object feature matrix f o as privileged information to inject the local details into the global features. Moreover, the position-aware object features can also guide the model to attend salient area. Thus, we need to fuse f o with f v and f n , respectively. Besides, it's necessary to avoid jumbling the object feature matrix f o . To these ends, we perform a concatenation operation on the object feature matrix and the broadcasting global feature vectors. After that, we use a nonlinear transformation to enhance the fused feature. Formally, this operation can be presented as follow:</p><formula xml:id="formula_0">fn = ReLU(W n f f n + W o f f o + b f ),<label>(1)</label></formula><p>where W n f , W o f ∈ R C×C , b f ∈ R C and fn ∈ R N ×C . Each column in fn represent a object-centric feature, which integrates the global noun appearance with a explicit local object information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gate Weights Attention Weights</head><p>Attention Weights</p><formula xml:id="formula_1">N × C N × C N × C 1 × C 1 × C 1 × C 1 × C N × C N × C 1 × C 1 × C N × 1 N × 1 1 × C 1 × C Noun Feature Stream Verb Feature Stream Noun Cls Video Segment ⊗ ⊙ Element-wise Multiplication Matrix Multiplication 'Cut' 'Cucurbita' 'Cut' 'Cucurbita'</formula><p>Gate Weights</p><p>Verb Cls <ref type="figure">Figure 2</ref>: The proposed method. Our framework consists of three feature extractors and one interaction module SAP. VerbNet and NounNet produce global features. Detection Model generates a set of local object features as privileged information. The privileged information is integrated into the global features to obtain object-centric feature matrices. These feature matrices are normalized by a cross stream gating mechanism. After that, the object-centric matrices are attended by the other branch to select the most action-relevant information. The outputs of SAP are used to classify the verb and noun, respectively.</p><p>Cross Stream Gating. The fused object-centric feature matrix contains useful local details. However, due to the existence of inaccurate detection regions, there are quite a few disturbing background noises in the features. To address this problem, we propose a gated channel attention to underline the action relevant information. Furthermore, to enhance the interaction between the verb stream and noun stream, we utilize a cross gating mechanism. For an input noun feature matrix fn, we generate gating weights for it using the verb feature f v :</p><formula xml:id="formula_2">g n = Sigmoid(W n g f v + b g ),<label>(2)</label></formula><p>where W n g ∈ R C×C , b g ∈ R C and g n ∈ R C . The output is produced by rescaling the noun feature matrix with the gating weights:</p><formula xml:id="formula_3">f n g = g n fn,<label>(3)</label></formula><p>where f n g ∈ R N ×C and denotes the element-wise multiplication. After re-calibrating the object-centric noun feature by the verb feature, the action-unrelated noise can be suppressed. Moreover, the cross gating mechanism enables mutual communication between the two branches, which exploits the correlations of verbs and nouns adaptively.</p><p>Action-attended Relation Module The calibrated objectcentric feature matrix contains the action-relevant information and implicit guidance about the spatio-temporal position of an on-going action. To make full use of the information, we consider uncovering the relationships among the features. First, we propose to assess the relevance between the global feature and position-aware object-centric feature. Second, we sum the object-centric features weighted by the relevance coefficients. Specifically, we perform attention mechanism on the normalized object-centric noun features f n g and the original verb feature f v by f n a = Softmax(f n g f v )f n g ,</p><p>where the final noun feature f n a ∈ R C . Through the interaction of global feature and object-centric features, our model selects the most action-relevant feature for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Objectives</head><p>We use Faster R-CNN with ResNeXt-101-FPN backbone as our object detector. Following the training procedure in <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>), we first pre-train the detector on Visual Genome and then finetune it on EPIC-Kitchens object detection set. For VerbNet and NounNet, we adopt 3D Resnet-50 (Hara, Kataoka, and Satoh 2018) as our backbones. The two nets are both initialized with Kinetics pretrained weights. We first individually train the VerbNet and NounNet with the corresponding Cross-Entropy Loss: L v and L n . After the base training stage, we cascade our SAP module and fine-tune the entire model in an end-to-end manner. The objective for the fine-tuning is the sum of L v and L n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Re-weighting</head><p>The actions are determined by the pairs of verb and noun. The basic method of obtaining the action score is to calculate the multiplication of verb probability and noun probability. However, there are thousands of combinations and most verb-noun pairs that do not exist in reality, e.g. "open the knife". In fact, there are only 149 action classes that have more than 50 samples in EPIC-Kitchens dataset <ref type="bibr" target="#b2">(Damen et al. 2018)</ref>. Following the approach in <ref type="bibr" target="#b22">(Wu et al. 2019a)</ref>, we re-weight the final action probability by a prior, i.e. P (action = y) = µ(y v , y n )P (verb = y v )P (noun = y n ), (5) where µ is the occurrence frequency of action in training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We evaluate our method on two large-scale egocentric datasets: EPIC-Kitchens <ref type="bibr" target="#b2">(Damen et al. 2018</ref>) and EGTEA (Li, Liu, and Rehg 2018).</p><p>EPIC-Kitchens is the largest dataset in first-person vision so far. It consists of 55 hours of recordings capturing all daily activities in the kitchens. The activities performed are non-scripted, which makes the dataset very challenging and close to real-world data. The dataset contains 39,594 action segments which are annotated with 125 verb classes and 321 noun classes. We split the original training set to new training and validation set following <ref type="bibr" target="#b0">(Baradel et al. 2018)</ref>. We focus on the recognition task on EPIC-Kitchens, which is to predict the verb, noun, and the combination pair in each video segment.</p><p>EGTEA is a large-scale egocentric video dataset which consists of 10,321 video clips annotated with 19 verb classes, 51 noun classes, and 106 action classes. There are no bounding boxes annotations in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>We implement and test our method using Caffe2, PaddlePaddle and Pytorch. We observe similar performance from these deep learning frameworks. Specifically, we pretrain the base models (VerbNet and NounNet) individually and then finetune the entire model in an end-to-end manner. We find in our experiments that end-to-end training consistently yields better performance than two-stage training, which only updates the SAP model at the second stage. Next, we first illustrate the details on how to pre-train the backbones (Backbone details) and how to extract privileged information (Detection details). Finally, we show the details of the end-toend fine-tuning (SAP details).</p><p>Backbone details. We take the Kinetics pre-trained ResNet50-3D model as the initialization of our backbone model. We then train the backbone models (VerbNet and NounNet) individually on the target dataset. The input of the two models is the trimmed video clips with 64 frames. The targets for the VerbNet and NounNet are the verb label and noun label, respectively. Videos are decoded at 60 FPS for the EPIC-Kitchens dataset, and 24 FPS for the EGTEA dataset. We adopt the stochastic gradient descent (SGD) with momentum 0.9 and weight decay 0.0001 to optimize the parameters for 40 epochs. The overall learning rate is initialized to 0.003 and then changed to 0.0003 in the last 10 epochs. The batch size is 32. During training, the frame size is 224 × 224 pixels, randomly cropped from a random scaled video whose side is randomly sampled in <ref type="bibr">[224,</ref><ref type="bibr">288]</ref>. We sample 64 frames with stride=2 for both datasets. During testing, for each input video segment, we use the center clip and scale it to the size 256. Then we use the center cropped frames with size 224 as the input.</p><p>Detection details. Following <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>), we use the same Faster R-CNN to detect objects and extract object features. The detector is first pre-trained on Visual Genome <ref type="bibr" target="#b10">(Krishna et al. 2016)</ref> and then fine-tuned on the training split of the EPIC-Kitchens dataset. For fine-tuning, we use a batch size of 12 and train the model for 15k iterations. We use an initial learning rate of 0.005, which is decreased by a factor of 10 at iteration 116k and 133k. Finally, our object features are extracted using RoIAlign from the detector's feature maps. Considering the efficiency, for a video clip, we extract object features on the center window with a size of 6 seconds. The sample rate is two frames per second. For each frame, we keep the top five features according to the confidence scores. Therefore, we have 60 detection features for a video clip. For EGTEA, we use the detection model pretrained on EPIC-Kitchens to extract object features.</p><p>SAP details. With the pre-trained backbone models and the detection results, we fine-tune the backbone models with our SAP module in an end-to-end manner. We use the same SGD to optimize the parameters for 40 epochs with batchsize of 32. The learning rate is initialized to 0.0001 and then reduced to 0.00001 in the last 20 epochs. The rest training details are the same as the backbone details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art Results</head><p>We compare our model with the following state-of-the-art methods. TSN (Price and Damen 2019) is a two-stream model for video recognition. The performance is provided by the dataset authors. ORN (Baradel et al. 2018) introduces object relation reasoning upon detection features, while the interactions between verb and noun branches are largely ignored. LFB <ref type="bibr" target="#b22">(Wu et al. 2019a</ref>) combines Long-Term Feature Banks (detection features) with 3D CNN to improve the accuracy of object recognition. "LFB Max" denotes their best operation in EPIC-Kitchens, which leverages max pooling for feature bank aggregation. LSTA (Sudhakaran, Escalera, and Lanz 2019) is an attention-based method, they only report the top-1 action accuracy on the test set. <ref type="table">Table 1</ref> summarizes the top-1 and top-5 accuracy for verb, noun, and action predictions on the EPIC-Kitchens dataset.</p><p>Our model outperforms the state-of-the-art methods by a large margin on all three evaluation splits, i.e., the validation set, the test seen (S1) set and the test unseen (S2) set. On the validation set, compared to our baseline model ("Ours Baseline"), our SAP on the noun prediction significantly improves the top-1 accuracy from 23.8% to 35.0%. Compared to "LFB Max", which also utilizes the detection features, our method outperforms them by 3.2% at top-1 accuracy. For the verb prediction, our SAP obtains 1.3% (from 54.6% to 55.9%) top-1 accuracy improvement compared to our baseline model. For the final action classification, our method achieves 25.0% top-1 accuracy, which is higher than "LFB Max" by 2.2%. The significant improvement mainly benefits from the interactions between the verb branch, noun branch, and privileged information. Similar performance improve- ment is also observed on the test seen (S1) set and the test unseen (S2) set. For the final action prediction, Our SAP outperforms the state-of-the-art method "LFB Max" by 2.1% on S1 and 2.7% on S2. Althogh(Ghadiyaram, Tran, and Mahajan 2019) use much more videos (65M videos) and extreme deep 3D CNN to train the model, we still outperform their best model on noun prediction and action prediction on S1. The results on the EGTEA dataset is shown in <ref type="table" target="#tab_4">Table 3</ref>. The results of Two Stream, I3D and TSN, are provided by the dataset developer <ref type="bibr" target="#b10">(Li, Liu, and Rehg 2018)</ref>. Ego-RNN <ref type="bibr" target="#b16">(Sudhakaran and Lanz 2018)</ref> and LSTA <ref type="bibr" target="#b16">(Sudhakaran, Escalera, and Lanz 2019)</ref> utilize RNNs and attention mechanism for egocentric video recognition. Our method achieves higher accuracy on all splits than the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We conduct extensive ablation studies to evaluate the effectiveness of each component in our SAP model. <ref type="table" target="#tab_2">Table 2</ref> shows the noun prediction accuracy of several variants of our method. The first row "CNN Baseline" indicates we only use the noun branch, without the help of the verb and detection feature. "Noun + Verb" is the model that we take the verb feature as the gate to enhance the noun feature. Specifically, the implementation of the gating operation is the same as the single branch in CSG. "Avg Pooling" and "Max Pooling" take only the privileged information as input. We apply the corresponding pooling operation on the M * N detection features and use a fully connected layer to output the noun prediction. The last six rows show the effectiveness of  Importance of the privileged information. The first two rows in <ref type="table" target="#tab_2">Table 2</ref> shows the importance of the privileged information guidance. Without the detection feature, both the noun branch ("CNN Baseline") and the two branches with 4.3152e-01, 3.0828e-04, 3.9726e-01,   communication of noun and verb branches ("Noun + Verb") fail in the noun prediction. It is consistent with our motivation that the local information provided by the detection features is a critical clue in the noun prediction. In addition, the third row ("Avg Pooling") and the fourth row ("Max Pooling") also achieve comparable performance compared to the "CNN Baseline" model, which indicates the detection features contains rich information for noun prediction. Even without the input of the whole video clip, the local details from the detection model are enough for predicting the interacted object.</p><p>Importance of Cross Stream Gating. The performance comparison between the model "Ours w/o CSG &amp; ARM" and the model "Ours w/o ARM" validates the effectiveness of the CSG module. The CSG module enables mutual communication between the verb branch and the noun branch. Therefore, the CSG can improve the performance from 30.4% to 32.7%. Moreover, the results of "Ours w/o Cross Stream" and "Ours w/o Gating" demonstrate the two components both benefits the noun prediction.</p><p>Importance of Action-attended Relation Module. The last rows of <ref type="table" target="#tab_2">Table 2</ref> shows the improvement of the proposed ARM. ARM can select the most action-relevant information from the object-centric features and explore the relationships in the spatio-temporal context. Therefore, the ARM further improves the performance from 32.7% to 35.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we show some qualitative results on the EPIC-Kitchens dataset. The colored boxes in the figure indicate the top confident objects found by the pre-trained detection model. We do not use labels of detected objects since they are not accurate. Instead, we use the detection feature to guide the mutual communication of the verb and noun branch. The numbers below each image are the value of ARM attention weights for the five object-centric features. Taking the first one (the left-top one) as an example, the ground truth of this video clip is "chop parsley". The pretrained detection model generates five proposals. Our ARM module correctly finds the interacted area with "parsley" and generates a high value (0.78) that describes the contribution of the enhanced feature to the final noun classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel framework named Symbiotic Attention with Privileged Information for egocentric action recognition. We introduce a new attention mecha-nism called symbiotic attention that can interactively leverage sources from the verb branch, the noun branch, and the privileged information. Our experimental results demonstrate the effectiveness of our framework, and we outperform the state-of-the-art methods on large-scale egocentric video datasets. In the future, we will explore a hierarchical structure that can readily interpret the multi-step attention process. It is also promising to design new models to suppress background distractors directly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results of our SAP model. The colored boxes show the top-5 detected regions and the numbers are the corresponding attention weights generated by our action-attended relation module. Red indicates the failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: The comparison with the state-of-the-art methods on the EPIC-Kitchens dataset.</figDesc><table><row><cell>Method</cell><cell>Pre-training</cell><cell cols="6">Verbs top-1 top-5 top-1 top-5 top-1 top-5 Nouns Actions</cell></row><row><cell></cell><cell>Validation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORN (Baradel et al. 2018)</cell><cell>ImageNet</cell><cell>40.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>I3D GFA (Wang et al. 2019)</cell><cell>Kinetics+ImageNet</cell><cell>-</cell><cell>-</cell><cell>34.1</cell><cell>60.4</cell><cell>-</cell><cell>-</cell></row><row><cell>R(2+1)D 34 (Ghadiyaram, Tran, and Mahajan 2019)</cell><cell>Kinetics</cell><cell>46.8</cell><cell>79.2</cell><cell>25.6</cell><cell>47.5</cell><cell>15.3</cell><cell>29.4</cell></row><row><cell>LFB Max (Wu et al. 2019a)</cell><cell cols="2">Kinetics+ImageNet 52.6</cell><cell>81.2</cell><cell>31.8</cell><cell>56.8</cell><cell>22.8</cell><cell>41.1</cell></row><row><cell>Ours Baseline</cell><cell>Kinetics</cell><cell>54.6</cell><cell>80.9</cell><cell>23.8</cell><cell>45.1</cell><cell>19.5</cell><cell>36.0</cell></row><row><cell>Ours SAP</cell><cell>Kinetics</cell><cell>55.9</cell><cell>81.9</cell><cell>35.0</cell><cell>60.4</cell><cell>25.0</cell><cell>44.7</cell></row><row><cell></cell><cell>Test seen (S1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSN RGB (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>48.0</cell><cell>87.0</cell><cell>38.9</cell><cell cols="3">65.5 22.40 44.8</cell></row><row><cell>TSN Flow (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>51.7</cell><cell>84.6</cell><cell>26.8</cell><cell>50.6</cell><cell>16.8</cell><cell>33.8</cell></row><row><cell>TSN Fusion (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>54.7</cell><cell>87.2</cell><cell>40.1</cell><cell>65.8</cell><cell>25.4</cell><cell>45.7</cell></row><row><cell>R(2+1)D 34 (Ghadiyaram, Tran, and Mahajan 2019)</cell><cell>Kinetics</cell><cell>59.1</cell><cell>87.4</cell><cell>38.0</cell><cell>62.7</cell><cell>26.8</cell><cell>46.1</cell></row><row><cell>LSTA (Sudhakaran, Escalera, and Lanz 2019)</cell><cell>ImageNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.2</cell><cell>-</cell></row><row><cell>LFB Max (Wu et al. 2019a)</cell><cell cols="2">Kinetics+ImageNet 60.0</cell><cell>88.4</cell><cell>45.0</cell><cell>71.8</cell><cell>32.7</cell><cell>55.3</cell></row><row><cell>Ours SAP</cell><cell>Kinetics</cell><cell>63.2</cell><cell>86.1</cell><cell>48.3</cell><cell>71.5</cell><cell>34.8</cell><cell>55.9</cell></row><row><cell></cell><cell>Test Unseen (S2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TSN RGB (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>36.5</cell><cell>74.4</cell><cell>22.6</cell><cell>46.9</cell><cell>11.3</cell><cell>26.3</cell></row><row><cell>TSN Flow (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>47.4</cell><cell>77.0</cell><cell>21.2</cell><cell>42.5</cell><cell>13.5</cell><cell>27.5</cell></row><row><cell>TSN Fusion (Price and Damen 2019)</cell><cell>ImageNet</cell><cell>46.1</cell><cell>76.7</cell><cell>24.3</cell><cell>49.3</cell><cell>14.8</cell><cell>29.8</cell></row><row><cell>R(2+1)D 34 (Ghadiyaram, Tran, and Mahajan 2019)</cell><cell>Kinetics</cell><cell>48.4</cell><cell>77.2</cell><cell>26.6</cell><cell>50.4</cell><cell>16.8</cell><cell>31.2</cell></row><row><cell>LSTA (Sudhakaran, Escalera, and Lanz 2019)</cell><cell>ImageNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>15.9</cell><cell>-</cell></row><row><cell>LFB Max (Wu et al. 2019a)</cell><cell cols="2">Kinetics+ImageNet 50.9</cell><cell>77.6</cell><cell>31.5</cell><cell>57.8</cell><cell>21.2</cell><cell>39.4</cell></row><row><cell>Ours SAP</cell><cell>Kinetics</cell><cell>53.2</cell><cell>78.2</cell><cell>33.0</cell><cell>58.0</cell><cell>23.9</cell><cell>40.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation Study based on noun prediction on the EPIC-Kitchens validation set. "Pril" denotes the privileged information from finer object detection features, ARM denotes the action-attended relation module, CSG denotes the cross stream gating. our component CSG and ARM. Specifically, we decompose CSG to two part: Cross Stream and Gating. We also investigate the impact of each part. In the table, "Ours w/o Cross Stream" indicates using the same stream to gate and attend the object-centric matrix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The comparison with the state-of-the-art methods on the EGTEA dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abu-El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
	</analytic>
	<monogr>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>and fully connected crfs. T-PAMI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhadi</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<idno>Goyal et al. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kataoka</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kazakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename></persName>
		</author>
		<editor>NeurIPS. [Li, Liu, and</editor>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Hinton</pubPlace>
		</imprint>
	</monogr>
	<note>Imagenet classification with deep convolutional neural networks. In the eye of beholder: Joint learning of gaze and actions in first person video. In ECCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelhamer</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An evaluation of action recognition models on epic-kitchens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00867</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Going deeper into first-person activity recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ECCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Simonyan and Zisserman</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC. [Sudhakaran, Escalera, and Lanz</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and Gupta</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention matching for audio-visual event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ACCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
