<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-specific Adaptive Threshold for Face Recognition and Authentication</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Rung</forename><surname>Chou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-specific Adaptive Threshold for Face Recognition and Authentication</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many face recognition systems boost the performance using deep learning models, but only a few researches go into the mechanisms for dealing with online registration. Although we can obtain discriminative facial features through the state-of-the-art deep model training, how to decide the best threshold for practical use remains a challenge. We develop a technique of adaptive threshold mechanism to improve the recognition accuracy. We also design a face recognition system along with the registering procedure to handle online registration. Furthermore, we introduce a new evaluation protocol to better evaluate the performance of an algorithm for real-world scenarios. Under our proposed protocol, our method can achieve a 22% accuracy improvement on the LFW dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Convolutional Neural Networks (CNNs) have achieved great success for face recognition. Especially after the unified framework proposed by Schroff et al. <ref type="bibr" target="#b8">[9]</ref>. In most works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, researchers use a fixed threshold for the face verification tasks. The threshold is usually the optimal value that can separate different identities of the testing data. However, we argue that this method is deficient for the real-world scenarios, because the optimal threshold is usually case-specific, i.e. the best thresholds for different data sets are often different. As we do not have the testing data in practical applications, the optimal threshold is hard to find or even unobtainable. Furthermore, The content of a face database would change frequently, which also raise the need for threshold value tuning.</p><p>We propose the technique of feature-specific adaptive thresholding to improve the recognition accuracy. The adaptive threshold serves for two purposes: it performs the task of face verification, and it acts as a gatekeeper of the database. We also design a system to simulate the real-world scenario, which consists of a deep CNN and a database; we use the deep CNN to extract the embedded feature vector of a facial image and store it in the database along with the threshold and its identity.</p><p>To have a fair comparison with the results of using a fixed threshold, we introduce a new evaluation protocol that has the same registration flow as the proposed system. The experimental results show that our method outperforms the traditional one. It strengthens the robustness of the threshold and makes the selection process more tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, deep CNNs play an important role in the research of face recognition. These works attempt to obtain a better deep learning model by either modifying the objective functions or redesigning the network structures. Their general goal is to utilize the deep learning model to produce discriminative facial features. For example, FaceNet <ref type="bibr" target="#b8">[9]</ref> proposes triplet loss; NormFace <ref type="bibr" target="#b12">[13]</ref> optimizes cosine similarity directly; A-Softmax loss <ref type="bibr" target="#b5">[6]</ref> introduces the angular margin into the objective and AM-Softmax loss <ref type="bibr" target="#b11">[12]</ref> improves A-Softmax to stabilize the training; VGGFace <ref type="bibr" target="#b6">[7]</ref> combines VGG-net <ref type="bibr" target="#b9">[10]</ref> with triplet loss and achieves a great success on the LFW benchmark; Recently, Mobile-FaceNets [1] designs a lightweight face recognition model with Global Depthwise Convolution.</p><p>Nevertheless, it is still challenging to choose the optimal threshold for differentiating different identities. In the benchmark of LFW <ref type="bibr" target="#b3">[4]</ref>, a canonical list of face verification pairs is provided and researchers are asked to evaluate the algorithm via 10-fold cross validation (CV). The best threshold for L 2 -distance is then decided accordingly. The CV-based methods are sufficient for comparing the experimental results of different methods, but the thresholds selected usually do not suffice for practical applications. In this paper, instead of applying a fixed threshold to the entire database, we introduce an adaptive thresholding method that assigns a suitable threshold per each registered face in the database, and show that the approach performs more favorably on face recognition and authentication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We have two operations in our system: registration and recognition. In the operation of registration, a feature vector (or embedding) is extracted form an input face image by using a deep network. We assume that one face image is registered on the system at a time. The face could belong either to some person already registered on the system, or a new person not registered before. We assign a threshold to the registered face during each registration, and the thresholds of the other registered faces will be modified accordingly.</p><p>For recognition, given a query image, we extract its feature embedding and compute the similarity scores between it and all of the other stored embeddings. Then we use the similarity scores to determine the identity of the query image. The system structure is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In the following sections, we first review the procedure of extracting the facial features by using state-of-the-art face detection and recognition methods; then we describe the details of registering and recognizing an image with adaptive threshold in sections 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Convolutional Neural Network</head><p>Given a query image, we first utilize the Multi-task Cascaded Convolutional Networks (MTCNN) <ref type="bibr" target="#b13">[14]</ref> to detect and align the face. Next, we utilize a well-trained face recognition model trained on Inception-ResNet-v1 <ref type="bibr" target="#b10">[11]</ref> with a L 2 normalization layer as proposed by FaceNet <ref type="bibr" target="#b8">[9]</ref>. In the inference phase, we extract the output of the L 2 normalization layer as the unified facial feature embedding. For the following section, we use embedding as a shorthand for the extracted facial features as introduced by FaceNet. As the embeddings are L 2 −normalized, we use the inner product between two embeddings to compute their similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Registration with Adaptive Threshold</head><p>Given a sequence of face images I = {I 1 · · · I t · · · I T }, the identity labels P = {P 1 · · · P t · · · P T } and the embed-</p><formula xml:id="formula_0">dings F = {F 1 · · · F t · · · F T } extracted by the deep CNN, ! " ! # ! $ A B C Name</formula><p>Embedding Threshold we register the embeddings into the database one-by-one as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. At each registration t, we insert the feature embedding F t and its identity P t into the database; then we update the threshold σ τ t accordingly. σ τ t denotes the threshold of registered feature embedding F τ when F t is registering into the database (τ = 1, · · · , t).</p><formula xml:id="formula_1">% &amp; " = max(% &amp;," " , .(/, 1)) ! 2 C ! " ! # Register feature ! &amp; 3 &amp; = 4 .(/, 1) 5 (1)Extract &amp; Compare (3)Update threshold (2)Insert into database Dot product % &amp; # = max(% &amp;,</formula><p>We assign a different threshold for each facial embedding in the database. For threshold σ τ t (associated with the τ -th image at the t-th time), we first compute the similarity score between the embeddings F τ and F v in the database</p><formula xml:id="formula_2">(v = 1 · · · t): S(τ, v) = F τ · F v<label>(1)</label></formula><p>Then we compute σ τ t as the maximum value among all facial embeddings not belonging to the same person at the current time:</p><formula xml:id="formula_3">σ τ t = max(S(τ, v)), v = 1 · · · t, where P τ = P v (2)</formula><p>For the implementation of updating the thresholds, as the face images are registered one at a time, we can take advantage of the recursion of σ τ t−1 and σ τ t for an efficient computation during registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recognition and authentication</head><p>Given a query facial image I λ without its identity label, we first extract the embedding F λ by the deep CNN. Then we compute the similarity scores with all the embeddings that are already stored in the database as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We extract the one which has the highest similarity score with F λ and denote its index as u:</p><formula xml:id="formula_4">u = arg max v (S(λ, v)), for v = 1 · · · t<label>(3)</label></formula><p>Once the most similar embedding F u is found, the system compare the associated threshold σ u t with the similarity score S(λ, u). If S(λ, u) ≥ σ u t , then the image I λ will be  classified as identity P u ; else it dissociates from any registered identities, in this case, we call it an intruder and reject the authentication request:</p><formula xml:id="formula_5">P λ = P u , if S(λ, u) ≥ σ u t intruder, if S(λ, u) &lt; σ u t (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and Experiment</head><p>Current open-set identification protocol such as <ref type="bibr" target="#b4">[5]</ref> evaluates the algorithm under determinate probe (or query) and gallery (or database) sets. This setting standardizes the evaluation and makes the result measurable, but it does not consider the intruders nor changes in the gallery, which often occurs in industrial applications.</p><p>We introduce timeline in our evaluation protocol. At tth time, we present an image to the system and check the correctness; we move the face image from the probe to the gallery set one at a time to simulate the real registration rocess and the changes in the gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation protocol for singly registering and recognizing</head><p>In our evaluation protocol, we have T images in the testing data, the gallery begins with an empty set. When a probe feature embedding F t is presented to the system, it will be used to calculate the similarity scores with all the gallery feature embeddings F τ , τ = 1 . . . t. The gallery feature embedding that gets the highest similarity score with F t is denoted as F u . The threshold function is denoted as Φ(t, u). For fixed threshold, Φ(t, u) always returns a constant value; for adaptive threshold, Φ(t, u) = σ u t . We use 10-fold CV to compute the fixed threshold can compare the results with those that obtained using our adative thresholding method. When F t is presented to the system, if S(t, u) ≥ Φ(t, u), and P t = P u , then we define this case as true accept:</p><formula xml:id="formula_6">TA(t) = {S(t, u) ≥ Φ(t, u), and P t = P u }<label>(5)</label></formula><p>If P t is an identity in the gallery but S(t, u) &lt; Φ(t, u), then no matter whether P t = P u , we define these cases as false reject:</p><formula xml:id="formula_7">FR(t) = {S(t, u) &lt; Φ(t, u), and P t ∈ P}<label>(6)</label></formula><p>If P t is not contained in the gallery, then we call it an intruder. Thus, we define false accept and true reject as:</p><formula xml:id="formula_8">FA(t) = {S(t, u) ≥ Φ(t, u), and P t / ∈ P}<label>(7)</label></formula><formula xml:id="formula_9">TR(t) = {S(t, u) &lt; Φ(t, u), and P t / ∈ P}<label>(8)</label></formula><p>For the last possible case, if S(t, u) ≥ Φ(t, u) and P t is someone in the gallery, but P t and P u are different identities, then we define this case as identification error:</p><formula xml:id="formula_10">IE(t) = {S(t, u) ≥ Φ(t, u), where P t = P u , and P t ∈ P}<label>(9)</label></formula><p>We add F t to the gallery set iteratively to simulate the registration operation (t = 1 · · · T ); we extract F t+1 from the probe set and repeat the process until all of the T images are examined. The final accuracy, ACC, is then defined as the average correctness of all the T sessions: </p><formula xml:id="formula_11">ACC = T t=1 |TA(t)| + |TR(t)| T<label>(10)</label></formula><formula xml:id="formula_12">S(t, u) ≥ Φ(t, u) S(t, u) &lt; Φ(t, u) P t = P u True accept False reject P t = P u , Identification error False reject P t ∈ P P t = P u , False accept True reject P t / ∈ P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Facial Datasets</head><p>In all our experiments we use the same deep CNN trained on MS-Celeb-1M <ref type="bibr" target="#b2">[3]</ref> dataset to extract the feature embedding. We evaluate our algorithm under the aforementioned protocol on three datasets: Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b3">[4]</ref>, Adience <ref type="bibr" target="#b1">[2]</ref> and Color FERET <ref type="bibr" target="#b7">[8]</ref>. As some faces in the images are undetectable, we did not use all images in the datasets. The statistics of the images used in our experiments are show in <ref type="table" target="#tab_1">Table 2</ref>. For each experiment, we randomly shuffle all of the images in the dataset, and then register the images one-by-one according to the random order. We perform the experiments 10 times on each dataset and average the accuracy. To fairly compare with the results of using a fixed threshold, we also conduct the fixed-thresholding experiments under our evaluation protocol with the same registration order.</p><p>As for the selection of fixed thresholds, following the verification benchmark of LFW, we use 6,000 image pairs randomly generated from the dataset. As done in the protocol of LFW for the verification performance evaluation, 10-fold CV is used and a threshold is selected for each foldsplitting. The 10 thresholds selected are then averaged to yield the fixed threshold used in our experiment. The same procedure is performed for the fixed thresholds selection of the color FERET and Adience datasets too. After that, the fixed thresholds also serve as the initial values (t = 1) of our adaptive thresholding procedure.</p><p>The accuracy (ACC defined in Eq. 10) is shown in Table 3. In the case of fewer samples per identity like LFW, the adaptive-threshold approach outperforms the fixedthreshold method by around 22.5% in accuracy. In Adience or Color FERET, the adaptive-threshold approach still consistently outperforms the fixed one. The temporary accuracy during each registration is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The adaptive-threshold approach converges earlier, showing it's promising in sample limited applications, e.g., small factory security systems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>System Structure. It consists of a deep CNN with a L 2 normalization layer, and a database for storing feature embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Registration Flowchart. F t is the embedding of a registering image with identity C. (1) Compute the similarity scores of F t and all embeddings other than C. (2) Store F t and its name in the database. (3) Update the thresholds of all the embeddings accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Recognizing Flowchart. F λ is the embedding of a query image I λ . (1) Compute the similarity score of F λ with all embeddings. (2) Get the maximal similarity score. (3) Compare the score with the stored threshold to determine whether the query is an intruder or a registered identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons of temporary accuracy during each registration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Summary of the evaluation protocol</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . The number of aligned images, identi- ties, and number of samples per identity (with the standard deviation) in facial datasets.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">#images #classes #images/class</cell></row><row><cell>LFW</cell><cell>13,233</cell><cell>5,749</cell><cell>2.3 ± 9.01</cell></row><row><cell>Adience</cell><cell>19,339</cell><cell cols="2">2,284 8.46 ± 23.13</cell></row><row><cell>Color FERET</cell><cell>11,285</cell><cell>994</cell><cell>11.35 ± 8.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Final Accuracy Adaptive Fixed / Threshold LFW 76.46% 53.97%/0.3779 Adience 84.30% 80.60%/0.2487 Color FERET 83.79% 80.72%/0.3968</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We solve the long-term threshold-selection problem for face recognition and authentication, and also tackle the deficiency of current evaluation protocol. The introduced technique of adaptive thresholding can decide more favorable thresholds. We also design an on-line registration system for real-world scenarios, which can handle varied conditions for practical applications.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCBR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
