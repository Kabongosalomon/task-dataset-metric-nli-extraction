<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Compose Task-Specific Tree Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
							<email>jhchoi@europa.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Kang</surname></persName>
							<email>kangminyoo@europa.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Yoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Compose Task-Specific Tree Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For years, recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of RvNNs is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel tree-structured long short-term memory architecture that learns how to compose task-specific tree structures only from plain text data efficiently. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Techniques for mapping natural language into vector space have received a lot of attention, due to their capability of representing ambiguous semantics of natural language using dense vectors. Among them, methods of learning representations of words, e.g. word2vec <ref type="bibr" target="#b31">(Mikolov et al. 2013)</ref> or GloVe , are relatively well-studied empirically and theoretically <ref type="bibr" target="#b0">(Baroni, Dinu, and Kruszewski 2014;</ref><ref type="bibr" target="#b25">Levy and Goldberg 2014)</ref>, and some of them became typical choices to consider when initializing word representations for better performance at downstream tasks.</p><p>Meanwhile, research on sentence representation is still in active progress, and accordingly various architecturesdesigned with different intuition and tailored for different tasks-are being proposed. In the midst of them, three architectures are most frequently used in obtaining sentence representation from words. Convolutional neural networks (CNNs) <ref type="bibr" target="#b21">(Kim 2014;</ref><ref type="bibr" target="#b18">Kalchbrenner, Grefenstette, and Blunsom 2014)</ref> utilize local distribution of words to encode sentences, similar to n-gram models. Recurrent neural networks (RNNs) <ref type="bibr" target="#b9">(Dai and Le 2015;</ref><ref type="bibr" target="#b23">Kiros et al. 2015;</ref><ref type="bibr" target="#b13">Hill, Cho, and Korhonen 2016)</ref> encode sentences by reading words in sequential order. Recursive neural networks Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>(RvNNs 1 ) <ref type="bibr" target="#b40">(Socher et al. 2013;</ref><ref type="bibr" target="#b16">Irsoy and Cardie 2014;</ref><ref type="bibr" target="#b3">Bowman et al. 2016</ref>), on which this paper focuses, rely on structured input (e.g. parse tree) to encode sentences, based on the intuition that there is significant semantics in the hierarchical structure of words. It is also notable that RvNNs are generalization of RNNs, as linear chain structures on which RNNs operate are equivalent to left-or right-skewed trees.</p><p>Although there is significant benefit in processing a sentence in a tree-structured recursive manner, data annotated with parse trees could be expensive to prepare and hard to be computed in batches <ref type="bibr" target="#b3">(Bowman et al. 2016</ref>). Furthermore, the optimal hierarchical composition of words might differ depending on the properties of a task.</p><p>In this paper, we propose Gumbel Tree-LSTM, which is a novel RvNN architecture that does not require structured data and learns to compose task-specific tree structures without explicit guidance. Our Gumbel Tree-LSTM model is based on tree-structured long short-term memory (Tree-LSTM) architecture <ref type="bibr" target="#b42">(Tai, Socher, and Manning 2015;</ref><ref type="bibr" target="#b49">Zhu, Sobihani, and Guo 2015)</ref>, which is one of the most renowned variants of RvNN.</p><p>To learn how to compose task-specific tree structures without depending on structured input, our model introduces composition query vector that measures validity of a composition. Using validity scores computed by the composition query vector, our model recursively selects compositions until only a single representation remains. We use Straight-Through (ST) Gumbel-Softmax estimator <ref type="bibr" target="#b17">(Jang, Gu, and Poole 2017;</ref><ref type="bibr" target="#b27">Maddison, Mnih, and Teh 2017)</ref> to sample compositions in the training phase. ST Gumbel-Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass, thus our model can be trained via the standard backpropagation. Also, since the computation is performed layer-wise, our model is easy to implement and naturally supports batched computation.</p><p>From experiments on natural language inference and sentiment analysis tasks, we find that our proposed model outperforms or is at least comparable to previous sentence encoder models and converges significantly faster than them.</p><p>The contributions of our work are as follows:</p><p>• We designed a novel sentence encoder architecture that learns to compose task-specific trees from plain text data.</p><p>• We showed from experiments that the proposed architecture outperforms or is competitive to state-of-the-art models. We also observed that our model converges faster than others.</p><p>• Specifically, we saw that our model significantly outperforms previous RvNN works trained on parse trees in all conducted experiments, from which we hypothesize that syntactic parse tree may not be the best structure for every task and the optimal structure could differ per task.</p><p>In the next section, we briefly introduce previous works which have similar objectives to that of our work. Then we describe the proposed model in detail and present findings from experiments. Lastly we summarize the overall content and discuss future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There have been several works that aim to learn hierarchical latent structure of text by recursively composing words into sentence representation. Some of them carry unsupervised learning on structures by making composition operations soft. To the best of our knowledge, gated recursive convolutional neural network (grConv) <ref type="bibr" target="#b6">(Cho et al. 2014</ref>) is the first model of its kind and used as an encoder for neural machine translation. The grConv architecture uses gating mechanism to control the information flow from children to parent. grConv and its variants are also applied to sentence classification tasks <ref type="bibr" target="#b4">(Chen et al. 2015;</ref><ref type="bibr" target="#b48">Zhao, Lu, and Poupart 2015)</ref>. Neural tree indexer (NTI) (Munkhdalai and Yu 2017b) utilizes soft hierarchical structures by using Tree-LSTM instead of grConv.</p><p>Although models that operate with soft structures are naturally capable of being trained via backpropagation, the structures predicted by them are ambiguous and thus it is hard to interpret them. CYK Tree-LSTM <ref type="bibr" target="#b29">(Maillard, Clark, and Yogatama 2017)</ref> resolves this ambiguity while maintaining the soft property by introducing the concept of CYK parsing algorithm <ref type="bibr" target="#b19">(Kasami 1965;</ref><ref type="bibr" target="#b45">Younger 1967;</ref><ref type="bibr" target="#b8">Cocke 1970</ref>). Though their model reduces the ambiguity by explicitly representing a node as a weighted sum of all candidate compositions, it is memory intensive since the number of candidates linearly increases by depth.</p><p>On the other hand, there exist some previous works that maintain the discreteness of tree composition processes, instead of relying on the soft hierarchical structure. The architecture proposed by <ref type="bibr" target="#b39">Socher et al. (2011)</ref> greedily selects two adjacent nodes whose reconstruction error is the smallest and merges them into the parent. In their work, rather than directly optimized on classification loss, a composition function is optimized to minimize reconstruction error. <ref type="bibr" target="#b44">Yogatama et al. (2017)</ref> introduce reinforcement learning to achieve the desired effect of discretization. They show that REINFORCE <ref type="bibr" target="#b43">(Williams 1992)</ref> algorithm can be used in estimating gradients to learn a tree composition function minimizing classification error. However, slow convergence due to the reinforcement learning setting is one of its drawbacks, according to the authors.</p><p>In the research area outside the RvNN, compositionality in vector space also has been a longstanding subject <ref type="bibr" target="#b37">(Plate 1995;</ref><ref type="bibr" target="#b32">Mitchell and Lapata 2010;</ref><ref type="bibr" target="#b11">Grefenstette and Sadrzadeh 2011;</ref><ref type="bibr" target="#b46">Zanzotto and</ref> Dell'Arciprete 2012, to name a few). And more recently, there exist works aiming to learn hierarchical latent structure from unstructured data <ref type="bibr" target="#b7">(Chung, Ahn, and Bengio 2017;</ref><ref type="bibr" target="#b20">Kim et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Description</head><p>Our proposed architecture is built based on the treestructured long short-term memory network architecture. We introduce several additional components into the Tree-LSTM architecture to allow the model to dynamically compose tree structure in a bottom-up manner and to effectively encode a sentence into a vector. In this section, we describe the components of our model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree-LSTM</head><p>Tree-structured long short-term memory network (Tree-LSTM) <ref type="bibr" target="#b42">(Tai, Socher, and Manning 2015;</ref><ref type="bibr" target="#b49">Zhu, Sobihani, and Guo 2015)</ref> is an elegant variant of RvNN, where it controls information flow from children to parent using similar mechanism to long short-term memory (LSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber 1997)</ref>. Tree-LSTM introduces cell state in computing parent representation, which assists each cell to capture distant vertical dependencies.</p><p>The following are formulae that our model uses to compute parent representation from its children:</p><formula xml:id="formula_0">     i f l f r o g      =      σ σ σ σ tanh      W comp h l h r + b comp (1) c p = f l c l + f r c r + i g (2) h p = o tanh(c p ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_1">W comp ∈ R 5D h ×2D h b comp ∈ R 2D h ,</formula><p>and is the element-wise product. Note that our formulation is akin to that of SPINN <ref type="bibr" target="#b3">(Bowman et al. 2016</ref>), but our version does not include the tracking LSTM. Instead, our model can apply an LSTM to leaf nodes, which we will soon describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gumbel-Softmax</head><p>Gumbel-Softmax (Jang, Gu, and Poole 2017) (or Concrete distribution <ref type="bibr" target="#b27">(Maddison, Mnih, and Teh 2017)</ref>) is a method of utilizing discrete random variables in a network. Since it approximates one-hot vectors sampled from a categorical distribution by making them continuous, gradients of model parameters can be calculated using the reparameterization trick and the standard backpropagation. Gumbel-Softmax is known to have an advantage over score-functionbased gradient estimators such as REINFORCE <ref type="bibr" target="#b43">(Williams 1992)</ref> which suffer from high variance and slow convergence <ref type="bibr" target="#b17">(Jang, Gu, and Poole 2017)</ref>. Gumbel-Softmax distribution is motivated by Gumbel-Max trick <ref type="bibr" target="#b28">(Maddison, Tarlow, and Minka 2014)</ref>, an algorithm for sampling from a categorical distribution. Consider</p><formula xml:id="formula_2">s 1 s 2 · · · s k Gumbel-Softmax argmax t (a) Forward s 1 s 2 · · · s k Gumbel-Softmax t (b) Backward</formula><p>Figure 1: Visualization of forward and backward computation path of ST Gumbel-Softmax. In the forward pass, a model can maintain sparseness due to arg max operation. In the backward pass, since there is no discrete operation, the error signal can backpropagate.</p><p>a k-dimensional categorical distribution whose class probabilities p 1 , · · · , p k are defined in terms of unnormalized log probabilities π 1 , · · · , π k :</p><formula xml:id="formula_3">p i = exp(log(π i )) k j=1 exp(log(π j ))</formula><p>.</p><p>(4)</p><p>Then a one-hot sample z = (z 1 , · · · , z k ) ∈ R k from the distribution can be easily drawn by the following equations:</p><formula xml:id="formula_4">z i = 1 i = arg max j (log(π j ) + g j ) 0 otherwise (5) g i = − log(− log(u i )) (6) u i ∼ Uniform(0, 1).<label>(7)</label></formula><p>Here, g i , namely Gumbel noise, perturbs each log(π i ) term so that taking arg max becomes equivalent to drawing a sample weighted on p 1 , · · · , p k . In Gumbel-Softmax, the discontinuous arg max function of Gumbel-Max trick is replaced by the differentiable softmax function. That is, given unnormalized probabilities π 1 , · · · , π k , a sample y = (y 1 , · · · , y k ) from the Gumbel-Softmax distribution is drawn by</p><formula xml:id="formula_5">y i = exp((log(π i ) + g i )/τ ) k j=1 exp((log(π j ) + g j )/τ ) ,<label>(8)</label></formula><p>where τ is a temperature parameter; as τ diminishes to zero, a sample from the Gumbel-Softmax distribution becomes cold and resembles the one-hot sample. Straight-Through (ST) Gumbel-Softmax estimator <ref type="bibr" target="#b17">(Jang, Gu, and Poole 2017)</ref>, whose name reminds of Straight-Through estimator (STE) <ref type="bibr" target="#b1">(Bengio, Léonard, and Courville 2013)</ref>, is a discrete version of the continuous Gumbel-Softmax estimator. Similar to the STE, it maintains sparsity by taking different paths in the forward and backward propagation. Obviously ST estimators are biased, however they perform well in practice, according to several previous works <ref type="bibr" target="#b7">(Chung, Ahn, and Bengio 2017;</ref><ref type="bibr" target="#b12">Gu, Im, and Li 2017)</ref> and our own result.</p><p>In the forward pass, it discretizes a continuous probability vector y sampled from the Gumbel-Softmax distribution into the one-hot vector y ST = (y ST 1 , · · · , y ST k ), where</p><formula xml:id="formula_6">y ST i = 1 i = arg max j y j 0 otherwise .<label>(9)</label></formula><p>And in the backward pass it simply uses the continuous y, thus the error signal is still able to backpropagate. See <ref type="figure">Figure  1</ref> for the visualization of the forward and backward pass. ST Gumbel-Softmax estimator is useful when a model needs to utilize discrete values directly, for example in the case that a model alters its computation path based on samples drawn from a categorical distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gumbel Tree-LSTM</head><p>In our Gumbel Tree-LSTM model, an input sentence composed of N words is represented as a sequence of word vectors (x 1 , · · · , x N ), where x i ∈ R Dx . Our basic model applies an affine transformation to each x i to obtain the initial hidden and cell state:</p><formula xml:id="formula_7">r 1 i = h 1 i c 1 i = W leaf x i + b leaf ,<label>(10)</label></formula><p>which we call leaf transformation. In Eq. 10, W leaf ∈ R 2D h ×Dx and b leaf ∈ R 2D h . Note that we denote the representation of i-th node at t-th layer as r t i = h t i ; c t i . Assume that t-th layer consists of M t node representations: (r t 1 , · · · , r t Mt ). If two adjacent nodes, say r t i and r t i+1 , are selected to be merged, then Eqs. 1-3 are applied by assuming [h l ; c l ] = r t i and [h r ; c r ] = r t i+1 to obtain the parent representation [h p ; c p ] = r t+1 i . Node representations which are not selected are copied to the corresponding positions at layer t+1. In other words, the (t+1)-th layer is composed of</p><formula xml:id="formula_8">M t+1 = M t − 1 representations (r t+1 1 , · · · , r t Mt+1 ), where r t+1 j =    r t j j &lt; i Tree-LSTM r t j , r t j+1 j = i r t j+1 j &gt; i .<label>(11)</label></formula><p>This procedure is repeated until the model reaches N -th layer and only a single node is left. It is notable that the property of selecting the best node pair at each stage resembles that of easy-first parsing <ref type="bibr" target="#b10">(Goldberg and Elhadad 2010)</ref>. For implementation-wise details, please see the supplementary material. . Then the validity score of each candidate is computed using the query vector q (denoted as v 1 , v 2 , v 3 ). In the training time, the model samples a parent node among candidates weighted on v 1 , v 2 , v 3 , using ST Gumbel-Softmax estimator, and in the testing time the model selects the candidate with the highest validity. At layer t + 1 (the top layer), the representation of the selected candidate ('the cat') is used as a parent, and the rest are copied from those of layer t ('sat', 'on'). Best viewed in color.</p><p>Parent selection. Since information about the tree structure of an input is not given to the model, a special mechanism is needed for the model to learn to compose taskspecific tree structures in an end-to-end manner. We now describe the mechanism for building up the tree structure from an unstructured sentence. First, our model introduces the trainable composition query vector q ∈ R D h . The composition query vector measures how valid a representation is. Specifically, the validity score of a representation r = [h; c] is defined by q · h.</p><p>At layer t, the model computes candidates for the parent representations using Eqs. 1-3: (r t+1 1 , · · · ,r t+1 Mt+1 ). Then, it calculates the validity score of each candidate and normalize it so that</p><formula xml:id="formula_9">Mt+1 i=1 v i = 1: v i = exp(q ·h t+1 i ) Mt+1 j=1 exp(q ·h t+1 j ) .<label>(12)</label></formula><p>In the training phase, the model samples a parent from candidates weighted on v i , using the ST Gumbel-Softmax estimator described above. Since the continuous Gumbel-Softmax function is used in the backward pass, the error backpropagation signal safely passes through the sampling operation, hence the model is able to learn to construct the task-specific tree structures that minimize the loss by backpropagation.</p><p>In the validation (or testing) phase, the model simply selects the parent which maximizes the validity score.</p><p>An example of the parent selection is depicted in <ref type="figure" target="#fig_0">Figure  2</ref>.</p><p>LSTM-based leaf transformation. The basic leaf transformation using an affine transformation (Eq. 10) does not consider information about the entire sentence of an input and thus the parent selection is performed based only on local information.</p><p>SPINN <ref type="bibr" target="#b3">(Bowman et al. 2016</ref>) addresses this issue by using the tracking LSTM which sequentially reads input words. The tracking LSTM makes the SPINN model hybrid, where the model takes advantage of both tree-structured composition and sequential reading. However, the tracking LSTM is not applicable to our model, since our model does not use shift-reduce parsing or maintain a stack. In the tracking LSTM's stead, our model applies an LSTM on input representations to give information about previous words to each leaf node:</p><formula xml:id="formula_10">r 1 i = h 1 i c 1 i = LSTM(x i , h 1 i−1 , c 1 i−1 ),<label>(13)</label></formula><p>where h 1 0 = c 1 0 = 0. From the experimental results, we validate that the LSTM applied to leaf nodes has a substantial gain over the basic leaf transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate performance of the proposed Gumbel Tree-LSTM model on two tasks: natural language inference and sentiment analysis. The implementation is made publicly available. <ref type="bibr">2</ref> The detailed experimental settings are described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Language Inference</head><p>Natural language inference (NLI) is a task of predicting the relationship between two sentences (hypothesis  and premise). In the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b2">(Bowman et al. 2015)</ref>, which we use for NLI experiments, a relationship is either contradiction, entailment, or neutral. For a model to correctly predict the relationship between two sentences, it should encode semantics of sentences accurately, thus the task has been used as one of standard tasks for evaluating the quality of sentence representations. The SNLI dataset is composed of about 550,000 sentences, each of which is binary-parsed. However, since our model operate on plain text, we do not use the parse tree information in both training and testing. The classifier architecture used in our SNLI experiments follows <ref type="bibr" target="#b33">(Mou et al. 2016;</ref><ref type="bibr" target="#b5">Chen et al. 2017)</ref>. Given the premise sentence vector (h pre ) and the hypothesis sentence vector (h hyp ) which are encoded by the proposed Gumbel Tree-LSTM model, the probability of relationship r ∈ {entailment, contradiction, neutral} is computed by the following equations:</p><formula xml:id="formula_11">p(r|h pre , h hyp ) = softmax(W r clf a + b r clf ) (14) a = Φ(f ) (15) f =    h pre h hyp h pre − h hyp h pre h hyp    ,<label>(16)</label></formula><p>where W r clf ∈ R 1×Dc , b r clf ∈ R 1 , and Φ is a multi-layer perceptron (MLP) with the rectified linear unit (ReLU) activation function.</p><p>For 100D experiments (where D x = D h = 100), we use a single-hidden layer MLP with 200 hidden units (i.e. D c = 200. The word vectors are initialized with GloVe  100D pretrained vectors 3 and fine-tuned during training.</p><p>For 300D experiments (where D x = D h = 300), we set the number of hidden units of a single-hidden layer MLP 3 http://nlp.stanford.edu/data/glove.6B.zip to 1024 (D c = 1024) and added batch normalization layers (Ioffe and Szegedy 2015) followed by dropout (Srivastava et al. 2014) with probability 0.1 to the input and the output of the MLP. We also apply dropout on the word vectors with probability 0.1. Similar to 100D experiments, we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training.</p><p>Since our model converges relatively fast, it is possible to train a model of larger size in a reasonable time. In the 600D experiment, we set D x = 300, D h = 600, and an MLP with three hidden layers (D c = 1024) is used. The dropout probability is set to 0.2 and word embeddings are not updated during training.</p><p>The size of mini-batches is set to 128 in all experiments, and hyperparameters are tuned using the validation split. The temperature parameter τ of Gumbel-Softmax is set to 1.0, and we did not find that temperature annealing improves performance. For training models, Adam optimizer  is used.</p><p>The results of SNLI experiments are summarized in <ref type="table" target="#tab_1">Table  1</ref>. First, we can see that LSTM-based leaf transformation has a clear advantage over the affine-transformation-based one. It improves the performance substantially and also leads to faster convergence.</p><p>Secondly, comparing ours with other models, we find that our 100D and 300D model outperform all other models of similar numbers of parameters. Our 600D model achieves the accuracy of 86.0%, which is comparable to that of the state-of-the-art model <ref type="bibr" target="#b36">(Nie and Bansal 2017)</ref>, while using far less parameters.</p><p>It is also worth noting that our models converge much faster than other models. All of our models converged within a few hours on a machine with NVIDIA Titan Xp GPU.</p><p>We also plot validation accuracies of various models during first 5 training epochs in <ref type="figure" target="#fig_1">Figure 3</ref>, and validate that our Model SST-2 (%) SST-5 (%) DMN <ref type="bibr" target="#b24">(Kumar et al. 2016)</ref> 88.6 52.1 NSE <ref type="bibr" target="#b34">(Munkhdalai and Yu 2017a)</ref> 89.7 52.8 byte-mLSTM <ref type="bibr" target="#b38">(Radford, Jozefowicz, and Sutskever 2017)</ref> 91.8 52.9 BCN+Char+CoVe <ref type="bibr" target="#b30">(McCann et al. 2017)</ref> 90.3 53.7 RNTN <ref type="bibr" target="#b40">(Socher et al. 2013)</ref> 85.4 45.7 Constituency Tree-LSTM <ref type="bibr" target="#b42">(Tai, Socher, and Manning 2015)</ref> 88.0 51.0 NTI-SLSTM-LSTM <ref type="bibr" target="#b35">(Munkhdalai and Yu 2017b)</ref> 89.3 53.1 Latent Syntax Tree-LSTM  86.5 -Constituency Tree-LSTM + Recurrent Dropout <ref type="bibr" target="#b26">(Looks et al. 2017)</ref> 89.4 52.3 Gumbel Tree-LSTM (Ours) 90.7 53.7 models converge significantly faster than others, not only in terms of total training time but also in the number of iterations. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis</head><p>To evaluate the performance of our model in single-sentence classification, we conducted experiments on Stanford Sentiment Treebank (SST) <ref type="bibr" target="#b40">(Socher et al. 2013)</ref> dataset. In the SST dataset, each sentence is represented as a binary parse tree, and each subtree of a parse tree is annotated with the corresponding sentiment score. Following the experimental setting of previous works, we use all subtrees and their labels for training, and only the root labels are used for evaluation. The classifier has a similar architecture to SNLI experiments. Specifically, for a sentence embedding h, the probability for the sentence to be predicted as label s ∈ {0, 1} (in the binary setting, SST-2) or s ∈ {1, 2, 3, 4, 5} (in the fine-grained setting, SST-5) is computed as follows:</p><formula xml:id="formula_12">p(s|h) = softmax(W s clf a + b s clf ) (17) a = Φ(h),<label>(18)</label></formula><p>where W s clf ∈ R 1×Dc , b s clf ∈ R 1 , and Φ is a single-hidden layer MLP with the ReLU activation function. Note that subtrees labeled as neutral are ignored in the binary setting in both training and evaluation.</p><p>We trained our SST-2 model with hyperparameters D x = 300, D h = 300, D c = 300. The word vectors are initialized with GloVe 300D pretrained vectors and fine-tuned during training. We apply dropout (p = 0.5) on the output of the word embedding layer and the input and the output of the MLP layer. The size of mini-batches is set to 32 and Adadelta  optimizer is used for optimization.</p><p>For our SST-5 model, hyperparameters are set to D x = 300, D h = 300, D c = 1024. Similar to the SST-2 model, we optimize the model using Adadelta optimizer with batch size 64 and apply dropout with p = 0.5. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results of SST experiments. Our SST-2 model outperforms all other models substantially 5 In the figure, our models and 300D NSE are trained with batch size 128. 100D CYK and 300D SPINN are trained with batch size 16 and 32 respectively, as in the original papers. We observed that our models still converge faster than others when a smaller batch size (16 or 32) is used. except byte-mLSTM <ref type="bibr" target="#b38">(Radford, Jozefowicz, and Sutskever 2017)</ref>, where a byte-level language model trained on the large product review dataset is used to obtain sentence representations.</p><p>We also see that the performance of our SST-5 model is on par with that of the current state-of-the-art model <ref type="bibr" target="#b30">(McCann et al. 2017)</ref>, which is pretrained on large parallel datasets and uses character n-gram embeddings alongside word embeddings, even though our model does not utilize external resources other than GloVe vectors and only uses wordlevel representations. The authors of <ref type="bibr" target="#b30">(McCann et al. 2017)</ref> stated that utilizing pretraining and character n-gram embeddings improves validation accuracy by 2.8% (SST-2) or 1.7% (SST-5).</p><p>In addition, from the fact that our models substantially outperform all other RvNN-based models, we conjecture that task-specific tree structures built by our model help encode sentences into vectors more efficiently than constituency-based or dependency-based parse trees do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>We conduct a set of experiments to observe various properties of our trained models. First, to see how well the model encodes sentences with similar meaning or syntax into close vectors, we find nearest neighbors of a query sentence. Second, to validate that the trained composition functions are non-trivial and task-specific, we visualize trees composed by SNLI and SST model given identical sentence.</p><p>Nearest neighbors We encode sentences in the test split of SNLI dataset using the trained 300D model and find nearest neighbors given a query sentence. <ref type="table">Table 3</ref> presents five nearest neighbors for each selected query sentence. In finding nearest neighbors, cosine distance is used as metric. The result shows that our model effectively maps similar sentences into vectors close to each other; the neighboring sentences are similar to a query sentence not only in terms of word overlap, but also in semantics. For example in the second column, the nearest sentence is 'the woman is looking at a dog', whose meaning is almost same as the query sentence. We can also see that other neighbors partially share semantics with the query sentence.</p><p># sunshine is on a man 's face . a girl is staring at a dog . the woman is wearing boots . 1 a man is walking on sunshine .</p><p>the woman is looking at a dog . the girl is wearing shoes 2 a guy is in a hot , sunny place a girl takes a photo of a dog . a person is wearing boots . 3 a man is working in the sun . a girl is petting her dog . the woman is wearing jeans . 4 it is sunny . a man is taking a picture of a dog , while a woman watches . a woman wearing sunglasses . 5 a man enjoys the sun coming through the window . a woman is playing with her dog . the woman is wearing a vest . <ref type="table">Table 3</ref>: Nearest neighbor sentences of query sentences. Each query sentence is unseen in the dataset.</p><p>i love this very much .</p><p>(a) SNLI i love this very much .</p><p>(b) SST this is the song which i love the most .</p><p>(c) SNLI this is the song which i love the most . Tree examples <ref type="figure" target="#fig_2">Figure 4</ref> show that two models (300D SNLI and SST-2) generate different tree structures given an identical sentence. In <ref type="figure" target="#fig_2">Figure 4a</ref> and 4b, the SNLI model groups the phrase 'i love this' first, while the SST model groups 'this very much' first. <ref type="figure" target="#fig_2">Figure 4c</ref> and 4d present how differently the two models process a sentence containing relative pronoun 'which'. It is intriguing that the models compose visually plausible tree structures, where the sentence is divided into two phrases by relative pronoun, even though they are trained without explicit parse trees. We hypothesize that these examples demonstrate that each model generates a distinct tree structure based on semantic properties of the task and learns non-trivial tree composition scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose Gumbel Tree-LSTM, a novel Tree-LSTM-based architecture that learns to compose taskspecific tree structures. Our model introduces the composition query vector to compute validity of the candidate parents and selects the appropriate parent according to validity scores. In training time, the model samples the parent from candidates using ST Gumbel-Softmax estimator, hence it is able to be trained by standard backpropagation while maintaining its property of discretely determining the computation path in forward propagation. From experiments, we validate that our model outperforms all other RvNN models and is competitive to state-ofthe-art models, and also observed that our model converges faster than other complex models. The result poses an important question: what is the optimal input structure for RvNN? We empirically showed that the optimal structure might differ per task, and investigating task-specific latent tree structures could be an interesting future research direction.</p><p>For future work, we plan to apply the core idea beyond sentence encoding. The performance could be further improved by applying intra-sentence or inter-sentence attention mechanisms. We also plan to design an architecture that generates sentences using recursive structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example of the parent selection. At layer t (the bottom layer), the model computes parent candidates (the middle layer)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Validation accuracies during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Tree structures built by models trained on SNLI and SST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of SNLI experiments. The above two sections group models of similar numbers of parameters. The bottom section contains results of state-of-the-art models. Word embedding parameters are not included in the number of parameters. * : values reported in the original papers. †: values estimated from per-epoch training time on the same machine our models trained on. ‡: cuDNN library is used in RNN computation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of SST experiments. The bottom section contains results of RvNN-based models. Underlined score indicates the best among RvNN-based models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jihunchoi/ unsupervised-treelstm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://nlp.stanford.edu/data/glove.840B. 300d.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/jihunchoi/ unsupervised-treelstm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is part of SNU-Samsung smart campus research program, which is supported by Samsung Electronics. The authors would like to thank anonymous reviewers for valuable comments and Volkan Cirik for helpful feedback on the early version of the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The supplementary material is available at https://github.com/jihunchoi/ unsupervised-treelstm/blob/master/ aaai18/supp.pdf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for "Learning to Compose Task-Specific Tree Structures"</head><p>Jihun Choi, Kang Min Yoo, Sang-goo Lee Seoul National University, Seoul 08826, <ref type="bibr">Korea {jhchoi, kangminyoo, sglee}@europa.snu.ac.kr</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Implementation-wise, we used multiple mask matrices in implementing the proposed Gumbel Tree-LSTM model. Using the mask matrices, Eq. 11 can be rewritten as a single equation:</p><p>The mask matrices are defined by the following equations.</p><p>m p =ȳ 1:Mt+1 (S7) Here, cumsum(c) is a function that takes a vector c = [c 1 · · · c k ] T and outputs a vector</p><p>is a vector which will be defined below, and 1 ∈ R Mt+1 is a vector whose values are all ones.</p><p>In the forward pass,ȳ 1:Mt+1 is defined by a one-hot vector y ST 1:Mt+1 , which is sampled from the categorical distribution of validity scores (v 1 , · · · , v Mt+1 ) using Gumbel-Max trick.</p><p>u i ∼ Uniform(0, 1) (S10) Note that = 10 −20 is added when calculating g i for numerical stability.</p><p>In the backward pass, instead of the one-hot version, the continuous vector y 1:Mt+1 obtained from Gumbel-Softmax is used asȳ 1:Mt+1 . Note that the Gumbel noise samples g 1 , · · · , g Mt+1 drawn in the forward pass are reused in the backward pass (i.e. noise values are not resampled in the backward pass).</p><p>In typical deep learning libraries supporting automatic differentiation (e.g. PyTorch, TensorFlow), this discrepancy between forward and backward pass can be implemented as y 1:Mt+1 = detach(y ST 1:Mt+1 − y 1:Mt+1 ) + y 1:Mt+1 , (S11) where detach(·) is a function that prevents error from backpropagating through its input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Experimental Settings</head><p>All experiments are conducted using the publicized codebase. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI</head><p>The composition query vector is initialized by sampling from Gaussian distribution N (0, 0.01 2 ). The last linear transformation that outputs the unnormalized log probability for each class is initialized by sampling from uniform distribution U(−0.005, 0.005). All other parameters are initialized following the scheme proposed by <ref type="bibr" target="#b50">He et al. (2015)</ref>. We used Adam optimizer  with default hyperparameters and halved learning rate if there is no improvement in accuracy for one epoch. The size of minibatch is set to 128 in all experiments.</p><p>In 100D experiments (D x = D h = 100, D c = 200, single-hidden layer MLP classifier), GloVe (6B, 100D)  pretrained word embeddings are used in initializing word representations. We fine-tuned word embedding parameters during training.</p><p>In 300D (D x = D h = 300, D c = 1024, single-hidden layer MLP classifier) and 600D (D x = 300, D h = 600, D c = 1024, MLP classifier with three hidden layers) experiments, GloVe (840B, 300D) pretrained word embeddings are used as word representations and fixed during training. Batch normalization is applied before the input and after the output of the MLP. Dropout is applied to word embeddings and the input and the output of the MLP with dropout probability 0.1 (300D) or 0.2 (600D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST</head><p>The composition query vector is initialized by sampling from Gaussian distribution N (0, 0.01 2 ). The last linear transformation that outputs the unnormalized log probability for each class is initialized by sampling from uniform distribution U(−0.002, 0.002). All other parameters are initialized following the scheme proposed by <ref type="bibr" target="#b50">He et al. (2015)</ref>. We used Adadelta optimizer  with default hyperparameters and halved learning rate if there is no improvement in accuracy for two epochs. In both SST-2 and SST-5 experiments, we set D x = D h = 300, used GloVe (840B, 300D) pretrained vectors with fine-tuning, and single-hidden layer MLP is used as classifier. Dropout is applied to word embeddings and the input and the output of the MLP classifier with probability 0.5.</p><p>In the SST-2 experiment, we set D c to 300 and set batch size to 32. In the SST-5 experiment, D c is increased to 1024, and mini-batches of 64 sentences are fed to the model during training.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. contextpredicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Sentence modeling with gated recursive neural network. In EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="793" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01353</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSST-8</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Programming languages and their compilers: Preliminary notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cocke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
		<respStmt>
			<orgName>Courant Institute Mathematical Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv:1706:07518</idno>
		<title level="m">Neural machine translation with Gumbel-Greedy decoding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An efficient recognition and syntax analysis algorithm for context-free languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kasami</surname></persName>
		</author>
		<idno>AFCRL- 65-758</idno>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
		<respStmt>
			<orgName>Air Force Cambridge Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-thought vectors. In NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning with dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3086" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Jointly learning sentence embeddings and syntax with unsupervised Tree-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09189</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00107</idno>
		<title level="m">Learned in translation: Contextualized word vectors</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shortcut-stacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>GloVe: Global vectors for word representation</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compose words into sentences with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context-free languages in time n 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distributed tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dell&amp;apos;arciprete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on Im-ageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
