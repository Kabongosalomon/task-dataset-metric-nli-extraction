<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCNET AND DCNET++: MAKING THE CAPSULES LEARN BETTER Dense and Diverse Capsule Networks: Making the Capsules Learn Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Samarth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Ropar Punjab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Phaye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Ropar Punjab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorva</forename><surname>Sikka</surname></persName>
							<email>apoorva.sikka@iitrpr.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Ropar Punjab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
							<email>abhinav@iitrpr.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Ropar Punjab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Bathula</surname></persName>
							<email>bathula@iitrpr.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Ropar Punjab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCNET AND DCNET++: MAKING THE CAPSULES LEARN BETTER Dense and Diverse Capsule Networks: Making the Capsules Learn Better</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Past few years have witnessed exponential growth of interest in deep learning methodologies with rapidly improving accuracies and reduced computational complexity. In particular, architectures using Convolutional Neural Networks (CNNs) have produced state-of-the-art performances for image classification and object recognition tasks. Recently, Capsule Networks (CapsNet) achieved significant increase in performance by addressing an inherent limitation of CNNs in encoding pose and deformation. Inspired by such advancement, we asked ourselves, can we do better? We propose Dense Capsule Networks (DCNet) and Diverse Capsule Networks (DCNet++). The two proposed frameworks customize the CapsNet by replacing the standard convolutional layers with densely connected convolutions. This helps in incorporating feature maps learned by different layers in forming the primary capsules. DCNet, essentially adds a deeper convolution network, which leads to learning of discriminative feature maps. Additionally, DCNet++ uses a hierarchical architecture to learn capsules that represent spatial information in a fine-to-coarser manner, which makes it more efficient for learning complex data. Experiments on image classification task using benchmark datasets demonstrate the efficacy of the proposed architectures. DCNet achieves state-of-the-art performance (99.75%) on MNIST dataset with twenty fold decrease in total training iterations, over the conventional CapsNet. Furthermore, DCNet++ performs better than CapsNet on SVHN dataset (96.90%), and outperforms the ensemble of seven CapsNet models on CIFAR-10 by 0.31% with seven fold decrease in number of parameters. * indicates authors have contributed equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep networks have been applied to the challenging tasks of image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>, object recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> and have shown substantial improvement. Many variants of CNN have been proposed by making them deeper and more complex over the time. Adding more depth in various combinations has lead to significant improvement in performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. However, increase in depth leads to vanishing gradient problem. This issue has been addressed by ResNets <ref type="bibr" target="#b4">[5]</ref>, FractalNets <ref type="bibr" target="#b8">[9]</ref> by adding connections from the initial layers to the later layers. Another similar structure, which further simplifies the way skip connections are added was proposed by Huang et al. popularly known as the DenseNets <ref type="bibr" target="#b6">[7]</ref>. The network adds dense connections between every other layer in a feedforward manner. Adding these dense connections leads to lesser number of parameters, when compared with the traditional CNN. Another benefit of concatenating these feature maps is better gradient flow across the network, which allows to train deeper networks. CNNs have performed really well on various computer vision and machine learning tasks, however, it has a few drawbacks, which have been highlighted by Sabour et al. <ref type="bibr" target="#b11">[12]</ref>. One is that CNNs are not robust to affine transformations i.e. a slight shift in the position of object make CNNs change their prediction. Although this problem can be reduced to some extent by data augmentation while training, this does not make network robust to any new pose or variation that might be present in the test data. Another major drawback is that generic CNNs do not consider spatial relationships between objects in an image while making any decision. Simply explained, CNNs only use mere presence of certain local objects in an image to make a decision while actually the spatial context of objects present is equally important. The reason is mainly the pooling operation performed in the network, which gives importance to the presence of features and ignores positional information of features, which is mainly done to decrease the parameters as the network grows. To overcome these drawbacks, Sabor et al. <ref type="bibr" target="#b11">[12]</ref> proposed a seminal architecture called the capsule networks (CapsNet). In this model, the information is stored at the vector level instead of scalar (as in the case of the simple neural networks) and these group of neurons acting together are named as capsule. Sabour et al. have used the concept of routing-byagreement and layer based squashing to achieve state-of-the-art accuracy on the MNIST dataset and detecting overlapping digits in a better way using reconstruction regularization. CapsNets are really powerful, however, at the same time, there is scope for improvement in terms of complexity as the authors did not use any pooling layers and depth, as the network is currently using only one layer of convolution and capsules. On the other hand, DenseNets have the capability to achieve very high performance by feature concatenation. We compared a Dense Convolution Network having 8 layers of convolution with feature concatenation and a simple CNN having 8 layers of convolution without concatenation and 32 kernels in each layer. Both quantitative analysis ( <ref type="table" target="#tab_7">Table 2</ref>) and visualizations ( <ref type="figure" target="#fig_0">Figure 1</ref>) show that DenseNets are better at capturing diversified features. We borrow this idea of feature concatenation across layers from DenseNets <ref type="bibr" target="#b6">[7]</ref> as it has the potential to learn diversified features, which otherwise would require a much deeper network. We use this concept as an input to the dynamic routing algorithm of Sabour et al. <ref type="bibr" target="#b11">[12]</ref>. The motivation behind our work is to: a) improve the performance of CapsNet, in terms of faster convergence and better results; b) achieve better performance than the CapsNet on complicated datasets such as CIFAR-10; c) try to reduce the model complexity.</p><p>Further, we follow the intuition behind DenseNets to design a modified decoder network with concatenated dense layers, which results in improvement in the reconstruction outputs. Our results <ref type="table" target="#tab_5">(Table 1)</ref> are at par with state-of-art performance on the MNIST dataset <ref type="bibr" target="#b9">[10]</ref> in 50 epochs, which required 1000 epochs to train conventional CapsNet. We also evaluate the proposed method on various classification datasets such as the FashionMNIST <ref type="bibr" target="#b18">[19]</ref>, The Street View House Numbers (SVHN) <ref type="bibr" target="#b10">[11]</ref>, AffNIST Dataset 1 and brain tumor dataset presented in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>, and compare the performance of the Capsule Network and Dense-CapsNet for 50 epochs keeping learning rate, learning decay rate and number of capsule parameters same. Further, we focus on why the method was not giving promising results on CIFAR-10 dataset and propose Diverse Capsule Network to improve the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The literature for CNN is vast. Architectures proposed using convolutions has increased significantly due to increase in computational power. The CNN tries to learn in a hierarchical manner from lower to higher layers where lower layers learn basic features like edges and higher layers learns complex features by combination of these low level features. Although, deeper networks have lead to improvement in performance, they are much more difficult to train due to huge increase in number of parameters. Recent architectures proposed, aim to improve the performance while jointly optimizing the number of parameters. Highway network <ref type="bibr" target="#b13">[14]</ref> was the first architecture proposed in this direction to train deeper network with large number of layers. They added bypassing paths to easily train the model. ResNet <ref type="bibr" target="#b4">[5]</ref> model improves training by adding residual connections. Another such network proposed by Huang et. al. <ref type="bibr" target="#b6">[7]</ref> created a novel way of adding skip connections by introducing connections from initial convolution layers to deeper layers, naming it one dense block. The capsule networks <ref type="bibr" target="#b11">[12]</ref> have been recently introduced to overcome the drawbacks of CNNs discussed in Section 1. Capsules are group of neurons that depict properties of various entities present in an image. There could be various properties of an image which can be captured like position, size, texture. Capsules use routing-by-agreement where output is sent to all final capsules. Each capsule makes a prediction for the parent capsule, which is then compared with the actual output of parent capsule. If the outputs matches, the coupling coefficient between the two capsules is increased. Let u i be an output of a capsule i, and j be the parent capsule, the prediction is calculated as:</p><formula xml:id="formula_0">u i| j = W i j u i c i j = exp(b i j ) ∑ k exp(b ik )</formula><p>where W i j is the weighting matrix. Then coupling coefficients c i j are computed using a simple softmax function as shown. Here b i j is log probability of capsule i being coupled with capsule j. This value is 0 when routing is started. Input vector to parent capsule j is calculated as:</p><formula xml:id="formula_1">s j = ∑ i c i jû j|i v j = ||s j || 2 1 + ||s j || 2 s j ||s j || ,</formula><p>The output of these capsule vectors represent probability that an object represented by capsule is present in given input or not. But the output of these capsule vectors can exceed one depending on the output. Thus, a non linear squashing function defined above is used to restrict the vector length to 1, where s j is input to capsule j and v j is output. The log probabilities are updated by computing the inner product of v j andû j|i . If two vectors agree, the product would be larger leading to longer vector length. In the final layer, a loss value is computed for each capsule. Loss value is high when entity is absent and capsule has high instantiation parameters. Loss is defined as:</p><formula xml:id="formula_2">L k = T k max(0, m + − ||v k ||) 2 + λ (1 − T k ) max (0, ||v k || − m − ) 2</formula><p>Here l k is loss function for a capsule k, T k is 1 when label is true and 0 otherwise. CapsNet applies a convolution and generates 256 feature maps. It is then fed to primary capsule layer where 32, 8D capsules are formed using 9 × 9 kernel with stride 2 followed by squashing. Finally, a DigitCaps layer is applied which forms final capsules of 16D. A paper by Afshar et al. <ref type="bibr" target="#b0">[1]</ref> shows that capsule networks are powerful enough to work on the images containing complex datasets. We aim to work on empowering capsules by using DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We tried to customize capsule network in two frameworks which is explained in the following subsections. Furthermore, we explain the intuition behind choosing densely connected networks and then refine it to improve the performance on complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Capsule Networks (DCNet)</head><p>The feature maps learned by the first convolution layer in the baseline CapsNet model learns very basic features, which might not be enough to build capsules for complex datasets. Hence, we tried increasing the convolution layers to two and eight in the initial layer and observed that it did not lead to any improvement as shown in <ref type="table" target="#tab_5">Table 1</ref>. In DCNet, we try to modify capsule networks to form a deeper architecture where we create an eight-layered dense convolutional subnetwork based on skip connections. Every layer is concatenated to the next layer in feed-forward manner, adding up to make a final convolution layer. This leads to better gradient flow as compared to directly stacked convolution layers.   <ref type="figure" target="#fig_1">Figure 2</ref> shows the detailed pipeline of the proposed architecture for MNIST dataset. The input sample goes into 8 levels of convolutions and each of those convolution levels generates 32 new feature maps followed by concatenation with feature maps of all previous layers which results in 257 feature maps (input image is included). These diversified feature maps act as input to the capsule layer which applies a convolution of 9 × 9 with a stride of 2. The feature maps obtained act as the primary capsules of the CapsNet. The work of Sabour et al. <ref type="bibr" target="#b11">[12]</ref> mainly focuses on equivariance instead of invariance which we totally agree with, so we did not use any of the max. or average pooling layers as used in DenseNets, which results in spatial information loss. It is important to note that while Sabour et al. <ref type="bibr" target="#b11">[12]</ref> created the primary capsules from 256 feature maps created by the same complexity level of convolution, DCNet's primary capsules are generated by combining all the features of different levels of complexity, which further improves the classification.</p><p>These feature maps act as thirty-two 8D capsules, which are passed to a squash activation layer, followed by the routing algorithm. 16D final capsules are generated for each of the 10 classes (digits) which further generates one-hot 10D output vector, similar to the conventional capsule network used for MNIST.   Inspired by the dense connections implemented by Huang et al. we also modified the reconstruction model of the capsule network. The decoder is a four-layered model with concatenation of features of first layer and second layer resulting in better reconstructions, which takes the DigitCaps layer as it's input (masked by output label during training). If the size of image is more than 32 × 32, the number of neurons are changed from 512 to 600 and 1024 to 1200. Our experiments show significant increase in performances over various datasets like MNIST, Fashion-MNIST, SVHN. We noticed that DCNet's performance on CIFAR-10 dataset increased over single baseline CapsNet model trained with same parameters, but it did not outperform the seven ensemble model <ref type="bibr" target="#b11">[12]</ref> of the capsule networks having 89.40% accuracy. As the images in CIFAR-10 are quite complex when compared to MNIST dataset, it is not easy for the network to encode the part-whole relationships. We address this problem by creating DCNet++ which learns well on such complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diverse Capsule Networks (DCNet++)</head><p>Adding skip connections to the convolutions wasn't enough to improve performance on the CIFAR-10 dataset. This might be due to the presence of simple primary capsules which are not sufficient to encode the information present in such complex images. We visualized what part of an image is activated by a 8D primary capsule of DCNet by guided back-propagation and noticed that every primary capsule is generated by a small spatial area of the input image, which then act together to make a decision but these aren't enough for complex images. To overcome this, we implemented a novel way of creating primary capsules which carry information of various scales of the image, hence diversifying the capsules. This in turn helped to find connections between primary capsules of various levels of image. <ref type="figure" target="#fig_3">Figure  3</ref> shows which part of the image is activated in the DCNet++ model. If we consider just one level, it will be similar to image activations in DCNets and baseline CapsNet, except for the fact that the activations will have no diffusion out of the square region in Capsule Network. The activated region is diffused out in DCNet and DCNet++ because of 'same' padded convolutions in the densely connected convolution layers. The ensemble of baseline CapsNet models (by Sabour et al. <ref type="bibr" target="#b11">[12]</ref>) give 89.40% accuracy on CIFAR-10 dataset, which is probably due to the fact that patches of 24 × 24 are used as input, which helps to activate larger spatial area for generating each primary capsule. Although it achieves reasonably higher accuracy over a single Capsule model, ensemble of seven such models leads to a huge increase in number of parameters which can be reduced for such small sized images. We focus to reduce the number of parameters used to model the data and learn better information via creating multiple levels of capsules. In the proposed model, we pass the complete image by first down sampling it to 32 × 32 size. A single three layered DCNet++ having 13.4M parameters achieves 89.71% test accuracy.   There are twelve capsules in each DCNet. A strided convolution (9 × 9 size and 2 stride) is applied which reduces the size of the image fed to next level which is similar to the concept of Pyramid of Gaussian <ref type="bibr" target="#b1">[2]</ref>. This also resembles with functioning of brain which separates the information into channels. For example, there are separate pathways for high and low spatial frequency content and color information.</p><p>In addition to these three DigitCaps (output) layers, we created one more DigitCaps output layer by routing the concatenation of three PrimaryCaps layers. The reason for adding this another level is to allow the model to learn combined features from various levels of capsules. We observed that in case of simple stacking of DigitCaps and joint back-propagation, the losses of last level PrimaryCaps dominate others leading to a poor learning and the former levels act as simple convolution layers. Thus, to avoid any imbalanced learning the model was jointly trained but the losses of four layers were back propagated separately.</p><p>While testing, the four DigitCaps layers are concatenated to form a 54D final capsule for each of the ten classes, and the reconstructions were created for only one channel of image using these fifty-four capsules. The reconstructions for CIFAR-10 were not very good, which we believe is due to the dominance of background noise present in the samples and presence of complex information of the image which the decoder is not robust enough to recreate. Interestingly, we notice the effect of noise in DigitCaps of different levels on the reconstruction outputs over MNIST dataset by subtracting 0.2 from each digit one at a time in the 54D DigitCaps. It is observed that the effect on reconstructions decrease from first level to the last level of capsules. DigitCaps generated from the concatenation of PrimaryCaps play a major role in affecting reconstructions, which is an additive effect of different layers of capsules (shown in <ref type="figure" target="#fig_6">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate the potential of our model on multiple datasets and compare it with the CapsNet architecture. All the experiments were performed using GeForce GTX 1080 with 8GB RAM. Due to resource constraints, we ran all our models for 50 -100 epochs. The initial learning rate was set to 0.001 and decay rate 0.9 with Adam as optimizer. We changed multiplier factor to scale down the reconstruction loss according to the image size so that it does not dominate margin loss. We used publicly available code <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref> for CapsNet and DenseNet to create our models 2 . The test errors were computed once for each model. Three routings were used for all the experiments. For fair comparisons, we mostly kept the parameters of the proposed DCNet model after PrimaryCaps layer, same as the conventional CapsNet. Following are the implementation details corresponding to the datasets used:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Level One DigitCaps Level Two DigitCaps</head><p>Level Three DigitCaps Merged DigitCaps MNIST Handwritten digits dataset and Fashion-MNIST dataset have 60K and 10K training and testing images with each image size being 28 × 28 in size. We did not use any data augmentation scheme and repeated the experiment 3 times. It can be clearly seen in <ref type="table" target="#tab_5">Table 1</ref> that DCNet is able to learn the input data variations quickly, as compared to the CapsNet, i.e., it is able to achieve 99.75% test accuracy on MNIST and 94.64% on Fashion-MNIST dataset with 20 fold decrease in total iterations. We changed the stride from two to one in the PrimaryCaps layer of second level of DCNet++ to fit the image. We did not observe any improvement in the performance of 3-level DCNet++ on MNIST dataset which  <ref type="bibr" target="#b11">[12]</ref> 99.65% (1000E, NR) <ref type="bibr" target="#b11">[12]</ref> 93.65% (100E) 93.23% (100E) 95.7% (&gt; 100E) <ref type="bibr" target="#b11">[12]</ref> 89.56% (50E) 78% (10E) <ref type="bibr" target="#b0">[1]</ref> 87.5% (50E)  is as expected because we are capturing fine-to-coarse level features, due to which every tiny variation in the writing of a particular number will be captured from the training set. These 'fine' features might be causing a problem during the testing phase. We need to invest more time into improving it. CIFAR-10 dataset is an image dataset having 50K and 10K samples for training and testing. We used channel means and standard deviations for normalizing the data. We compare our proposed model with the ensemble of four and seven capsule network models <ref type="bibr" target="#b11">[12]</ref> for first 50 epochs. We used similar parameters for DCNet model that were used for MNIST and the test accuracy improved over the ensemble of four CapsNet models (implemented by Xi et al. <ref type="bibr" target="#b17">[18]</ref>) to 82.68% from 71.55%. The proposed DCNet++ model resulted in 89.7% accuracy in 13.4M parameters, which is significantly less than 7 ensemble model proposed by <ref type="bibr">Sabour</ref>    <ref type="bibr" target="#b11">[12]</ref>.</p><p>Brain Tumor Dataset contains 3,064 MRI images of 233 patients diagnosed with one of the three brain tumor types (i.e., meningioma, glioma, and pituitary tumor). Afshar et al. <ref type="bibr" target="#b0">[1]</ref> changed the capsule network architecture with initial convolution layer having 64 feature maps. We created an equivalent DCNet model (for uniform results) by modifying the eight initial convolution layers to four layers with 16 kernels each, totaling to 64 kernels and decreased the primary capsules to 6. We also changed the learning rate of our model to 1E − 4 for effective learning. The DCNet++ is a 3-level hierarchical model of the modified DCNet. We trained the models on eight fold cross validation (results in <ref type="table" target="#tab_5">Table 1)</ref>.</p><p>SmallNORB dataset has 24K and 24K images for training and testing. We normalized the images and add random noise and contrast to each image. The proposed model was compared with the CapsNet <ref type="bibr" target="#b11">[12]</ref> for first 50 epochs, by modifying DCNet taking 6D 16 primary capsules with 8D final capsule from four convolution layers with 18 kernels each. The results of DCNet improved over the replicated CapsNet model to give a test accuracy of 95.58%. The DCNet++ model is same as what we used for CIFAR-10 dataset, resulting in 96.90% accuracy, compared to CapsNet <ref type="bibr" target="#b11">[12]</ref>, which achieved 95.70% in more than 50 epochs.</p><p>We find that the loss of the modified decoder in the DCNet decrease by a considerable amount (shown in <ref type="figure" target="#fig_4">Figure 4</ref>), having same loss multiplier factor as that of CapsNet. Also, <ref type="figure">Figure 6</ref> shows the test accuracy comparison of CapsNet and DCNet from which we can clearly infer that DCNet has a faster convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a modification of Capsule Network, DCNet that replaces standard convolution layers in CapsNet with densely connected convolution. Addition of direct connections between two consecutive layers help learn better feature maps, which in turn helps in forming better quality primary capsules. The effectiveness of this architecture is demonstrated by state-of-the-art performance (99.75%) on MNIST data with twenty fold decrease in total training iterations, over conventional CapsNets. Although the same DCNet model performed better (82.63%) than a single, baseline CapsNet model on CIFAR-10 data, it was below par compared to seven ensemble model of CapsNet with 89.40% accuracy.</p><p>Performance of CapsNet on real-life, complex data (CIFAR-10, ImageNet, etc.) is known to be substandard compared to simpler datasets like MNIST. DCNet++, addresses this limitation by stacking multiple layers of DCNet to enhance the representational power of the network. The hierarchical structure helps learn intricate relationships between fine-to-coarse level features. A single, three-level DCNet++ achieves 89.71% accuracy on CIFAR-10, which is an increase of 0.31% accuracy over seven ensemble CapsNet model with significantly less number of parameters. The proposed networks substantially improve the performance of existing Capsule Networks on other datasets such as SVHN, SmallNORB and Tumor Dataset. In future, we plan to incorporate EM routing <ref type="bibr" target="#b5">[6]</ref> in DCNet and DCNet++ and work on reducing the computational complexity of the model even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Motivation for using dense convolutions with CapsNet: Here (A) and (B) are the feature maps at the same depth from DenseNets and CNN, respectively. It is clear that DenseNets learn clearer discriminative features as compared to CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed prediction pipeline for MNIST (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8D</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Activated image region on CIFAR-10 by corresponding capsules of different levels. Row A depicts how a primary capsule gets activated on the input image. Row B shows sample activation regions on input image from capsules of different levels in DCNet++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Pipeline of three-level DCNet++ for CIFAR-10 (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>depicts the detailed pipeline of the DCNet++ model used to train the CIFAR-10 dataset. It is a hierarchical model where a DCNet model is created and it's intermediate representation is used as an input to the next DCNet which in turn generates a representation fed to the next DCNet layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Reconstruction outputs by adding noise in different digits of the DigitCaps layer of DCNet++ on MNIST. Merged DigitCaps are affected the most due to the addition of all PrimaryCaps having fine-coarse activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Comparison of the performance of DCNet and the CapsNet model on MNIST dataset. Reconstruction MSE loss of Cap-sNet+Baseline Decoder, CapsNet+Proposed Decoder and DCNet+Proposed Decoder on MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Performance of CapsNets, DCNets and DCNet++ over various datasets. NR -No Reconstruction SubNetwork, E -training epochs, BR -baseline Reconstruction SubNetwork.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>et al. (7 × 14.5M, 89.40%) and DCNet (16M parameters for 32 × 32 image).</figDesc><table><row><cell>Model</cell><cell>Description</cell><cell cols="2">Parameters Test Acc.</cell></row><row><cell>CapsNet (Baseline)</cell><cell>Conv -Primary Capsules -Final Capsules</cell><cell>8.2M</cell><cell>99.67%</cell></row><row><cell>CapsNet Variant</cell><cell>Added one more initial convolution layer having 256 9x9 kernels with same padding, stride=1</cell><cell>13.5M</cell><cell>99.66%</cell></row><row><cell>DCNet Variant One</cell><cell cols="2">Used 3 convolution layers with 8 feature maps each 6.9M</cell><cell>99.66%</cell></row><row><cell>DCNet Variant Two</cell><cell>Removed the concatenations (no skip connections) acting like simple 8 layered convolutions</cell><cell>4M</cell><cell>99.68%</cell></row><row><cell>DCNet Variant Three</cell><cell>Used 8th convolution layer only (no concatenation in the last layer) to create Primary Capsules</cell><cell>7.2M</cell><cell>99.72%</cell></row><row><cell>Final DCNet</cell><cell>DenseConv -Primary Capsules -Final Capsules</cell><cell>11.8M</cell><cell>99.75%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Comparison of various model variations after 50 epochs on MNIST dataset. For details of proposed DCNet, refer to Section 3.1. Street View House Numbers (SVHN) contains 73K and 26K real-life digit images for training and testing. To compare results with the original capsule network model [12] for first 50 epochs, we modified DCNet by using 6D 16 primary capsules with 8D final capsule from four convolution layers with 18 feature maps. The results of the DCNet model improved over the replicated CapsNet model to test accuracy of 95.59% from 93.23%. The model for DCNet++ is same as what we used for CIFAR-10 dataset, resulting in 96.9% accuracy, compared to 95.70% by CapsNets (more than 50 epochs)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at: http://www.cs.toronto.edu/~tijmen/affNIST/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will be sharing the code after acceptance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Brain tumor type classification via capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parnian</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos N</forename><surname>Plataniotis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10200</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Readings in computer vision: Issues, problems, principles, and paradigms. chapter The Laplacian Pyramid As a Compact Image Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adelson</surname></persName>
		</author>
		<idno>0-934613-33-8</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=33517.33571" />
		<imprint>
			<date type="published" when="1987" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="671" to="679" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced performance of brain tumor classification via tumor region augmentation and partition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ru</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqiang</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianjin</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">140381</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=HJWLfGWRb" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-nepdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1605.07648</idno>
		<ptr target="http://arxiv.org/abs/1605.07648" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<ptr target="http://arxiv.org/abs/1505.00387" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4842</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.220</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dense net in keras</title>
		<idno>titu1994</idno>
		<ptr target="https://github.com/titu1994/DenseNet" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Capsule network performance on complex data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selina</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03480</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xifengguo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Capsnet-Keras</surname></persName>
		</author>
		<ptr target="https://github.com/XifengGuo/CapsNet-Keras" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Retrieval of brain tumors by adaptive spatial pooling and fisher vector representation 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhao Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianjin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wufan</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
