<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Ojha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose FineGAN, a novel unsupervised GAN framework, which disentangles the background, object shape, and object appearance to hierarchically generate images of fine-grained object categories. To disentangle the factors without supervision, our key idea is to use information theory to associate each factor to a latent code, and to condition the relationships between the codes in a specific way to induce the desired hierarchy. Through extensive experiments, we show that FineGAN achieves the desired disentanglement to generate realistic and diverse images belonging to fine-grained classes of birds, dogs, and cars. Using FineGAN's automatically learned features, we also cluster real images as a first attempt at solving the novel problem of unsupervised fine-grained object category discovery. Our code/models/demo can be found at https://github.com/kkanshul/finegan * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A D B C</head><p>Consider the figure above: if tasked to group any of the images together, as humans we can easily tell that birds A and B should not be grouped with C and D as they have completely different backgrounds and shapes. But how about C and D? They share the same background, shape, and rough color. However, upon close inspection, we see that even C and D should not be grouped together as C's beak is yellow and its tails have large white spots while D's beak is black and its tails have thin white strips. <ref type="bibr" target="#b0">1</ref> This example demonstrates that clustering fine-grained object categories requires not only disentanglement of the background, <ref type="figure">Figure 1</ref>. FineGAN disentangles the background, object shape (parent), and object appearance (child) to hierarchically generate fine-grained objects, without mask or fine-grained annotations.</p><p>shape, and appearance (color/texture), but that it is naturally facilitated in a hierarchical fashion.</p><p>In this work, we aim to develop a model that can do just that: model fine-grained object categories by hierarchically disentangling the background, object's shape, and its appearance, without any manual fine-grained annotations. Specifically, we make the first attempt at solving the novel problem of unsupervised fine-grained object clustering (or "discovery"). Although both unsupervised object discovery and fine-grained recognition have a long history, prior work on unsupervised object category discovery focus only on clustering entry-level categories (e.g., birds vs. cars vs. dogs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b14">15]</ref>, while existing work on fine-grained recognition focus exclusively on the supervised setting in which ground-truth fine-grained category annotations are provided <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Why unsupervised discovery for such a difficult problem? We have two key motivations. First, fine-grained annotations require domain experts. As a result, the overall annotation process is very expensive and standard crowdsourcing techniques cannot be used, which restrict the amount of training data that can be collected. Second, unsupervised learning enables the discovery of latent structure in the data, which may not have been labeled by annotators. For example, fine-grained image datasets often have an in-herent hierarchical organization in which the categories can first be grouped based on one feature (e.g., shape) and then differentiated based on another (e.g., appearance).</p><p>Main Idea. We hypothesize that a generative model with the capability of hierarchically generating images with finegrained details can also be useful for fine-grained grouping of real images. We therefore propose FineGAN, a novel hierarchical unsupervised Generative Adversarial Networks framework to generate images of fine-grained categories.</p><p>FineGAN generates a fine-grained image by hierarchically generating and stitching together a background image, a parent image capturing one factor of variation of the object, and a child image capturing another factor. To disentangle the two factors of variation of the object without any supervision, we use information theory, similar to InfoGAN <ref type="bibr" target="#b8">[9]</ref>. Specifically, we enforce high mutual information between (1) the parent latent code and the parent image, and (2) the child latent code, conditioned on the parent code, and the child image. By imposing constraints on the relationship between the parent and child latent codes (specifically, by grouping child codes such that each group has the same parent code), we can induce the parent and child codes to capture the object's shape and color/texture details, respectively; see <ref type="figure">Fig. 1</ref>. This is because in many fine-grained datasets, objects often differ in appearance conditioned on a shared shape (e.g., 'Yellow-billed Cuckoo' and 'Blackbilled Cuckoo', which share the same shape but differ in their beak color and wing patterns).</p><p>Moreover, FineGAN automatically generates masks at both the parent and child stages, which help condition the latent codes to focus on the relevant object factors as well as to stitch together the generated images across the stages. Ultimately, the features learned through this unsupervised hierarchical image generation process can be used to cluster real images into their fine-grained classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. Our work has two main contributions:</head><p>(1) We introduce FineGAN, an unsupervised model that learns to hierarchically generate the background, shape, and appearance of fine-grained object categories. Through various qualitative evaluations, we demonstrate FineGAN's ability to accurately disentangle background, object shape, and object appearance. Furthermore, quantitative evaluations on three benchmark datasets (CUB <ref type="bibr" target="#b46">[47]</ref>, Stanforddogs <ref type="bibr" target="#b27">[28]</ref>, and Stanford-cars <ref type="bibr" target="#b29">[30]</ref>) demonstrate FineGAN's strength in generating realistic and diverse images.</p><p>(2) We use FineGAN's learned disentangled representation to cluster real images for unsupervised fine-grained object category discovery. It produces fine-grained clusters that are significantly more accurate than those of state-ofthe-art unsupervised clustering approaches (JULE <ref type="bibr" target="#b52">[53]</ref> and DEPICT <ref type="bibr" target="#b14">[15]</ref>). To our knowledge, this is the first attempt to cluster fine-grained categories in the unsupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Fine-grained category recognition involves classifying subordinate categories within entry-level categories (e.g., different species of birds), which requires annotations from domain experts <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b47">48]</ref>. Some methods require additional part <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b54">55]</ref>, attribute <ref type="bibr" target="#b13">[14]</ref>, or text <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19]</ref> annotations. Our work makes the first attempt to overcome the dependency on expert annotations by performing unsupervised fine-grained category discovery without any class annotations.</p><p>Visual object discovery and clustering. Early work on unsupervised object discovery <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> use handcrafted features to cluster object categories from unlabeled images. Others explore the use of natural language dialogue for object discovery <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref>. Recent unsupervised deep clustering approaches <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b14">15]</ref> demonstrate stateof-the-art results on datasets whose objects have large variations in high-level detail like shape and background. On fine-grained category datasets, we show that FineGAN significantly outperforms these methods as it is able to focus on the fine-grained object details.</p><p>Disentangled representation learning has a vast literature (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref>). The most related work in this space is InfoGAN <ref type="bibr" target="#b8">[9]</ref>, which learns disentangled representations without any supervision by maximizing the mutual information between the latent codes and generated data. Our work builds on the same principles of information theory, but we extend it to learn a hierarchical disentangled representation. Specifically, unlike InfoGAN in which all details of an object are generated together, FineGAN provides explicit distentanglement and control over the generation of background, shape, and appearance, which we show is especially important when modeling fine-grained categories.</p><p>GANs and Stagewise image generation. Unconditional GANs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> can generate realistic images without any supervision. However, unlike our approach, these methods do not generate images hierarchically and do not have explicit control over the background, object's shape, and object's appearance. Some conditional supervised approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b4">5]</ref> learn to generate finegrained images with text descriptions. One such approach, FusedGAN <ref type="bibr" target="#b4">[5]</ref>, generates fine-grained objects with specific pose and shape but it cannot decouple them, and lacks explicit control over the background. In contrast, FineGAN can generate fine-grained images without any text supervision and with full control over the background, pose, shape, and appearance. Also related are stagewise image generators <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b26">27]</ref>. In particular, LR-GAN <ref type="bibr" target="#b51">[52]</ref> generates the background and foreground separately and stitches them. However, both are controlled by a single random vec-tor, and it does not disentangle the object's shape from appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Let X = {x 1 , x 2 , . . . , x N } be a dataset containing unlabeled images of fine-grained object categories. Our goal is to learn an unsupervised generative model, FineGAN, which produces high quality images matching the true data distribution p data (x), while also learning to disentangle the relevant factors of variation associated with images in X .</p><p>We consider background, shape, appearance, and pose/location of the object as the factors of variation in this work. If FineGAN can successfully associate each latent code to a particular fine-grained category aspect (e.g., like a bird's shape and wing color), then its learned features can also be used to group the real images in X for unsupervised find-grained object category discovery. <ref type="figure">Fig. 2</ref> shows our FineGAN architecture for modeling and generating fine-grained object images. The overall process has three interacting stages: background, parent, and child. The background stage generates a realistic background image B. The parent stage generates the outline (shape) of the object and stitches it onto B to produce parent image P. The child stage fills in the object's outline with the appropriate color and texture, to produce the final child image C. The objective function of the complete process is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hierarchical fine-grained disentanglement</head><formula xml:id="formula_0">L = λL b + βL p + γL c</formula><p>where L b , L p and L c denote the objectives for the background, parent, and child stage respectively, with λ, β and γ denoting their weights. We train all stages end-to-end.</p><p>The different stages get conditioned with different latent codes, as seen from <ref type="figure">Fig. 2</ref>. FineGAN takes as input: i) a continuous noise vector z ∼ N (0, 1); ii) a categorical background code b ∼ Cat(K = N b , p = 1/N b ); iii) a categorical parent code p ∼ Cat(K = N p , p = 1/N p ); and iv) a categorical child code c ∼ Cat(K = N c , p = 1/N c ).</p><p>Relationship between latent codes: (1) Parent code and child code. We assume the presence of an implicit hierarchy in X -as mentioned previously, fine-grained categories can often be grouped first based on a common shape and then differentiated based on appearance. To help discover this hierarchy, we impose two constraints: (i) the number of categories of parent code is set to be less than that of child code (N p &lt; N c ), and (ii) for each parent code p, we tie a fixed number of child codes c to it (multiple child codes share the same parent code). We will show that these constraints help push p to capture shape and c to capture appearance. For example, if the shape identity captured by p is that of a duck, then the list of c's tied to this p would all share the same duck shape, but vary in their color and texture.</p><p>(2) Background code and child code. There is usually some correlation between an object and the background in which it is found (e.g., ducks in water). Thus, to avoid conflicting object-background pairs (which a real/fake discriminator could easily exploit to tell that an image is fake), we set the background code to be the same as the child code during training (b = c). However, we can easily relax this constraint during testing (e.g., to generate a duck in a tree).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Background stage</head><p>The background stage synthesizes a background image B, which acts as a canvas for the parent and child stages to stitch different foreground aspects on top of B. Since we aim to disentangle background as a separate factor of variation, B should not contain any foreground information. We hence separate the background stage from the parent and child stages, which share a common feature pipeline. This stage consists of a generator G b and a discriminator pair, D b and D aux . G b is conditioned on latent background code b, which controls the different (unknown) background classes (e.g., trees, water, sky), and on latent code z, which controls intra-class background details (e.g., positioning of leaves). To generate the background, we assume access to an object bounding box detector that can detect instances of the super-category (e.g., bird). We use the detector to locate non-object background patches in each real image x i . We then train G b and D b using two objectives: L b = L bg adv + L bg aux , where L bg adv is the adversarial loss <ref type="bibr" target="#b15">[16]</ref> and L bg aux is the auxiliary background classification loss.</p><p>For the adversarial loss L bg adv , we employ the discriminator D b on a patch level <ref type="bibr" target="#b25">[26]</ref> (we assume background can easily be modeled as texture) to predict an N × N grid with each member indicating the real/fake score for the corresponding patch in the input image:</p><formula xml:id="formula_1">L bg adv = min Gb max Db E x [log(D b (x))] + E z,b [log(1 − D b (G b (z, b)))]</formula><p>The auxiliary classification loss L bg aux makes the background generation task more explicit, and is also computed on a patch level. Specifically, patches inside (r i ) and outside (r o ) the detected object in real images constitute the training set for foreground (1) and background (0) respectively, and is used to train a binary classifier D aux with cross-entropy loss. We then use D aux to train the generator G b :</p><formula xml:id="formula_2">L bg aux = min G b E z,b [log(1 − D aux (G b (z, b)))]</formula><p>This loss updates G b so that D aux assigns a high background probability to the generated background patches.</p><formula xml:id="formula_3">z b z p G c F p F c Background: B Parent: P Child: C Child stage C m C f Inverse P m P f Inverse Parent stage L p_info L c_info L bg_adv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stitching process</head><p>Generative modules</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative modules</head><p>Element wise multiplication</p><formula xml:id="formula_4">Element wise addition b -Background code p -Parent code c -Child code P f,m c C f,m G p,m G p,f G c,m G c,f G p G b D p D aux D b D c D adv</formula><p>Background stage L bg_aux L c_adv <ref type="figure">Figure 2</ref>. FineGAN architecture for hierarchical fine-grained image generation. The background stage, conditioned on random vector z and background code b, generates the background image B. The parent stage, conditioned on z and parent code p, uses B as a canvas to generate parent image P , which captures the shape of the object. The child stage, conditioned on c, uses P as a canvas to generate the final child image C with the object's appearance details stitched into the shape outline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Parent stage</head><p>As explained previously, we model the real data distribution p data (x) through a two-level foreground generation process via the parent and child stages. The parent stage can be viewed as modeling higher-level information about an object like its shape, and the child stage, conditioned on the parent stage, as modeling lower-level information like its color/texture. Capturing multi-level information in this way can have potential advantages. First, it makes the overall image generation process more principled and easier to interpret; different sub-networks of the model can focus only on synthesizing entities they are concerned with, in contrast to the case where the entire network performs single-shot image generation. Second, for fine-grained generation, it should be easier for the model to generate appearance details conditioned on the object's shape, without having to worry about the background and other variations. With the same reasoning, such hierarchical features-parent capturing shape and child capturing appearance-should also be beneficial for fine-grained categorization compared to a flat-level feature representation.</p><p>We now discuss the working details of the parent stage. As shown in <ref type="figure">Fig. 2</ref>, G p , which consists of a series of convolutional layers and residual blocks, maps z and p to feature representation F p . As discussed previously, the requirement from this stage is only to generate a foreground entity, and stitch it to the existing background B. Consequently, two generators G p,f and G p,m transform F p into parent foreground (P f ) and parent mask (P m ) respectively, so that P m can be used to stitch P f on B, to obtain the parent image P:</p><formula xml:id="formula_5">P = P f,m + B m where P f,m = P m P f and B m = (1 − P m ) B</formula><p>denote masked foreground and inverse masked background image respectively; see green arrows in <ref type="figure">Fig. 2</ref>. This idea of generating a mask and using it for stitching is inspired by LR-GAN <ref type="bibr" target="#b51">[52]</ref>.</p><p>We again employ a discriminator at the parent stage, and denote it as D p . Its functioning however, differs from the discriminators employed at the other stages. This is because in contrast to the background and child stages where we know the true distribution to be modeled, the true distribution for P or P f,m is unknown (i.e., we have real patch samples of background and real image samples of the object, but we do not have any real intermediate image samples in which the object exhibits one factor like shape but lacks another factor like appearance). Consequently, we cannot use the standard GAN objective to train D p .</p><p>Thus, we only use D p to induce the parent code p to represent the hierarchical concept i.e., the object's shape. With no supervision from image labels, we exploit information theory to discover this concept in a completely unsupervised manner, similar to InfoGAN <ref type="bibr" target="#b8">[9]</ref>. Specifically, we maximize the mutual information I(p, P f,m ), with D p approximating the posterior P (p|P f,m ):</p><formula xml:id="formula_6">L p = L p inf o = max Dp,G p,f ,Gp,m E z,p [log D p (p|P f,m )]</formula><p>We use P f,m instead of P so that D p makes its decision solely based on the foreground object (shape) and not get influenced by the background. In simple words, D p is asked to reconstruct the latent hierarchical category information (p) from P f,m , which already has this information encoded during its synthesis. Given our constraints from Sec. 3.1 that there are less parent categories than child ones (N p &lt; N c ) and multiple child codes share the same parent code, FineGAN tries encoding p into P f,m as an attribute that: (i) by itself cannot capture all fine-grained category details, and (ii) is common to multiple fine-grained categories, which is the essence of hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Child stage</head><p>The result of the previous stages is an image that is a composition of the background and object's outline. The task that remains is filling in the outline with appropriate texture/color to generate the final fine-grained object image.</p><p>As shown in <ref type="figure">Fig. 2</ref>, we encode the color/texture information about the object with child code c, which is itself conditioned on parent code p. Concatenated with F p , the resulting feature chunk is fed into G c , which again consists of a series of convolutional and residual blocks. Analogous to the parent stage, two generators G c,f and G c,m map the resulting feature representation F c into child foreground (C f ) and child mask (C m ) respectively. The stitching process to obtain the complete child image C is:</p><formula xml:id="formula_7">C = C f,m + P c,m where C f,m = C m C f , and P c,m = (1 − C m ) P.</formula><p>We now discuss the requirements for the child stage discriminative networks, D adv and D c : (i) discriminate between real samples from X and fake samples from the generative distribution using D adv ; (ii) use D c to approximate the posterior P (c|C f,m ) to associate the latent code c to a fine-grained object detail like color and texture. The loss function can hence be broken down into two components L c = L c adv + L c inf o , where:</p><formula xml:id="formula_8">L c adv = min Gc max Dadv E x [log(D adv (x))] + E z,b,p,c [log(1 − D adv (C))], L c inf o = max Dc,Gc,f ,Gc,m E z,p,c [log D c (c|C f,m )].</formula><p>Again, we use C f,m instead of C so that D c makes its decision solely based on the object (color/texture and shape) and not get influenced by the background. With shape already captured though the parent code p, the child code c can now solely focus to correspond to the texture/color inside the shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fine-grained object category discovery</head><p>Given our trained FineGAN model, we can now use it to compute features for the real images x i ∈ X to cluster them into fine-grained object categories. In particular, we can make use of the final synthetic images {C j } and their associated parent and child codes to learn a mapping from images to codes. Note that we cannot directly use the parent and child discriminators D p and D c -which each categorize {P f,m } and {C f,m } into one of the parent and child codes respectively--on the real images due to the unavailability of real foreground masks. Instead, we train a pair of convolutional networks (φ p and φ c ) to predict the parent and child codes of the final set of synthetic images {C j }:</p><p>1. Randomly sample a batch of codes: z ∼ N (0, 1), p ∼ p p , c ∼ p c , b ∼ p b to generate child images {C j }. 2. Feed forward this batch through φ p and φ c . Compute cross-entropy loss CE(p, φ p (C j )) and CE(c, φ c (C j )). 3. Update φ p and φ c . Repeat till convergence.</p><p>To accurately predict parent code p from C j , φ p has to solely focus on the object's shape as no sensible supervision can come from the randomly chosen background and child codes. With similar reasoning, φ c has to solely focus on the object's appearance to accurately predict child code c. Once φ p and φ c are trained, we use them to extract features for each real image x i ∈ X . Finally, we use their concatenated features to group the images with k-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first evaluate FineGAN's ability to disentangle and generate images of fine-grained object categories. We then evaluate FineGAN's learned features for fine-grained object clustering with real images.</p><p>Datasets and implementation details. We evaluate on three fine-grained datasets: (1) CUB <ref type="bibr" target="#b46">[47]</ref>: 200 bird classes. We use the entire dataset (11,788 images); (2) Stanford Dogs <ref type="bibr" target="#b27">[28]</ref>: 120 dog classes. We use its train data <ref type="bibr">(12,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-grained image generation</head><p>We first analyze FineGAN's image generation in terms of realism and diversity. We compare to:</p><p>• Simple-GAN: Generates a final image (C) in one shot without the parent and background stages. Only has L c adv loss at the child stage. This baseline helps gauge the importance of disentanglement learned by L c inf o . For fair comparison, we use FineGAN's backbone architecture. • InfoGAN <ref type="bibr" target="#b8">[9]</ref>: Same as Simple-GAN but with additional L c inf o . This baseline helps analyze the importance of hierarchical disentanglement between background, shape, and appearance during image generation, which is lacking in InfoGAN. N c is set to be the same as FineGAN for each dataset. We again use FineGAN's backbone architecture. • LR-GAN <ref type="bibr" target="#b51">[52]</ref>: It also generates an image stagewise, which is similar to our approach. But its stages only consist of foreground and background, and that too controlled by single random vector z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Mask</head><p>Parent Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Child Mask</head><p>Child Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB Birds</head><p>Stanford Cars Stanford Dogs <ref type="figure">Figure 3</ref>. FineGAN's stagewise image generation. Background stage generates a background which is retained over the child and parent stages. Parent stage generates a hollow image with only the object's shape, and child stage fills in the appearance to complete the image.</p><p>• StackGAN-v2 <ref type="bibr" target="#b56">[57]</ref>: Its unconditional version generates images at multiple scales with L c adv at each scale. This helps gauge how FineGAN fares against a state-of-the-art unconditional image generation approach.</p><p>For LR-GAN and StackGAN-v2, we use the authors' publicly-available code. We evaluate image generation using Inception Score (IS) <ref type="bibr" target="#b41">[42]</ref> and Frechet Inception Distance (FID) <ref type="bibr" target="#b19">[20]</ref>, which are computed on 30K randomly generated images (equal number of images for each child code c), using an Inception Network fine-tuned on the respective datasets <ref type="bibr" target="#b1">[2]</ref>. We evaluate on 128 x 128 generated images for all methods except LR-GAN, for which 64 x 64 generated images give better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Quantitative evaluation of image generation</head><p>FineGAN obtains Inception Scores and FIDs that are favorable when compared to the baselines (see <ref type="table">Table 1</ref>), which shows it can generate images that closely match the real data distribution.</p><p>In particular, the lower scores by Simple-GAN, LR-GAN, and StackGAN-v2 show that relying on a single adversarial loss can be insufficient to model fine-grained details. Both FineGAN and InfoGAN learn to associate a c code to a variation factor (L c inf o ) to generate more detailed images. But by further disentangling the background and object shape (parent), FineGAN learns to generate more diverse images. LR-GAN also generates an image stagewise but we believe it has lower performance as it only separates foreground and background, which appears to be insufficient for capturing fine-grained details. These results strongly suggest that FineGAN's hierarchical disentanglement is important for better fine-grained image generation. the number of children fixed (200). IS remains consistently high unless we have very small (N p =5) or large (N p =40) number of parents. With very small N p , we limit diversity in the number of object shapes, and with very high N p , the model has less opportunity to take advantage of the implicit hierarchy in the data. With variable number of children per parent (N p =mixed: 6 parents with 5 children, 3 parents with 20 children, and 11 parents with 10 children), IS remains high, which shows there is no need to have the same number of children for each parent. These results show that FineGAN is robust to a wide range of parent choices.  the parent image (3rd row) together with the background. The final child stage produces a more detailed mask (4th row) and the final composed image (last row), which has the same foreground shape as that of the parent image with added texture/color details. Note that the generation of accurate masks at each stage is important for the final composed image to retain the background, and is obtained without any mask supervision during training. We present additional quantitative analyses on the quality of the masks in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How sensitive is FineGAN to the number of parents?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Qualitative evaluation of image generation</head><p>Disentanglement of factors of variation. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the discovered grouping of parent and child codes by Fine-GAN. Each row corresponds to different instances with the same child code. Two observations can be made as we move left to right: (i) there is a consistency in the appearance and shape of the foreground objects; (ii) background changes slightly, giving an impression that the background across a row belongs to the same class, but with slight modifications. For each dataset, each set of three rows corresponds to three distinct children of the same parent, which is evident from their common shape. Notice that different child codes for the same parent can capture fine-grained differences in the appearance of the foreground object (e.g., dogs in the third row differ from those in first only because of small brown patches; similarly, birds in the 7th and last rows differ only in their wing color). Finally, the consistency in object viewpoint and pose along each column shows that FineGAN has learned to associate z with these factors.</p><p>Disentanglement of parent vs. child. <ref type="figure">Fig. 5</ref> further analyzes the disentanglement of parent (shape) and child code (appearance). Across the rows, we vary parent code p while keeping child code c constant, which changes the bird's shape but keeps the texture/color the same. Across the columns, we vary child code c while keeping parent code p constant, which changes the bird's color/texture but keeps the shape the same. This result illustrates the control that FineGAN has learned without any corresponding supervision over the shape and appearance of a bird. Note that we keep background code b to be same across each column.</p><p>Disentanglement of background vs. foreground. The figure below shows disentanglement of background from object. In (a), we keep background code b constant and vary the parent and child code, which generates different birds same child code, varying parent code same parent code, varying child code <ref type="figure">Figure 5</ref>. Disentanglement of parent vs. child codes. Shape is retained over the column, appearance is retained over the row.</p><p>over the same background. In (b), we keep the parent and child codes constant and vary the background code, which generates an identical bird with different backgrounds. Comparison with InfoGAN. In InfoGAN <ref type="bibr" target="#b8">[9]</ref>, the latent code prediction is based on the complete image, in contrast to FineGAN which uses the masked foreground. Due to this, InfoGAN's child code prediction can be biased by the background (see <ref type="figure">Fig. 6</ref>). Furthermore, InfoGAN <ref type="bibr" target="#b8">[9]</ref> does not hierarchically disentangle the latent factors. To enable InfoGAN to model the hierarchy in the data, we tried conditioning its generator on both the parent and child codes, and ask the discriminator to predict both. This improves performance slightly (IS: 48.06, FID: 12.84 for birds), but is still worse than FineGAN. This shows that simply adding a parent code constraint to InfoGAN does not lead it to produce the hierarchical disentanglement that FineGAN achieves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-grained object category discovery</head><p>We next evaluate FineGAN's learned features for clustering real images into fine-grained object categories. We compare against the state-of-the-art deep clustering approaches JULE <ref type="bibr" target="#b52">[53]</ref> and DEPICT <ref type="bibr" target="#b14">[15]</ref>. To make them even more competitive, we also create a JULE variant with ResNet-50 backbone (JULE-ResNet-50) and DE-PICT with double the number of filters in each conv layer (DEPICT-Large). We use code provided by the authors. All methods cluster the same image regions.</p><p>For evaluation we use Normalized Mutual Information (NMI) <ref type="bibr" target="#b49">[50]</ref> and Accuracy (of best mapping between clus- <ref type="figure">Figure 6</ref>. InfoGAN results. Images in each group have same child code. The birds are the same, but so are their backgrounds. This strongly suggests InfoGAN takes background into consideration when categorizing the images. In contrast, FineGAN's generated images <ref type="figure" target="#fig_2">(Fig. 4)</ref>   <ref type="table">Table 3</ref>. Our approach outperforms existing clustering methods.</p><p>ter assignments and true labels) following <ref type="bibr" target="#b14">[15]</ref>. Our approach outperforms the baselines on all three datasets (see <ref type="table">Table 3</ref>). This indicates that FineGAN's features learned for hierarchical image generation are better able to capture the fine-grained object details necessary for fine-grained object clustering. JULE and DEPICT are unable to capture those details to the same extent; instead, they focus more on highlevel details like background and rough shape (see supp. for examples). Increasing their capacity (JULE-RESNET-50 and DEPICT-Large) gives little improvement. Finally, if we only use our child code features, then performance drops (0.017 in Accuracy on birds). This shows that the parent code and child code features are complementary and capture different aspects (shape vs. appearance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and limitations</head><p>There are some limitations of FineGAN worth discussing. First, although we have shown that our model is robust to a wide range of number of parents <ref type="table">(Table 2)</ref>, it along with the number of children are hyperparameters that a user must set, which can be difficult when the true number of categories is unknown (a problem common to most unsupervised grouping methods). Second, the latent modes of variation that FineGAN discovers may not necessarily correspond to those defined/annotated by a human. For example, our results in <ref type="figure" target="#fig_2">Fig. 4</ref> for cars show that the children are grouped based on color rather than car model type. This highlights the importance of a good evaluation metric for unsupervised methods. Third, in our current implementation, FineGAN requires bounding boxes to model the background. In preliminary experiments, we observe that approximating the background with patches lying along the border of real images also gives reasonable results. Finally, while we significantly outperform unsupervised clustering methods, we are far behind fully-supervised fine-grained recognition methods. Nonetheless, we feel that this paper has taken important initial steps in tackling the challenging problem of unsupervised fine-grained object modeling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Qualitative clustering results</head><p>In this section, we visualize the clusters formed using FineGAN's learned features, JULE <ref type="bibr" target="#b52">[53]</ref>, and DEPICT <ref type="bibr" target="#b14">[15]</ref>. JULE performs alternating deep representation learning and clustering whereas DEPICT minimizes a relative entropy loss with a regularization term for balanced clustering.</p><p>In <ref type="figure" target="#fig_5">Fig. 7</ref>, each row corresponds to members of a cluster formed by JULE and DEPICT. We can see that these clusters are mainly formed on the basis of background and object shape rather than fine-grained object similarity. For example, the clusters formed by JULE in the first and third rows have consistent blue and green background, respectively. Similarly, the clusters in the second row of JULE and first and third rows of DEPICT have consistent object shape and pose. Overall, these qualitative results reflect the worse <ref type="figure">Figure 8</ref>. Our clusters. Each row corresponds to a cluster. We can see that each cluster captures fine-grained appearance details. Also, the background varies for each cluster which shows that the clustering does not significantly dependent on background. quantitative performance of JULE and DEPICT shown in <ref type="table">Table 3</ref> of the main paper.</p><p>Next, in <ref type="figure">Fig. 8</ref>, we show our clusters formed using FineGAN's learned features (obtained by concatenating the penultimate layer outputs of φ p and φ c ); again, each row corresponds to members of a cluster. Here, many clusters have a representative shape and appearance, which correspond to a true fine-grained category. For example, the cluster in the first row has birds which have consistent shape with a red body and black stripe. Also, the background varies within each cluster which indicates that background is not used as a significant cue for clustering. These results show that FineGAN's learned features are able to better capture fine-grained details. To further investigate, we analyzed specific classes in CUB <ref type="bibr" target="#b46">[47]</ref> which only differ at the fine-grained level that our approach is able to discover as separate clusters. Example classes are shown in <ref type="figure">Fig. 9</ref>. Our approach creates clusters in which the majority of images of 'Summer Tanager' and 'Vermilion Flycatcher' are placed in separate clusters. We observe similar results for the 'Red Bellied Woodpecker' and 'Red Cockaded Woodpecker' classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vermilion Flycatcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summer Tanager</head><p>Red Bellied Woodpecker Red Cockaded Woodpecker <ref type="figure">Figure 9</ref>. Fine-grained clusters. Our approach is able to group classes which only vary at the fine-grained level into different clusters. For example, 'Summer Tanger' and 'Vermillion Flycatcher' which only differ by a black stripe on their body are grouped into different clusters. Similarly, 'Red Bellied Woodpecker' and 'Red Cockaded Woodpecker' are grouped into different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Quantitative evaluation of mask generation</head><p>In <ref type="figure">Fig. 3</ref> of the main paper, we show the masks formed by FineGAN at the parent and child stages. In this section, we quantitatively evaluate the quality of the masks.</p><p>We first train a network which takes our final generated image C as input and predicts the corresponding parent mask which we obtained automatically as part of the generation process. We then use this network to predict the foreground mask on real images. To evaluate, we use the entire CUB dataset <ref type="bibr" target="#b46">[47]</ref>, which has ground-truth foreground masks for birds. Our model obtains a high 75.6% mIU <ref type="bibr" target="#b35">[36]</ref> when compared against the ground-truth segmentation masks. This shows the accuracy of the masks produced by FineGAN. Next, we measure how much of the parent mask is retained at the child stage. Ideally, a high percentage of the parent mask should be retained at the child stage, because our hypothesis is that the child stage should focus on generating the object's appearance conditioned on the shape produced in the parent stage. To measure this, we compute the intersection between the binarized parent and child masks, and normalize the result by the child mask area. This value is 0.70; i.e., a large portion of the changes at the child stage is within the parent mask. This also shows that most of the background is retained at the child stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attempting to model hierarchy with Info-GAN</head><p>As explained in "Comparison with InfoGAN" in the main paper, conditioning InfoGAN's generator on both the parent and child codes and asking its discriminator to predict both only results in minor improvement in image generation (IS: 48.06, FID: 12.84 for birds), that is still worse than FineGAN. <ref type="figure">Fig. 10</ref> shows qualitative results. Here, all four child groups have the same parent code, but do not seem to share any meaningful attribute (shape, color, or background). This shows that simply adding a parent code constraint to InfoGAN does not lead it to produce the hierarchical disentanglement that FineGAN achieves. <ref type="figure">Figure 10</ref>. Images generated by an InfoGAN model trained to predict both parent and child codes. Here all four child groups have the same parent, but do not share a noticeable common pattern (e.g., consistent in either shape, color, or background). <ref type="figure">Figure 11</ref>. Relaxing the latent code relationship constraints during inference. The generated ducks have unusual green/leafy backgrounds. The first image also has a rare color considering its shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Relaxing relationship constraints during inference</head><p>In Section 3.1 of the main paper, we described the constraints that we impose during training on the relationships between background, parent, and child codes to facilitate the desired entanglement. In this section, we qualitatively reason about the importance of these relationships, and show how these constraints can be relaxed during inference. <ref type="figure">Fig. 11</ref> shows FineGAN's generated images when the background code (b) corresponding to a green/leafy background is chosen along with the parent code (p) corresponding to a duck-like category. During training, D adv would easily categorize these as fake images since the ducks in the CUB dataset mostly appear in water-like blue backgrounds. Furthermore, the first image in <ref type="figure">Fig. 11</ref> illustrates the need for tying a fixed number of child codes to the same parent code. The combination of blue, green, and red texture is rare for duck-like birds in CUB; and again D adv could use this to categorize the image as fake. However, during inference, we can relax these constraints as the generator no longer needs to fool the discriminator.</p><p>FineGAN can easily generate images like these with 'inconsistent' background and texture, only because it has learned a disentangled representation of these factors, and can hence compose an image by choosing any combination of the factors. <ref type="figure">Fig. 5</ref> in the main paper shows additional examples (e.g., a red seagull).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Combining factors from multiple real images</head><p>We show additional examples of fusing different factors of variation from multiple real images to synthesize a new image. Specifically, we use φ c , φ p , and similarly-trained φ b , to compute c, p, and b codes respectively for different real images. We then input these codes to FineGAN to generate an image with their combined factors. <ref type="figure" target="#fig_8">Fig. 12</ref> shows images generated in this way. The 4th column are generated images that have the background of the 1st column real image, shape of the 2nd column real image, and appearance of the 3rd column real image. This application of FineGAN could potentially be useful for artists who want to generate new images by combining factors from multiple visual examples.  Discriminative modules. D b consists of four convolutional layers, with leaky Relu used as the non-linearity except last convolutional layer, for which we use sigmoid nonlinearity to output a 24 x 24 activation map of 0/1 (real/fake) scores. The network design is chosen such that each member from the 24 x 24 matrix represents the score for a 34 x 34 receptive field in the input image. We only consider background patches which lie completely outside the detected bounding box. D aux shares all convolutional layers with D b except the last one, where it branches out with a separate convolutional layer, again giving a 24 x 24 activation map of 0/1 scores indicating background/foreground classification. D p consists of eight convolutional layers, with all except last followed by BatchNorm and leaky Relu layers. This network outputs an N p dimensional parent class distribution. D c has an identical architecture as that of D p , except it outputs a N c dimensional child class distribution. Similar to the background stage, D adv shares all convolutional layers except the last two, where it branches out using a separate convolutional layer which outputs a single 0/1 scalar value indicating the real/fake score for the final child image C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional image generation visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Training details</head><p>We optimize FineGAN using Adam with learning rate 2 x 10 −4 , β 1 = 0.5, β 2 = 0.999. We use a minibatch of 16 images and train for 600 epochs. Following StackGAN <ref type="bibr" target="#b56">[57]</ref>, we crop all images to 1.5× their available bounding boxes.</p><p>Hard negative training Upon complete training of our G/D models, we find that some noise vectors z = {z 1 , z 2 , . . . } result in degenerate images. This property is characteristic of z, and not of any other latent code; i.e. I = G(z, b, p, c) is a degenerate image ∀ b ∼ p b , p ∼ p p , c ∼ p c if z ∈ z. We also find that almost all of these degenerate images have very low scores predicted for the activated class corresponding to the input child code; i.e., D c (c|G c (z, b, p, c)) ≈ 0. We exploit this observation and continue training the G/D models for a further 1-2 epochs. This time we train alternatively on a normal batch of images and on a batch of degenerate images, which is formed using images I i = G c (z i , b, p, c) for which D c (c|I i ) values are low. More specifically, we choose 16 (same as minibatch size) lowest scoring I i out of 160 randomly generated I i . This way of hard negative training alleviates the problem of degenerate images to a great extent. Note that this is not something that can be applied to standard unconditional GANs as they do not produce class-conditional scores. Although the same technique can be used for Info-GAN, in our experiments, it did not result in any significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Mask</head><p>Parent Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Child Mask</head><p>Child Image <ref type="figure">Figure 13</ref>. FineGAN's stagewise image generation for CUB. Background stage generates a background which is retained over the child and parent stages. Parent stage generates a hollow image with only the object's shape, and child stage fills in the appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Mask</head><p>Parent Image Child Mask Child Image <ref type="figure" target="#fig_2">Figure 14</ref>. FineGAN's stagewise image generation for dogs. Background stage generates a background which is retained over the child and parent stages. Parent stage generates a hollow image with only the object's shape, and child stage fills in the appearance. same child code same parent code same z vector <ref type="figure">Figure 15</ref>. Varying p vs. c vs. z for CUB. We show 6 different parent groups (with different p's), each with 4 children (with different c's). For the same parent, the object's shape remains consistent while the appearance changes with different child codes. For the same child, the appearance remains consistent. Each column has the same random vector z -we can see that it controls the object's pose and position. same child code same parent code same z vector <ref type="figure">Figure 16</ref>. Varying p vs. c vs. z for dogs. We show 6 different parent groups (with different p's), each with 4 children (with different c's). For the same parent, the object's shape remains consistent while the appearance changes with different child codes. For the same child, the appearance remains consistent. Each column has the same random vector z -we can see that it controls the object's pose and position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parent Mask</head><p>Parent Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Child Mask</head><p>Child Image <ref type="figure" target="#fig_5">Figure 17</ref>. FineGAN's stagewise image generation for cars. Background stage generates a background which is retained over the child and parent stages. Parent stage generates a hollow image with only the object's shape, and child stage fills in the appearance. same parent code same z vector same child code <ref type="figure">Figure 18</ref>. Varying p vs. c vs. z for cars. We show 4 different parent groups (with different p's), each with 3 children (with different c's). For the same parent, the object's shape remains consistent while the appearance changes with different child codes. For the same child, the appearance remains consistent. Each column has the same random vector z -we can see that it controls the object's pose and position.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>000 images); (3) Stanford Cars<ref type="bibr" target="#b29">[30]</ref>: 196 car classes. We use its train data (8,144 images). We do not use any of the provided labels for training. The labels are only used for evaluation. Number of parents and children are set as: (1) CUB: N p = 20, N c = 200; (2) Stanford dogs: N p = 12, N c = 120; and (3) Cars: N p = 20, N c = 196. N c matches the groundtruth number of fine-grained classes per dataset. We set λ = 10, β = 1 and γ = 1 for all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We next qualitatively analyze FineGAN's (i) image generation process; (ii) disentanglement of the factors of variation; and provide (iii) in-depth comparison to InfoGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Varying p vs. c vs. z. Every three rows correspond to the same parent code p and each row has a different child code c. For the same parent, the object's shape remains consistent while the appearance changes with different child codes. For the same child, the appearance remains consistent. Each column has the same random vector z -we see that it controls the object's pose and position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Fixed b, varying p and c (b) Fixed p and c, varying b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>JULE and DEPICT clusters. Each row corresponds to a cluster. The clusters are mainly formed on the basis of non fine-grained elements like background and rough shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Finally</head><label></label><figDesc>, we show additional qualitative results that supplement Figs. 3 and 4 in the main paper. In Figs. 13, 14, and 17, we show the stagewise image generation results for CUB, dogs, and cars respectively. Figs. 15, 16, and 18 show the discovered grouping of parent and child codes by Fine-GAN for CUB, dogs, and cars respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7 .</head><label>7</label><figDesc>Architecture and training details 7.1. FineGAN architecture Generative modules. G b consists of six convolutional layers with BatchNorm [25] and nearest neighbor upsampling applied after each convolutional layer except the last one. G p has an identical initial architecture as that of G b . The intermediate feature representation obtained is concatenated with the parent code p, similar to StackGAN [57]. This representation is then passed through a residual block and a pair of convolutional layers, which gives us F p in Fig.2 of the main paper. G p,f and G p,m each consist of a single convolutional layer to transform the feature representation to have a resolution that matches the output image. Similar to the later part of G p , G c consists of a residual block and a pair of convolutional layers which transform the concatenation of F p and child code c into F c . Each of G c,f and G c,m , similar to G p,f and G p,m , consist of a single convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Combining factors from multiple real images. We can generate fake images (fourth column) which combine the background (first column), shape (second column), and appearance (third column) of real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 showsTable 1 .</head><label>21</label><figDesc>the Inception Score (IS) on CUB of Fine-GAN trained with varying number of parents while keeping ± 0.77 43.16 ± 0.42 28.62 ± 0.44 13.20 29.34 17.63 LR-GAN [52] 13.50 ± 0.20 10.22 ± 0.21 5.25 ± 0.05 34.91 54.91 88.80 StackGANv2 [57] 43.47 ± 0.74 37.29 ± 0.56 33.69 ± 0.44 13.60 31.39 16.28 FineGAN (ours) 52.53 ± 0.45 46.92 ± 0.61 32.62 ± 0.37 11.25 25.66 16.03 Inception Score (higher is better) and FID (lower is better). FineGAN consistently generates diverse and real images that compare favorably to those of state-of-the-art baselines. CUB) 52.53 52.11 49.62 46.68 51.</figDesc><table><row><cell></cell><cell></cell><cell>IS</cell><cell></cell><cell>FID</cell></row><row><cell></cell><cell>Birds</cell><cell>Dogs</cell><cell>Cars</cell><cell>Birds Dogs Cars</cell></row><row><cell>Simple-GAN</cell><cell cols="4">31.85 ± 0.17 6.75 ± 0.07 20.92 ± 0.14 16.69 261.85 33.35</cell></row><row><cell>InfoGAN [9]</cell><cell cols="4">47.32 Np=20 Np=10 Np=40 Np=5 Np=mixed</cell></row><row><cell cols="2">Inception Score (</cell><cell></cell><cell></cell></row></table><note>83 Table 2. Varying number of parent codes Np, with number of chil- dren Nc fixed to 200. FineGAN is robust to a wide range of Np.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for same c show reasonable variety in background.</figDesc><table><row><cell></cell><cell>NMI</cell><cell>Accuracy</cell></row><row><cell></cell><cell cols="2">Birds Dogs Cars Birds Dogs Cars</cell></row><row><cell>JULE [53]</cell><cell cols="2">0.204 0.142 0.232 0.045 0.043 0.046</cell></row><row><cell cols="3">JULE-ResNet-50 [53] 0.203 0.148 0.237 0.044 0.044 0.050</cell></row><row><cell>DEPICT [15]</cell><cell cols="2">0.290 0.182 0.329 0.061 0.052 0.063</cell></row><row><cell>DEPICT-Large [15]</cell><cell cols="2">0.297 0.183 0.330 0.061 0.054 0.062</cell></row><row><cell>Ours</cell><cell cols="2">0.403 0.233 0.354 0.126 0.079 0.078</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Image generation process.Fig. 3shows the intermediate images generated for CUB, Stanford Cars, and Stanford Dogs. The background images (1st row) capture the context of each dataset well; e.g., they contain roads for cars, gardens or indoor scene for dogs, leafy backgrounds for birds. The parent stage produces parent masks that capture each object's shape (2nd row), and a textureless, hollow entity as</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by NSF IIS-1751206, IIS-1748387, AWS ML Research Award, Google Cloud Platform research credits, and GPUs donated by NVIDIA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Here, we provide qualitative clustering results, additional quantitative analysis on the quality of the generated masks, and architecture and training details. We also provide additional clustering and image generation results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poof: Part-based onevs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semisupervised fusedgan for conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained recognition in the wild: A multi-task domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Kamran Ghasedi Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of categories from sets of partially matching image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation by mixing them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Szab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generating images with recurrent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Dongjoo</forename><surname>Daniel Jiwoong Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memisevic</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1602.05110" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05387</idno>
		<title level="m">Generating images part by part with composite generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of categories from sets of partially matching image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning the easy things first: Self-paced visual category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of visual object class hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weaklysupervised discriminative patch learning via cnn for finegrained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Lr-gan: Layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combining randomization and discrimination for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shaoting Zhang, Ahmed Elgammal, and Dimitris Metaxas. Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10916</idno>
		<title level="m">Xiaolei Huang, and Dimitris Metaxas. Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ross Girshick, and Trevor Darrell. Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ian Reid, and Anton van den Hengel. Parallel attention: A unified framework for visual object discovery through dialogs and queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
