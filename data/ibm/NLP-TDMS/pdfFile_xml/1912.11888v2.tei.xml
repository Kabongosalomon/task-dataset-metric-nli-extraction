<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">W-PoseNet: Dense Correspondence Regularized Pixel Pair Pose Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">W-PoseNet: Dense Correspondence Regularized Pixel Pair Pose Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a novel 6D pose estimation algorithm -W-PoseNet, which directly regresses object rotation and translation from a sparse set of pixel pair representations, via a low-rank bilinear pooling on dense features of input RGB-D images. Moreover, those pixel-wise deep features are regularized by explicitly learning a dense correspondence mapping onto their 3D coordinates in a canonical space as an auxiliary task, which can thus improve robustness against ambiguities caused by pose symmetries and inter-object occlusion. Experiment results on two popular benchmarks show that our W-PoseNet consistently achieves state-of-the-art performance on 6D pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The problem of six degree-of-freedom pose (simply put, 6D pose) estimation aims to predicting a rotation together with a translation of an object instance in 3D space relative to a canonical CAD model, which plays a vital role in a number of applications such as augmented reality <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, grasp and manipulation in robotics <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and 3D semantic analysis <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This paper concerns on the problem of estimating 6D pose of an object given a RGB-D image. Such a problem remains challenging in view of intrinsic inconsistent texture and shape of objects and inter-object occlusion in the cluttered scenes, in addition to extrinsic varying illumination and sensor noises. In light of this, encoding a discriminative and robust feature representation for each instance is essential for predicting its 6D pose.</p><p>Most of existing RGB-D methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> depend on iterative-closest point (ICP <ref type="bibr" target="#b12">[13]</ref>) to refine pose predictions, which leads to less efficient inference compared to learning-based refinement (such as the refinement network adopted in <ref type="bibr" target="#b13">[14]</ref> achieving hundreds of magnitude faster) and thus can be less favorable for real-time applications such as robotic grasp and planning. DenseFusion <ref type="bibr" target="#b13">[14]</ref> has recently been developed to combine both textural and geometric features in a pixel-wise fusion manner, which can obtain accurate pose estimation in a real-time inference processing. Cheng et al. <ref type="bibr" target="#b14">[15]</ref> further exploit both intra-modality and <ref type="figure">Fig. 1</ref>. Visualization of our W-PoseNet in comparison with its competitor DenseFusion <ref type="bibr" target="#b13">[14]</ref>. The key differences lie in that our W-PoseNet introduces pixel pair pose estimation based on a low-rank bilinear pooling (LRBP, highlighted in a green block) and dense correspondence mapping (DCM) from each pixel to its 3D coordinate (highlighted in yellow blocks), whose geometric structure is similar to the font W highlighted in orange. Illustrative examples at the bottom rows are from the YCB-Video <ref type="bibr" target="#b9">[10]</ref>.</p><p>inter-modality correlation to learn more discriminative local feature based on DenseFusion <ref type="bibr" target="#b13">[14]</ref>. However, these state-ofthe-art dense pose regressors heavily depend on the quality and resolution of the acquired data and also suffer from pixelwise feature ambiguities caused by pose symmetries.</p><p>We consider that extracting discriminative textural and geometric features in local regions and generating a robust global representation of each object instance are important to alleviate the aforementioned challenges in 6D pose estimation. To this end, different from pixel-wise regression methods such as DenseFusion <ref type="bibr" target="#b13">[14]</ref> or Correlation Fusion <ref type="bibr" target="#b14">[15]</ref>, this paper is the first attempt to explore pixel-pair pose regression -W-PoseNet by developing two novel components of in the proposed deep model: a joint loss function and a W-shape pose regression module, which is shown in <ref type="figure">Fig. 1</ref>.</p><p>On one hand, beyond one loss term as other pixel-wise pose regressors <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> on minimizing pose predictions and their corresponding ground truth, the proposed W-PoseNet additionally encodes the textural and geometric information favoring for reconstructing pose-sensitive point sets, whose points' 3D coordinates are generated by transforming the observed point cloud sampling from object models (e.g. CAD models) with the inverse of ground truth pose. In detail, our W-PoseNet incorporates an auxiliary task of dense correspondence mapping from each pixel of input data to its corresponding coordinate in the canonical space, in order to regularize local feature learning for pose regression by extra pixel-wise supervision self-generated from object models and ground truth pose. Intuitively, compared to sparse semantic keypoints pre-defined in keypoint-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, dense correspondence mapping in our scheme treats each pixel as a keypoint to regress its corresponding 3D coordinate in object model space, which makes each pixel-wise feature more discriminative, and thus our pixel-wise feature encoding is more robust to occlusion.</p><p>On the other hand, inspired by the point-pair feature <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> on point clouds, this paper proposes a novel pixel pair feature encoding layer based on the low-rank bilinear pooling <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> to generate pixel-pair features from two sampled pixels' vectors in feature map output of the encoder, which are then mapped onto 6-DoF pose. Similar to the concept in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, our pixel-pair features describe the relative geometric structure of two sampled 3D points on the object models, but also contain texture information of their 2D projection on input RGB images. Moreover, local features anchored on single pixels concern on extracting textural and geometric information in local regions, which can be less discriminative to some symmetric poses. Such an observation encourages our motivation to combine pixel-wise features into a set of pixel-pair features. A global representation for each instance, which combines a number of pixel-pairs' features, thus generates a pose prediction.</p><p>The main contributions of this paper are three-fold.</p><p>• This paper is the first attempt to introduce the concept of a pair-wise combination of dense features to 6D pose estimation. To this end, this paper proposes a novel pixel-pair pose regression network -W-PoseNet, which aims to learn a global representation of each object instance consisting of sparse pixel-pair features (PiPF). • This paper designs an auxiliary task of dense correspondence mapping from input data to 3D coordinates in object model space sensitive to 6D pose, which can regularize feature encoding in deep pose regression to improve pixel-wise local feature discrimination. • Extensive experiments are conducted on the popular benchmarks, whose results can demonstrate that the proposed W-PoseNet consistently outperforms the state-ofthe-art 6D pose estimation algorithms.</p><p>Source codes and pre-trained models will be released after acceptance at https://github.com/xzlscut/ W-PoseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Keypoint-based 6D Pose Estimation -The algorithms belonging to keypoint-based pose estimation share a twostage pipeline: first localizing 2D projection of predefined 3D keypoints and then generate pose predictions via 2D-to-3D keypoint correspondence mapping with a PnP <ref type="bibr" target="#b25">[26]</ref>. Existing methods can be categorized into two groups: object detection based <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref> and dense heatmap based <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The former focuses on solving the problem of sparse keypoint localization via object detection on the whole object to reduce negative effects of background <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, but are sensitive to occlusion <ref type="bibr" target="#b16">[17]</ref>. The latter group of methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> pay more attention to discovering latent correlation across all the keypoints and thus are more robust to inter-object occlusion. Recently, PVNet <ref type="bibr" target="#b19">[20]</ref> is proposed to detect keypoints via voting on pixel-wise predictions of the directional vector that points to keypoints and is robust to truncation and occlusion. In contrast, dense correspondence mapping in our method share similar concepts as keypoint-based methods but makes pixel-wise predictions on 3D keypoints directly instead of their projection in 2D images, which performs robustly to handle with occlusion. Dense 6D Pose Estimation -An alternative group of algorithms is to produce dense pose predictions for each pixel or local patches with hand-crafted features <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, CNN patch-based feature encoding <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b7">[8]</ref> and CNN pixelbased feature encoding <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, whose final pose output is selected via a voting scheme. In <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, random forests are adopted to regress 3d coordinates for each pixel and discover 2D-3D correspondence to produce pose predictions. The proposed W-PoseNet is designed to both predict poses from dense features and generate pose-sensitive coordinates in a unique framework. In general, the dense feature encoding module in our method follows the same pipeline as <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, but the key difference lies in incorporating an extra branch to constrain feature encoding with dense correspondence mapping from input data to its corresponding 3D coordinates in a canonical space.</p><p>Bilinear Pooling in CNNs -The bilinear pooling <ref type="bibr" target="#b32">[33]</ref> is an effective tool to calculate second-order statistics of local features in visual recognition but suffers from expensive computational cost due to its high-dimensionality. A number of its variants have been proposed to cope with the challenge via approximation with Random Maclaurin <ref type="bibr" target="#b33">[34]</ref> or Tensor Sketch <ref type="bibr" target="#b34">[35]</ref>. A low-rank bilinear model is proposed by Kong et al. <ref type="bibr" target="#b22">[23]</ref> to avoid computing the bilinear feature matrix, which is approximated via two decomposed lowrank matrices. Similarly, <ref type="bibr" target="#b23">[24]</ref> introduces Grassmann Pooling based on SVD decomposition to approximate feature maps. A quadratic transformation with the low-rank constraint is exploited on pairwise feature interaction either from spatial locations <ref type="bibr" target="#b35">[36]</ref> or from feature maps across network layers <ref type="bibr" target="#b36">[37]</ref>. Our pixel-pair pose regression shares a similar concept as bilinear models to construct pair-wise features, but a sparsely sampled set of pixel-pair features in W-PoseNet can achieve more robust estimation, owing to the pair-wise combination of spatially localized features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The problem of 6D pose estimation given RGB-D images is to detect object instances in the scenes and estimate their rotation R ∈ SO(3) and translation t ∈ R 3 . In detail, a 6D pose can be defined as a rigid transformation p = [R|t] from the object coordinate system with respect to the camera coordinate system. This paper aims to improve the discrimination and robustness of a global representation of each object instance having large variations of texture and shape, from the perspective of pairwise feature interaction on dense local features. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the whole pipeline of our W-PoseNet, which consists of several main stages in addition to iterative pose refinement as DenseFusion <ref type="bibr" target="#b13">[14]</ref> (see Sec. III-D). Encouraged by the recent success of dense pose regression on RGB-D data, we employ the same feature encoding and fusion part of DenseFusion <ref type="bibr" target="#b13">[14]</ref> to extract and fuse pixelwise features from heterogeneous data (see Sec. III-A). Pixelwise features are sampled to generate a sparse set of pixelpair features via a low-rank bilinear pooling on local features, which produces a global feature via a max pooling and its pose prediction (see Sec. III-B). For robust pixel-wise features, an auxiliary task of learning a dense correspondence mapping from each pixel to its 3D coordinate together with pixel-pair pose regression is designed in Sec. III-C. For testing, given an input RGB-D image, the proposed W-PoseNet produces a pose prediction, which as the output of our W-PoseNet is then fed into the iterative pose refinement network to generate a final pose prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation and Pixel-Wise Feature Encoding</head><p>We adopt the identical modules of the DenseFusion <ref type="bibr" target="#b13">[14]</ref> and briefly introduce the following steps: 1) semantic segmentation on RGB images and point clouds converted by depth images; 2) dense feature extraction and 3) pixel-wise feature fusion, which is shown in the top row of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Following the segmentation algorithm adopted in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b9">[10]</ref>, an RGB image is first fed into the autoencoderbased segmentation network to produce N + 1 binary masks belonging to N object classes and the background class respectively. The bounding box of each mask is used to crop the corresponding depth image, and the depth intensities within the object mask are converted to a point cloud.</p><p>Cropped image patches and point clouds are fed into 2D CNN based and PointNet <ref type="bibr" target="#b37">[38]</ref> based feature encoders to respectively extract texture and geometric features from heterogeneous data sources. Specifically, given H ×W RGB-D images, the texture branch aims to mapping H × W × 3 images to H × W × d rgb -dimensional feature maps, which correspond to a feature vector f ∈ R drgb at each spatially localized pixel. The other branch extracts geometric features g from orderless points via two shared-MLP and produces P × d depth -dimensional feature maps where P denotes the size of sampled points in point cloud generated from the depth image.</p><p>We combine textural features f from those pixels in cropped RGB image patches corresponding to these P points with g in the manner of pixel-wise fusion. Specifically speaking, to capture correlation across RGB and depth modalities, geometric and appearance features belonging to the same pixels are first concatenated and then fed into a MLP-based feature fusion network (i.e. PointNet++ <ref type="bibr" target="#b24">[25]</ref>) to generate P pixel-wise fusion feature x ∈ R dfusion . Instead of concatenating the fused feature with a global feature obtained by average pooling as in the DenseFusion <ref type="bibr" target="#b13">[14]</ref>, we adopted the hierarchical pooling of PointNet++ <ref type="bibr" target="#b24">[25]</ref> to aggregate local neighboring features. Our motivation for not using global features here is to get rid of the global information about object instances to verify the effectiveness of a combination of local features as a global description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pixel Pair Pose Regression</head><p>This section presents the key components of our pixel-pair pose regression: 1) pixel-pair feature generation; 2) pixel-pair feature encoding; and 3) pose regression, which are shown in the bottom row of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>The seminal work of point-pair feature (PPF) <ref type="bibr" target="#b20">[21]</ref> constructs 4-dimensional features of a relative position in the manner of Euclidean distance and relative angles between normals and between normals and the determined line from a pair of oriented points sampled from an observed 3D scene. Motivated from its elegant design that can precisely determine a pose when the pair of points is sampled from the object of interest, we propose to pair-wisely aggregate the learned pixel-wise features for a pixel-pair feature encoding (PiPE) to alleviate the suffering of dense features from ambiguities caused by pose symmetries and occlusion. Generation on Pixel Pair Features -In 3D domain, local descriptors for point clouds can encode the neighboring geometric structure of each point but suffer from sensor noises and sparse point distribution. In view of this, multiple point-pair features <ref type="bibr" target="#b20">[21]</ref> consisting of relative position and orientation of two oriented points, are collected as a global representation of an object model. Such a feature representation can still perform robustly to occlusion and sparse point clouds, which is widely adopted for practical applications. Inspired by the robustness and low computational cost of point-pair features <ref type="bibr" target="#b20">[21]</ref>, we introduce a novel feature encoding layer inserted between the pose regression module and feature fusion module investigated in the previous section. Specifically, given the set {x} P of P pixel-wise features, we replicate the features into another set. Each feature vector in one set is combined with one randomly selected from the other set, then the P pixels will eventually generate P pixel-pairs. Other combination strategies such as dense pair generation by combining any two pixels can also be employed, but we adopt such a sparse generation owing to its computational simplicity. Pixel-Pair Feature Encoding (PiPE) -Given the output of pixel pairs, a naive approach to combine local features (i.e., x k where k = 1, 2, . . . , P ) into a compact one is direct concatenation, which reveals the 1-order statistical information of local features. An alternative to exploit 1order statistical information is to generate an element-wise sum of two feature vectors. Encouraged by bilinear models in visual recognition <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b23">[24]</ref>, our pixel-pair feature encoding is designed based on a low-rank bilinear pooling <ref type="bibr" target="#b38">[39]</ref>. Specifically, given two pixel-wise feature vectors (e.g. a and b from the set {x} P ), the second-order pooling method <ref type="bibr" target="#b39">[40]</ref> can be written as</p><formula xml:id="formula_0">G PiPE (a, b) = vec(ab T ) ∈ R d 2 fusion .<label>(1)</label></formula><p>In view of high-dimensionality d 2 fusion of ab T , the low-rank bilinear pooling <ref type="bibr" target="#b38">[39]</ref> can be adopted to obtain two low-rank matrices U ∈ R dfusion×l and V ∈ R dfusion×l to avoid computing ab T as</p><formula xml:id="formula_1">G PiPE (a, b) = P T σ(U T a • V T b)<label>(2)</label></formula><p>where • denotes the Hadamard product, P is the project matrix to determine the output dimension, and we follow <ref type="bibr" target="#b38">[39]</ref> use the relu nonlinearity activation function σ(·). For generating a d fusion -dimensional pixel-pair feature (the setting in our experiments for a fair comparison), we compare computational complexities of direct concatenation and projection via a fully-connected layer (2 × d fusion , d fusion ) and a low-rank bilinear pooling. The former desires 2 × d 2 fusion network parameters, while the latter needs 3 × d fusion × l where l denoting the dimension of the compressed vector projected from x is usually much smaller than d fusion . As a result, our method adopts the low-rank bilinear pooling in view of the lighter computational cost and the more richer information than direct concatenation, which is verified in our experiments. Pixel-Pair Pose Regression -All P pixel-pair features G ppf are fed into a max pooling to generate a global representation for 6D pose prediction of each object instance. Specifically, the error of pose predictions of the global representation as follows <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_2">L PR = 1 M j ||(Rx j + t) − (Rx j +t)||,<label>(3)</label></formula><p>where x j denotes the j-th point in a point set randomly sampled from the object model, M is the size of the point set, [R|t] is the ground-truth pose and [R|t] is the predicted pose output by the pixel-pair feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auxiliary Dense Correspondence Mapping</head><p>For obtaining discriminative and robust features per pixels, our method designs an auxiliary dense correspondence mapping task to regularize feature learning. As shown in <ref type="figure" target="#fig_1">Fig.  3</ref>, dense correspondence mapping is achieved via regressing per-pixel features x to its corresponding 3D coordinate in the canonical coordinate system. To this goal, the object point cloud obtained by a depth input image is firstly transformed with the inverse of ground truth pose [R|t] to obtain its corresponding point cloud in the canonical space. Each 3D coordinate in the transformed point cloud is used as a pixel-wise supervision of its corresponding point in the dense correspondence mapping branch. The loss term on this auxiliary branch, termed as a Dense Correspondence Mapping (DCM) Loss, can be thus written as</p><formula xml:id="formula_3">L DCM = 1 M i ||p i −p i ||<label>(4)</label></formula><p>where M represents the points sampled from the observed point cloud,p is the prediction of the DCM branch of our network, and p denotes the ground truth coordinate corresponding top. A joint loss L joint to combine both terms for pose regression and dense correspondence mapping can thus be written as:</p><formula xml:id="formula_4">L joint = L PR + λL DCM<label>(5)</label></formula><p>where λ is the trade-off parameter between two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Iterative Pose Refinement</head><p>Iterative refinement in the DenseFusion <ref type="bibr" target="#b13">[14]</ref> has gained significant performance improvement for 6D pose estimation. Similarly, we adopt its identical refinement post-processing to improve final pose predictions, which encodes the resulting point clouds after transformation with previous pose predictions to gradually refining pose in a residual learning manner. Experimental results can demonstrate the boosted performance of the proposed W-PoseNet together with the refinement network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Datasets -To evaluate W-PoseNet comprehensively, we conduct experiments on two popular benchmarks -the YCB-Video dataset <ref type="bibr" target="#b9">[10]</ref> and the LineMOD dataset <ref type="bibr" target="#b40">[41]</ref>. The YCB-Video dataset consists of 21 objects with different textures and sizes and has 92 videos in total. Following recent work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we adopt 80 videos for training, 2949 keyframes from the other 12 videos for testing, and also used an additional 80,000 synthetic images provided by <ref type="bibr" target="#b9">[10]</ref>. For a fair comparison, we used the semantic segmentation mask provided by the PoseCNN in evaluation by following <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The LineMOD dataset contains 15,783 images belonging to 13 low-textural objects placed under different cluttered environments suffering from the challenges of occlusion and illumination changes. We follow the prior works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> use 15% of images for training and use the other 85% of images for testing without using additional synthetic data.</p><p>Performance Metrics -We adopt the ADDS and ADD(S) metrics in the YCB-Video dataset following recent work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref> for comparative evaluation. For asymmetric objects, we use the average distance (ADD) between 3D model points transformed by ground-truth poses and predicted poses to measure the pose estimation error. For symmetric objects, the average closest point distance (ADDS) is employed to measure the mean error. 6d pose predictions are considered to be correct if the error is smaller than a predefined threshold, which varies from 0 to 10cm to plot an accuracy-threshold curve to compute the area under the curve (AUC). Similarly, for the LineMOD dataset, we use the ADDS distance for symmetric objects (i.e. eggbox and glue) and the ADD distance for the remaining objects having an asymmetric geometry while taking 10% of the diameter as threshold following <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>Implementation Details -As the DenseFusion <ref type="bibr" target="#b13">[14]</ref>, in our experiments, the CNN used for texture feature encoding is composed of Resnet-18 <ref type="bibr" target="#b41">[42]</ref> followed by 4 blocks of one upsampling and one convolution layer as the decoder. RGB and depth images are respectively encoded into 128-dimensional vectors, which are fused into a pixel-wise dense feature x fusion . The PointNet++ <ref type="bibr" target="#b24">[25]</ref> aggregating pixel-wise features consists of two set abstraction and two feature propagation layers. For training our model, learning rate is set 10 −4 with the Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with State-of-the-art Methods</head><p>We compare the proposed W-PoseNet and the state-of-theart methods on the YCB-Video and the LineMOD datasets, and the results are visualized in <ref type="table" target="#tab_0">Tables I and II</ref>. In general, our method can consistently achieve state-of-the-art performance with and without iterative pose refinement on the YCB-Video and LineMOD. Specifically, on the YCB-Video, our network consistently performs better than its direct competitor DenseFusion especially for asymmetric objects with and without post-processing refinement, while similar results are observed on the LineMOD dataset. As the dense feature extraction and fusion part of our W-PoseNet and DenseFusion are identical, the performance gain can only be explained by our design on a pixel-pair combination of local features and auxiliary dense correspondence mapping. To verify the robustness of W-PoseNet against inter-object occlusion, we first calculate the invisible surface percentage for each object instance on the YCB-Video dataset as the DenseFusion <ref type="bibr" target="#b13">[14]</ref> did, then evaluate the proposed W-PoseNet with different degrees of occlusion. Specifically, we sampled a certain number of points on object models, projected these points onto their image plane by using the ground-truth poses and the camera intrinsic parameters. Given the observation (i.e. depth d(p) on each pixel) in depth images and 2D projection d(p) of 3D coordinates, if any pixel satisfying |d(p) − d(p)|&gt;h (h is set to 2cm in our experiment), the pixel is considered under occlusion and can be marked as an invisible pixel. The invisible surface percentage for each instance can be generated via the ratio between the size of invisible points and the total number of sampled points. <ref type="figure" target="#fig_2">Fig. 4</ref> shows comparative evaluation of several methods in terms of the ADD(S) metric. Our W-PoseNet can consistently outperform the PoseCNN+ICP <ref type="bibr" target="#b9">[10]</ref> and the DenseFusion <ref type="bibr" target="#b13">[14]</ref> with increasing the invisible surface percentage. On one hand, both W-PoseNet and DenseFusion are more robust than the PoseCNN+ICP against inter-object occlusion in view of dense feature encoding. On the other hand, the performance gain of our W-PoseNet sharing the identical feature encoding module of the DenseFusion can demonstrate its superior robustness of our dense correspondence regularized pixel pair pose regression to pixel-wise regression in the DenseFusion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>In order to verify the effectiveness of each module of W-PoseNet, we conducted a number of ablation experiments on the LineMOD dataset without iterative pose refinement. Effects of Pixel-Pair Pose Prediction -Compared to pixelwise pose estimation in the DenseFusion <ref type="bibr" target="#b13">[14]</ref>, our W-PoseNet without the DCM Loss and post-refining gain 88.2% on the mean ADD(S), while the DenseFusion without refinement can only achieve 86.2%. The only difference between W-PoseNet without the DCM Loss and the DenseFusion lies in the usage of pixel-pair pose regression, and thus 2% improvement gain can be credited to the effects of Pixel-Pair Pose Prediction. Evaluation on the DCM Loss -We compare the Dense-Fusion and our W-PoseNet without pixel-pair regression, obtaining 86.2% and 93.2% respectively, which only differ on the usage of additional dense correspondence mapping branch. It is evident that, owing to introducing the auxiliary DCM branch, the W-PoseNet can significantly outperform the state-of-the-art DenseFusion by 7%, which indicates that the DCM branch improves the quality of per-pixel features. Effects of the Feature Fusion Module -We explore a variety of feature fusion methods, including the elementwise sum (ELS), direct concatenation (CON), and lowrank bilinear pooling (LRBP) under two settings, i.e. with and without the auxiliary correspondence mapping (DCM). Among these methods, the LRBP consistently achieves the best performance in both settings. Such a study verifies the efficacy of a bilinear pooling to exploit the richer 2-order statistics of local features.</p><p>Effects of Iterative Pose Refinement -We employ the identical refinement network with the same settings as the DenseFusion. <ref type="table" target="#tab_0">From Tables I and II</ref>, iterative pose refinement can further boost estimation performance of our W-PoseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper introduces a novel 6D pose estimation network based on two key observations: extracting discriminative dense features in local regions and generating a robust global representation of each object instance. To this end, we introduce two novel modules into the state-of-the-art Dense-Fusion -pixel-pair pose prediction and dense corresponding mapping utilizing object models, which are verified their effectiveness in the experiments respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline of the proposed W-PoseNet. The method first detects and segments the foreground containing object instances on RGB images. RGB and depth images are respectively fed into feature encoders and then fused with the PointNet++ [25]. Pixel-wise features are sparsely sampled and combined to generate pixel pair features, which produce 6D pose [R|t]. The branch about Dense Correspondence Mapping is to regress pose-specific 3D coordinates from per-pixel features, providing additional geometric constraints for feature learning. A joint loss on dense correspondence mapping and pixel-pair pose regression branches are used to supervise network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Dense Correspondence Mapping (DCM). This module aims to regularizing feature learning in pose regression with mapping onto 3D coordinates sensitive to 6D poses. Specifically, the point cloud in blue is generated by transforming the point clouds sampled from depth image into the model coordinate system, which are used as supervision signals in DCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of our W-PoseNet and two state-of-the-art methods under different degree of occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARATIVE</head><label>I</label><figDesc>EVALUATION OF 6D POSE ESTIMATION ON THE YCB-VIDEO DATASET IN TERMS OF THE ADDS AUC AND ADD(S) AUC METRICS. OBJECTS IN BOLD ARE SYMMETRIC. RESULTS ARE REPORT IN UNITS OF %.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">Without Iterative Refinement</cell><cell></cell><cell></cell><cell cols="3">With Iterative Refinement</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">PoseCNN [10]</cell><cell cols="2">DenseFusion [14]</cell><cell cols="2">W-PoseNet</cell><cell cols="2">PoseCNN+ICP [10]</cell><cell cols="2">DenseFusion [14]</cell><cell cols="2">W-PoseNet</cell></row><row><cell></cell><cell>ADDS</cell><cell>ADD(S)</cell><cell cols="2">ADDS ADD(S)</cell><cell>ADDS</cell><cell>ADD(S)</cell><cell>ADDS</cell><cell>ADD(S)</cell><cell cols="2">ADDS ADD(S)</cell><cell>ADDS</cell><cell>ADD(S)</cell></row><row><cell>002 master chef can</cell><cell>83.9</cell><cell>50.2</cell><cell>95.3</cell><cell>70.7</cell><cell>96.0</cell><cell>82.2</cell><cell>95.8</cell><cell>68.1</cell><cell>96.4</cell><cell>73.2</cell><cell>96.0</cell><cell>84.0</cell></row><row><cell>003 cracker box</cell><cell>76.9</cell><cell>53.1</cell><cell>92.5</cell><cell>86.9</cell><cell>93.0</cell><cell>87.1</cell><cell>92.7</cell><cell>83.4</cell><cell>95.8</cell><cell>94.1</cell><cell>95.5</cell><cell>93.0</cell></row><row><cell>004 sugar box</cell><cell>84.2</cell><cell>68.4</cell><cell>95.1</cell><cell>90.8</cell><cell>96.7</cell><cell>94.9</cell><cell>98.2</cell><cell>97.1</cell><cell>97.6</cell><cell>96.5</cell><cell>97.8</cell><cell>96.8</cell></row><row><cell>005 tomato soup can</cell><cell>81.0</cell><cell>66.2</cell><cell>93.8</cell><cell>84.7</cell><cell>94.3</cell><cell>89.0</cell><cell>94.5</cell><cell>81.8</cell><cell>94.5</cell><cell>85.5</cell><cell>94.5</cell><cell>89.9</cell></row><row><cell>006 mustard bottle</cell><cell>90.4</cell><cell>81.0</cell><cell>95.8</cell><cell>90.9</cell><cell>97.3</cell><cell>95.6</cell><cell>98.6</cell><cell>98.0</cell><cell>97.3</cell><cell>94.7</cell><cell>98.1</cell><cell>97.5</cell></row><row><cell>007 tuna fish can</cell><cell>88.0</cell><cell>70.7</cell><cell>95.7</cell><cell>79.6</cell><cell>96.5</cell><cell>80.5</cell><cell>97.1</cell><cell>83.9</cell><cell>97.1</cell><cell>81.9</cell><cell>97.3</cell><cell>81.8</cell></row><row><cell>008 pudding box</cell><cell>79.1</cell><cell>62.7</cell><cell>94.3</cell><cell>89.3</cell><cell>95.1</cell><cell>90.8</cell><cell>97.9</cell><cell>96.6</cell><cell>96.0</cell><cell>93.3</cell><cell>96.6</cell><cell>94.3</cell></row><row><cell>009 gelatin box</cell><cell>87.2</cell><cell>75.2</cell><cell>97.2</cell><cell>95.8</cell><cell>96.9</cell><cell>95.0</cell><cell>98.8</cell><cell>98.1</cell><cell>98.0</cell><cell>96.7</cell><cell>98.5</cell><cell>97.3</cell></row><row><cell>010 potted meat can</cell><cell>78.5</cell><cell>59.5</cell><cell>89.3</cell><cell>79.6</cell><cell>90.8</cell><cell>78.9</cell><cell>92.7</cell><cell>83.5</cell><cell>90.7</cell><cell>83.6</cell><cell>91.6</cell><cell>80.4</cell></row><row><cell>011 banana</cell><cell>86.0</cell><cell>72.3</cell><cell>90.0</cell><cell>76.7</cell><cell>95.8</cell><cell>91.2</cell><cell>97.1</cell><cell>91.9</cell><cell>96.2</cell><cell>83.3</cell><cell>97.2</cell><cell>93.9</cell></row><row><cell>019 pitcher base</cell><cell>77.0</cell><cell>53.3</cell><cell>93.6</cell><cell>87.1</cell><cell>96.3</cell><cell>94.2</cell><cell>97.8</cell><cell>96.9</cell><cell>97.5</cell><cell>96.9</cell><cell>98.3</cell><cell>98.1</cell></row><row><cell>021 bleach cleanser</cell><cell>71.6</cell><cell>50.3</cell><cell>94.4</cell><cell>87.5</cell><cell>95.2</cell><cell>88.6</cell><cell>96.9</cell><cell>92.5</cell><cell>95.9</cell><cell>89.9</cell><cell>96.3</cell><cell>91.8</cell></row><row><cell>024 bowl</cell><cell>69.6</cell><cell>69.6</cell><cell>86.0</cell><cell>86.0</cell><cell>94.3</cell><cell>94.3</cell><cell>81.0</cell><cell>81.0</cell><cell>89.5</cell><cell>89.5</cell><cell>96.2</cell><cell>96.2</cell></row><row><cell>025 mug</cell><cell>78.2</cell><cell>58.5</cell><cell>95.3</cell><cell>83.8</cell><cell>96.5</cell><cell>89.7</cell><cell>94.9</cell><cell>81.1</cell><cell>96.7</cell><cell>88.9</cell><cell>97.1</cell><cell>91.8</cell></row><row><cell>035 power drill</cell><cell>72.7</cell><cell>55.3</cell><cell>92.1</cell><cell>83.7</cell><cell>95.8</cell><cell>93.1</cell><cell>98.2</cell><cell>97.7</cell><cell>96.0</cell><cell>92.7</cell><cell>97.4</cell><cell>96.2</cell></row><row><cell>036 wood block</cell><cell>64.3</cell><cell>64.3</cell><cell>89.5</cell><cell>89.5</cell><cell>91.5</cell><cell>91.5</cell><cell>87.6</cell><cell>87.6</cell><cell>92.8</cell><cell>92.8</cell><cell>91.7</cell><cell>91.7</cell></row><row><cell>037 scissors</cell><cell>56.9</cell><cell>35.8</cell><cell>90.1</cell><cell>77.4</cell><cell>88.0</cell><cell>60.6</cell><cell>91.7</cell><cell>78.4</cell><cell>92.0</cell><cell>77.9</cell><cell>89.7</cell><cell>73.0</cell></row><row><cell>040 large marker</cell><cell>71.7</cell><cell>58.3</cell><cell>95.1</cell><cell>89.1</cell><cell>97.1</cell><cell>91.1</cell><cell>97.2</cell><cell>85.3</cell><cell>97.6</cell><cell>93.0</cell><cell>97.5</cell><cell>90.8</cell></row><row><cell>051 large clamp</cell><cell>50.2</cell><cell>50.2</cell><cell>71.5</cell><cell>71.5</cell><cell>75.7</cell><cell>75.7</cell><cell>75.2</cell><cell>75.2</cell><cell>72.5</cell><cell>72.5</cell><cell>76.1</cell><cell>76.1</cell></row><row><cell>052 extra large clamp</cell><cell>44.1</cell><cell>44.1</cell><cell>70.2</cell><cell>70.2</cell><cell>73.3</cell><cell>73.3</cell><cell>64.4</cell><cell>64.4</cell><cell>69.9</cell><cell>69.9</cell><cell>74.6</cell><cell>74.6</cell></row><row><cell>061 foam brick</cell><cell>88.0</cell><cell>88.0</cell><cell>92.2</cell><cell>92.2</cell><cell>95.8</cell><cell>95.8</cell><cell>97.2</cell><cell>97.2</cell><cell>92.0</cell><cell>92.0</cell><cell>96.9</cell><cell>96.9</cell></row><row><cell>ALL</cell><cell>75.8</cell><cell>59.9</cell><cell>91.2</cell><cell>82.9</cell><cell>93.1</cell><cell>87.1</cell><cell>93.0</cell><cell>85.4</cell><cell>93.2</cell><cell>86.1</cell><cell>94.1</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARATIVE</head><label>II</label><figDesc>EVALUATION OF 6D POSE ESTIMATION IN TERMS OF ADD(S) ON THE LINEMOD DATASET. OBJECTS IN BOLD ARE SYMMETRIC. THE TOP GROUP OF ALGORITHMS ONLY USE RGB IMAGES AS INPUT, WHILE THE BOTTOM GROUP USES RGB-D IMAGES. REF. DENOTES THE POST-PROCESSING REFINEMENT. RESULTS ARE REPORTED IN UNITS OF %.</figDesc><table><row><cell>Method</cell><cell>ape</cell><cell>ben.</cell><cell>cam</cell><cell>can</cell><cell>cat</cell><cell>drill.</cell><cell>duck</cell><cell>egg.</cell><cell>glue</cell><cell>hole.</cell><cell>iron</cell><cell>lamp</cell><cell cols="2">phone MEAN</cell></row><row><cell>DeepIM [43]</cell><cell>77.0</cell><cell>97.5</cell><cell>93.5</cell><cell>96.5</cell><cell>82.1</cell><cell>95.0</cell><cell>77.7</cell><cell>97.1</cell><cell>99.4</cell><cell>52.8</cell><cell>98.3</cell><cell>97.5</cell><cell>87.7</cell><cell>88.6</cell></row><row><cell>PVNet [20]</cell><cell>43.6</cell><cell>99.9</cell><cell>86.9</cell><cell>95.5</cell><cell>79.3</cell><cell>96.4</cell><cell>52.6</cell><cell>99.2</cell><cell>95.7</cell><cell>82.0</cell><cell>98.9</cell><cell>99.3</cell><cell>92.4</cell><cell>86.3</cell></row><row><cell>CDPN [44]</cell><cell>64.4</cell><cell>97.8</cell><cell>91.7</cell><cell>95.9</cell><cell>83.8</cell><cell>96.2</cell><cell>66.8</cell><cell>99.7</cell><cell>99.6</cell><cell>85.8</cell><cell>97.9</cell><cell>97.9</cell><cell>90.8</cell><cell>89.9</cell></row><row><cell>Imp.+ICP [11]</cell><cell>20.6</cell><cell>64.3</cell><cell>63.2</cell><cell>76.1</cell><cell>72.0</cell><cell>41.6</cell><cell>32.4</cell><cell>98.6</cell><cell>96.4</cell><cell>49.9</cell><cell>63.1</cell><cell>91.7</cell><cell>71.0</cell><cell>64.7</cell></row><row><cell>SSD6D+ICP [12]</cell><cell>65.0</cell><cell>80.0</cell><cell>78.0</cell><cell>86.0</cell><cell>70.0</cell><cell>73.0</cell><cell>66.0</cell><cell>100.0</cell><cell>100.0</cell><cell>49.0</cell><cell>78.0</cell><cell>73.0</cell><cell>79.0</cell><cell>79.0</cell></row><row><cell>DenseFusion [14]</cell><cell>79.5</cell><cell>84.2</cell><cell>76.5</cell><cell>86.6</cell><cell>88.8</cell><cell>77.7</cell><cell>76.3</cell><cell>99.9</cell><cell>99.4</cell><cell>79.0</cell><cell>92.1</cell><cell>92.3</cell><cell>88.0</cell><cell>86.2</cell></row><row><cell>DenseFusion+Ref. [14]</cell><cell>92.3</cell><cell>93.2</cell><cell>94.4</cell><cell>93.1</cell><cell>96.5</cell><cell>87.0</cell><cell>92.3</cell><cell>99.8</cell><cell>100.0</cell><cell>92.1</cell><cell>97.0</cell><cell>95.3</cell><cell>92.8</cell><cell>94.3</cell></row><row><cell>W-PoseNet (ours)</cell><cell>91.7</cell><cell>98.8</cell><cell>98.4</cell><cell>96.5</cell><cell>97.7</cell><cell>96.3</cell><cell>95.0</cell><cell>99.8</cell><cell>99.9</cell><cell>94.4</cell><cell>97.7</cell><cell>99.6</cell><cell>96.8</cell><cell>97.2</cell></row><row><cell>W-PoseNet+Ref. (ours)</cell><cell>94.9</cell><cell>98.9</cell><cell>99.1</cell><cell>97.8</cell><cell>98.8</cell><cell>97.1</cell><cell>97.7</cell><cell>99.8</cell><cell>100.0</cell><cell>96.5</cell><cell>97.9</cell><cell>99.3</cell><cell>97.6</cell><cell>98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDIES ABOUT COMBINATION OF LOCAL FEATURES IN TERMS OF ADD(S). RESULTS ARE REPORT IN UNITS OF %.</figDesc><table><row><cell></cell><cell>ELS</cell><cell cols="2">CON LRBP</cell><cell cols="2">DCM+ELS DCM+CON</cell><cell>DCM+LRBP</cell></row><row><cell>ADD(S)</cell><cell>91.4</cell><cell>92.5</cell><cell>92.8</cell><cell>96.0</cell><cell>97.0</cell><cell>97.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Spindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TVCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality applications using genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Y.</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Grasp pose detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gualtieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent-class hough forests for 3d object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving semantic analysis on point clouds via auxiliary supervision of local geometric priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04803</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12936</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>6d pose estimation with correlation fusion</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going further with point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">independent object class detection using 3d feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schertler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth-encoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recovering 6d object pose and predicting next-best-view in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random feature maps for dot product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorized bilinear models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7678" to="7687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
