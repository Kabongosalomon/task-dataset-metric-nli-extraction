<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Representation Learning in Graph Neural Networks with Node Decimation Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bianchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Grattarola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Livi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Representation Learning in Graph Neural Networks with Node Decimation Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph neural networks</term>
					<term>Graph pooling</term>
					<term>Maxcut optimization</term>
					<term>Kron reduction</term>
					<term>Graph classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In graph neural networks (GNNs), pooling operators compute local summaries of input graphs to capture their global properties, and they are fundamental for building deep GNNs that learn hierarchical representations. In this work, we propose the Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarser graphs while preserving the overall graph topology. During training, the GNN learns new node representations and fits them to a pyramid of coarsened graphs, which is computed offline in a pre-processing stage.</p><p>NDP consists of three steps. First, a node decimation procedure selects the nodes belonging to one side of the partition identified by a spectral algorithm that approximates the MAXCUT solution. Afterwards, the selected nodes are connected with Kron reduction to form the coarsened graph. Finally, since the resulting graph is very dense, we apply a sparsification procedure that prunes the adjacency matrix of the coarsened graph to reduce the computational cost in the GNN. Notably, we show that it is possible to remove many edges without significantly altering the graph structure.</p><p>Experimental results show that NDP is more efficient compared to state-of-the-art graph pooling operators while reaching, at the same time, competitive performance on a significant variety of graph classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Generating hierarchical representations across the layers of a neural network is key to deep learning methods. This hierarchical representation is usually achieved through pooling operations, which progressively reduce the dimensionality of the inputs encouraging the network to learn high-level data descriptors. Graph Neural Networks (GNNs) are machine learning models that learn abstract representations of graphstructured data to solve a large variety of inference tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Differently from neural networks that process vectors, images, or sequences, the graphs processed by GNNs have an arbitrary topology. As a consequence, standard pooling operations that leverage on the regular structure of the data and physical locality principles cannot be immediately applied to GNNs. <ref type="bibr">*</ref> Graph pooling aggregates vertex features while reducing, at the same time, the underlying structure in order to maintain a meaningful connectivity in the coarsened graph. By alternating graph pooling and message-passing (MP) operations <ref type="bibr" target="#b5">[6]</ref>, a GNN can gradually distill global properties from the graph, which are then used in tasks such as graph classification.</p><p>In this paper, we propose Node Decimation Pooling (NDP), a pooling operator for GNNs. NDP is based on node decimation, a procedure developed in the field of graph signal processing for the design of multi-scale graph filters <ref type="bibr" target="#b6">[7]</ref>. In particular, we build upon the multi-resolution framework <ref type="bibr" target="#b7">[8]</ref> that consists of removing some nodes from a graph and then building a coarsened graph from the remaining ones. The NDP procedure that we propose pre-computes off-line (i.e., before training) a pyramid of coarsened graphs, which are then used as support for the node representations computed at different levels of the GNN architecture.</p><p>The contributions of our work are the following.</p><p>1) We introduce the NDP operator that allows to implement deep GNNs that have a low complexity (in terms of execution time and memory requirements) and achieve high accuracy on several downstream tasks. 2) We propose a simple and efficient spectral algorithm that partitions the graph nodes in two sets by maximizing a MAXCUT objective. Such a partition is exploited to select the nodes to be discarded when coarsening the graph. 3) We propose a graph sparsification procedure that reduces the computational cost of MP operations applied after pooling and has a small impact on the representations learned by the GNN. In particular, we show both analytically and empirically that many edges can be removed without significantly altering the graph structure.</p><p>When compared to other methods for graph pooling, NDP performs significantly better than other techniques that precompute the topology of the coarsened graphs, while it achieves a comparable performance with respect to state-of-the-art feature-based pooling methods. The latter, learn both the topology and the features of the coarsened graphs end-to-end via gradient descent, at the cost of a larger model complexity and higher training time. The efficiency of NDP brings a significant advantage when GNNs are deployed in real-world scenarios subject to computational constraints, like in embedded devices and sensor networks.</p><p>The paper is organized as follows: in Sect. II, we formalize the problem and introduce the nomenclature; in Sect. III, we present the proposed method; Sect. IV provides formal analyses and implementation details; related works are discussed in Sect. V, and Sect. VI reports experimental results. Further results and analyses are deferred to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>Let G = {V, E} be a graph with node set V, |V| = N , and edge set E described by a symmetric adjacency matrix A ∈ R N ×N . Define as graph signal X ∈ R N ×F the matrix containing the features of the nodes in the graph (the i-th row of X corresponds with the features x i ∈ R F of the i-th node). For simplicity, we will only consider undirected graphs without edge annotations.</p><p>Let L = D − A be the Laplacian of the graph, where D is a diagonal degree matrix s.t. d ii is the degree of node i. We also define the symmetric Laplacian as L s = I − D −1/2 AD −1/2 . The Laplacian characterizes the dynamics of a diffusion process on the graph and plays a fundamental role in the proposed graph reduction procedure. We note that in the presence of directed edges it is still possible to obtain a symmetric and positivesemidefinite Laplacian <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> for which the derivations presented in this paper hold.</p><p>We consider a GNN composed of a stack of MP layers, each one followed by a graph pooling operation. The (l)-th pooling operation reduces N l nodes to N l+i &lt; N l , producing a pooled version of the node features X (l+1) ∈ R N l+1 ×F l+1 and adjacency matrix A (l+1) ∈ R N l+1 ×N l+1 (see <ref type="figure">Fig. 2</ref>). To implement the MP layer, we consider a simple formulation that operates on the first-order neighbourhood of each node and accounts for the original node features through a separate set of weights acting as a layer-wise skip connection. The computation carried out by the (j)-th MP layer is given by</p><formula xml:id="formula_0">X j+1 = MP(X j , A; Θ MP ) = ReLU(D − 1 2 AD − 1 2 X j W + X j V),<label>(1)</label></formula><p>where Θ MP = {W ∈ R Fj ×Fj+1 , V ∈ R Fj ×Fj+1 } are the trainable weights relative to the mixing and skip component of the layer, respectively. Several other types of MP (e.g., those proposed in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>) can seamlessly be used in conjunction with NDP pooling. In the presence of annotated edges, the MP operation can be extended by following <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPH COARSENING WITH NODE DECIMATION POOLING</head><p>In this section, we describe the proposed NDP operation that consists of the three steps depicted in <ref type="figure">Fig. 1</ref>: (a) decimate the nodes by dropping one of the two sides of the MAXCUT partition; (b) connect the remaining nodes with a link construction procedure; (c) sparsify the adjacency matrix resulting from the coarsened Laplacian, so that only strong connections are kept, i.e., those edges whose weight is associated to an entry of the adjacency matrix above a given threshold . The proposed method is completely unsupervised and the coarsened graphs are pre-computed before training the GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Node decimation with MAXCUT spectral partitioning</head><p>Similarly to pooling operations in Convolutional Neural Networks (CNNs) that compute local summaries of neighboring pixels, we propose a pooling procedure that provides an effective coverage of the whole graph and reduces the number of nodes approximately by a factor of 2. This can be achieved by partitioning nodes in two sets, so that nodes in one set are strongly connected to the complement nodes of the partition, and then dropping one of the two sets. The rationale is that strongly connected nodes exchange a lot of information after a MP operation and, as a result, they are highly dependent and their features become similar. Therefore, one set alone can represent the whole graph sufficiently well. This is similar to pooling in CNNs, where the maximum or the average is extracted from a small patch of neighboring pixels, which are assumed to be highly correlated and contain similar information. In the following, we formalize the problem of finding the optimal subset of vertices that can be used to represent the whole graph.</p><p>The partition of the vertices (a cut) that maximizes the volume of edges whose endpoints are on opposite sides of the partition is the solution of the MAXCUT problem <ref type="bibr" target="#b18">[19]</ref>. The MAXCUT objective is expressed by the integer quadratic problem</p><formula xml:id="formula_1">max z i,j∈V a ij (1 − z i z j ) s.t. z i ∈ {−1, 1},<label>(2)</label></formula><p>where z is the vector containing the optimization variables z i for i = 1, . . . , N indicating to which side of the bi-partition the node i is assigned to; a ij is the entry at row i and column j of A. Problem (2) is NP-hard and heuristics must be considered to solve it. The heuristic that gives the best-known MAXCUT approximation in polynomial time is the Goemans-Williamson algorithm, which is based on the Semi-Definite Programming (SDP) relaxation <ref type="bibr" target="#b19">[20]</ref>. Solving SDP is cumbersome and requires specific optimization programs that scale poorly on large graphs. Therefore, we hereby propose a simple algorithm based on the Laplacian spectrum. First, we rewrite the objective function in (2) as a quadratic form of the graph Laplacian:</p><formula xml:id="formula_2">i,j a ij (1 − z i z j ) = i,j a ij z 2 i + z 2 j 2 − z i z j = 1 2 i j a ij z 2 i + 1 2 j i a ij z 2 j − i,j a ij z i z j = 1 2 i d ii z 2 i + 1 2 j d jj z 2 j − z T Az = z T Dz − z T Az = z T Lz.</formula><p>Then, we consider a continuous relaxation of the integer problem (2) by letting the discrete partition vector z assume continuous values, contained in a vector c:</p><formula xml:id="formula_3">max c c T Lc, s.t. c ∈ R N and c 2 = 1.<label>(3)</label></formula><p>Eq. 3 can be solved by considering the Lagrangian c T Lc + λc T c to find the maximum of c T Lc under constraint c 2 = 1. By setting the gradient of the Lagrangian to zero, we recover </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Links construction Nodes decimation</head><p>Graph sparsification (keep) (discard) <ref type="figure">Fig. 1</ref>. Depiction of the proposed graph coarsening procedure. First, the nodes are partitioned in two sets according to a MAXCUT objective and then are decimated by dropping one of the two sets (V − ). Then, a coarsened Laplacian is built by connecting the remaining nodes with a graph reduction procedure. Finally, the edges with low weights in the new adjacency matrix obtained from the coarsened Laplacian are dropped to make the resulting graph sparser. the eigenvalue equation Lc + λc = 0. All the eigenvalues of L are non-negative and, by restricting the space of feasible solutions to vectors of unitary norm, the trivial solution c * = ∞ is excluded. In particular, if c 2 = 1, c T Lc is a Rayleigh quotient and reaches its maximum λ max (the largest eigenvalue of L) when c * corresponds to v max , the eigenvector associated with λ max .</p><p>Since the components of v max are real, we apply a rounding procedure to find a discrete solution. Specifically, we are looking for a partition z * ∈ Z, where Z = {z : z ∈ {−1, 1} N } is the set of all feasible cuts, so that z * is the closest (in a least-square sense) to c * . This amounts to solving the problem</p><formula xml:id="formula_4">z * = arg min{ c * − z 2 : z ∈ Z},<label>(4)</label></formula><p>with the optimum given by</p><formula xml:id="formula_5">z * i = 1, c * i ≥ 0, −1, c * i &lt; 0.<label>(5)</label></formula><p>By means of the rounding procedure in <ref type="bibr" target="#b4">(5)</ref>, the nodes in V are partitioned in two sets, V + and V − = V \ V + , such that</p><formula xml:id="formula_6">V + = {i ∈ V : v max [i] ≥ 0}.<label>(6)</label></formula><p>In the NDP algorithm we always drop the nodes in V − , i.e., the nodes associated with a negative value in v max . However, it would be equivalent to drop the nodes in V + . The node decimation procedure offers two important advantages: i) it removes approximately half of the nodes when applied, i.e., |V + | ≈ |V − |; ii) the eigenvector v max can be quickly computed with the power method <ref type="bibr" target="#b20">[21]</ref>. There exists an analogy between the proposed spectral algorithm for partitioning the graph nodes and spectral clustering <ref type="bibr" target="#b21">[22]</ref>. However, spectral clustering solves a minCUT problem <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, which is somehow orthogonal to the MAXCUT problem considered here. In particular, spectral clustering identifies K ≥ 2 clusters of densely connected nodes by cutting the smallest volume of edges in the graph, while our algorithm cuts the largest volume of edges yielding two sets of nodes, V + and V − , that cover the original graph in a similar way. Moreover, spectral clustering partitions the nodes in K clusters based on the values of the eigenvectors associated with the M ≥ 1 smallest eigenvalues, while our algorithm partitions the nodes in two sets based only on the last eigenvector v max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Links construction on the coarsened graph</head><p>After dropping nodes in V − and all their incident edges, the resulting graph is likely to be disconnected. Therefore, we use a link construction procedure to obtain a connected graph supported by the nodes in V + . Specifically, we adopt the Kron reduction <ref type="bibr" target="#b24">[25]</ref> to generate a new Laplacian L <ref type="bibr" target="#b0">(1)</ref> , which is computed as the Schur complement of L with respect to the nodes in V − . In detail, the reduced Laplacian L (1) is</p><formula xml:id="formula_7">L (1) = L\L V − ,V − = L V + ,V + −L V + ,V − L −1 V − ,V − L V − ,V + (7)</formula><p>where L V + ,V − identifies a sub-matrix of L with rows (columns) corresponding to the nodes in V + (V − ). It is possible to show that L V − ,V − is always invertible if the associated adjacency matrix A is irreducible. We note that A is irreducible when the graph is not disconnected (i.e., has a single component), a property that holds when the algebraic multiplicity of the eigenvalue λ min = 0 is 1.</p><p>Let us consider the case where A has no self loops. The Laplacian is by definition a weakly diagonally dominant matrix, since L ii = N j=1,j =i |L ij | for all i ∈ V. If A is irreducible, then L is also irreducible. This implies that the strict inequality L ii &gt; n j=1,j =i |L ij | holds for at least one vertex i ∈ V − . It follows that the Kron-reduced Laplacian L V − ,V − is also irreducible, diagonally dominant, and has at least one row with a strictly positive row sum. Hence, L V − ,V − is invertible, as proven by <ref type="bibr" target="#b25">[26]</ref> in Corollary 6.2.27. When A contains selfloops, the existence of the inverse of L V − ,V − is still guaranteed through a small work-around, which is discussed in App. A. Finally, if the graph is disconnected then A is reducible (i.e., it can be expressed in an upper-triangular block form by simultaneous row/column permutations); in this case, the Kron reduction can be computed by means of the generalized inverse L † V − ,V − and the solution corresponds to a generalized Schur complement of L.</p><p>L <ref type="bibr" target="#b0">(1)</ref> in <ref type="formula">(7)</ref> is a well-defined Laplacian where two nodes are connected if and only if there is a path between them in L (connectivity preservation property). Also, L (1) does not introduce self-loops and guarantees the preservation of resistance distance <ref type="bibr" target="#b7">[8]</ref>. Finally, Kron reduction guarantees spectral interlacing between the original Laplacian L ∈ R N ×N and the new coarsened one L (1) ∈ R N1×N1 , with N 1 ≤ N . Specifically, we have λ i ≥ λ (1) i ≥ λ N −N1+i , ∀i = 1, . . . , N 1 , where λ i and λ (1) i are the eigenvalues of L and L <ref type="bibr" target="#b0">(1)</ref> , respectively. The adjacency matrix of the new coarsened graph is recovered from the coarsened Laplacian:</p><formula xml:id="formula_8">A (1) =   −L (1) + diag({ N1 j=1,j =i L (1) ij } N1 i=1 )   .<label>(8)</label></formula><p>We note that A <ref type="bibr" target="#b0">(1)</ref> does not contain self-loops; we refer to App. A for a discussion on how to handle the case with self-loops in the original adjacency matrix.</p><p>A pyramid of coarsened Laplacians is generated by recursively applying node decimation followed by Kron reduction. At the end of the procedure, the adjacency matrices A = {A (0) , A (1) , . . . , A (l) , . . . } of the coarsened graphs are derived from the associated coarsened Laplacians. Note that we interchangeably refer with A (0) or A to the original adjacency matrix. The adjacency matrices in A are used to implement hierarchical pooling in deep GNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph sparsification</head><p>To implement multiple pooling operations the graph must be coarsened several times. Due to the connectivity preservation property, by repeatedly applying Kron reduction the graph eventually becomes fully-connected. This implies a high computational burden in deeper layers of the network, since the complexity of MP operations scales with the number of edges.</p><p>To address this issue, it is possible to apply the spectral sparsification algorithm proposed in <ref type="bibr" target="#b26">[27]</ref> to obtain a sparser graph. However, we found that this procedure leads to numerical instability and poor convergence during the learning phase. Therefore, to limit the number of edges with non-zero weights we propose a sparsification procedure that removes from the adjacency matrix of the coarsened graph the edges whose weight is below a small user-defined threshold :</p><formula xml:id="formula_9">A (i) = ā (l) ij = 0, if |a (l) ij | ≤ ā (l) ij = a (l) ij , otherwise.<label>(9)</label></formula><p>D. Pooling with decimation matrices.</p><p>To pool the node features with a differentiable operation that can be applied while training the GNN, we multiply the graph signal X (l) with a decimation matrix S (l) ∈ N N l+1 ×N l . S (l) is obtained by selecting from the identity matrix I N l ∈ N N l ×N l the rows corresponding to the vertices in V + :</p><formula xml:id="formula_10">X (l+1) = S (l) X (l) = [I N l ] V + ,: X (l) .<label>(10)</label></formula><p>As discussed in Sec. III-A, NDP approximately halves the the nodes of the current graph at each pooling stage. This is a consequence of the MAXCUT objective that splits the nodes in two sets so that the volume of edges crossing the partition, i.e., the edges to be cut, is maximized. Intuitively, if one of the two sets is much smaller than the other, more edges are cut by moving some nodes to the smaller set. For this reason, the application of a single decimation matrix S (l) shares similarities with a classic pooling with stride 2 in CNNs.</p><p>It follows that a down-sampling ratio of ≈ 2 k can be obtained in NDP by applying k decimation matrices in cascade. This enables moving from level l to level l+k (k &gt; 1) in the pyramid of coarsened graphs. <ref type="figure">Fig. 2</ref> shows an example of pooling with downsampling ratio ≈ 8, where the GNN performs messagepassing on A (3) right after A (0) . <ref type="figure">Fig. 2</ref>. This example shows how it is possible to skip some MP operations on intermediate levels of the pyramid of coarsened graphs. Such a procedure shares analogies with pooling with a larger stride in traditional CNNs and can be considered as a higher-order graph pooling. After the first MP operation on A (0) , the node features are pooled by applying in cascade 3 decimation matrices, S (0) , S <ref type="bibr" target="#b0">(1)</ref> , and S <ref type="bibr" target="#b1">(2)</ref> . Afterwards, it is possible to directly perform a MP operation on A <ref type="bibr" target="#b2">(3)</ref> , skipping the MP operations on A <ref type="bibr" target="#b0">(1)</ref> and A <ref type="bibr" target="#b1">(2)</ref> .  The entries of v max associated with low-degree nodes assume very small values and their signs may be flipped due to numerical errors. The partition obtained by using v s max , i.e., the eigenvector of the symmetric Laplacian L s , in (6) is analytically the same. Indeed, since v s max = D −1/2 v max , the values of the two eigenvectors are rescaled by positive numbers and, therefore, the sign of their components is the same. However, a positive effect of the degree normalization is that the values in v s max associated to nodes with a low degree are amplified. <ref type="figure" target="#fig_2">Fig. 3</ref> compares the values in the eigenvectors v max and v s max , computed on the same graph. Since many values in v max are concentrated around zero, partitioning the nodes according to the sign of the entries in v max gives numerically unstable results. On the other hand, since the values in v s max are distributed more evenly the nodes can be partitioned more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ANALYSIS OF THE GRAPH COARSENING PROCEDURE AND IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Numerical precision in eigendecomposition</head><p>Note that, even if the indexes of V + are identified from the eigenvector of L s , the Kron reduction is still performed on the Laplacian L. In the supplementary material we report numerical differences in the size of the cut obtained on random graphs when using v max or v s max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of the approximate MAXCUT solution</head><p>Since computing the optimal MAXCUT solution is NP-hard, it is generally not possible to evaluate the quality of the cut found by the proposed spectral method (Sect. III-A) in terms of discrepancy from the MAXCUT. Therefore, to assess the quality of a solution we consider the following bounds</p><formula xml:id="formula_11">0.5 ≤ MAXCUT |E| ≤ λ s max 2 ≤ 1.<label>(11)</label></formula><p>The value λ s max /2 is an upper-bound of MAXCUT/|E|, where λ s max is the largest eigenvalue of L s and |E| = i,j a ij . The lower-bound 0.5 is given by the random cut, which uniformly assigns the nodes to the two sides of the partition. <ref type="bibr" target="#b0">1</ref> The derivation of the upper-bound is in App. B.</p><p>To quantify the size of a cut induced by a partition vector z, such as the one in (5), we introduce the function</p><formula xml:id="formula_12">0 ≤ γ(z) = z T Lz 2 i,j a ij ≤ MAXCUT |E| ,<label>(12)</label></formula><p>which measures the proportion of edges cut by z. Note that γ(·) depends also on L, but we keep it implicit to simplify the notation.</p><p>Let us now consider the best-and worst-case scenarios. The best case is the bipartite graph, where the MAXCUT is known and it cuts all the graph edges. The partition z found by our spectral algorithm on bipartite graphs is optimal, i.e., γ(z) = MAXCUT/|E| = 1. In graphs that are close to be bipartite or, in general, that have a very sparse and regular connectivity, a large percentage of edges can be cut if the nodes are partitioned correctly. Indeed, for these graphs the MAXCUT is usually large and is closer to the upper-bound in <ref type="bibr" target="#b10">(11)</ref>. On the other hand, in very dense graphs the MAXCUT is smaller, as well as the gap between the upper-and lower-bound in <ref type="bibr" target="#b10">(11)</ref>. Notably, the worstcase scenario is a complete graph where is not possible to cut more than half of the edges, i.e., MAXCUT = 0.5. We note that in graphs made of a sparse regular part that is weakly connected to a large dense part, the gaps in (11) can be arbitrarily large.</p><p>The proposed spectral algorithm is not designed to handle very dense graphs; an intuitive explanation is that v s max can be interpreted as the graph signal with the highest frequency, since its sign oscillates as much as possible when transiting from <ref type="bibr" target="#b0">1</ref> A random cut z is, on average, at least 0.5 of the optimal cut z * :</p><formula xml:id="formula_13">E[|z|] = (i,j)∈E E[z i z j ] = (i,j)∈E Pr[(i, j) ∈ z] = |E| 2 ≥ |z * | 2</formula><p>a node to one of its neighbors. While such oscillation in the sign is clearly possible on bipartite graphs, in complete graphs it is not possible to find a signal that assumes an opposite sign on neighboring nodes, because all nodes are connected with each other. Remarkably, the solution <ref type="formula" target="#formula_5">(5)</ref> found by the spectral algorithm on very dense graphs can be worse than the random cut. A theoretical result found by Trevisan <ref type="bibr" target="#b27">[28]</ref> states that a spectral algorithm, like the one we propose, is guaranteed to yield a cut larger than the random partition only when λ s max ≥ 2(1 − τ ) = 1.891 (see App. C for details). To illustrate how the size of the cut found by the spectral algorithm changes between the best-and worst-case scenarios, we randomly add edges to a bipartite graph until it becomes complete. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates how the size of the cut γ(z) induced by the spectral partition z changes as more edges are added and the original structure of the graph is corrupted (blue line). The figure also reports the size of the random cut (orange line) and the MAXCUT upper bound from Eq. (12) (green line). The black line indicates the threshold from <ref type="bibr" target="#b27">[28]</ref>, i.e., the value of λ 2 max /2 below which the spectral cut is no longer guaranteed to be larger than the random cut. The graph used to generate the figure is a regular grid; however, similar results hold also for other families of random graphs and are reported in the supplementary material.  <ref type="bibr" target="#b27">[28]</ref> indicating the value of λ s max /2 below which one should switch to the random cut to obtain a solution guaranteed to be ≥ 0.53·MAXCUT. The x-axis indicates the density of the graph connectivity, which increases by randomly adding edges. <ref type="figure" target="#fig_3">Fig. 4</ref> shows that the spectral algorithm finds a better-thanrandom cut even when λ s max /2 &lt; 1 − τ (i.e., when the result from <ref type="bibr" target="#b27">[28]</ref> does not hold), and only approaches the size of the random cut when the edge density is very high (70%-80%).</p><p>Importantly, when the size of the spectral partition becomes smaller than the random partition, the upper-bound λ s max /2 ≈ 0.5, meaning that the random cut is very close to the MAXCUT. To obtain a cut that is always at least as good as the random cut, we first compute the partition z as in <ref type="bibr" target="#b4">(5)</ref> and evaluate its size γ(z): if γ(z) &lt; 0.5, we return a random partition instead.</p><p>We conclude by noticing that, due to the smoothing effect of MP operations, the nodes belonging to densely connected graph components are likely to have very similar representations computed by the GNN; it is, therefore, not important which of these nodes are dropped by a random cut. The random cut in these cases not only is optimal in terms of the MAXCUT objective, but it also introduces stochasticity that provides robustness when training the GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudocode</head><p>The procedure for generating the pyramid of coarsened adjacency matrices and the pooling matrices used for decimation is reported in Alg. 1. L is a list of positive integers indicating the levels in the pyramid of coarsened Laplacians that we want to compute. For instance, given levels L = [1, 3, 5] for a graph of N nodes, the algorithm will return the coarsened graphs with approximately N/2, N/8, and N/32 nodes (in general, N/2 li for each l i in L). Matrix R is a buffer that accumulates decimation matrices when one or more coarsening levels are skipped. This happens when the GNN implements high order pooling, as discussed in Sect III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Graph coarsening</head><p>Input: adjacency matrix A, coarsening levels L, sparsification threshold Output: coarsened adjacency matricesĀ, decimation matrices S 1:</p><formula xml:id="formula_14">A (0) = A, R = I N , A = {}, S = {}, l = 0 2: while l ≤ max(L) do 3: A (l+1) , S (l+1) = pool(A (l) ) 4:</formula><p>if l ∈ L then 5:</p><formula xml:id="formula_15">A = A ∪ A (l+1) , S = S ∪ S (l+1) R 6: R = I N l 7: else 8: R = S (l+1) R 9: l = l + 1 10:Ā = {Ā (l) :ā (l) ij = a (l) ij if a (l) ij &gt; and 0 otherwise, ∀A (l) ∈ A}</formula><p>Alg. 2 shows the details of the pooling function, used in line 3 of Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 pool(·) function</head><formula xml:id="formula_16">Input: adjacency matrix A (l) ∈ R N l ×N l Output: coarsened adjacency matrix A (l+1) ∈ R N l+1 ×N l+1 , decimation matrix S (l+1) ∈ N N l+1 ×N l 1: get L (l) = D (l) − A (l) and L (l) s = I − (D (l) ) − 1 2 A (l) (D (l) ) − 1 2 2: compute the eigenvector v s max of L (l) s 3: partition vector z s.t. z i = 1 if v s max [i] ≥ 0, z i = −1 if v s max [i] &lt; 0 4: if γ(z) &lt; 0.5 then 5: random sample z i ∼ {−1, 1}, ∀i = 1, . . . , N l (random cut) 6: V + = {i : z i = 1}, V − = {i : z i = −1} 7: L (l+1) = L (l) \ L (l) V − ,V − (Kron reduction) 8: A (l+1) = −L (l+1) + diag( j =i L (l+1) ij ) 9: S (l+1) = [I N l+1 ] V + ,:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational cost analysis</head><p>The most expensive operations in the NDP algorithm are i) the cost of computing the eigenvector v s max , and ii) the cost of inverting the submatrix L V − ,V − within the Kron reduction.</p><p>Computing all eigenvectors has a cost O(N 3 ), where N is the number of nodes. However, computing only the eigenvector corresponding to the largest eigenvalue is fast when using the power method <ref type="bibr" target="#b28">[29]</ref>, which requires only few iterations (usually 5-10), each one of complexity O(N 2 ). The cost of inverting</p><formula xml:id="formula_17">L V − ,V − is O(|V − | 3 ),</formula><p>where |V − | is the number of nodes that are dropped.</p><p>We notice that the coarsened graphs are pre-computed before training the GNN. Therefore, the computational time of graph coarsening is much lower compared to training the GNN for several epochs, since each MP operation in the GNN has a cost O(N 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Structure of the sparsified graphs</head><p>When applying the sparsification, the spectrum of the resulting adjacency matrixĀ is preserved, up to a small factor that depends on , with respect to the spectrum of A. Theorem 1. Let Q be a matrix used to remove small values in the adjacency matrix A, which is defined as</p><formula xml:id="formula_18">Q = q ij = −a ij , if |a ij | ≤ q ij = 0, otherwise.<label>(13)</label></formula><p>Each eigenvalueᾱ i of the sparsified adjacency matrixĀ = A + Q is bounded byᾱ</p><formula xml:id="formula_19">i ≤ α i + u T i Qu i ,<label>(14)</label></formula><p>where α i and u i are eigenvalue-eigenvector pairs of A.</p><p>Proof. Let P be a matrix with elements p ij = sign(q ij ) and consider the perturbation A+ P, which modifies the eigenvalue problem</p><formula xml:id="formula_20">Au i = α i u i in (A + P)(u i + u ) = (α i + α )(u i + u ).<label>(15)</label></formula><p>where α is a small number and u a small vector, which are unknown and indicate a perturbation on α i and u i , respectively. By expanding <ref type="bibr" target="#b14">(15)</ref>, then canceling the equation Au i = α i u i and the high order terms O( 2 ), one obtains</p><formula xml:id="formula_21">Au + Pu i = α i u + α u i .<label>(16)</label></formula><p>Since A is symmetric, its eigenvectors can be used as a basis to express the small vector u</p><formula xml:id="formula_22">u = N j=1 δ j u j ,<label>(17)</label></formula><p>where δ j are (small) unknown coefficients. Substituting <ref type="bibr" target="#b16">(17)</ref> in <ref type="bibr" target="#b15">(16)</ref> and bringing A inside the summation, gives N j=1</p><formula xml:id="formula_23">δ j Au j + Pu i = α i N j=1 δ j u j + α u i .<label>(18)</label></formula><p>By considering the original eigenvalue problem that gives N j=1 δ j Au j = N j=1 δ j α j u j and by left-multiplying each term</p><formula xml:id="formula_24">with u T i , (18) becomes u T i N j=1 δ j α j u j + u T i Pu i = u T i α i N j=1 δ j u j + u T i α u i .<label>(19)</label></formula><p>Since eigenvectors are orthogonal, u T i u j = 0, ∀j = i and</p><formula xml:id="formula_25">u T i u j = 1, for j = i, Eq. (19) becomes u T i δ i α i u i + u T i Pu i = u T i α i δ i u i + u T i α i u i , u T i Pu i = u T i α u i = α ,<label>(20)</label></formula><p>which, in turn, gives</p><formula xml:id="formula_26">α = u T i Pu i ≥ u T i Qu i ,<label>(21)</label></formula><p>as Q ≤ P.</p><p>A common way to measure the similarity of two graphs is to compare the spectrum of their Laplacians. To extend the results of Theorem 1 to the spectra of the Laplacians L andL, respectively associated with the original and sparsified adjacency matrices A andĀ, it is necessary to consider the relationships between the eigenvalues of A and L. For a dregular graph, the relationship λ i = d − α i links the i-th eigenvalue λ i of L to the i-th eigenvalue α i of A <ref type="bibr" target="#b29">[30]</ref>. However, for a general graph it is only possible to derive a loose bound, d max − α n ≤ λ n ≤ d max − α 1 , that depends on the maximum degree d max of the graph [31, Lemma 2.21].</p><p>Therefore, we numerically compare the spectra of the Laplacians associated with the matrices in A andĀ. In particular, <ref type="figure" target="#fig_4">Fig. 5</ref>(top-left) depicts the spectrum of the Laplacian associated to the original graph A (0) (black dotted line) and the spectra Λ(L (1) ), Λ(L <ref type="bibr" target="#b1">(2)</ref> ), Λ(L (3) ) of the Laplacians associated with A (1) , A <ref type="bibr" target="#b1">(2)</ref> , and A <ref type="bibr" target="#b2">(3)</ref> . <ref type="figure" target="#fig_4">Fig. 5</ref>(top-right) depicts the spectra of the LaplaciansL <ref type="bibr" target="#b0">(1)</ref> ,L <ref type="bibr" target="#b1">(2)</ref> ,L (3) associated with the sparsified matricesĀ <ref type="bibr" target="#b0">(1)</ref> ,Ā <ref type="bibr" target="#b1">(2)</ref> , andĀ <ref type="bibr" target="#b2">(3)</ref> . It is possible to observe that the spectra of L (l) andL (l) are almost identical and therefore, to better visualize the differences, we show in <ref type="figure" target="#fig_4">Fig. 5</ref>(bottom) the absolute differences |Λ(L (l) ) − Λ(L (l) )|. The graph used in <ref type="figure" target="#fig_4">Fig. 5</ref> is a random sensor network and the sparsification threshold is = 10 −2 , which is the one adopted in all our experiments.  <ref type="bibr" target="#b0">(1)</ref> ,Ā <ref type="bibr" target="#b1">(2)</ref> , andĀ <ref type="bibr" target="#b2">(3)</ref> . Bottom: Absolute difference between the spectra of the Laplacians.</p><p>To quantify how much the coarsened graph changes as a function of , we consider the spectral distance that measures a dissimilarity between the spectra of the Laplacians associated with A andĀ <ref type="bibr" target="#b31">[32]</ref>. The spectral distance is computed as</p><formula xml:id="formula_27">SD(L,L; ) = 1 K K+1 k=2 |λ k ( ) − λ k | λ k ,<label>(22)</label></formula><p>where {λ k } K+1 k=2 and {λ k ( )} K+1 k=2 are, respectively, the K smallest non-zero eigenvalues of L andL. Non-zero edges <ref type="figure">Fig. 6</ref>. In blue, the variation of spectral distance between the Laplacian L and the LaplacianL, associated with the adjacency matrix A sparsified with threshold . In red, the number of edges that remain inL. <ref type="figure">Fig. 6</ref> depicts in blue the variation of spectral distance between L andL, as we increase the threshold used to computeĀ. The red line indicates the number of edges that remain inĀ after sparsification. It is possible to see that for small increments of the spectral distance increases linearly, while the number of edges in the graph drops exponentially. Therefore, with a small it is possible to discard a large amount of edges with minimal changes in the graph spectrum.</p><p>The graph used to generate <ref type="figure">Fig. 6</ref> is a sensor network; results for other types of graph are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK ON GRAPH POOLING</head><p>We discuss related work on GNN pooling by distinguishing between topological and feature-based pooling methods. a) Topological pooling methods: similarly to NDP, topological pooling methods pre-compute coarsened graphs before training based on their topology. Topological pooling methods are usually unsupervised, as they define how to coarsen the graph outside of the learning procedure. The GNN is then trained to fit its node representations to these pre-determined structures. Pre-computing graph coarsening not only makes the training much faster by avoiding to perform graph reduction at every forward pass, but it also provides a strong inductive bias that prevents degenerate solutions, such as entire graphs collapsing into a single node or entire graph sections being discarded. This is important when dealing with small datasets or, as we show in the following section, in tasks such as graph signal classification.</p><p>The approach that is most related to NDP and that has been adopted in several GNN architectures to perform pooling <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, consists of coarsening the graph with GRACLUS, a hierarchical spectral clustering algorithm <ref type="bibr" target="#b36">[37]</ref>. At each level l, two vertices i and j are clustered together in a new vertex k. Then, a standard pooling operation (average or max pool) is applied to compute the node feature x j . This approach has several drawbacks. First, the connectivity of the original graph is not preserved in the coarsened graphs and the spectrum of their associated Laplacians is usually not contained in the spectrum of the original Laplacian. Second, GRACLUS pooling adds "fake" nodes so that they can be exactly halved at each pooling step;</p><p>this not only injects noisy information in the graph signal, but also increases the computational complexity in the GNN. Finally, clustering depends on the initial ordering of the nodes, which hampers stability and reproducibility.</p><p>An alternative approach is to directly cluster the rows (or the columns) of the adjacency matrix, as done by approach proposed in <ref type="bibr" target="#b37">[38]</ref>, which decomposes A in two matrices W ∈ R N ×K and H ∈ R K×N using the Non-negative Matrix Factorization (NMF) A ≈ WH. The NMF inherently clusters the columns of A since the minimization of the NMF objective is equivalent to the objective in k-means clustering <ref type="bibr" target="#b38">[39]</ref>. In particular, W is interpreted as the cluster representatives matrix and H as a soft-cluster assignment matrix of the columns in A. Therefore, the pooled node features and the coarsened graph can be obtained as X (1) = H T X and A (1) = H T AH, respectively. The main drawback is that NMF does not scale well to large graphs. b) Feature-based pooling methods: these methods compute a coarsened version of the graph through differentiable functions, which are parametrized by weights that are optimized for the task at hand. Differently from topological pooling, these methods account for the node features, which change as the GNN is trained. While this gives more flexibility in adapting the coarsening on the data and the task at hand, GNNs with feature-based pooling have more parameters; as such, training is slower and more difficult.</p><p>DiffPool <ref type="bibr" target="#b39">[40]</ref> is a pooling method that learns differentiable soft assignments to cluster the nodes at each layer. DiffPool uses two MP layers in parallel: one to update the node features, and one to generate soft cluster assignments. The original adjacency matrix acts as a prior when learning the cluster assignments, while an entropy-based regularization encourages sparsity in the cluster assignments. The application of this method to large graphs is not practical, as the cluster assignment matrix is dense and its size is N × K, where K is the number of nodes of the coarsened graph.</p><p>A second approach, dubbed Top-K pooling <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, learns a projection vector that is applied to each node feature to obtain a score. The nodes with the K highest scores are retained, while the remaining ones are dropped. Since the top-K selection is not differentiable, the scores are also used as a gating for the node features, allowing gradients to flow through the projection vector during backpropagation. Top-K is more memory efficient than DiffPool as it avoids generating cluster assignments. A variant proposed in <ref type="bibr" target="#b42">[43]</ref> introduces in Top-K pooling a soft attention mechanism for selecting the nodes to retain. Another variant of Top-K, called SAGPool, processes the node features with an additional MP layer before using them to compute the scores <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We consider two tasks on graph-structured data: graph classification and graph signal classification. The code used in all experiments is based on the Spektral library <ref type="bibr" target="#b44">[45]</ref>, and the code to replicate all experiments of this paper is publicly available at GitHub. 2 2 github.com/danielegrattarola/decimation-pooling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph classification</head><p>In this task, the i-th sample is a graph represented by the pair {A i , X i } which must be classified with a label y i . We consider 2 synthetic datasets (Bench-easy and Bench-hard) <ref type="bibr" target="#b2">3</ref> and 8 datasets of real-world graphs: Proteins, Enzymes, NCI1, MUTAG, Mutagenicity, D&amp;D, COLLAB, and Reddit-Binary. <ref type="bibr" target="#b3">4</ref> When node features X are not available, we use node degrees and clustering coefficients as a surrogate. Moreover, we also use node labels as node features whenever they are available.</p><p>In the following, we compare NDP with GRACLUS <ref type="bibr" target="#b10">[11]</ref>, NMF <ref type="bibr" target="#b37">[38]</ref>, DiffPool <ref type="bibr" target="#b39">[40]</ref>, and Top-K <ref type="bibr" target="#b40">[41]</ref>. In each experiment we adopt a fixed network architecture, MP(32)-P(2)-MP(32)-P(2)-MP(32)-AvgPool-Softmax, where MP(32) stands for a MP layer as described in (1) configured with 32 hidden units and ReLU activations, P(2) is a pooling operation with stride 2, AvgPool is a global average pooling operation on all the remaining graph nodes, and Softmax indicates a dense layer with Softmax activation. As training algorithm, we use Adam <ref type="bibr" target="#b45">[46]</ref> with initial learning rate 5e-4 and L 2 regularization with weight 5e-4. As an exception, for the Enzymes dataset we used MP(64).</p><p>Additional baselines are the Weisfeiler-Lehman (WL) graph kernel <ref type="bibr" target="#b46">[47]</ref>, a GNN with only MP layers (Flat), and a network with only dense layers (Dense). The comparison with Flat helps to understand whether pooling operations are useful for a given task. The results obtained by Dense, instead, help to quantify how much additional information is brought by the graph structure compared to considering the node features alone. While recent graph kernels <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> and GNN architectures <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> could be considered as further baselines for graph classification, the focus of our analysis and discussion is on graph pooling operators and, therefore, we point the interested reader towards the referenced papers.</p><p>To train the GNN on mini-batches of graphs with a variable number of nodes, we consider the disjoint union of the graphs in each mini-batch and train the GNN on the combined Laplacians and graph signals. See the supplementary material for an illustration.</p><p>We evaluate the model's performance by splitting the dataset in 10 folds. Each fold is, in turn, selected as the test set, while the remaining 9 folds become the training set. For each different train/test split, we set aside 10% of the training data as validation set, which is used for early stopping, i.e., we interrupt the training procedure after the loss on the validation set does not decrease for 50 epochs.</p><p>We report in <ref type="table" target="#tab_1">Table I</ref> the test accuracy averaged over the 10 folds. We note that no architecture outperforms every other in all tasks. The WL kernel achieves the best results on NCI1 and Mutagenicity, but it does not perform well on the other datasets. Interestingly, the Dense architecture achieves the best performance on MUTAG, indicating that in this case, the connectivity of the graps does not carry useful information for the classification task. The performance of the Flat baseline indicates that in Enzymes and COLLAB pooling operations are not necessary to improve the classification accuracy. NDP consistently achieves a higher accuracy compared to GRACLUS and NMF, which are also topological pooling methods. We argue that the lower performance of GRACLUS is due to the fake nodes, which introduce noise in the graphs. Among the two feature-based pooling methods, DiffPool always outperforms Top-K. The reason is arguably that Top-K drops entire parts of the graphs, thus discarding important information for the classification <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In <ref type="figure">Fig. 7</ref>, we report the training time for the five different pooling methods. As expected, GNNs configured with GRA-CLUS, NMF, and NDP are much faster to train compared to those based on DiffPool and TopK, with NDP being slightly faster than the other two topological methods. In <ref type="figure">Fig. 8</ref>, we plot the average training time per epoch against the average accuracy obtained by each pooling method on the 10 datasets taken into account. The scatter plot is obtained from the data reported in Tab. I and <ref type="figure">Fig. 7</ref>. On average, NDP obtains the highest classification accuracy, slightly outperforming even Diffpool, while being, at the same time, the fastest among all pooling methods.</p><p>To understand the differences between the topological pooling methods, we randomly selected one graph from the Proteins dataset and show in <ref type="figure" target="#fig_8">Fig. 9</ref> the coarsened graphs computed by GRACLUS, NMF, and NDP. From <ref type="figure" target="#fig_8">Fig. 9(b)</ref> we notice that the graphs A <ref type="bibr" target="#b0">(1)</ref> and A <ref type="bibr" target="#b1">(2)</ref> in GRACLUS have additional nodes that are disconnected. As discussed in Sect. V, these are the fake nodes that are added to the graph so that its size can be halved at every pooling operation. <ref type="figure" target="#fig_8">Fig. 9(c)</ref> shows that NMF produces graphs that are very dense, as a consequence of the multiplication with the dense soft-assignment matrix to construct the coarsened graph. Finally, <ref type="figure" target="#fig_8">Fig. 9(d)</ref> shows that NDP produces coarsened graphs that are sparse and preserve well the topology of the original graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph signal classification</head><p>In this task, different graph signals X i , defined on the same adjacency matrix A, must be classified with a label y i . We use the same architecture adopted for graph classification, with the only difference that each pooling operation is now implemented with stride 4: MP(32)-P(4)-MP(32)-P(4)-MP(32)-AvgPool-Softmax. We recall that when using NDP a stride of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DiffPool</head><p>Top-K GRACLUS NMF NDP</p><p>24.00 ± 0.0 11.00 ± 0.0 96.21 ± 0.18 94.15± 0.17 97.09 ± 0.01 4 is obtained by applying two decimation matrices in cascade, S (1) S (0) and S (3) S (2) (cf. Sec. III-D). We perform two graph signal classification experiments: image classification on MNIST and sentiment analysis on IMDB dataset.</p><p>MNIST. For this experiment, we adopt the same settings described in <ref type="bibr" target="#b10">[11]</ref>. To emulate a typical convolutional network operating on a regular 2D grid, an 8-NN graph is defined on the 28×28 pixels of the MNIST images, using as edge weights the following similarity score between nodes:</p><formula xml:id="formula_28">a ij = exp − p i − p j 2 σ 2 ,<label>(23)</label></formula><p>where p i and p j are the 2D coordinates of pixel i and j. The graph signal X i ∈ R 784×1 is the i-th vectorized image. Tab. II reports the average results achieved over 10 independent runs by a GNN implemented with different pooling operators. Contrarily to graph classification, DiffPool and TopK fail to solve this task and achieve an accuracy comparable to random guessing. On the contrary, the topological pooling methods obtain an accuracy close to a classical CNN, with NDP significantly outperforming the other two techniques.</p><p>We argue that the poor performance of the two featurebased pooling methods is attributable to 1) the low information content in the node features, and 2) a graph that has a regular structure and is connected only locally. This means that the graph has a very large diameter (maximum shortest path), where information propagates slowly through MP layers. Therefore, even after MP, nodes in very different parts of the graph will end up having similar (if not identical) features, which leads feature-based pooling methods to assign them to the same cluster. As a result the graph collapses, becoming densely connected and losing its original structure. On the other hand, topological pooling methods can preserve the graph structure by operating on the whole adjacency matrix at once to compute  the coarsened graphs and are not affected by uninformative node features. IMDB. We consider the IMDB sentiment analysis dataset of movies reviews, which must be classified as positive or negative. We use a graph that encodes the similarity of all words in the vocabulary. Each graph signal represents a review and consists of a binary vector with size equal to the vocabulary, which assumes value 1 in correspondence of a word that appears at least once in the review, and 0 otherwise.</p><p>The graph is built as follows. First, we extract a vocabulary from the most common words in the reviews. For each review, we consider at most 256 words, padding with a special token the reviews that are shorter and truncating those that are longer. Then, we train a simple classifier consisting of a word embedding layer <ref type="bibr" target="#b52">[53]</ref> of size 200, followed by a dense layer with a ReLU activation, a dropout layer <ref type="bibr" target="#b53">[54]</ref> with probability 0.5, and a dense layer with sigmoid activation. After training, we extract the embedding vector of each word in the vocabulary and construct a 4-NN graph, according to the Euclidean similarity between the embedding vectors.</p><p>As baselines, we consider the network used to generate the word embeddings (Dense) and two more advanced architectures. The first (LSTM), is a network where the dense hidden layer is replaced by an LSTM layer <ref type="bibr" target="#b54">[55]</ref>, which allows capturing the temporal dependencies in the sequence of words in the review. The other baseline (TCN) is a network where the hidden (a) Original graph.  Similarly to the MNIST experiment, we notice that neither DiffPool nor TopK are able to solve this graph signal classification task. The reason can be once again attributed to the low information content of the individual node features and in the sparsity of the graph signal (most node features are 0), which makes it difficult for the feature-based pooling methods to infer global properties of the graph by looking at local sub-structures.</p><formula xml:id="formula_29">A 1 A 2 A 3 (b) GRACLUS coarsening. A 1 A 2 A 3 (c) NMF coarsening. A 1 A 2 A 3 (d) NDP coarsening.</formula><p>On the other hand, NDP consistently outperforms the baselines, GRACLUS, and NMF. The coarsened graphs generated by NMF when the vocabulary has 10k words are too dense to fit in the memory of the GPU (Nvidia GeForce RTX 2080). Interestingly, the GNNs configured with GRACLUS and NDP always achieve better results than the Dense network, even if the latter generates the word embeddings used to build the graph on which the GNN operates. This can be explained by the fact that the Dense network immediately overfits the dataset, whereas the graph structure provides a strong regularization, as the GNN combines only words that are neighboring on the vocabulary graph.</p><p>The LSTM baseline generally achieves a better accuracy than Dense, since it captures the sequential ordering of the words in the reviews, which also helps to prevent overfitting on training data. Finally, the TCN baseline always outperforms LSTM, both in terms of accuracy and computational costs. This substantiates recent findings showing that convolutional architectures may be more suitable than recurrent ones for tasks involving sequential data <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>We proposed Node Decimation Pooling (NDP), a pooling strategy for Graph Neural Networks that reduces a graph based on properties of its Laplacian. NDP partitions the nodes into two disjoint sets by optimizing a MAXCUT objective. The nodes of one set are dropped, while the others are connected with Kron reduction to form a new smaller graph. Since Kron reduction yields dense graphs, a sparsification procedure is used to remove the weaker connections.</p><p>The algorithm we proposed to approximate the MAXCUT solution is theoretically grounded on graph spectral theory and achieves good results while being, at the same time, simple and efficient to implement. To evaluate the MAXCUT solution, we considered theoretical bounds and we introduced an experimental framework to empirically assess the quality of the solution.</p><p>We demonstrated that the graph sparsification procedure proposed in this work preserves the spectrum of the graph up to an arbitrarily small constant. In particular, we first derived an analytical relationship between the eigenvalues of the adjacency matrix of the original and sparsified graphs. Then, we performed numerical experiments to study how much the spectrum of the graph Laplacian varies, in practice, after sparsification.</p><p>We compared NDP with two main families of pooling methods for GNNs: topological (to which NDP belongs) and feature-based methods. NDP has advantages compared to both types of pooling. In particular, experimental results showed that NDP is computationally cheaper (in terms of both time and memory) than feature-based methods, while it achieves competitive performance on all the downstream tasks taken into account. An important finding in our results indicates that topological methods are the only viable approach in graph signal classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for critically reading our manuscript and for giving us important suggestions, which allowed us to significantly improve our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Kron reduction in graph with self-loops</head><p>If A contains self loops, the existence of the strict inequality condition L ii &gt; n j=1,j =i |L ij | discussed in Sec. III-B is no more guaranteed. However, it is sufficient to consider the loopy-</p><formula xml:id="formula_30">Laplacian Q = D−A+2diag(A), where diag(A) is the diagonal of A, defined as {A ii } N i=1 .</formula><p>Q is now an irreducible matrix and Q ii &gt; n j=1,j =i |Q ij | + A ii holds for at least least one vertex i ∈ V + . We notice that the adjacency matrix can be univocally recovered:</p><formula xml:id="formula_31">A = −Q + diag({ N j=1,j =i Q ij } N i=1</formula><p>). Therefore, from the Kron reduction Q (1) of Q we can first recover A <ref type="bibr" target="#b0">(1)</ref> and then compute the reduced Laplacian as L (1) = D (1) − A <ref type="bibr" target="#b0">(1)</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation of the MAXCUT upperbound</head><p>Let us consider the Rayleigh quotient</p><formula xml:id="formula_32">r(z, L) = z T Lz z T z ,<label>(24)</label></formula><p>which assumes its maximum value λ max when z is the largest eigenvector of the Laplacian L. When z is the partition vector in (5), we have r(z, L) ≤ λ max . As shown in Sect. III-A, the numerator in <ref type="bibr" target="#b23">(24)</ref> can be rewritten as <ref type="bibr" target="#b4">(5)</ref>, and where cut(z) is the volume of edges crossing the partition induced by z. From (5) also follows that the denominator in (24) is z T z = N , since z 2 i = 1, ∀i. By combining the results we obtain</p><formula xml:id="formula_33">z T Lz = i,j∈E a ij (z i − z j ) 2 = i,j∈V + a ij (z i − z j ) 2 + i,j∈V − a ij (z i − z j ) 2 + i∈V + ,j∈V − a ij (z i − z j ) 2 = 0 + 0 + i∈V + ,j∈V − a ij 2 2 = 4 · cut(z), since z i = 1 if i ∈ V + and z i = −1 if i ∈ V − according to</formula><formula xml:id="formula_34">4 · cut(z) N ≤ λ max , ∀z ∈ R N → MAXCUT ≤ λ max N 4 .<label>(25)</label></formula><p>When considering the symmetric Laplacian L s , we multiply (24) on both sides by D −1/2 , changing the denominator into</p><formula xml:id="formula_35">z T Dz = i,i d ii z 2 i = 2|E|.</formula><p>Replacing in (25) N with 2|E| and λ max with λ s max , we get the bound MAXCUT/|E| ≤ λ s max /2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relationship with Trevisan [28] spectral algorithm</head><p>The main result in <ref type="bibr" target="#b27">[28]</ref> states that if λ s max ≥ 2(1 − τ ), then there exist a set of vertices V and a partition (</p><formula xml:id="formula_36">V 1 , V 2 ) of V so that |e(V 1 , V 2 )| ≥ 1 2 (1 √ 16τ )vol(V), where vol(V) = i∈V d i and e(V 1 , V 2 )</formula><p>are the edges with one endpoint in V 1 and the other in V 2 . In cases where an optimal solution cuts 1 − τ fraction of the edges, a partition found by a recursive spectral algorithm will remove 1 − 4 √ τ + 8τ of the edges. The optimal τ is value 0.0549 for which 1−4 √ τ +8τ 1−τ reaches its minimum 0.5311. When the largest eigenvalue λ s max is too small, the expected random cut is larger than the solution found by the spectral algorithm. The analysis in <ref type="bibr" target="#b27">[28]</ref> shows that the spectral cut is guaranteed to be larger than the random cut only when λ s max ≥ 2(1 − τ ), i.e., when λ s max ≥ 1.891 given the optimal value τ = 0.0549. Therefore, an algorithm that recursively cuts a fraction of edges according to the values in v s max until λ s max ≥ 2(1 − τ ) and then performs a random cut, finds a solution that is always ≥ 0.5311 MAXCUT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cut size on regular and random graphs</head><p>In the following, we consider two different types of bipartite graphs, a regular grid and a ring graph, and four classes of random graphs, which are the Stochastic Block Model (SBM), a sensor network, the Erdos-Renyi graph, and a community graph. A graphical representation in the node space and the adjacency matrix of one instance of each graph type is depicted in <ref type="figure" target="#fig_9">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regular grid Ring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Block Model Sensor network</head><p>Erdos-Renyi Community graph In Tab. IV we report the size of the cut γ(z) induced by the partition z, which is obtained with the proposed spectral algorithm. We consider both the partitions obtained from the eigenvectors v max and v s max , associated with the largest eigenvalue of the Laplacian L and the symmetric Laplacian L s , respectively. The values in Tab. IV are the mean and standard deviation of γ(z) obtained on 50 different instances of each class. We also report the MAXCUT upperbound, λ s max /2 and the size of the cut induced by a random partition. Consistently better performance are obtained when the partition is based on v s max rather than v max ; as discussed in Sect. IV-A, this is because many entries in v max have small values that cannot be partitioned precisely according to the sign, due to numerical errors. The results show that on the two regular graphs, which are bipartite, the cut obtained with the spectral algorithm coincides with the MAXCUT upper bound and, therefore, also with the optimal solution. For every other graph, the cut yielded by the spectral algorithm is always larger than the random cut. We recall that in those cases the MAXCUT is unknown and the gaps between the lower bound (0.5) and the upper bound (λ s max /2) can be arbitrarily large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spectral and random cut as a function of edge density</head><p>We replicate for each graph type the experiment in Sect. IV-B, which illustrates how the size of the cut obtained with the proposed algorithm changes as we randomly add edges. <ref type="figure" target="#fig_10">Fig. 11</ref> reports in blue the size of the cut associated with the partition yielded by the spectral algorithm; in orange the size of the cut yielded by the random partition; in green the MAXCUT upper bound; in black the theoretical threshold that indicates when to switch to the random partition to obtain a cut with size ≥ 0.53</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAXCUT.</head><p>The examples encompass the two extreme cases where the MAXCUT solution is known: a bipartite graph where MAXCUT is 1 and the complete graph where MAXCUT is 0.5. In every example, when λ s max becomes lower than 1 − τ the solution of the spectral algorithm is still larger than the cut induced by the random partition. In fact, the spectral cut remains larger than the random cut until when the density is approximately 70-80%. Importantly, when the solution of the spectral algorithm become worse than the random cut, the MAXCUT upper bound is close to 0.5. Therefore, when the spectral cut is lower than 0.5 it is possible to return the random partition instead, which yields a nearly-optimal solution. <ref type="figure">Fig. 12</ref> shows for the result of the NDP coarsening procedure on the 6 types of graphs. The first column shows the subset of nodes of the original graph that are selected (V + , in red) and discarded (V − , in blue) after each pooling step. The second column shows the coarsened graph obtained after each pooling operation. Finally, columns 3 and 4 show the coarsened graphs after applying sparsification with different thresholds .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visual examples of coarsening with NDP pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spectral similarity in sparsified graphs</head><p>In Sec. IV-E we introduced the spectral similarity distance to quantify how much the spectrum of the Laplacian associated with the sparsified adjacency matrix changes when edges smaller than are dropped. In <ref type="figure" target="#fig_2">Fig. 13</ref> we show how the graph structure (in terms of spectral similarity) varies, when the value of increases and more edges are dropped. In every example, for small values of the structure of the graphs changes only slightly while a large amount of edges is dropped. Notably, the spectral similarity increases almost linearly with , while the edge density decreases exponentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Mini-batch training</head><p>Problems such as graph classification and graph regression are characterized by samples of graphs that, generally, have a variable number of vertices. In order to apply MP and pooling operations when training a GNN on mini-batches, one solution is to perform zero-padding and obtain all graphs with N max vertices, where N max is the number of vertices in the largest graph of the dataset. However, this solution is particularly inefficient in terms of memory cost, especially when there are many graphs with less than N max vertices. A more efficient solution is to build the disjoint union of the graphs in each mini-batch and train the GNN on the combined Laplacian and graph signal. This is the solution adopted in our experiments; <ref type="figure" target="#fig_3">Fig. 14</ref>    <ref type="figure" target="#fig_2">Fig. 13</ref>. In blue, the variation of spectral distance between the Laplacian L associated with A and the LaplacianL associated with the adjacency matrixĀ sparsified with a varying threshold . In red, the number of edges that remain inL. Example of the implementation used in the graph classification task, where the GNN is fed with a disjoint union of the graphs in mini-batch. The illustration shows an example for a mini-batch of size three. <ref type="figure" target="#fig_4">Fig. 15</ref> reports the evolution of the loss during training for 4 different graph classification datasets. Notice that in our experiments we used early stopping. However, to provide a more extended profile of the training procedure, we show the training curves obtained when the GNN is trained for 1000 epochs on different datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training curves</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(Left) distribution and values assumed by vmax. (Right) distribution and values assumed by vmax. The entries of the eigenvectors are sorted by node degree. A Stochastic Block Model graph was used in this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>0.03 0.13 0.22 0.32 0.42 0.51 0.61 0.71 0.8 0.9 Blue line: fraction of edges cut by the partition yield by the spectral algorithm. Orange line: fraction of edges removed by a random cut. Green line: the MAXCUT upper bound as a function of the largest eigenvalue λ s max of the symmetric Laplacian. Black line: the threshold from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Top-left: Spectrum of the Laplacians associated with the original adjacency A (0) and the coarsened versions A (1) , A (2) , and A (3) obtained with the NDP algorithm. Top-right: Spectrum of the Laplacians associated with the sparsified adjacency matricesĀ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Average training time per epoch (in seconds) for different pooling methods. The bars height is in logarithmic scale. Simulations were performed with an Nvidia RTX 2080 Ti. Average training time per epoch against average accuracy, computed for each pooling method over the 10 graph classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Example of coarsening on one graph from the Proteins dataset. In (a), the original adjacency matrix of the graph. In (b), (c), and (d) the edges of the Laplacians at coarsening level 0, 1, and 2, as obtained by the 3 different pooling methods GRACLUS, NMF, and the proposed NDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Graphical representation and adjacency matrix of 2 regular graphs (Regular grid and Ring) and an instance of 4 random graphs (Stochastic Block Model (SBM), Sensor network, Erdos-Renyi graph, and Community graph).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Blue line: fraction of edges cut by the partition yielded by the spectral algorithm. Orange line: fraction of edges removed by a random cut. Green line: the MAXCUT upper bound λ s max /2. Black line: the threshold from [28] indicating the value of λ s max /2 below which one should switch to the random cut to obtain a solution ≥ 0.53 MAXCUT. The x-axis indicates the density of the graph connectivity, which increases by randomly adding edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 14. Example of the implementation used in the graph classification task, where the GNN is fed with a disjoint union of the graphs in mini-batch. The illustration shows an example for a mini-batch of size three.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Accuracy in training and validation over 1000 epochs on 4 different datasets. The curves are averaged over 10 runs per method and per dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>filippombianchi@gmail.com F. M. Bianchi is with the Dept. of Mathematics and Statistics, UiT the Arctic University of Norway and with NORCE, Norwegian Research Centre D. Grattarola is with the Faculty of Informatics, Università della Svizzera italiana, Switzerland L. Livi is with Dept.s. of Computer Science and Mathematics, University of Manitoba, Canada, and Dept. of Computer Science, University of Exeter, United Kingdom</figDesc><table /><note>C. Alippi is with Faculty of Informatics, Università della Svizzera italiana, Switzerland, and Dept. of Electronics, Information, and Bioengineering, Politecnico di Milano, Italy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I GRAPH</head><label>I</label><figDesc>CLASSIFICATION ACCURACY. SIGNIFICANTLY BETTER RESULTS (p &lt; 0.05) ARE IN BOLD.</figDesc><table><row><cell>Dataset</cell><cell>WL</cell><cell>Dense</cell><cell>Flat</cell><cell>Diffpool</cell><cell>Top-K</cell><cell>GRACLUS</cell><cell>NMF</cell><cell>NDP</cell></row><row><cell>Bench-easy</cell><cell>92.6</cell><cell>29.3±0.3</cell><cell>98.5±0.3</cell><cell>98.6±0.4</cell><cell>82.4±8.9</cell><cell>97.5±0.5</cell><cell>97.4±0.8</cell><cell>97.4±0.9</cell></row><row><cell>Bench-hard</cell><cell>60.0</cell><cell>29.4±0.3</cell><cell>67.6±2.8</cell><cell>69.9±1.9</cell><cell>42.7±15.2</cell><cell>69.0±1.5</cell><cell>68.6±1.6</cell><cell>71.9±0.8</cell></row><row><cell>Proteins</cell><cell>71.2±2.6</cell><cell>68.7±3.3</cell><cell>72.6±4.8</cell><cell>72.8±3.5</cell><cell>69.6±3.3</cell><cell>70.3±2.6</cell><cell>71.6±4.1</cell><cell>73.4±3.1</cell></row><row><cell>Enzymes</cell><cell>33.6±4.1</cell><cell>45.7±9.9</cell><cell>52.0±12.3</cell><cell>24.6±5.3</cell><cell>31.4±6.8</cell><cell>42.0±6.7</cell><cell>39.9±3.6</cell><cell>44.5±7.4</cell></row><row><cell>NCI1</cell><cell>81.1±1.6</cell><cell>53.7±3.0</cell><cell>74.4±2.5</cell><cell>76.5±2.2</cell><cell>71.8±2.6</cell><cell>69.5±1.7</cell><cell>68.2±2.2</cell><cell>74.2±1.7</cell></row><row><cell>MUTAG</cell><cell>78.9±13.1</cell><cell>91.1±7.1</cell><cell>87.1±6.6</cell><cell>90.5±3.9</cell><cell>85.5±11.0</cell><cell>84.9±8.1</cell><cell>76.7±14.4</cell><cell>87.9±5.7</cell></row><row><cell>Mutagenicity</cell><cell>81.7±1.1</cell><cell>68.4±0.3</cell><cell>78.0±1.3</cell><cell>77.6±2.7</cell><cell>71.9±3.7</cell><cell>74.4±1.8</cell><cell>75.5±1.7</cell><cell>77.9±1.4</cell></row><row><cell>D&amp;D</cell><cell>78.6±2.7</cell><cell>70.6±5.2</cell><cell>76.8±1.5</cell><cell>79.3±2.4</cell><cell>69.4±7.8</cell><cell>70.5±4.8</cell><cell>70.6±4.1</cell><cell>72.8±5.4</cell></row><row><cell>COLLAB</cell><cell>74.8±1.3</cell><cell>79.3±1.6</cell><cell>82.1±1.8</cell><cell>81.8±1.4</cell><cell>79.3±1.8</cell><cell>77.1±2.1</cell><cell>78.5±1.8</cell><cell>79.1±1.3</cell></row><row><cell>Reddit-Binary</cell><cell>68.2±1.7</cell><cell>48.5±2.6</cell><cell>80.3±2.6</cell><cell>86.8±2.1</cell><cell>74.7±4.5</cell><cell>79.2±0.4</cell><cell>52.0±2.1</cell><cell>88.0±1.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II GRAPH</head><label>II</label><figDesc>SIGNAL CLASSIFICATION ACCURACY ON MNIST.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III GRAPH</head><label>III</label><figDesc>SIGNAL CLASSIFICATION ACCURACY ON IMDB SENTIMENT ANALYSIS DATASET.</figDesc><table><row><cell># Words</cell><cell>Dense</cell><cell>LSTM</cell><cell>TCN</cell><cell>DiffPool</cell><cell>Top-K</cell><cell>GRACLUS</cell><cell>NMF</cell><cell>NDP</cell></row><row><cell>1k</cell><cell>82.65±0.01</cell><cell>86.58±0.03</cell><cell>85.61±0.14</cell><cell>50.00±0.0</cell><cell>50.00±0.0</cell><cell>85.03±0.10</cell><cell>82.51±0.11</cell><cell>85.77±0.03</cell></row><row><cell>5k</cell><cell>86.26±0.03</cell><cell>86.59±0.06</cell><cell>87.42±0.09</cell><cell>50.00±0.0</cell><cell>50.00±0.0</cell><cell>87.55±0.15</cell><cell>85.66±0.11</cell><cell>87.79±0.02</cell></row><row><cell>10k</cell><cell>83.75±0.02</cell><cell>85.98±0.04</cell><cell>87.38±0.07</cell><cell>50.00±0.0</cell><cell>50.00±0.0</cell><cell>87.29±0.07</cell><cell>OOM</cell><cell>87.82±0.02</cell></row><row><cell cols="5">layers are 1D convolutions with different dilation rates [56]. In</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">particular, we used a Temporal Convolution Network [57] with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">7 residual blocks with dilations [1, 2, 4, 8, 16, 32, 64], kernel</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">size 6, causal padding, and dropout probability 0.3. The results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">averaged over 10 runs for vocabularies of different sizes (#</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Words) are reported in Tab. III.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV SIZE</head><label>IV</label><figDesc>OF THE CUT OBTAINED WITH OUR SPECTRAL ALGORITHM ON DIFFERENT TYPES OF GRAPH. REPORTED IS THE MEAN AND STANDARD DEVIATION OF THE CUT OBTAINED FROM vMAX AND v s MAX ON 50 INSTANCES OF EACH GRAPH TYPE AND THE MAXCUT UPPERBOUND, λ S MAX /2. FOR COMPLETENESS, WE SHOW ALSO THE RESULTS OBTAINED BY THE RANDOM CUT.</figDesc><table><row><cell></cell><cell>Grid</cell><cell>Ring</cell><cell>SBM</cell><cell>Sensor</cell><cell cols="2">Erdos-Renyi Community</cell></row><row><cell>MAXCUT upperbound</cell><cell>1.0</cell><cell>1.0</cell><cell>0.63±0.0</cell><cell>0.77±0.05</cell><cell>0.67±0.0</cell><cell>0.89±0.06</cell></row><row><cell>Cut with vmax</cell><cell>1.0</cell><cell>1.0</cell><cell cols="2">0.51±0.03 0.53±0.03</cell><cell>0.55±0.02</cell><cell>0.5±0.05</cell></row><row><cell>Cut with v s max</cell><cell>1.0</cell><cell>1.0</cell><cell cols="2">0.58±0.01 0.58±0.02</cell><cell>0.61±0.0</cell><cell>0.54±0.04</cell></row><row><cell>Random cut</cell><cell cols="2">0.5±0.03 0.5±0.05</cell><cell>0.5±0.01</cell><cell>0.51±0.02</cell><cell>0.5±0.0</cell><cell>0.5±0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>reports a visualization of the procedure.</figDesc><table><row><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell></row><row><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell></row><row><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(a) Regular grid</cell><cell></cell><cell cols="2">(b) Ring graph</cell></row><row><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell></row><row><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell></row><row><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(c) Stochastic Block Model</cell><cell></cell><cell cols="2">(d) Sensor network</cell></row><row><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell><cell>1st pooling</cell><cell>A (1)</cell><cell>A (1) ( =0.01) A (1) ( =0.1)</cell></row><row><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell><cell>2nd pooling</cell><cell>A (2)</cell><cell>A (2) ( =0.01) A (2) ( =0.1)</cell></row><row><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell><cell>3rd pooling</cell><cell>A (3)</cell><cell>A (3) ( =0.01) A (3) ( =0.1)</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(e) Erdos-Renyi</cell><cell></cell><cell></cell><cell></cell></row></table><note>(f) Community graph Fig. 12. Coarsened graphs obtained with the NDP algorithm. The 3rd and 4th column show graphs sparsified with different threshold .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/FilippoMB/Benchmark_dataset_for_graph_ classification 4 http://graphlearning.io</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depthbased subgraph convolutional auto-encoder for network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning vertex convolutional networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09936</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep depth-based representations of graphs through deep learning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">336</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Design of graph filters and filterbanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Borgnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cooperative and Graph Signal Processing</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="299" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multiscale pyramid transform for graph signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Faraji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2119" to="2134" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Laplacians and the cheeger inequality for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computational approaches to max-cut,&quot; in Handbook on semidefinite, conic and polynomial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piccialli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rendl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiegele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="821" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Goemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1115" to="1145" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An agent-based algorithm exploiting multiple local dissimilarities for clusters mining and knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maiorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1347" to="1369" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Departmental Papers</title>
		<imprint>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2000" />
			<publisher>CIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2729" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kron reduction of graphs with applications to electrical networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dorfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral sparsification of graphs: theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max cut and the smallest eigenvalue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1769" to="1786" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fundamentals of matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Comparing graph spectra of adjacency and laplacian matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lutzeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03769</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparison of spectral methods through the adjacency matrix and the laplacian of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zumstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TH Diploma</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph reduction with spectral and cut guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kernel k-means: spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A non-negative factorization approach to node pooling in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Sotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference of the Italian Association for Artificial Intelligence. AIIA</title>
		<meeting>the 18th International Conference of the Italian Association for Artificial Intelligence. AIIA</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2005 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning withdifferentiable pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hongyang Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International conference on Machine learning (ICML)</title>
		<meeting>the 36th International conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velicković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems -Relational Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8673-understanding-attention-and-generalization-in-graph-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12138</idno>
		<title level="m">Graph neural networks in tensorflow and keras with spektral</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">Sep</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">(hyper) graph embedding and classification via simplicial complexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giuliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">223</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wasserstein weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6439" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning backtrackless aligned-spatial graph convolutional networks for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Walksteered convolution for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
