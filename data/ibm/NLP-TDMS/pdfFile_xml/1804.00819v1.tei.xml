<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Dense Video Captioning with Masked Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
							<email>luozhou@umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
							<email>yingbo.zhou@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
							<email>jjcorso@eecs.umich.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>richard@socher.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salesforce</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Dense Video Captioning with Masked Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video has become an important source for humans to learn and acquire knowledge (e.g. video lectures, making sandwiches <ref type="bibr" target="#b20">[21]</ref>, changing tires <ref type="bibr" target="#b0">[1]</ref>). Video content consumes high cognitive bandwidth, and thus is slow for humans to digest. Although the visual signal itself can some- <ref type="bibr">Figure 1</ref>. Dense video captioning is to localize (temporal) events from a video, which are then described with natural language sentences. We leverage temporal convolutional networks and selfattention mechanisms for precise event proposal generation and captioning. times disambiguate certain semantics, one way to make video content more easily and rapidly understood by humans is to compress it in a way that retains the semantics. This is particularly important given the massive amount of video being produced everyday. Video summarization <ref type="bibr" target="#b41">[42]</ref> is one way of doing this, but it loses the language components of the video, which are particularly important in instructional videos. Dense video captioning <ref type="bibr" target="#b19">[20]</ref>-describing events in the video with descriptive natural language-is another way of achieving this compression while retaining the language components.</p><p>Dense video captioning can be decomposed into two parts: event detection and event description. Existing methods tackle these two sub-problems using event proposal and captioning modules, and exploit two ways to combine them for dense video captioning. One way is to train the two modules independently and generate descriptions for the best event proposals with the best captioning model <ref type="bibr" target="#b12">[13]</ref>. The other way is to alternate training <ref type="bibr" target="#b19">[20]</ref> between the two modules, i.e., alternate between i) training the proposal module only and ii) training the captioning module on the positive event proposals while fine-tuning the proposal module. However, in either case, the language information cannot have direct impacts on the event proposal.</p><p>Intuitively, the video event segments and language are closely related and the language information should be able to help localize events in the video. To this end, we propose an encoder-decoder based end-to-end model for doing dense video captioning (see <ref type="figure">Fig. 1</ref>). The encoder encodes the video frames (features) into the proper representation. The proposal decoder then decodes this representation with different anchors to form event proposals, i.e., start and end time of the event, and a confidence score. The captioning decoder then decodes the proposal specific representation using a masking network, which converts the event proposal into a differentiable mask. This continuous mask enables both the proposal and captioning decoder to be trained consistently, i.e. the proposal module now learns to adjust its prediction based on the quality of the generated caption. In other words, the language information from caption now is able to guide the visual model to generate more plausible proposals. In contrast to the existing methods where the proposal module solves a class-agnostic binary classification problem regardless the details in the video content, our model enforces the consistency between the content in the proposed video segment and the semantic information in the language description.</p><p>Another challenge for dense video captioning, and more broadly for sequence modeling tasks, is the need to learn a representation that is capable of capturing long term dependencies. Recurrent Neural Networks (RNN) are possible solutions to this problem, however, learning such representation is still difficult <ref type="bibr" target="#b23">[24]</ref>. Self-attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref> allows for an attention mechanism within a module and is a potential way to learn this long-range dependence. In selfattention the higher layer in the same module is able to attend to all states below it. This made the length of the paths of states from the higher layer to all states in the lower layer to be one, and thus facilitates more effective learning. The shorter path length facilitates learning these dependencies because larger gradients can now pass to all states. Transformer <ref type="bibr" target="#b29">[30]</ref> implements a fast self-attention mechanism and has demonstrated its effectiveness in machine translation. Unlike traditional sequential models, transformer does not require unrolling across time, and therefore trains and tests much faster as compared to RNN based models. We employ transformer in both the encoder and decoder of our model.</p><p>Our main contributions are twofold. First, we propose an end-to-end model for doing dense video captioning. A differentiable masking scheme is proposed to ensure the consistency between proposal and captioning module during training. Second, we employ self-attention: a scheme that facilitates the learning of long-range dependencies to do dense video captioning. To the best of our knowledge, our model is the first one that does not use a RNN-based model for doing dense video captioning. In addition, we achieve competitive results on ActivityNet Captions <ref type="bibr" target="#b19">[20]</ref> and YouCookII <ref type="bibr" target="#b42">[43]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image and Video Captioning. In contrast to earlier video captioning papers, which are based on models like hidden Markov models and ontologies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b6">7]</ref>, recent work on captioning is dominated by deep neural network-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27]</ref>. Generally, they use Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref> for encoding video frames, followed by a recurrent language decoder, e.g., Long Short-Term Memory <ref type="bibr" target="#b17">[18]</ref>. They vary mainly based on frame encoding, e.g., via mean-pooling <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b10">11]</ref>, recurrent nets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, and attention mechanisms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11]</ref>. The attention mechanism was initially proposed for machine translation <ref type="bibr" target="#b2">[3]</ref> and has achieved top performance in various language generation tasks, either as temporal attention <ref type="bibr" target="#b35">[36]</ref>, semantic attention <ref type="bibr" target="#b10">[11]</ref> or both <ref type="bibr" target="#b22">[23]</ref>. Our work falls into the first of the three types. In addition to using cross-module attention, we apply self-attention <ref type="bibr" target="#b29">[30]</ref> within each module. Temporal Action Proposals. Temporal action proposals (TAP) aim to temporally localize action-agnostic proposals in a long untrimmed video. Existing methods formulate TAP as a binary classification problem and differ in how the proposals are proposed and discriminated from the background. Shuo et al. <ref type="bibr" target="#b27">[28]</ref> propose and classify proposal candidates directly over video frames in a sliding window fashion, which is computationally expensive. More recently, inspired by the anchoring mechanism from object detection <ref type="bibr" target="#b25">[26]</ref>, two types of methods have been proposedexplicit anchoring <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> and implicit anchoring <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>. In the former case, each anchor is an encoding of the visual features between the anchor temporal boundaries and is classfied as action or background. In implicit anchoring, recurrent networks encode the video sequence and, at each anchor center, multiple anchors with various sizes are proposed based on the same visual feature. So far, explicit anchoring methods accompanied with location regression yield better performance <ref type="bibr" target="#b11">[12]</ref>. Our proposal module is based upon Zhou et al. <ref type="bibr" target="#b42">[43]</ref>, which is designed to detect long complicated events rather than actions. We further improve the framework with a temporal convolutional proposal network and self-attention based context encoding. Dense Video Captioning. The video paragraph captioning method proposed by Yu et al. <ref type="bibr" target="#b40">[41]</ref> generates sentence descriptions for temporally localized video events. However, the temporal locations of each event are provided beforehand. Das et al. <ref type="bibr" target="#b6">[7]</ref> generates dense captions over the entire video using sparse object stitching, but their work relies on a top-down ontology for the actual description and is not data-driven like the recent captioning methods. The most similar work to ours is Krishna et al. <ref type="bibr" target="#b19">[20]</ref> who introduce a dense video captioning model that learns to propose the event locations and caption each event with a sentence. However, they combine the proposal and the captioning modules through co-training and are not able to take advantage of language to benefit the event proposal <ref type="bibr" target="#b16">[17]</ref>. To this end, we propose an end-to-end framework for doing dense video captioning that is able to produce proposal and description simultaneously. Also, our work directly incorporates the semantics from captions to the proposal module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>In this section we introduce some background on Transformer <ref type="bibr" target="#b29">[30]</ref>, which is the building block for our model. We start by introducing the scaled dot-product attention, which is the foundation of transformer. Given a query q i ∈ R d from all T queries, a set of keys k t ∈ R d and values v t ∈ R d where t = 1, 2, ..., T , the scaled dot-product attention outputs a weighted sum of values v t , where the weights are determined by the dot-products of query q and keys k t . In practice, we pack k t and v t into matricies K = (k 1 , ..., k T ) and V = (v 1 , ..., v T ), respectively. The attention output on query q is:</p><formula xml:id="formula_0">A(q i , K, V ) = V exp K T q i / √ d T t=1 exp{k T t q i / √ d}<label>(1)</label></formula><p>The multi-head attention consists of H paralleled scaled dot-product attention layers called "head", where each "head" is an independent dot-product attention. The attention output from multi-head attention is as below:</p><formula xml:id="formula_1">MA(q i , K, V ) = W O   head 1 · · · head H   (2) head j = A(W q j q i , W K j K, W V j V )<label>(3)</label></formula><p>where W q j , W K j , W V j ∈ R d H ×d are the independent head projection matrices, j = 1, 2, ..., H, and</p><formula xml:id="formula_2">W O ∈ R d×d .</formula><p>This formulation of attention is quite general, for example when the query is the hidden states from the decoder, and both the keys and values are all the encoder hidden states, it represents the common cross-module attention. Self-attention <ref type="bibr" target="#b29">[30]</ref> is another case of multi-head attention where the queries, keys and values are all from the same hidden layer (see also in <ref type="figure" target="#fig_0">Fig. 2</ref>). Now we are ready to introduce Transformer model, which is an encoder-decoder based model that is originally proposed for machine translation <ref type="bibr" target="#b29">[30]</ref>. The building block for Transformer is multi-head attention and a pointwise feed-forward layer. The pointwise feed-forward layer takes the input from multi-head attention layer, and further transforms it through two linear projections with ReLU activation. The feed-forward layer can also be viewed as two convolution layers with kernel size one. The encoder and decoder of Transformer is composed by multiple such building blocks, and they have the same number of layers. The decoder from each layer takes input from the encoder of the same layer as well as the lower layer decoder output. Self-attention is applied to both encoder and decoder. Cross-module attention between encoder and decoder is also applied. Note that the self-attention layer in the decoder can only attend to the current and previous positions to preserve the auto-regressive property. Residual connection <ref type="bibr" target="#b15">[16]</ref> is applied to all input and output layers. Additionally, layer normalization <ref type="bibr" target="#b1">[2]</ref> (LayerNorm) is applied to all layers. <ref type="figure" target="#fig_0">Fig. 2</ref> shows a one layered transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">End-to-End Dense Video Captioning</head><p>Our end-to-end model is composed of three parts: a video encoder, a proposal decoder, and a captioning decoder that contains a mask prediction network to generate text description from a given proposal. The video encoder is composed of multiple self-attention layers. The proposal decoder takes the visual features from the encoder and outputs event proposals. The mask prediction network takes the proposal output and generates a differentiable mask for a certain event proposal. To make the decoder caption the current proposal, we then apply this mask by element-wise multiplication between it, the input visual embedding and all outputs from proposal encoder. In the following sections, we illustrate each component of our model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Encoder</head><p>Each frame x t of the video X = {x 1 , . . . , x T } is first encoded to a continuous representation F 0 = {f 0 1 , . . . , f 0 T }. It is then fed forward to L encoding layers, where each layer learns a representation F l+1 = V (F l ) by taking input from previous layer l,</p><formula xml:id="formula_3">V(F l ) = Ψ(PF(Γ(F l )), Γ(F l )) (4) Γ(F l ) =   Ψ(MA(f l 1 , F l , F l ), f l 1 ) · · · Ψ(MA(f l T , F l , F l ), f l T )   (5) Ψ(α, β) = LayerNorm(α + β)<label>(6)</label></formula><formula xml:id="formula_4">PF(γ) = M l 2 max(0, M l 1 γ + b l 1 ) + b l 2<label>(7)</label></formula><p>where Ψ(·) represents the function that performs layer normalization on the residual output, PF(·) denotes the 2layered feed-forward neural network with ReLU nonlinearity for the first layer, M l 1 , M l 2 are the weights for the feedforward layers, and b l 1 , b l 2 are the biases. Notice the selfattention used in eq. 5. At each time step t, f l t is given as the query to the attention layer and the output is the weight sum of f l t , t = 1, 2, ..., T , which encodes not only the information regarding the current time step, but also all other time steps. Therefore, each time step of the output from the self-attention is able to encode all context information. In addition, it is easy to see that the length of the path between time steps is only one. In contrast to recurrent models, this makes the gradient update independent with respect to their position in time, and thus makes learning potential dependencies amongst distant frames easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposal Decoder</head><p>Our event proposal decoder is based on ProcNets <ref type="bibr" target="#b42">[43]</ref>, for its state-of-the-art performance on long dense event proposals. We adopt the same anchor-offset mechanism as in ProcNets and design a set of N explicit anchors for event proposals. Each anchor-based proposal is represented by an event proposal score P e ∈ [0, 1] and two offsets: center θ c and length θ l . The associated anchor has length l a and center c a . The proposal boundaries (S p , E p ) are determined by the anchor locations and offsets: c p = c a + θ c l a l p = l a exp{θ l },</p><formula xml:id="formula_5">S p = c p − l p /2 E p = c p + l p /2.<label>(8)</label></formula><p>These proposal outputs are obtained from temporal convolution (i.e. 1-D convolutions) applied on the last layer output of the visual encoder. The score indicates the likelihood for a proposal to be an event. The offsets are used to adjust the proposed segment boundaries from the associated anchor locations. We made following changes to ProcNets:</p><p>• The sequential prediction module in ProcNets is removed, as the event segments in a video are not closely coupled and the number of events is small in general. • Use input from a multi-head self-attention layer instead of a bidirectional LSTM (Bi-LSTM) layer <ref type="bibr" target="#b14">[15]</ref>. • Use multi-layer temporal convolutions to generate the proposal score and offsets. The temporal convolutional network contain three 1-D conv. layers, with batch normalization <ref type="bibr" target="#b18">[19]</ref>. We use ReLU activation for hidden layers. • In our model, the conv. stride depends on kernel size ( kernel size s ) versus always 1 in ProcNets 1 .</p><p>We encode the video context by a self-attention layer as it has potential to learn better context representation. Changing stride size based on kernel size reduces the number of longer proposals so that the training samples is more balanced, because a larger kernel size makes it easier to get good overlap with ground truth. It also speeds up training as the number of long proposals is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Captioning Decoder</head><p>Masked Transformer. The captioning decoder takes input from both the visual encoder and the proposal decoder. Given a proposal tuple (P e , S p , E p ) and visual representations {F 1 , . . . , F L }, the L-layered captioning decoder generates the t-th word by doing the following</p><formula xml:id="formula_6">Y l+1 ≤t = C(Y l ≤t ) = Ψ(PF(Φ(Y l ≤t )), Φ(Y l ≤t )) (9) Φ(Y l ≤t ) =   Ψ(MA(Ω(Y l ≤t ) 1 ,F l ,F l ), Ω(Y l ≤t ) 1 ) · · · Ψ(MA(Ω(Y l ≤t ) t ,F l ,F l ), Ω(Y l ≤t ) t )   (10) Ω(Y l ≤t ) =   Ψ(MA(y l 1 , Y l , Y l ), y l 1 ) · · · Ψ(MA(y l t , Y l , Y l ), y l t )   (11) F l = f M (S p , E p ) F l (12) p(w t+1 |X, Y L ≤t ) = softmax(W V y L t+1 )<label>(13)</label></formula><p>where y 0 i represents word vector, Y l ≤t = {y l 1 , . . . , y l t }, w t+1 denotes the probability of each word in the vocabulary for time t+1, W V ∈ R ν×d denotes the word embedding matrix with vocabulary size ν, and indicates elementwise multiplication. C(·) denotes the decoder representation, i.e. the output from feed-forward layer in <ref type="figure">Fig. 1</ref>. Φ(·) denotes the cross module attention that use the current decoder states to attend to encoder states (i.e. multi-head attention in <ref type="figure">Fig. 1)</ref>. Ω(·) represents the self-attention in decoder. Notice that the subscript ≤ t restricts the attention only on the already generated words. f M : R 2 → [0, 1] T is a masking function that output values (near) zero when outside the predicted starting and ending locations, and (near) one otherwise. With this function, the receptive region of the model is restricted to the current segment so that the visual representation focuses on describing the current event. Note that during decoding, the encoder performs the forward propagation again so that the representation of each encoder layer contains only the information for the current proposal (see eq. 12). This is different from simply multiplying the mask with the existing representation from the encoder during proposal prediction, since the representation of the latter still contains information that is outside the proposal region. The representation from the L-th layer of captioning decoder is then used for predicting the next word for the current proposal using a linear layer with softmax activation (see eq. 13). Differentiable Proposal Mask. We cannot choose any arbitrary function for f M as a discrete one would prevent us from doing end-to-end training. We therefore propose to use a fully differentiable function to obtain the mask for visual events. This function f M maps the predicted proposal location to a differentiable mask M ∈ R T for each time</p><formula xml:id="formula_7">step i ∈ {1, . . . , T }. f M (S p , E p , S a , E a , i) = σ(g( (14) [ρ(S p , :), ρ(E p , :), ρ(S a , :), ρ(E e , :), Bin(S a , E a , :)])) ρ(pos, i) = sin(pos/10000 i/d ) i is even cos(pos/10000 (i−1)/d ) otherwise<label>(15)</label></formula><formula xml:id="formula_8">Bin(S a , E a , i) = 1 if i ∈ [S a , E a ] 0 otherwise<label>(16)</label></formula><p>where S a and E a are the start and end position of anchor, [·] denotes concatenation, g(·) is a continuous function, and σ(·) is the logistic sigmoid function. We choose to use a multilayer perceptron to parameterize g. In other words, we have a feed-forward neural network that takes the positional encoding from the anchor and predicted boundary positions and the corresponding binary mask to predict the continuous mask. We use the same positional encoding strategy as in <ref type="bibr" target="#b29">[30]</ref>. Directly learning the mask would be difficult and unnecessary, since we would already have a reasonable boundary prediction from the proposal module. Therefore, we use a gated formulation that lets the model choose between the learned continuous mask and the discrete mask obtained from the proposal module. More precisely, the gated masking function f GM is</p><formula xml:id="formula_9">f GM (S p , E p , S a , E a , i) = P e Bin(S p , E p , i) + (1 − P e )f M (S p , E p , S a , E a , i) (17)</formula><p>Since the proposal score P e ∈ [0, 1], it now acts as a gating mechanism. This can also be viewed as a modulation between the continuous and proposal masks, the continuous mask is used as a supplement for the proposal mask in case the confidence is low from the proposal module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Learning</head><p>Our model is fully differentiable and can be trained consistently from end-to-end The event proposal anchors are sampled as follows. Anchors that have overlap greater than 70% with any ground-truth segments are regarded as positive samples and ones that have less than 30% overlap with all ground-truth segments are negative. The proposal boundaries for positive samples are regressed to the groundtruth boundaries (offsets). We randomly sample U = 10 anchors from positive and negative anchor pools that correspond to one ground-truth segment for each mini-batch.</p><p>The loss for training our model has four parts: the regression loss L r for event boundary prediction, the binary cross entropy mask prediction loss L m , the event classification loss L e (i.e. prediction P e ), and the captioning model loss L c . The final loss L is a combination of these four losses,</p><formula xml:id="formula_10">L r = Smooth 1 (θ c , θ c ) + Smooth 1 (θ l , θ l ) L i m = BCE(Bin(S p , E p , i), f M (S p , E p , S a , E a , i)) L e = BCE(P e , P e ) L t c = CE(ŵ t , p(w t |X, Y L ≤t−1 )) L = λ 1 L r + λ 2 i L i m + λ 3 L e + λ 4 t L t c</formula><p>where Smooth 1 is the smooth 1 loss defined in <ref type="bibr" target="#b13">[14]</ref>, BCE denotes binary cross entropy, CE represents cross entropy loss,θ c andθ l represent the ground-truth center and length offset with respect to the current anchor,P e is the groundtruth label for the proposed event,ŵ t denotes the groundtruth word at time step t, and λ 1...4 ∈ R + are the coefficients that balance the contribution from each loss. Simple Single Stage Models. The key for our proposed model to work is not the single stage learning of a compositional loss, but the ability to keep the consistency between the proposal and captioning. For example, we could make a single-stage trainable model by simply sticking them together with multi-task learning. More precisely, we can have the same model but choose a non-differentiable masking function f M in eq. 12. The same training procedure can be applied for this model (see the following section). Since the masking function would then be non-differentiable, error from the captioning model cannot be back propagated to modify the proposal predictions. However, the captioning decoder is still able to influence the visual representation that is learned from the visual encoder. This may be undesirable, as the updates the visual representation may lead to worse performance for the proposal decoder. As a baseline, we also test this single-stage model in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>For the proposal decoder, the temporal convolutional networks take the last encoding output from video encoder as the input. The sizes of the temporal convolution kernels vary from 1 to 251 and we set the stride factor s to 50. For our Transformer model, we set the model dimension d = 1024 (same as the Bi-LSTM hidden size) and set the hidden size of feed-forward layer to 2048. We set number of heads (H) to 8. In addition to the residual dropout and attention dropout layers in Transformer, we add a 1-D dropout layer at the visual input embedding to avoid overfitting. We use recurrent dropout proposed in <ref type="bibr" target="#b9">[10]</ref> for this 1-D dropout. Due to space limits, more details are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>ActivityNet Captions <ref type="bibr" target="#b19">[20]</ref> and YouCookII <ref type="bibr" target="#b42">[43]</ref> are the two largest datasets with temporal event segments annotated and described by natural language sentences. Ac-tivityNet Captions contains 20k videos, and on average each video has 3.65 events annotated. YouCookII has 2k videos and the average number of segments per video is 7.70. The train/val/test splits for ActivityNet Captions are 0.5:0.25:0.25 while for YouCookII are 0.66:0.23:0.1. We report our results from both datasets on the validation sets. For ActivityNet Captions, we also show the testing results on the evaluation server while the testing set for YouCookII is not available. Data Preprocessing. We down-sample the video every 0.5s and extract the 1-D appearance and optical flow features per frame, as suggested by Xiong et al. <ref type="bibr" target="#b33">[34]</ref>. For appearance features, we take the output of the "Flatten-673" layer in ResNet-200 <ref type="bibr" target="#b15">[16]</ref>; for optical flow features, we extract the optical flow from 5 contiguous frames, encode with BN-Inception <ref type="bibr" target="#b18">[19]</ref> and take output of the "globalpool" layer. Both networks are pre-trained on the Activi-tyNet dataset <ref type="bibr" target="#b5">[6]</ref> for the action recognition task. We then concatenate the two feature vector and further encode with a linear layer. We set the window size T to 480. The input is zero padded in case the number of sampled frames is smaller than the size of the window. Otherwise, the video is truncated to fit the window. Note that we do not fine-tune the visual features for efficiency considerations, however, allowing fine-tuning may lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baseline and Metrics</head><p>Baselines. Most of the existing methods can only caption an entire video or specified video clip. For example, LSTM-YT <ref type="bibr" target="#b31">[32]</ref>, S2YT <ref type="bibr" target="#b30">[31]</ref>, TempoAttn <ref type="bibr" target="#b35">[36]</ref>, H-RNN <ref type="bibr" target="#b40">[41]</ref> and DEM <ref type="bibr" target="#b19">[20]</ref>. The most relevant baseline is TempoAttn, where the model temporally attends on visual sequence inputs as the input of LSTM language encoder. For a fair comparison, we made the following changes to the original TempoAttn. First, all the methods take the same visual feature input. Second, we add a Bi-LSTM context encoder to TempoAttn while our method use self-attention context encoder. Third, we apply temporal attention on Bi-LSTM output for all the language decoder layers in TempoAttn since our decoder has attention each layer. We name this baseline Bi-LSTM+TempoAttn. Since zero inputs deteriorates Bi-LSTM encoding, we only apply the masking on the output of the LSTM encoder when it is passed to the decoder. We also compare with a a simple single-stage Masked Transformer baseline as mentioned in section 4.4, where the model employs a discrete binary mask.</p><p>For event proposals, we compare our self-attention transformer-based model with ProcNets and our own baseline with Bi-LSTM. For captioning-only models, we use the same baseline as the full dense video captioning but instead, replace the learned proposals with ground-truth proposals. Results for other dense captioning methods (e.g. the best published method DEM <ref type="bibr" target="#b19">[20]</ref>) are not available on the validation set nor is the source code released. So, we compare our methods against those methods that participated in CVPR 2017 ActivityNet Video Dense-captioning Challenge <ref type="bibr" target="#b12">[13]</ref> for test set performance on ActivityNet. Evaluation Metrics. For ground-truth segment captioning, we measure the captioning performance with most commonly-used evaluation metrics: BLEU{3,4} and ME-TEOR. For dense captioning, the evaluate metric takes both proposal accuracy and captioning accuracy into account. Given a tIoU threshold, if the proposal has an overlapping larger than the threshold with any ground-truth segments, the metric score is computed for the generated sentence and the corresponding ground-truth sentence. Otherwise, the metric score is set to 0. The scores are then averaged across all the proposals and finally averaged across all the tIoU thresholds-0.3, 0.5, 0.7, 0.9 in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison with State-of-the-Art Methods</head><p>We compare our proposed method with baselines on the ActivityNet Caption dataset. The validation and testing set results are shown in Tab. 1 and 2, respectively. All our models outperform the LSTM-based models by a large margin, which may be attributed to their better ability of modeling long-range dependencies.</p><p>We also test the performance of our model on the YouCookII dataset, and the result is shown in Tab. 3. Here, we see similar trend on performance. Our transformer based model outperforms the LSTM baseline by a significant amount. However, the results on learned proposals are much worse as compared to the ActivityNet dataset. This is possibly because of small objects, such as utensils and ingredients, are hard to detect using global visual features but are crucial for describing a recipe. Hence, one future extension for our work is to incorporate object detectors/trackers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> into the current captioning system.  We show qualitative results in <ref type="figure" target="#fig_1">Fig. 3</ref> where the proposed method generates captions with more relevant semantic information. More visualizations are in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Model Analysis</head><p>In this section we perform experiments to analyze the effectiveness of our model on different sub-tasks of dense video captioning. Video Event Proposal. We first evaluate the effect of selfattention on event proposal, and the results are shown in Tab. 4. We use standard average recall (AR) metric <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> given 100 proposals. Bi-LSTM indicates our improved ProcNets-prop model by using temporal convolutional and large kernel strides. We use our full model here, where the context encoder is replaced by our video encoder. We have noticed that the anchor sizes have a large impact on the results. So, for fair comparison, we maintain the same anchor sizes across all three methods. Our proposed Bi-LSTM model gains a 7% relative improvement from the baseline results from the deeper proposal network and more balanced anchor candidates. Our video encoder further yields <ref type="table">Table 4</ref>. Event proposal results from ActivityNet Captions dataset. We compare our proposed methods with our baseline method ProcNets-prop on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Recall (%) ProcNets-prop <ref type="bibr" target="#b42">[43]</ref> 47.01 Bi-LSTM (ours) 50.65 Self-Attn (our) 52.95  <ref type="figure" target="#fig_2">Fig. 4</ref> follow the convention <ref type="bibr" target="#b19">[20]</ref>. DAPs <ref type="bibr" target="#b8">[9]</ref>, is initially proposed for short action proposals and adapted later for long event proposal <ref type="bibr" target="#b19">[20]</ref>. The proposed models outperforms DAPs-event and ProcNets-prop by significant margins. Transformer based and Bi-LSTM based models yield similar recall results given sufficient number of proposals (100), while our self-attention encoding model is more accurate when the allowed number of proposals is small. Dense Video Captioning. Next, we look at the dense video captioning results in an ideal setting: doing the captioning based on the ground-truth event segments. This will give us an ideal captioning performance since all event proposals are accurate. Because we need access to ground-truth event proposal during test time, we report the results on validation set 3 (see Tab. 5). The proposed Masked Transformer (section 4.3) outperforms the baseline by a large margin (by more than 1 METEOR point). This directly substantiates the effectiveness of the transformer on both visual and language encoding and multi-head temporal attention. We notice that as the number of encoder and decoder layers increases, the performance gets further boosts by 1.3%-1.7%. As can be noted here, the 2-layer transformer strikes a good balance point between performance and computation, and thus we use 2-layer transformer for all our experiments. Analysis on Long Events. As mentioned in section 4.1, learning long-range dependencies should be easier with self-attention, since the next layer observes information from all time steps of the previous layer. To validate this hypothesis directly, we test our model against the LSTM Ground-truth Event 0: Two teams are playing volleyball in a indoor court. Event 1: Two teams wearing dark uniforms are doing a volleyball competition, then appears a team with yellow t-shirts. Event 2: Then, a boy with a red t-shirt serves the ball and the teams start to hit and running to pass the ball, then another team wearing green shorts enters the court. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Trans. (ours)</head><p>Event 0: a man is seen standing in a large circle and leads into a man holding a ball and Event 1: the man spins the ball around and throws the ball Event 2: the man throws the ball and his throw the distance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-LSTM+TempoAttn</head><p>Event 0: a man is seen standing on a field with a man standing on a field Event 1: he throws the ball and throws it back and forth Event 2: he throws the ball and throws it back and forth   baseline on longer event segments (where the events are at least 50s long) from the ActivityNet Caption dataset, where learning the long-range dependencies are crucial for achieving good performance. It is clear from the result (see Tab.</p><p>6) that our transformer based model performs significantly better than the LSTM baseline. The discrepancy is even larger when the model needs to learn both the proposal and captioning, which demonstrate the effectiveness of selfattention in facilitate learning long range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose an end-to-end model for dense video captioning. The model is composed of an encoder and two decoders. The encoder encodes the input video to proper visual representations. The proposal decoder then decodes from this representation with different anchors to form video event proposals. The captioning decoder employs a differentiable masking network to restrict its attention to the proposal event, ensures the consistency between the proposal and captioning during training. In addition, we propose to use self-attention for dense video captioning. We achieved significant performance improvement on both event proposal and captioning tasks as compared to RNNbased models. We demonstrate the effectiveness of our models on ActivityNet Captions and YouCookII dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Implementation Details</head><p>The sizes of the temporal convolution kernels in the proposal module are <ref type="bibr">1 2, 3, 4, 5, 7, 9, 11, 15, 21, 29, 41, 57, 71, 111, 161, 211</ref> and 251. We set the hyper-parameters for End-to-end Masked Transformer as follows. The dropout ratio for Transformer is set to 0.2 and that for visual input embedding is set to 0.1. We set the loss coefficients λ1, λ2, λ3, λ4 to 10, 1, 1, 0.25. For training, we use stochastic gradient descent (SGD) with Nesterov momentum, the learning rate is set between 0.01 and 0.1 depending on the convergence, and the momentum is set at 0.95. We decay the learning rate by half on plateau. We also clip the gradient <ref type="bibr" target="#b23">[24]</ref> to have global 2 norm of 1. For inference, we first pick event proposals with prediction score higher than a pre-defined threshold (0.7). We remove proposals that have high overlap (i.e. ≥ 0.9) with each other. For each video, we have at least 50, and at most 500 event proposals. The descriptions are then generated for each of the proposal, and we use greedy decoding for text generation with at most 20 words. We implement the model in PyTorch and train it using 8 Tesla K80 GPUs with synchronous SGD. The model typically takes a day to converge.</p><p>The implementation for proposal-only and captioning-only model is slightly different. We apply Adam for training rather than SGD and set the learning rate to 0.0001. When training the captioning-only model, we apply scheduled sampling <ref type="bibr" target="#b3">[4]</ref>. We set the sampling ratio to 0.05 at the beginning of training, and increase it by 0.05 every 5 epoch until it reaches 0.25. Note that applying scheduled sampling to End-to-end Masked Transformer yield no improvements and hence disabled. In the proposal-only model, we report the results on a single-layer Transformer with the model size and hidden size to be 512 and 128. The temporal conv. stride factor s is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Additional Results</head><p>To see the effectiveness of self-attention, we performed additional ablation studies, where we apply self-attention module at the encoder or decoder of the LSTM-based baseline. From the result it is clear that self-attention have significant impact on the performance of the model (see Tab. 7), especially as in the language decoder.</p><p>Note that the performance of captioning models over groundtruth segments vary little from number of layers. We choose to use 2-layer transformer for the rest of the experiments because 1) the 4 and 6 layer models are more computational expensive; 2) the learning is more complicated when the learned proposals are approximate, and a 2-layer model give us more flexibility for handling this case (see Tab. 7 for results on the 1-layer model).</p><p>Self-attention facilitates the learning of long-range dependencies, which should not hurt the performance on modeling relative short-range dependencies. To validate this we tested our model on shorter activities, where the activities are at most 15 seconds long. The result is shown in Tab. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Additional Qualitative Results</head><p>We visualize the learned masks in <ref type="figure">Fig. 5</ref>. The first two correspond to the case where the proposal prediction is confident, i.e., proposal scores are high (&gt; 0.9) and the last two correspond to the case where the prediction is less confident, i.e., proposal scores are low (&lt; 0.7). We visualize the cross module attention in <ref type="figure">Fig. 7</ref>. For convenience, we randomly choose one of the attention matrices from the multi-head attention. Also, we notice that attention weights from higher-level self-attention layer is tend to be flatter than these from the lower-level layer. Qualitative results for YouCookII are shown in <ref type="figure">Fig. 6</ref>. The visual recognition is challenging result from the small and ambiguous objects (e.g., black pepper, lamb). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth</head><p>Event 0: stretch the dough Event 1: cut the dough into squares Event 2: lay pepperoni and cheese on the dough and roll into a ball Event 3: put the rolls in a pan Event 4: brush each pizza bite with some melted butter and sprinkle some italian seasoning on top</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Trans. (ours)</head><p>Event 0: knead the dough Event 1: cut the dough into thin slices roll Event 2: cut the meat into thin slices roll Event 3: dip the fish in the batter and place on the Event 4: dip the fish in the batter and coat the batter the batter the batter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-LSTM+TempoAttn</head><p>Event 0: cut the roll into pieces the edges and roll the dough Event 1: cut the salmon into thin slices the sheet Event 2: cut the salmon into thin slices the sheet Event 3: place the filling on the bread the bread Event 4: place the chicken on the pan the grill and serve Ground-truth Event 0: pour some oil into a hot pan Event 1: add chopped onions and carrots to the pan Event 2: add salt to the pan and mix Event 3: add butter and garlic to the pan and mix Event 4: add lamb to the pan and break it up Event 5: add salt black pepper and italian seasoning to the meat ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Trans. (ours)</head><p>Event 0: heat a pan with oil a pan add the pork Event 1: add the onions and garlic to the pan stir Event 2: add the onions and carrots to the pan stir Event 3: add the vegetables to the pan and stir Event 4: add the pork to the pan stir Event 5: add the vegetables to the pan and stir ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-LSTM+TempoAttn</head><p>Event 0: add oil to a pan heat &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; Event 1: add the chicken to the pan heat Event 2: add the chicken to the pan heat Event 3: add the chicken to the pan heat Event 4: add the chicken to the pan heat Event 5: add the chicken to the pan and stir heat ... <ref type="figure">Figure 6</ref>. Qualitative results on YouCookII videos. We only showed result for the first 6 events in the second example. <ref type="figure">Figure 7</ref>. Visualization of weights from the cross-module attention layer. X axis represents the generated words at each time step. Y axis indicates the sampled frames.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Transformer with 1-layer encoder and 1-layer decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on ActivityNet Captions. The color bars represent different events. Colored text highlight relevant content to the event. Our model generates more relevant attributes as compared to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Event proposal recall curve under tIoU threshold 0.8 with average 100 proposals per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>High proposal score. (b) High proposal score (b) Low proposal score (b) Low proposal score Figure 5. Visualization of differentiable masks and final masks under hight (a and b) and low proposal score (c and d). Videos from ActivityNet Captions validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Captioning results from ActivityNet Caption Dataset with learned event proposals. All results are on the validation set and all our models are based on 2-layer Transformer. We report BLEU (B) and METEOR (M). All results are on the validation set. Top scores are highlighted.</figDesc><table><row><cell>Method</cell><cell cols="2">B@3 B@4</cell><cell>M</cell></row><row><cell>Bi-LSTM +TempoAttn</cell><cell>2.43</cell><cell cols="2">1.01 7.49</cell></row><row><cell>Masked Transformer</cell><cell>4.47</cell><cell cols="2">2.14 9.43</cell></row><row><cell>End-to-end Masked Transformer</cell><cell>4.76</cell><cell cols="2">2.23 9.56</cell></row><row><cell cols="4">Table 2. Dense video captioning challenge leader board results.</cell></row><row><cell cols="3">For results from the same team, we keep the highest one.</cell></row><row><cell>Method</cell><cell cols="2">METEOR</cell></row><row><cell>DEM [20]</cell><cell></cell><cell>4.82</cell></row><row><cell>Wang et al.</cell><cell></cell><cell>9.12</cell></row><row><cell>Jin et al.</cell><cell></cell><cell>9.62</cell></row><row><cell>Guo et al.</cell><cell></cell><cell>9.87</cell></row><row><cell>Yao et al. 2 (Ensemble)</cell><cell cols="2">12.84</cell></row><row><cell>Our Method</cell><cell cols="2">10.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Recipe generation benchmark on YouCookII validation set. GT proposals indicate the ground-truth segments are given during inference.</figDesc><table><row><cell>Method</cell><cell cols="4">GT Proposals Learned Proposals B@4 M B@4 M</cell></row><row><cell>Bi-LSTM +TempoAttn</cell><cell>0.87</cell><cell>8.15</cell><cell>0.08</cell><cell>4.62</cell></row><row><cell>Our Method</cell><cell>1.42</cell><cell>11.20</cell><cell>0.30</cell><cell>6.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Captioning results from ActivityNet Caption Dataset with ground-truth proposals. All results are on the validation set. Top two scores are highlighted.</figDesc><table><row><cell>Method</cell><cell cols="2">B@3 B@4</cell><cell>M</cell></row><row><cell>Bi-LSTM +TempoAttn</cell><cell>4.8</cell><cell cols="2">2.1 10.02</cell></row><row><cell>Our Method</cell><cell></cell><cell></cell></row><row><cell>1-layer</cell><cell>5.80</cell><cell cols="2">2.66 10.92</cell></row><row><cell>2-layer</cell><cell>5.69</cell><cell cols="2">2.67 11.06</cell></row><row><cell>4-layer</cell><cell>5.70</cell><cell cols="2">2.77 11.11</cell></row><row><cell>6-layer</cell><cell>5.66</cell><cell cols="2">2.71 11.10</cell></row><row><cell cols="4">a 4.5% improvement from our recurrent nets-based model.</cell></row><row><cell cols="4">We show the recall curve under high tIoU threshold (0.8)</cell></row><row><cell>in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Evaluating only long events from ActivityNet Caption Dataset. GT proposals indicate the ground-truth segments are given during inference.</figDesc><table><row><cell></cell><cell cols="4">GT Proposals Learned Proposals</cell></row><row><cell>Method</cell><cell>B@4</cell><cell>M</cell><cell>B@4</cell><cell>M</cell></row><row><cell>Bi-LSTM +TempoAttn</cell><cell>0.84</cell><cell>5.39</cell><cell>0.42</cell><cell>3.99</cell></row><row><cell>Our Method</cell><cell>1.13</cell><cell>5.90</cell><cell>1.04</cell><cell>5.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Additional ablation experiments on ActivityNet. Evaluating only short events from ActivityNet.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">B@3 B@4</cell><cell>M</cell></row><row><cell cols="2">SelfAttn + LSTM TempoAttn</cell><cell>2.91</cell><cell cols="2">1.35 7.88</cell></row><row><cell>BiLSTM + SelfAttn</cell><cell></cell><cell>4.06</cell><cell cols="2">1.92 9.05</cell></row><row><cell>Our Method (1-layer)</cell><cell></cell><cell>4.49</cell><cell cols="2">2.10 9.27</cell></row><row><cell></cell><cell cols="4">GT Proposals Learned Proposals</cell></row><row><cell>Method</cell><cell>B@4</cell><cell>M</cell><cell>B@4</cell><cell>M</cell></row><row><cell>Bi-LSTM+TempoAttn</cell><cell>0.74</cell><cell>5.29</cell><cell>0.23</cell><cell>4.43</cell></row><row><cell>Our Method</cell><cell>0.87</cell><cell>5.82</cell><cell>0.68</cell><cell>5.06</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">s is a scalar that affects the convolution stride for different kernel size</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This work is unpublished. It employs external data for model training and the final prediction is obtained from an ensemble of models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The results are overly optimistic, however, it is fine here since we are interested in the best situation performance. The comparison is also fair, since all methods are tuned to optimize the validation set performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The technical work was performed while Luowei was an intern at Salesforce Research. This work is also partly supported by ARO W911NF-15-1-0354 and DARPA FA8750-17-2-0112. This article solely reflects the opinions and conclusions of its authors but not the funding agents.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4575" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khrisna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">Activitynet challenge 2017 summary</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A compositional framework for grounding language inference, generation, and acquisition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="601" to="713" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grounded language learning from video described with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Watch what you just said: Image captioning with text-conditional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia 2017</title>
		<meeting>the on Thematic Workshops of ACM Multimedia 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
