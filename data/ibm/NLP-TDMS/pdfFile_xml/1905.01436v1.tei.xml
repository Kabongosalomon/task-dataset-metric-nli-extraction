<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge-Labeling Graph Neural Network for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Edge-Labeling Graph Neural Network for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel edge-labeling graph neural network (EGNN), which adapts a deep neural network on the edge-labeling graph, for few-shot learning. The previous graph neural network (GNN) approaches in few-shot learning have been based on the node-labeling framework, which implicitly models the intra-cluster similarity and the inter-cluster dissimilarity. In contrast, the proposed EGNN learns to predict the edge-labels rather than the node-labels on the graph that enables the evolution of an explicit clustering by iteratively updating the edgelabels with direct exploitation of both intra-cluster similarity and the inter-cluster dissimilarity. It is also well suited for performing on various numbers of classes without retraining, and can be easily extended to perform a transductive inference. The parameters of the EGNN are learned by episodic training with an edge-labeling loss to obtain a well-generalizable model for unseen low-data problem. On both of the supervised and semi-supervised few-shot image classification tasks with two benchmark datasets, the proposed EGNN significantly improves the performances over the existing GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A lot of interest in meta-learning <ref type="bibr" target="#b0">[1]</ref> has been recently arisen in various areas including especially taskgeneralization problems such as few-shot learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, learn-to-learn <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, non-stationary reinforcement learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, and continual learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Among these meta-learning problems, few-shot leaning aims to automatically and efficiently solve new tasks with few labeled data based on knowledge obtained from previous experiences. This is in * Work done during an internship at Kakao Brain. Correspondence to kimjm0309@gmail.com <ref type="figure">Figure 1</ref>: Alternative node and edge feature update in EGNN with edge-labeling for few-shot learning contrast to traditional (deep) learning methods that highly rely on large amounts of labeled data and cumbersome manual tuning to solve a single task.</p><p>Recently, there has also been growing interest in graph neural networks (GNNs) to handle rich relational structures on data with deep neural networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. GNNs iteratively perform a feature aggregation from neighbors by message passing, and therefore can express complex interactions among data instances. Since few-shot learning algorithms have shown to require full exploitation of the relationships between a support set and a query <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, the use of GNNs can naturally have the great potential to solve the few-shot learning problem. A few approaches that have explored GNNs for few-shot learning have been recently proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. Specifically, given a new task with its few-shot support set, Garcia and Bruna <ref type="bibr" target="#b5">[6]</ref> proposed to first construct a graph where all examples of the support set and a query are densely connected. Each input node is represented by the embedding feature (e.g. an output of a convolutional neural network) and the given label information (e.g. one-hot encoded label). Then, it classifies the unlabeled query by iteratively updating node features from neighborhood aggregation. Liu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a transductive propagation network (TPN) on the node features obtained from a deep neu-ral network. At test-time, it iteratively propagates one-hot encoded labels over the entire support and query instances as a whole with a common graph parameter set. Here, it is noted that the above previous GNN approaches in fewshot learning have been mainly based on the node-labeling framework, which implicitly models the intra-cluster similarity and inter-cluster dissimilarity.</p><p>On the contrary, the edge-labeling framework is able to explicitly perform the clustering with representation learning and metric learning, and thus it is intuitively a more conducive framework for inferring a query association to an existing support clusters. Furthermore, it does not require the pre-specified number of clusters (e.g. class-cardinality or ways) while the node-labeling framework has to separately train the models according to each number of clusters. The explicit utilization of edge-labeling which indicates whether the associated two nodes belong to the same cluster (class) have been previously adapted in the naive (hyper) graphs for correlation clustering <ref type="bibr" target="#b34">[35]</ref> and the GNNs for citation networks or dynamical systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, but never applied to a graph for few-shot learning. Therefore, in this paper, we propose an edge-labeling GNN (EGNN) for few-shot leaning, especially on the task of few-shot classification.</p><p>The proposed EGNN consists of a number of layers in which each layer is composed of a node-update block and an edge-update block. Specifically, across layers, the EGNN not only updates the node features but also explicitly adjusts the edge features, which reflect the edgelabels of the two connected node pairs and directly exploit both the intra-cluster similarity and inter-cluster dissimilarity. As shown in <ref type="figure">Figure 1</ref>, after a number of alternative node and edge feature updates, the edge-label prediction can be obtained from the final edge feature. The edge loss is then computed to update the parameters of EGNN with a well-known meta-learning strategy, called episodic training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. The EGNN is naturally able to perform a transductive inference to predict all test (query) samples at once as a whole, and this has shown more robust predictions in most cases when a few labeled training samples are provided. In addition, the edge-labeling framework in the EGNN enables to handle various numbers of classes without remodeling or retraining. We will show by means of experimental results on two benchmark few-shot image classification datasets that the EGNN outperforms other few-shot learning algorithms including the existing GNNs in both supervised and semi-supervised cases.</p><p>Our main contributions can be summarized as follows:</p><p>• The EGNN is first proposed for few-shot learning with iteratively updating edge-labels with exploitation of both intra-cluster similarity and inter-cluster dissimilarity. It is also able to be well suited for performing on various numbers of classes without retraining.</p><p>• It consists of a number of layers in which each layer is composed of a node-update block and an edge-update block where the corresponding parameters are estimated under the episodic training framework.</p><p>• Both of the transductive and non-transductive learning or inference are investigated with the proposed EGNN.</p><p>• On both of the supervised and semi-supervised fewshot image classification tasks with two benchmark datasets, the proposed EGNN significantly improves the performances over the existing GNNs. Additionally, several ablation experiments show the benefits from the explicit clustering as well as the separate utilization of intra-cluster similarity and inter-cluster dissimilarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Graph Neural Network Graph neural networks were first proposed to directly process graph structured data with neural networks as of form of recurrent neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Li et al. <ref type="bibr" target="#b30">[31]</ref> further extended it with gated recurrent units and modern optimization techniques. Graph neural networks mainly do representation learning with a neighborhood aggregation framework that the node features are computed by recursively aggregating and transforming features of neighboring nodes. Generalized convolution based propagation rules also have been directly applied to graphs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, and Kipf and Welling <ref type="bibr" target="#b29">[30]</ref> especially applied it to semi-supervised learning on graph-structured data with scalability. A few approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> have explored GNNs for few-shot learning and are based on the node-labeling framework.</p><p>Edge-Labeling Graph Correlation clustering (CC) is a graph-partitioning algorithm <ref type="bibr" target="#b39">[40]</ref> that infers the edge labels of the graph by simultaneously maximizing intracluster similarity and inter-cluster dissimilarity. Finley and Joachims <ref type="bibr" target="#b40">[41]</ref> considered a framework that uses structured support vector machine in CC for noun-phrase clustering and news article clustering. Taskar <ref type="bibr" target="#b41">[42]</ref> derived a maxmargin formulation for learning the edge scores in CC for producing two different segmentations of a single image. Kim et al. <ref type="bibr" target="#b34">[35]</ref> explored a higher-order CC over a hypergraph for task-specific image segmentation. The attention mechanism in a graph attention network has recently extended to incorporate real-valued edge features that are adaptive to both the local contents and the global layers for modeling citation networks <ref type="bibr" target="#b35">[36]</ref>. Kipf et al. <ref type="bibr" target="#b36">[37]</ref> introduced a method to simultaneously infer relational structure with interpretable edge types while learning the dynamical model of an interacting system. Johnson <ref type="bibr" target="#b42">[43]</ref> introduced the Gated Graph Transformer Neural Network (GGT-NN) for natural language tasks, where multiple edge types and several graph transformation operations including node state update, propagation and edge update are considered.</p><p>Few-Shot Learning One main stream approach for fewshot image classification is based on representation learning and does prediction by using nearest-neighbor according to similarity between representations. The similarity can be a simple distance function such as cosine or Euclidean distance. A Siamese network <ref type="bibr" target="#b43">[44]</ref> works in a pairwise manner using trainable weighted L 1 distance. A matching network <ref type="bibr" target="#b1">[2]</ref> further uses an attention mechanism to derive an differentiable nearest-neighbor classifier and a prototypical network <ref type="bibr" target="#b2">[3]</ref> extends it with defining prototypes as the mean of embedded support examples for each class. DEML <ref type="bibr" target="#b44">[45]</ref> has introduced a concept learner to extract high-level concept by using a large-scale auxiliary labeled dataset showing that a good representation is an important component to improve the performance of few-shot image classification.</p><p>A meta-learner that learns to optimize model parameters extract some transferable knowledge between tasks to leverage in the context of few-shot learning. Meta-LSTM <ref type="bibr" target="#b7">[8]</ref> uses LSTM as a model updater and treats the model parameters as its hidden states. This allows to learn the initial values of parameters and update the parameters by reading few-shot examples. MAML <ref type="bibr" target="#b3">[4]</ref> learns only the initial values of parameters and simply uses SGD. It is a model agnostic approach, applicable to both supervised and reinforcement learning tasks. Reptile <ref type="bibr" target="#b45">[46]</ref> is similar to MAML but using only first-order gradients. Another generic metalearner, SNAIL <ref type="bibr" target="#b9">[10]</ref>, is with a novel combination of temporal convolutions and soft attention to learn an optimal learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, the definition of few-shot classification task is introduced, and the proposed algorithm is described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem definition: Few-shot classification</head><p>The few-shot classification aims to learn a classifier when only a few training samples per each class are given. Therefore, each few-shot classification task T contains a support set S, a labeled set of input-label pairs, and a query set Q, an unlabeled set on which the learned classifier is evaluated. If the support set S contains K labeled samples for each of N unique classes, the problem is called N -way K-shot classification problem.</p><p>Recently, meta-learning has become a standard methodology to tackle few-shot classification. In principle, we can train a classifier to assign a class label to each query sample with only the compact support set of the task. However, a small number of labeled support samples for each task are not sufficient to train a model fully reflecting the inter-and intra-class variations, which often leads to unsatisfactory classification performance. Meta-learning on explicit training set resolves this issue by extracting transferable knowledge that allows us to perform better few-shot learning on the support set, and thus classify the query set more successfully.</p><p>As an efficient way of meta-learning, we adopt episodic training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> which is commonly employed in various literatures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Given a relatively large labeled training dataset, the idea of episodic training is to sample training tasks (episodes) that mimic the few-shot learning setting of test tasks. Here, since the distribution of training tasks is assumed to be similar to that of test tasks, the performances of the test tasks can be improved by learning a model to work well on the training tasks.</p><p>More concretely, in episodic training, both training and test tasks of the N -way K-shot problem are formed as follows:</p><formula xml:id="formula_0">T = S Q where S = {(x i , y i )} N ×K i=1 and Q = {(x i , y i )} N ×K+T i=N ×K+1 .</formula><p>Here, T is the number of query samples, and x i and y i ∈ {C 1 , · · · C N } = C T ⊂ C are the ith input data and its label, respectively. C is the set of all classes of either training or test dataset. Although both the training and test tasks are sampled from the common task distribution, the label spaces are mutually exclusive, i.e. C train ∩ C test = ∅. The support set S in each episode serves as the labeled training set on which the model is trained to minimize the loss of its predictions over the query set Q. This training procedure is iteratively carried out episode by episode until convergence.</p><p>Finally, if some of N ×K support samples are unlabeled, the problem is referred to as semi-supervised few-shot classification. In Section 4, the effectiveness of our algorithm on semi-supervised setting will be presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>This section describes the proposed EGNN for few-shot classification, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Given the feature representations (extracted from a jointly trained convolutional neural network) of all samples of the target task, a fully-connected graph is initially constructed where each node represents each sample, and each edge represents the types of relationship between the two connected nodes; Let G = (V, E; T ) be the graph constructed with samples from the task T , where V := {V i } i=1,...,|T | and E := {E ij } i,j=1,...,|T | denote the set of nodes and edges of the graph, respectively. Let v i and e ij be the node feature of V i and the edge feature of E ij , respectively. |T | = N × K + T is the total number of samples in the task T . Each groundtruth edge-label y ij is defined by the ground-truth node labels as: Each edge feature e ij = {e ijd } 2 d=1 ∈ [0, 1] 2 is a 2dimensional vector representing the (normalized) strengths of the intra-and inter-class relations of the two connected nodes. This allows to separately exploit the intra-cluster similarity and the inter-cluster dissimilairity.</p><formula xml:id="formula_1">y ij = 1, if y i = y j , 0, otherwise.<label>(1)</label></formula><p>Node features are initialized by the output of the convolutional embedding network v 0 i = f emb (x i ; θ emb ), where θ emb is the corresponding parameter set (see <ref type="figure" target="#fig_1">Figure 3</ref>.(a)). Edge features are initialized by edge labels as follows:</p><formula xml:id="formula_2">e 0 ij =    [1||0], if y ij = 1 and i, j ≤ N × K, [0||1], if y ij = 0 and i, j ≤ N × K, [0.5||0.5], otherwise,<label>(2)</label></formula><p>where || is the concatenation operation. The EGNN consists of L layers to process the graph, and the forward propagation of EGNN for inference is an alternative update of node feature and edge feature through layers.</p><p>In detail, given v −1 i and e −1 ij from the layer − 1, node feature update is firstly conducted by a neighborhood aggregation procedure. The feature node v i at the layer is updated by first aggregating the features of other nodes proportional to their edge features, and then performing the feature transformation; the edge feature e −1 ij at the layer − 1 is used as a degree of contribution of the correspond-ing neighbor node like an attention mechanism as follows:</p><formula xml:id="formula_3">v i = f v ([ jẽ −1 ij1 v −1 j || jẽ −1 ij2 v −1 j ]; θ v ),<label>(3)</label></formula><p>whereẽ ijd = e ijd k e ikd , and f v is the feature (node) transformation network, as shown in <ref type="figure" target="#fig_1">Figure 3.(b)</ref>, with the parameter set θ v . It should be noted that besides the conventional intra-class aggregation, we additionally consider inter-class aggregation. While the intra-class aggregation provides the target node the information of "similar neighbors", the inter-class aggregation provides the information of "dissimilar neighbors".</p><p>Then, edge feature update is done based on the newly updated node features. The (dis)similarities between every pair of nodes are re-obtained, and the feature of each edge is updated by combining the previous edge feature value and the updated (dis)similarities such that</p><formula xml:id="formula_4">e ij1 = f e (v i , v j ; θ e )e −1 ij1 k f e (v i , v k ; θ e )e −1 ik1 /( k e −1 ik1 ) ,<label>(4)</label></formula><formula xml:id="formula_5">e ij2 = (1 − f e (v i , v j ; θ e ))e −1 ij2 k (1 − f e (v i , v k ; θ e ))e −1 ik2 /( k e −1 ik2 ) ,<label>(5)</label></formula><formula xml:id="formula_6">e ij =ē ij / ē ij 1 ,<label>(6)</label></formula><p>where f e is the metric network that computes similarity scores with the parameter set θ e (see <ref type="figure" target="#fig_1">Figure 3</ref>.(c)). In spe- cific, the node feature flows into edges, and each element of the edge feature vector is updated separately from each normalized intra-cluster similarity or inter-cluster dissimilarity. Namely, each edge update considers not only the relation of the corresponding pair of nodes but also the relations of the other pairs of nodes. We can optionally use two separate metric networks for the computations of each of similarity or dissimilarity (e.g. separate f e,dsim instead of (1 − f e,sim )). After L number of alternative node and edge feature updates, the edge-label prediction can be obtained from the final edge feature, i.e.ŷ ij = e L ij1 . Here,ŷ ij ∈ [0, 1] can be considered as a probability that the two nodes V i and V j are from the same class. Therefore, each node V i can be classified by simple weighted voting with support set labels and edge-label prediction results. The prediction probability of node V i can be formulated as P (</p><formula xml:id="formula_7">y i = C k |T ) = p (k) i : p (k) i = softmax {j:j =i∧(xj ,yj )∈S}ŷ ij δ(y j = C k )<label>(7)</label></formula><p>where δ(y j = C k ) is the Kronecker delta function that is equal to one when y j = C k and zero otherwise. Alternative approach for node classification is the use of graph clustering; the entire graph G can be first partitioned into clusters, using the edge prediction and an optimization for valid partitioning via linear programming <ref type="bibr" target="#b34">[35]</ref>, and then each cluster can be labeled with the support label it contains the most. However, in this paper, we simply apply Eq. <ref type="formula" target="#formula_7">(7)</ref> to obtain the classification results. The overall algorithm for the Algorithm 1: The process of EGNN for inference </p><formula xml:id="formula_8">1 Input: G = (V, E; T ), where T = S Q, S = {(x i , y i )} N ×K i=1 , Q = {x i } N ×K+T i=N ×K+1 2 Parameters: θ emb ∪ {θ v , θ e } L =1 3 Output: {ŷ i } N ×K+T i=N ×K+1 4 Initialize: v 0 i = f emb (x i ; θ emb ), e 0 ij ,</formula><formula xml:id="formula_9">{ŷ i } N ×K+T i=N ×K+1 ← Edge2NodePred({y i } N ×K i=1 , {e L ij })</formula><p>EGNN inference at test-time is summarized in Algorithm 1.</p><p>The non-transductive inference means the number of query samples T = 1 or it performs the query inference one-byone, separately, while the transductive inference classifies all query samples at once in a single graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Given M training tasks {T train m } M m=1 at a certain iteration during the episodic training, the parameters of the proposed EGNN, θ emb ∪ {θ v , θ e } L =1 , are trained in an end-toend fashion by minimizing the following loss function:</p><formula xml:id="formula_10">L = L =1 M m=1 λ L e (Y m,e ,Ŷ m,e ),<label>(8)</label></formula><p>where Y m,e andŶ m,e are the set of all ground-truth query edge-labels and the set of all (real-valued) query-edge predictions of the m th task at the th layer, respectively, and the edge loss L e is defined as binary cross-entropy loss. Since the edge prediction results can be obtained not only from the last layer but also from the other layers, the total loss combines all losses that are computed in all layers in order to improve the gradient flow in the lower layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluated and compared our EGNN 1 with state-ofthe-art approaches on two few-shot learning benchmarks, i.e. miniImageNet <ref type="bibr" target="#b1">[2]</ref> and tieredImageNet <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b0">1</ref> The code and models are available on https://github.com/khy0809/fewshot-egnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>miniImageNet It is the most popular few-shot learning benchmark proposed by <ref type="bibr" target="#b1">[2]</ref> derived from the original ILSVRC-12 dataset <ref type="bibr" target="#b46">[47]</ref>. All images are RGB colored, and of size 84 × 84 pixels, sampled from 100 different classes with 600 samples per class. We followed the splits used in <ref type="bibr" target="#b7">[8]</ref> -64, 16, and 20 classes for training, validation and testing, respectively.</p><p>tieredImageNet Similar to miniImageNet dataset, tieredImageNet <ref type="bibr" target="#b6">[7]</ref> is also a subset of ILSVRC-12 <ref type="bibr" target="#b46">[47]</ref>. Compared with miniImageNet, it has much larger number of images (more than 700K) sampled from larger number of classes (608 classes rather than 100 for miniImageNet). Importantly, different from miniImageNet, tieredImageNet adopts hierarchical category structure where each of 608 classes belongs to one of 34 higher-level categories sampled from the high-level nodes in the Imagenet. Each higher-level category contains 10 to 20 classes, and divided into 20 training (351 classes), 6 validation (97 classes) and 8 test (160 classes) categories. The average number of images in each class is 1281.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup</head><p>Network Architecture For feature embedding module, a convolutional neural network, which consists of four blocks, was utilized as in most few-shot learning models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> without any skip connections 2 . More concretely, each convolutional block consists of 3 × 3 convolutions, a batch normalization and a LeakyReLU activation. All network architectures used in EGNN are described in details in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Evaluation For both datasets, we conducted a 5-way 5shot experiment which is one of standard few-shot learning settings. For evaluation, each test episode was formed by randomly sampling 15 queries for each of 5 classes, and the performance is averaged over 600 randomly generated episodes from the test set. Especially, we additionally conducted a more challenging 10-way experiment on miniImagenet, to demonstrate the flexibility of our EGNN model when the number of classes are different between meta-training stage and meta-test stage, which will be presented in Section 4.5.</p><p>Training The proposed model was trained with Adam optimizer with an initial learning rate of 5 × 10 −4 and weight decay of 10 −6 . The task mini-batch sizes for meta-training were set to be 40 and 20 for 5-way and 10-way experiments, respectively. For miniImageNet, we cut the learn- <ref type="bibr" target="#b1">2</ref> Resnet-based models are excluded for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) miniImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Trans. 5-Way 5-Shot Matching Networks <ref type="bibr" target="#b1">[2]</ref> No 55.30 Reptile <ref type="bibr" target="#b45">[46]</ref> No 62.74 Prototypical Net <ref type="bibr" target="#b2">[3]</ref> No 65.77 GNN <ref type="bibr" target="#b5">[6]</ref> No ing rate in half every 15,000 episodes while for tieredIma-geNet, the learning rate is halved for every 30,000 because it is larger dataset and requires more iterations to converge. All our code was implemented in Pytorch <ref type="bibr" target="#b47">[48]</ref> and run with NVIDIA Tesla P40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Few-shot classification</head><p>The few-shot classification performance of the proposed EGNN model is compared with several state-of-the-art models in <ref type="table">Table 1a</ref> and 1b. Here, as presented in <ref type="bibr" target="#b11">[12]</ref>, all models are grouped into three categories with regard to three different transductive settings; "No" means nontransductive method, where each query sample is predicted independently from other queries, "Yes" means transductive method where all queries are simultaneously processed and predicted together, and "BN" means that query batch statistics are used instead of global batch normalization parameters, which can be considered as a kind of transductive inference at test-time.</p><p>The proposed EGNN was tested with both transductive and non-transductive settings. As shown in <ref type="table">Table 1a</ref>, EGNN shows the best performance in 5-way 5-shot set-ting, on both transductive and non-transductive settings on miniImagenet. Notably, EGNN performed better than nodelabeling GNN <ref type="bibr" target="#b5">[6]</ref>, which supports the effectiveness of our edge-labeling framework for few-shot learning. Moreover, EGNN with transduction (EGNN + Transduction) outperformed the second best method <ref type="figure" target="#fig_0">(TPN [12]</ref>) on both datasets, especially by large margin on miniImagenet. <ref type="table">Table 1b</ref> shows that the transductive setting on tieredImagenet gave the best performance as well as large improvement compared to the non-transductive setting. In TPN, only the labels of the support set are propagated to the queries based on the pairwise node feature affinities using a common Laplacian matrix, so the queries communicate to each other only via their embedding feature similarities. In contrast, our proposed EGNN allows us to consider more complicated interactions between query samples, by propagating to each other not only their node features but also edge-label information across the graph layers having different parameter sets. Furthermore, the node features of TPN are fixed and never changed during label propagation, which allows them to derive a closed-form, one-step label propagation equation. On the contrary, in our EGNN, both node and edge features are dynamically changed and adapted to the given task gradually with several update steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-supervised few-shot classification</head><p>For semi-supervised experiment, we followed the same setting described in <ref type="bibr" target="#b5">[6]</ref> for fair comparison. It is a 5-way 5-shot setting, but the support samples are only partially labeled. The labeled samples are balanced among classes so that all classes have the same amount of labeled and unlabeled samples. The obtained results on miniImagenet are presented in <ref type="table" target="#tab_3">Table 2</ref>. Here, "LabeledOnly" denotes learning with only labeled support samples, and "Semi" means the semi-supervised setting explained above. Different results are presented according to when 20% and 40%, 60% of support samples were labeled, and the proposed EGNN is compared with node-labeling GNN <ref type="bibr" target="#b5">[6]</ref>. As shown in Table 2, semi-supervised learning increases the performances in comparison to labeled-only learning on all cases. Notably, the EGNN outperformed the previous GNN <ref type="bibr" target="#b5">[6]</ref> by a large margin (61.88% vs 52.45%, when 20% labeled) on semi-supervised learning, especially when the labeled portion was small. The performance is even more increased on transductive setting (EGNN-Semi(T)). In a nutshell, our EGNN is able to extract more useful information from unlabeled samples compared to node-labeling framework, on both transductive and non-transductive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation studies</head><p>The proposed edge-labeling GNN has a deep architecture that consists of several node and edge-update layers. Therefore, as the model gets deeper with more layers, the   <ref type="table">Table 3</ref>: 5-way 5-shot results on miniImagenet with different numbers of EGNN layers and different feature types interactions between task samples should be propagated more intensively, which may leads to performance improvements. To support this statement, we compared the few-shot learning performances with different numbers of EGNN layers, and the results are presented in <ref type="table">Table 3</ref>. As the number of EGNN layers increases, the performance gets better. There exists a big jump on few-shot accuracy when the number of layers changes from 1 to 2 (67.99% → 73.19%), and a little additional gain with three layers (76.37 %).</p><p>Another key ingredient of the proposed EGNN is to use separate exploitation of intra-cluster similarity and intercluster dissimilarity in node/edge updates. To validate the effectiveness of this, we conducted experiment with only intra-cluster aggregation and compared the results with those obtained by using both aggregations. The results are also presented in <ref type="table">Table 3</ref>. For all EGNN layers, the use of separate inter-cluster aggregation clearly improves the performances.</p><p>It should also be noted that compared to the previous node-labeling GNN, the proposed edge-labeling framework is more conducive in solving the few-shot problem under arbitrary meta-test setting, especially when the number of few-shot classes for meta-testing does not match to the one used for meta-training. To validate this statement, we conducted a cross-way experiment with EGNN, and the result is presented in <ref type="table">Table 4</ref>. Here, the model was trained with 5way 5-shot setting and tested on 10-way 5-shot setting, and vice versa. Interestingly, both cross-way results are similar to those obtained with the matched-way settings. Therefore, we can observe that the EGNN can be successfully extended to modified few-shot setting without re-training of the model, while the previous node-labeling <ref type="bibr">GNN</ref>   <ref type="table">Table 4</ref>: Cross-way few-shot learning results on miniImagenet 5-shot setting.</p><p>not even applicable to cross-way setting, since the size of the model and parameters are dependent on the number of ways. <ref type="figure" target="#fig_2">Figure 4</ref> shows t-SNE <ref type="bibr" target="#b48">[49]</ref> visualizations of node features for the previous node-labeling GNN and EGNN. The GNN tends to show a good clustering among support samples after the first layer-propagation, however, query samples are heavily clustered together, and according to each label, query samples and their support samples never get close together, especially even with more layer-propagations, which means that the last fully-connect layer of GNN actually seems to perform most roles in query classification. In contrast, in our EGNN, as the layer-propagation goes on, both the query and support samples are pulled away if their labels are different, and at the same time, equally labeled query and support samples get close together.</p><p>For further analysis, <ref type="figure" target="#fig_3">Figure 5</ref> shows how edge features propagate in EGNN. Starting from the initial feature where all query edges are initialized with 0.5, the edge feature gradually evolves to resemble ground-truth edge label, as they are passes through the several EGNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work addressed the problem of few-shot learning, especially on the few-shot classification task. We proposed the novel EGNN which aims to iteratively update edgelabels for inferring a query association to an existing support clusters. In the process of EGNN, a number of alternative node and edge feature updates were performed using explicit intra-cluster similarity and inter-cluster dissimilarity through the graph layers having different parameter sets, and the edge-label prediction was obtained from the final edge feature. The edge-labeling loss was used to update the parameters of the EGNN with episodic training. Ex-  From left to right: initial edge feature, 1st layer, 2nd layer, ground-truth edge labels. Red color denotes higher value (e ij1 = 1), while blue color denotes lower value (e ij1 = 0). This illustration shows 5-way 3-shot setting, and 3 queries for each class, total 30 task-samples. The first 15 samples are support set, and latter 15 are query set.</p><p>perimental results showed that the proposed EGNN outperformed other few-shot learning algorithms on both of the supervised and semi-supervised few-shot image classification tasks. The proposed framework is applicable to a broad variety of other meta-clustering tasks. For future work, we can consider another training loss which is related to the valid graph clustering such as the cycle loss <ref type="bibr" target="#b34">[35]</ref>. Another promising direction is graph sparsification, e.g. constructing K-nearest neighbor graphs <ref type="bibr" target="#b49">[50]</ref>, that will make our algorithm more scalable to larger number of shots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework of the proposed EGNN model. In this illustration, a 2-way 2-shot problem is presented as an example. Blue and green circles represent two different classes. Nodes with solid line represent labeled support samples, while a node with dashed line represents the unlabeled query sample. The strength of edge feature is represented by the color in the square. Note that although each edge has a 2-dimensional feature, only the first dimension is depicted for simplicity. The detailed process is described in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Detailed network architectures used in EGNN. (a) Embedding network f emb . (b) Feature (node) transformation network f v . (c) Metric network f e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>t-SNE visualization of node features. From top to bottom: GNN [6], EGNN. From left to right: initial embedding, 1st layer, 2nd layer, 3rd layer. 'x' represents query, 'o' represents support. Different colors mean different labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of edge feature propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised few-shot classification accuracies on miniImageNet.</figDesc><table><row><cell></cell><cell cols="3"># of EGNN layers</cell></row><row><cell>Feature type</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell cols="4">Intra &amp; Inter 67.99 73.19 76.37</cell></row><row><cell>Intra Only</cell><cell cols="3">67.28 72.20 74.04</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metalearning: a survey of trends and technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Budka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung Yongxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural optimizer search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Wichrowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous adaptation via meta-learning in nonstationary and competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolved policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradly</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to adapt: Meta-learning for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignasi</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusha</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">S</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno>abs/1803.11347</idno>
		<ptr target="http://arxiv.org/abs/1803.11347" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Vuorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Yeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daejoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.06928" />
	</analytic>
	<monogr>
		<title level="j">Meta continual learning. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforced continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.01261" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>abs/1704.01212</idno>
		<ptr target="http://arxiv.org/abs/1704.01212" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Higher-order correlation clustering for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adaptive edge features guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02709</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04687</idno>
		<title level="m">Neural relational inference for interacting systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno>abs/1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Correlation clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="89" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning structured prediction models: a large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning graphical state transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep metalearning: Learning to learn in the concept space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1802.03596</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On firstorder meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
