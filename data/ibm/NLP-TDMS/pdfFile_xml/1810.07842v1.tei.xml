<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A NOVEL FOCAL TVERSKY LOSS FUNCTION WITH IMPROVED ATTENTION U-NET FOR LESION SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-18">18 Oct 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabila</forename><surname>Abraham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<addrLine>350 Victoria Street</addrLine>
									<settlement>Toronto</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naimul</forename><surname>Mefraz Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<addrLine>350 Victoria Street</addrLine>
									<settlement>Toronto</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A NOVEL FOCAL TVERSKY LOSS FUNCTION WITH IMPROVED ATTENTION U-NET FOR LESION SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-18">18 Oct 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-semantic segmentation</term>
					<term>attention net- works</term>
					<term>Tversky index</term>
					<term>data imbalance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a generalized focal loss function based on the Tversky index to address the issue of data imbalance in medical image segmentation. Compared to the commonly used Dice loss, our loss function achieves a better trade off between precision and recall when training on small structures such as lesions. To evaluate our loss function, we improve the attention U-Net model by incorporating an image pyramid to preserve contextual features. We experiment on the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and 21.4% of the images area and improve segmentation accuracy when compared to the standard U-Net by 25.7% and 3.6%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A common task in medical image analysis is the ability to detect and segment pathological regions that typically occupy a very small fraction of the full image. Such imbalance in the data can lead to instability in established generative and discriminative frameworks <ref type="bibr" target="#b0">[1]</ref>. In recent literature, convolutional neural networks (CNNs) have been successfully applied to automatically segment 2D and 3D biological data <ref type="bibr" target="#b0">[1]</ref>. Most of the current deep learning methods derive from a fully convolutional network architecture (FCN), where the fully connected layers are replaced by convolutional layers <ref type="bibr" target="#b1">[2]</ref>. The popular U-Net is an FCN variant which has become the defacto standard for image segmentation due to its multi-scale skip connections and learnable up-convolution layers <ref type="bibr" target="#b2">[3]</ref>.</p><p>A dominant research area in image segmentation is to develop strategies to deal with class imbalance. The focal loss function proposed in <ref type="bibr" target="#b3">[4]</ref> reshapes the cross-entropy loss function with a modulating exponent to down-weight errors assigned to well-classified examples. The focal loss prevents the vast number of easy negative examples from dominating the gradient to alleviate class-imbalance. In practice however, Code available at https://github.com/nabsabraham/focal-tversky-unet it faces difficulty balancing precision and recall due to small regions-of-interest (ROI) found in medical images. Research efforts to address small ROI segmentation propose more discriminative models such as attention gated networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. CNNs with attention gates (AGs) focus on the target region, with respect to the classification goal, and can be trained endto-end. At test time, these gates generate soft region proposals to highlight salient ROI features and suppress feature activations by irrelevant regions.</p><p>To address the issues of data imbalance and training performance, we combine attention gated U-Net with a novel variant of the focal loss function, better suited for small lesion segmentation. Our major contributions include (1) a novel focal Tversky loss function for highly imbalanced data and small ROI segmentation, where we modulate the Tversky index <ref type="bibr" target="#b6">[7]</ref> to improve precision and recall balance, and (2) a deeply supervised attention U-Net <ref type="bibr" target="#b4">[5]</ref>, improved with a multiscaled input image pyramid for better intermediate feature representations. Experiments were performed on the Breast Ultrasound Lesions 2017 dataset B (BUS) <ref type="bibr" target="#b7">[8]</ref> and the ISIC 2018 skin lesion dataset <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, both datasets suffering from class imbalance and large intra class variation in lesion sizes. On average, the lesions occupy 4.84% ± 5.43% and 21.4% ± 20.3% in ISIC 2018 dataset and BUS dataset B, respectively. When compared to the baseline U-Net, our methods improves Dice scores by 25.7% and 3.6% for BUS dataset B and ISIC 2018, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Focal Tversky Loss</head><p>In the medical community, the Dice score coefficient (DSC) is an overlap index that is widely used to asses segmentation maps. The 2-class DSC variant for class c is expressed in Equation 1, where g ic ∈ {0, 1} and p ic ∈ [0, 1] represent the ground truth label and the predicted label, respectively. The total number of pixels in an image is denoted by N . The ǫ provides numerical stability to prevent division by zero. </p><formula xml:id="formula_0">DSC c = N i=1 p ic g ic + ǫ N i=1 p ic + g ic + ǫ<label>(1)</label></formula><p>A common method to reduce the effects of class imbalance is to introduce a weight w c for each class c, which is inversely proportional to the label frequency <ref type="bibr" target="#b10">[11]</ref>. The linear Dice loss (DL) is therefore defined as a minimization of the overlap between the prediction and ground truth <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">DL c = c 1 − DSC c<label>(2)</label></formula><p>One of the limitations of the Dice loss function is that it equally weighs false positive (FP) and false negative (FN) detections. In practice, this results in segmentation maps with high precision but low recall. With highly imbalanced data and small ROIs such as skin lesions, FN detections need to be weighted higher than FPs to improve recall rate. The Tversky similarity index is a generalization of the Dice score which allows for flexibility in balancing FP and FNs:</p><formula xml:id="formula_2">T I c = N i=1 p ic g ic + ǫ N i=1 p ic g ic + α N i=1 p ic g ic + β N i=1 p ic g ic + ǫ</formula><p>(3) where, p ic is the probability that pixel i is of the lesion class c and p ic is the probability pixel i is of the non-lesion class,c. The same is true for g ic and g ic , respectively. Hyperparameters α and β can be tuned to shift the emphasis to improve recall in the case of large class imbalance. The Tversky index is adapted to a loss function (TL) in <ref type="bibr" target="#b6">[7]</ref> by minimizing</p><formula xml:id="formula_3">c 1 − T I c .</formula><p>Another issue with the DL is that it struggles to segment small ROIs as they do not contribute to the loss significantly. To address this, we propose the focal Tversky loss function (FTL), parametrized by γ, for control between easy background and hard ROI training examples. In <ref type="bibr" target="#b3">[4]</ref>, the focal parameter exponentiates the cross-entropy loss to focus on hard classes detected with lower probability. This idea has been extended in recent works where an exponent is applied to the Dice score <ref type="bibr" target="#b12">[13]</ref> or a combination of Dice and cross-entropy <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Similarly, we define our Focal Tversky Loss (FTL) function as:</p><formula xml:id="formula_4">F T L c = c (1 − T I c ) 1 /γ (4)</formula><p>where γ varies in the range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. In practice, if a pixel is misclassified with a high Tversky index, the FTL is unaffected. However, if the Tversky index is small and the pixel is misclassified, the FTL will decrease significantly.</p><p>When γ &gt; 1, the loss function focuses more on less accurate predictions that have been misclassified. However, we observe over-suppression of the FTL when the class accuracy is high, usually as the model is close to convergence. This trend is visualized in <ref type="figure" target="#fig_0">Figure 1</ref> as increasing values of Tversky index are mapped to flatter regions of the FTL curve with increasing values of γ. We experiment with high values of γ and observe the best performance with γ = <ref type="bibr" target="#b3">4</ref> 3 and therefore train all experiments with it. To combat the over-suppression of the loss function, we train intermediate layers with the FTL but supervise the last layer with the Tversky loss to provide a strong error signal and mitigate sub-optimal convergence.</p><p>We hypothesize using a higher α in our generalized loss function will improve model convergence by shifting the focus to minimize FN predictions. Therefore, we train all models with α = 0.7 and β = 0.3. It is important to note that in the case of α = β = 0.5, the Tversky index simplifies to the DSC. Moreover, when γ = 1, the FTL simplifies to the TL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Architecture</head><p>To achieve further balance between precision and recall, we propose an improved attention U-Net <ref type="bibr" target="#b4">[5]</ref> that incorporates the proposed FTL. This architecture is based on the popular U-Net which has been designed to work well with very small number of training examples <ref type="figure" target="#fig_1">(Figure 2</ref>). The network is composed of a contracting path to extract locality features and an expansive path, to resample the image maps with contextual information. Skip connections are used to combine highresolution local features with low-resolution global features and encourage more semantically meaningful outputs.</p><p>At the deepest stage of encoding, the network has the richest possible feature representation. However, with cascaded convolutions and non-linearities, spatial details tend to get lost in the high-level output maps. This makes it difficult to reduce false detections for small objects that show large shape variability <ref type="bibr" target="#b4">[5]</ref>. To address this issue, we use soft attention gates (AGs) to identify relevant spatial information from low-level feature maps and propagate it to the decoding stage.</p><p>AGs produce attention coefficients α i ∈ [0, 1] at each pixel i, that scale input feature maps x l i , at layer l, to output semantically relevant features,x i l , as depicted in 3. A gating signal, g, is used for each pixel i to determine focus regions. It is collected from a coarser scale than the input query signal, x l i to compute intermediate activation maps:</p><formula xml:id="formula_5">q l attn = ψ T (σ 1 (W T x x l i + W T g g i + b g )) + b ψ<label>(5)</label></formula><p>where the linear attention coefficients, q l attn , are computed by the element-wise sum and 1x1 linear transformations, parameterized by W x , b x , W g and b g . The intermediate maps are transformed by ReLU and sigmoid non-linearities applied as σ 1 and σ 2 , respectively:</p><formula xml:id="formula_6">α l i = σ 2 (q l attn (x l i , g i )<label>(6)</label></formula><p>The attention coefficients α i scale the low level query signal x l i by an element-wise product and retain only relevant activations. These pruned features are then concatenated with upsampled output maps at each scale in the expansive stage. The lowest-level feature maps, i.e. the first skip connections, are not used in the gating function as they do not represent input data in a high dimensional space <ref type="bibr" target="#b4">[5]</ref>. A 1x1x1 convolution and sigmoid activation is applied on each output map in the expansive stage. Every high dimensional feature representation is supervised with our FTL, with the exception of the last layer, to avoid loss over-suppression. This tactic of deep supervision, introduced in <ref type="bibr" target="#b14">[15]</ref>, forces intermediate layers to be semantically discriminative at every scale. Moreover, it helps to ensure that attention unit has the ability to influence the responses to a large range of image foreground content.</p><p>Moreover, since different kinds of class details are more easily accessible at different scales, we inject the encoder layers with an input image pyramid before each of the maxpooling layers. Combined with deep supervision, this method improves segmentation accuracy for datasets where small ROI features can get lost in cascading convolutions and facilitates the network learning more locality aware features with respect to the classification goal. <ref type="figure">Fig. 3</ref>. Schematic of additive attention gate (AG) adapted from <ref type="bibr" target="#b4">[5]</ref>. Input features x l are scaled with attention coefficients α i to propagate relevant features to the decoding layer outputx l . The coarser gating signal g provides contextual information while spatial regions from the input x l provide locality information. Feature map resampling is computed by bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We validate the FTL on two datasets where the ROI class is significantly smaller than the background class and observe large performance gains. We experiment with the Breast Ultrasound Dataset B (BUS) open-sourced in <ref type="bibr" target="#b7">[8]</ref>. This dataset consists of 163 ultrasound images of breast lesions from different women. The average image size is 760 x 570 pixels where each of the images presented one or more lesions. For our experiments, the data is resampled to 128 x 128 pixels with a 75-25 train-test split. To extend our proposed method to larger datasets, we extract training data from the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge dataset <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. This dataset consists of 2,594 RGB images of skin lesions with an average image size of 2166 x 3188 pixels. For our experiments, the dataset is resampled to 192 x 256 pixels with 75-25 train-test split.</p><p>To present a fair evaluation of our multi-scaled attention U-Net and the focal Tversky loss, we do not augment our datasets or incorporate any transfer learning. We study 7 cases of variations within U-Net and the Tversky loss function while comparing to the baseline U-Net trained with Dice loss. Ablation test results are recorded in Section 3 with 5fold cross validation for Dice scores, precision and recall.  The ISIC 2018 experiment was trained for 50 epochs with a batch size of 8. The BUS 2017 dataset was trained for 100 epochs with a batch size of 16. Both models were optimized using stochastic gradient descent with momentum, using an initial learning rate at 0.01 which decays by 10 −6 on every epoch. These parameters were optimized through a grid search method <ref type="bibr" target="#b15">[16]</ref>. All experiments are programmed using the Keras framework with the Tensorflow backend and trained using an NVIDIA GTX 1070 GPU. Open-source implementation with reproducible results for this paper can be obtained from https://github.com/nabsabraham/focal-tversky-unet. <ref type="table" target="#tab_0">Table 1</ref> shows that the baseline U-Net trained with the Dice loss function has the worst performance. The large standard deviation in the precision and recall scores suggest the learning is not stable. In contrast, U-Net models trained with TL and FTL show increased DSC and more balanced precisionrecall scores which occurs due to weighting α higher in the loss function than β. We observe incorporating attention in U-Net trained with DL depicts lower Dice scores than the baseline, probably due to the intra-lesion variation. Injecting an input pyramid into the model improves the DSC significantly suggesting features of small lesions are easily lost when class imbalance is high. Training the attention model with FTL combines the benefits of improved feature selection with focused training to outperform all other methods. The proposed architecture (last row) is able to segment lesions with a Dice score of 0.804 on training with a small subset of 100 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Contrary to the BUS scores, ISIC results in <ref type="table">Table 2</ref> show the baseline U-Net trained with DL performs well due to the large training sample size, variation in lesion structures and distinct features present in the RGB images. Training U-Net with TL and FTL, we observe an improved DSC score. However, when the Tversky index is high for misclassified examples, the focal exponent γ suppresses the contribution to the error signal and since α is weighted higher than β, the model converges to the highest reported recall at 0.926, but lowest precision. To address this issue, when training the proposed attention model, we supervise the last layer with TL so that a true error signal will still propogate back when the model is close to convergence. As a result, our improved attention U-Net model with FTL (last row) obtains slightly lower but overall better balanced recall and precision, and, consequently, the best DSC score. We outperform the baseline by 3.6% with a low spread of 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we propose a novel focal Tversky loss function to improve the precision and recall balance in semantic segmentation. Our experiments demonstrate the importance of the choice of loss function when dealing with highly imbalanced problems and with varying dataset sizes. Moreover, we improve the attention U-Net proposed in <ref type="bibr" target="#b4">[5]</ref> by incorporating a an input image pyramid into each scale in the model architecture. The redundancy in the features helps recover any lost contextual information which is crucial when segmenting small ROIs such as lesions. Our proposed method outperforms the baseline U-Net in Dice scores and presents balanced precision-recall scores with low standard deviations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The focal Tversky loss non-linearly focuses training on hard examples (where Tversky Index &lt; 0.5) and suppresses easy examples from contributing to the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FigureFig. 2 .</head><label>2</label><figDesc>Proposed Attention U-Net architecture with input image pyramid and deep supervised output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 2 .</head><label>2</label><figDesc>+ DL α = 0.5, β = 0.5 0.547 ± 0.04 0.653 ± 0.171 0.658 ± 0.146 U-Net + TL α = 0.7, β = 0.3 0.657 ± 0.02 0.732 ± 0.072 0.723 ± 0.074 U-Net + FTL α = 0.7, β = 0.3, γ = 4 /3 0.669 ± 0.033 0.775 ± 0.047 0.715 ± 0.057 Attn U-Net + DL α = 0.5, β = 0.5 0.615 ± 0.020 0.675 ± 0.042 0.658 ± 0.049 Attn U-Net + Multi-Input + DL α = 0.5, β = 0.5 0.716 ± 0.041 0.759 ± 0.092 0.751 ± 0.046 Attn U-Net + Multi-Input + TL α = 0.7, β = 0.3 0.751 ± 0.042 0.802 ± 0.073 0.768 ± 0.056 Attn U-Net + Multi-Input + FTL α = 0.7, β = 0.3, γ = 4 /3 0.804 ± 0.024 0.829 ± 0.027 0.817 ± 0.022 Performance on ISIC 2018 with 649 test images Model Parameters DSC Precision Recall U-Net + DL α = 0.5, β = 0.5 0.820 ± 0.013 0.849 ± 0.038 0.867 ± 0.048 U-Net + TL α = 0.7, β = 0.3 0.838 ± 0.026 0.822 ± 0.051 0.917 ± 0.033 U-Net + FTL α = 0.7, β = 0.3, γ = 4 /3 0.829 ± 0.027 0.797 ± 0.040 0.926 ± 0.012 Attn U-Net + DL α = 0.5, β = 0.5 0.806 ± 0.033 0.874 ± 0.080 0.827 ± 0.055 Attn U-Net + Multi-Input + DL α = 0.5, β = 0.5 0.827 ± 0.055 0.896 ± 0.019 0.829 ± 0.076 Attn U-Net + Multi-Input + TL α = 0.7, β = 0.3 0.841 ± 0.012 0.823 ± 0.038 0.912 ± 0.026 Attn U-Net + Multi-Input + FTL α = 0.7, β = 0.3, γ = 4 /3 0.856 ± 0.007 0.858 ± 0.020 0.897 ± 0.014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance on BUS 2017 Dataset B with 40 test images</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><forename type="middle">Le</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Where&apos;s your focus: Personalized attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07931</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tversky as a loss function for highly unbalanced image segmentation using 3d fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Seyed Raein Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadegh Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">K</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gholipour</surname></persName>
		</author>
		<idno>abs/1803.11078</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Moi Hoon Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melcior</forename><surname>Ganau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyer</forename><surname>Sentís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">K</forename><surname>Zwiggelaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabin</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI 2018)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The ham10000 dataset: A large collection of multisource dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10417</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d segmentation with exponential logarithmic loss for highly unbalanced object sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Ken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mic-Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>21st International Conference on Medical Image Computing and Computer Assisted Intervention<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11073</biblScope>
		</imprint>
	</monogr>
	<note>Part IV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Anatomynet: Deep 3d squeeze-and-excitation u-nets for fast and fully automated whole-volume anatomical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05238</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
