<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-identification: Past, Present and Future</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Person Re-identification: Past, Present and Future</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Large-scale person re-identification</term>
					<term>hand-crafted systems</term>
					<term>Convolutional Neural Network</term>
					<term>literature survey</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image-and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A CCORDING to Homer (Odyssey iv:412), Mennelaus was becalmed on his journey home from the Trojan War; He wanted to propitiate the gods and return safely home.</p><p>He was told that he should capture Proteus and force him to reveal the answer. Although Proteus transformed to a lion, a serpent, a leopard, water and also a tree, Mennelaus then succeeded in holding him as he emerged from the sea to sleep among the seals. Proteus was finally compelled to answer to him truthfully.</p><p>Perhaps this is one of the oldest stories about reidentifying a person even after intensive appearance changes. In 1961, when discussing the relationship between mental states and behavior, Alvin Plantinga <ref type="bibr" target="#b0">[1]</ref> provided one of the first definitions of re-identification:</p><p>"To re-identify a particular, then, is to identify it as (numerically) the same particular as one encountered on a previous occasion".</p><p>Person re-identification had thus been studied in various research and documentation areas such as metaphysics <ref type="bibr" target="#b0">[1]</ref>, psychology <ref type="bibr" target="#b1">[2]</ref>, and logic <ref type="bibr" target="#b2">[3]</ref>. All these works are grounded on Leibniz's Law which claims that "there cannot be separate objects or entities that have all their properties in common."</p><p>In the modern computer vision community, the task of person re-ID shares similar insights with the old times. In video surveillance, when being presented with a person-ofinterest (query), person re-ID tells whether this person has been observed in another place (time) by another camera. The emergence of this task can be attributed to 1) the increasing demand of public safety and 2) the widespread large camera • L. <ref type="bibr">Zheng</ref>  networks in theme parks, university campuses and streets, etc. Both causes make it extremely expensive to rely solely on brute-force human labor to accurately and efficiently spot a person-of-interest or to track a person across cameras. Technically speaking, a practical person re-ID system in video surveillance can be broken down into three modules, i.e., person detection, person tracking, and person retrieval. It is generally believed that the first two modules are independent computer vision tasks, so most re-ID works focus on the last module, i.e., person retrieval. In this survey, if not specified, person re-ID refers to the person retrieval module. From the perspective of computer vision, the most challenging problem in re-ID is how to correctly match two images of the same person under intensive appearance changes, such as lighting, pose, and viewpoint, which has important scientific values. Given its research and application significance, the re-ID community is fast growing, evidenced by an increasing number of publications in the top venues ( <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Organization of This Survey</head><p>Some person re-ID surveys exist <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this survey, we mainly discuss the vision part of re-ID, which is also a focus in the community, and refer readers to the camera calibration and view topology methods in <ref type="bibr" target="#b4">[5]</ref>. Another difference from previous surveys is that we focus on different re-ID subtasks currently available or likely to be visible in the future, instead of very detailed techniques or architectures. Special emphasis is given deep learning methods, end-toend re-ID and very large scale re-ID, which are currently popular topics or reflect future trends. This survey first introduces a brief history of person re-ID in Section 1.2 and its relationship with classification and retrieval in Section 1.3. We then describe previous literature in image-based and video-based person re-ID in Section 2 and Section 3, respectively. Both sections categorize methods into handcrafted and deeply-learned systems. In Section 4, since the arXiv:1610.02984v1 [cs.CV] 10 Oct 2016</p><p>Multi-cam tracking Huang and Russell "Person re-identificaiton" appears in multi-cam tracking Zajdel et al.</p><p>Person re-identification as an independent vision task Gheissari et al.  relationship between detection, tracking, and re-ID has not been extensively studied, we will discuss several previous works and point out future research emphasis. In Section 5, large-scale re-ID which resorts to state-of-the-art retrieval models will be introduced, which is also an important future direction. Some other open issues will be summarized in Section 6, and conclusions will be drawn in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">A Brief History of Person Re-ID</head><p>Person re-ID research started with multi-camera tracking <ref type="bibr" target="#b7">[8]</ref>.</p><p>Several important re-ID directions have been developed since then. In this survey, we briefly introduce some milestones in person re-ID history <ref type="figure">(Fig. 2)</ref>. Multi-camera tracking. In the early years, person re-ID, as a the term without being formally raised, was tightly twined with multi-camera tracking, in which appearance models were integrated with the geometry calibration among disjoint cameras. In 1997, Huang and Russell <ref type="bibr" target="#b8">[9]</ref> proposed a Bayesian formulation to estimate the posterior of predicting the appearance of objects in one camera given evidence observed in other camera views. The appearance model includes multiple spatial-temporal features such as color, vehicle length, height and width, velocity, and time of observation. A comprehensive survey of multi-camera tracking can be accessed in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Multi-camera tracking with explicit "re-identification". To our knowledge, the first work on multi-camera tracking where the term "person re-identification" is proposed, was published in 2005 by Wojciech Zajdel, Zoran Zivkovic and Ben J. A. Kröse from the University of Amsterdam <ref type="bibr" target="#b9">[10]</ref>. In their ICRA'05 paper entitled "Keeping track of humans: Have I seen this person before?", Zajedel et al. aims to "re-identify a person when it leaves the field of view and re-enters later". In their method, a unique, latent label is assumed for every person, and a dynamic Bayesian network is defined to encode the probabilistic relationship between the labels and features (color and spatial-temporal cues) from the tracklets. The ID of an incoming person is determined by the posterior label distributions computed by an approximate Bayesian inference algorithm.</p><p>The independence of re-ID (image-based). One year later in 2006, Gheissari et al. <ref type="bibr" target="#b10">[11]</ref> employed only the visual cues of persons after a spatial-temporal segmentation algorithm for foreground detection. Visual matching based on color and salient edgel histograms is performed by either an articulated pedestrian model or the Hessian-Affine interest point operator. Experiments are conducted on a dataset with 44 persons captured by 3 cameras with moderate view overlap. Note that, although Gheissari et al. <ref type="bibr" target="#b10">[11]</ref> design a spatial-temporal segmentation method using the video frames, neither the feature design nor matching processes use the video information, so we classify <ref type="bibr" target="#b10">[11]</ref> into imagebased re-ID. This work <ref type="bibr" target="#b10">[11]</ref> marks the separation of person re-ID from multi-camera tracking, and its beginning as an independent computer vision task.</p><p>Video-based re-ID. Initially intended for tracking in videos, most re-ID works focus on image matching instead. In the year 2010, two works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> were proposed for multishot re-ID, in which frames are randomly selected. Color is a common feature used in both works, and Farenzena et al. <ref type="bibr" target="#b12">[13]</ref> additionally employ a segmentation model to detect the foreground. For distance measurement, both works calculate the minimum distance among bounding boxes in two image sets, and Bazzani et al. further use the Bhattacharyya distance for the color and generic epitome features. It is shown that using multiple frames per person effectively improves over the single-frame version <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and that re-ID accuracy will saturate as the number of selected frames increases <ref type="bibr" target="#b11">[12]</ref>.</p><p>Deep learning for re-ID. The success of deep learning in image classification <ref type="bibr" target="#b13">[14]</ref> spreads to re-ID in 2014, when Yi et al. <ref type="bibr" target="#b14">[15]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref> both employ a siamese neural network <ref type="bibr" target="#b16">[17]</ref> to determine if a pair of input images belong to the same ID. The reason for choosing the siamese model is probably that the number of training samples for each identity is limited (usually two). Aside from some variations in parameter settings, the main differences are that <ref type="bibr" target="#b14">[15]</ref> adds</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Train Class Test Class Advantage Classification available seen discri. learning Retrieval not available unseen efficiency Person re-ID available unseen discri. + efficiency?  <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>, so the two methods are not directly comparable. Although its performance is not stable yet on the small datasets, deep learning methods has since become a popular option in re-ID End-to-end image-based re-ID. While a majority of works use hand-cropped boxes or boxes produced by a fixed detector in their experiments, it is necessary to study the impact of pedestrian detectors on re-ID accuracy. In 2014, Xu et al. <ref type="bibr" target="#b17">[18]</ref> addressed this topic by combining the detection (commonness) and re-ID (uniqueness) scores. It is shown that on the CAMPUS dataset, jointly considering detection and re-ID confidence leads to higher person retrieval accuracy than using them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Relationship with Classification and Retrieval</head><p>Person re-ID lies inbetween image classification <ref type="bibr" target="#b13">[14]</ref> and instance retrieval <ref type="bibr" target="#b18">[19]</ref> in terms of the relationship between training and testing classes <ref type="table" target="#tab_2">(Table 1)</ref>. For image classification, training images are available for each class, and testing images fall into these predefined classes, denoted as previously "seen" in <ref type="table" target="#tab_2">Table 1</ref>. For instance retrieval, usually there is no training data because one does not know the content of the query in advance and the gallery may contain various types of objects. So the training classes are "not available" and the testing classes (queries) are denoted as previously "unseen".</p><p>Compared to image classification, person re-ID is similar in that the training classes are available, which includes images of different identities. Person re-ID is also similar to instance retrieval in that the testing identities are unseen: they do not have overlap with the training identities, except that both training and testing images are of pedestrians.</p><p>As a consequence, person re-ID can be positioned to take advantage of both classification and retrieval. On the one hand, using training classes, discriminative distance metrics <ref type="bibr" target="#b19">[20]</ref> or feature embeddings <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[21]</ref> can be learned in the person space. On the other hand, when it comes to retrieval, efficient indexing structures <ref type="bibr" target="#b21">[22]</ref> and hashing techniques <ref type="bibr" target="#b22">[23]</ref> can be beneficial for re-ID in a large gallery. In this survey, both effective learning and efficient retrieval approaches will be introduced or pointed out as important future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IMAGE-BASED PERSON RE-ID</head><p>Since the work by <ref type="bibr">Gheissari et al. in 2006 [11]</ref>, person re-ID has mostly been explored using single images. Let us consider a closed-world toy model, in which G is a gallery (database) composed of N images, denoted as {g i } N i=1 . They belong to N different identities 1, 2, ..., N . Given a probe (query) image q, its identity is determined by:</p><formula xml:id="formula_0">i * = arg max i∈1,2,...,N sim(q, g i ),<label>(1)</label></formula><p>where i * is the identity of probe q, and sim(·, ·) is some kind of similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hand-crafted Systems</head><p>It is apparent from Eq. 1 that two components are necessary for a toy re-ID system, i.e., image description and distance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Pedestrian Description</head><p>In pedestrian descriptions, the most commonly used feature is color, while texture features are less frequent. In <ref type="bibr" target="#b12">[13]</ref>, the pedestrian foreground is segmented from the background, and a symmetrical axis is computed for each body part. Based on body configuration, the weighted color histogram (WH), the maximally stable color regions (MSCR), and the recurrent high-structured patches (RHSP) are computed. WH assigns larger weights to pixels near the symmetrical axis and forms a color histogram for each part. MSCR detects stable color regions and extracts features such as color, area, and centroid. RHSP instead is a texture feature capturing recurrent texture patches. Gheissari et al. <ref type="bibr" target="#b10">[11]</ref> propose a spatial-temporal segmentation method to detect stable foreground regions. For a local region, an HS histogram and an edgel histogram are computed. The latter encodes the dominant local boundary orientation and the RGB ratios on either sides of the edgel. Gray and Tao <ref type="bibr" target="#b23">[24]</ref> use 8 color channels (RGB, HS, and YCbCr) and 21 texture filters on the luminance channel, and the pedestrian is partitioned into horizontal stripes. A number of later works <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> employ the same set of features as <ref type="bibr" target="#b23">[24]</ref>. Similarly, Mignon et al. <ref type="bibr" target="#b27">[28]</ref> build the feature vector from RGB, YUV and HSV channels and the LBP texture histograms in horizontal stripes. Compared to the earlier works described above, handcrafted features have remained more or less the same in recent years <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. In a series of works by Zhao et al. <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the 32-dim LAB color histogram and the 128-dim SIFT descriptor are extracted from each 10 × 10 patch densely sampled with a step size of 5 pixels; this feature is also used in <ref type="bibr" target="#b34">[35]</ref>. Adjacency constrained search is employed to find the best match for a query patch in horizontal stripes with similar latitudes in a gallery image. Das et al. <ref type="bibr" target="#b35">[36]</ref> apply HSV histograms on the head, torso and legs from the silhouette proposed in <ref type="bibr" target="#b11">[12]</ref>. Li et al. <ref type="bibr" target="#b30">[31]</ref> also extract local color descriptors from patches but aggregate them using hierarchical Gaussianization <ref type="bibr" target="#b36">[37]</ref> to capture spatial information, a procedure followed by <ref type="bibr" target="#b37">[38]</ref>. Pedagadi et al. <ref type="bibr" target="#b38">[39]</ref> extract color histograms and moments from HSV and YUV spaces before dimension reduction using PCA. Liu et al. <ref type="bibr" target="#b39">[40]</ref> extract the HSV histogram, gradient histogram and the LBP histogram for each local patch. To improve the robustness of the RGB values against photometric variance, Yang et al. <ref type="bibr" target="#b40">[41]</ref> introduce the salient color names based color descriptor (SCNCD) for global pedestrian color descriptions. The influence of the background and different color spaces are also analysed. In <ref type="bibr" target="#b19">[20]</ref>, Liao et al. propose the local maximal occurrence (LOMO) descriptor, which includes the color and SILTP histograms. Bins in the same horizontal stripe undergo max pooling and a three-scale pyramid model is built before a log transformation. LOMO is later employed by <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and a similar set of features is used by Chen et al. <ref type="bibr" target="#b31">[32]</ref>. In <ref type="bibr" target="#b43">[44]</ref>, Zheng et al. propose extracting the 11-dim color names descriptor <ref type="bibr" target="#b44">[45]</ref> for each local patch, and aggregating them into a global vector through a Bag-of-Words (BoW) model. In <ref type="bibr" target="#b45">[46]</ref>, a hierarchical Gaussian feature is proposed to describe color and texture cues, which models each region by multiple Gaussian distributions. Each distribution represents a patch inside the region.</p><p>Apart from directly using low-level color and texture features, another good choice is the attribute-based features which can be viewed as mid-level representations. It is believed that attributes are more robust to image translations compared to low-level descriptors. In <ref type="bibr" target="#b46">[47]</ref>, Layne et al. annotate 15 binary attributes on the VIPeR dataset related to attire and soft biometrics. The low-level color and texture features are used to train the attribute classifiers. After attribute weighting, the resulting vector is integrated in the SDALF <ref type="bibr" target="#b12">[13]</ref> framework to fuse with other visual features. Liu et al. <ref type="bibr" target="#b47">[48]</ref> improve the latent Dirichlet allocation (LDA) model using annotated attributes to filter out noisy LDA topics. Liu et al. <ref type="bibr" target="#b48">[49]</ref> propose discovering some pedestrian prototypes with common attributes in an unsupervised manner and adaptively determine the feature weights of different query person according to the prototypes. Some recent works borrow external data for attribute learning. In <ref type="bibr" target="#b49">[50]</ref>, Su et al. embed the binary semantic attributes of the same person but different cameras into a continuous low-rank attribute space, so that the attribute vector is more discriminative for matching. Shi et al. <ref type="bibr" target="#b50">[51]</ref> propose learning a number of attributes including color, texture, and category labels from existing fashion photography datasets. These attributes are directly transferred to re-ID under surveillance videos and achieve competitive results. Recently, Li et al. <ref type="bibr" target="#b51">[52]</ref> collected a large-scale dataset with richly annotated pedestrian attributes to facilitate attributebased re-ID methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Distance Metric Learning</head><p>In hand-crafted re-ID systems, a good distance metric is critical for its success, because the high-dimensional visual features typically do not capture the invariant factors under sample variances. A comprehensive survey of the metric learning methods can be accessed in <ref type="bibr" target="#b52">[53]</ref>. These metric learning methods are categorized w.r.t supervised learning versus unsupervised learning, global learning versus local learning, etc. In person re-ID, the majority of works fall into the scope of supervised global distance metric learning.</p><p>The general idea of global metric learning is to keep all the vectors of the same class closer while pushing vectors of different classes further apart. The most commonly used formulation is based on the class of Mahalanobis distance functions, which generalizes Euclidean distance using linear scalings and rotations of the feature space. The squared distance between two vectors x i and x j can be written as,</p><formula xml:id="formula_1">d(x i , x j ) = (x i − x j ) T M(x i − x j ),<label>(2)</label></formula><p>where M is a positive semidefinite matrix. Equation 2 can be formulated into the convex programming problem suggested by Xing et al. <ref type="bibr" target="#b53">[54]</ref>.</p><p>In person re-ID, currently the most popular metric learning method, i.e., KISSME <ref type="bibr" target="#b54">[55]</ref> is based on Eq. 2. In this method <ref type="bibr" target="#b54">[55]</ref>, the decision on whether a pair (i, j) is similar or not is formulated as a likelihood ratio test. The pairwise difference (x i,j = x i − x j ) is employed and the difference space is assumed to be a Gaussian distribution with a zero mean. It is shown in <ref type="bibr" target="#b54">[55]</ref> that the Mahalanobis distance metric can be naturally derived from the log-likelihood ratio test and in practice, the principle component analysis (PCA) is applied to the data points to eliminate dimension correlations.</p><p>Based on Eq. 2, a number of other metric learning methods have been introduced. In the early days, some classic metric learning methods target at nearest neighbor classification. Weinberger et al. <ref type="bibr" target="#b55">[56]</ref> propose the large margin nearest neighbor Learning (LMNN) method which sets up a perimeter for the target neighbors (matched pairs) and punishes those invading the perimeter (imposters). This method belongs to the supervised local distance metric learning category <ref type="bibr" target="#b52">[53]</ref>. To avoid the overfitting problems encountered in LMNN, Davis et al. <ref type="bibr" target="#b56">[57]</ref> propose the information-theoretic metric learning (ITML) as a trade-off between satisfying the given similarity constraints and ensuring that the learned metric is close to the initial distance function.</p><p>In recent years, Hirzer et al. <ref type="bibr" target="#b57">[58]</ref> proposed relaxing the positivity constraint which provides a sufficient approximation for the matrix M with a much lower computational cost. Chen et al. <ref type="bibr" target="#b37">[38]</ref> add a bilinear similarity in addition to the Mahalanobis distance, so that cross-patch similarities can be modeled. In <ref type="bibr" target="#b30">[31]</ref>, the global distance metric is coupled with the local adaptive threshold rule which additionally contains the orthogonal information of (x i , x j ). In <ref type="bibr" target="#b58">[59]</ref>, Liao et al. suggest perserving with a positive semidefinite constraint and propose weighting the positive and negative samples differently. Yang et al. <ref type="bibr" target="#b59">[60]</ref> consider both the differences and commonness between image pairs and show that the covariance matrices of dissimilar pairs can be inferred from those of the similar pairs, which makes the learning process scalable to large datasets.</p><p>Other than learning distance metrics, some works focus on learning discriminative subspaces. Liao et al. <ref type="bibr" target="#b19">[20]</ref> propose learning the projection w to a low-dimensional subspace with cross-view data solved in a similar manner to linear discriminant analysis (LDA) <ref type="bibr" target="#b60">[61]</ref>,</p><formula xml:id="formula_2">J (w) = w T S b w w T S w w ,<label>(3)</label></formula><p>where S b and S w are the between-class and within-class scatter matrices, respectively. Then, a distance function is learned in the resulting subspace using KISSME. To learn w, Zhang et al. <ref type="bibr" target="#b41">[42]</ref> further employ the null Foley-Sammon transform to learn a discriminative null space which satisfies a zero within-class scatter and a positive between-class scatter. For dimension reduction, Pedagadi et al. <ref type="bibr" target="#b38">[39]</ref> sequentially combine the unsupervised PCA (principle component analysis) and supervised local Fisher discriminative analysis which preserves the local neighborhood structure. In <ref type="bibr" target="#b27">[28]</ref>, the pairwise constrained component analysis (PCCA) is proposed which learns a linear mapping function to be able to work directly on high-dimensional data, while ITML and KISSME should be preceded by a step of dimension reduction. In <ref type="bibr" target="#b61">[62]</ref>, Xiong et al. further propose improved versions of two existing subspace projection methods, i.e., regularized PCCA <ref type="bibr" target="#b27">[28]</ref> and kernel LFDA <ref type="bibr" target="#b38">[39]</ref>. Aside from the methods that use Mahalanobis distance (Eq. 2), some use other learning tools such as support vector machine (SVM) or boosting. Prosser et al. <ref type="bibr" target="#b24">[25]</ref> propose learning a set of weak RankSVMs which are subsequently</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Identification Verification rank-1 (%) mAP (%) rank-1 (%) mAP (%) AlexNet <ref type="bibr" target="#b13">[14]</ref> 56  assembled into a stronger ranker. In <ref type="bibr" target="#b62">[63]</ref>, a structural SVM is employed to combine different color descriptors at decision level. In <ref type="bibr" target="#b42">[43]</ref>, Zhang et al. learn a specific SVM for each training identity and map each testing image to a weight vector inferred from its visual features. Gray and Tao <ref type="bibr" target="#b23">[24]</ref> propose using the AdaBoost algorithm to select and combine many different kinds of simple features into a single similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deeply-learned Systems</head><p>CNN-based deep learning models have been popular since Krizhevsky et al. <ref type="bibr" target="#b13">[14]</ref> won ILSVRC'12 by a large margin. The first two works in re-ID to use deep learning were <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> as mentioned in Section 1.2 and <ref type="figure">Fig. 2</ref>. Generally speaking, two types of CNN models are commonly employed in the community. The first type is the classification model as used in image classification <ref type="bibr" target="#b13">[14]</ref> and object detection <ref type="bibr" target="#b63">[64]</ref>.</p><p>The second is the siamese model using image pairs <ref type="bibr" target="#b64">[65]</ref> or triplets <ref type="bibr" target="#b65">[66]</ref> as input. The major bottleneck of deep learning in re-ID is the lack of training data. Most re-ID datasets provide only two images for each identity such as VIPeR <ref type="bibr" target="#b23">[24]</ref>, so currently most CNN-based re-ID methods focus on the siamese model. In <ref type="bibr" target="#b14">[15]</ref>, an input image is partitioned into three overlapping horizontal parts, and the parts go through two convolutional layers plus a fully connected layer which fuses them and outputs a vector for this image. The similarity of the two output vectors are computed using the cosine distance. The architecture designed by Li et al. <ref type="bibr" target="#b15">[16]</ref> is different in that a patch matching layer is added which multiplies the convolution responses of two images in different horizontal stripes, similar to ACS <ref type="bibr" target="#b29">[30]</ref>   <ref type="bibr" target="#b71">[72]</ref> propose inserting a gating function after each convolutional layer to capture effective subtle patterns when a pair of testing images are fed into the network. This method achieves state-of-the-art accuracy on several benchmarks, but its disadvantage is also obvious. The query has to pair with each gallery image before being sent into the network -a time inefficient process in large datasets. Similar to <ref type="bibr" target="#b71">[72]</ref>, Liu et al. <ref type="bibr" target="#b72">[73]</ref> propose integrating a soft attention based model in a siamese network to adaptively focus on the important local parts of an input image pair; however, this method is also limited by computational inefficiency. While these works use image pairs, Cheng et al. <ref type="bibr" target="#b73">[74]</ref> design a triplet loss function that takes three images as input. After the first convolutional layer, four overlapping body parts are partitioned for each image and fused with a global one in the FC layer. Su et al. <ref type="bibr" target="#b74">[75]</ref> propose a three-stage learning process which includes attribute prediction using an independent dataset and an attributes triplet loss trained on datasets with ID labels. A drawback of the siamese model is that it does not make full use of re-ID annotations. In fact, the siamese model only needs to consider pairwise (or triplet) labels. Telling whether an image pair is similar (belong to the same identity) or not is a weak label in re-ID. Another potentially effective strategy consists of using a classification/identification mode, which makes full use of the re-ID labels. In <ref type="bibr" target="#b75">[76]</ref>, training identities from multiple datasets jointly form the training set and a softmax loss is employed in the classification network. Together with the proposed impact score for each FC neuron and a domain guided dropout based on the impact score, the learned generic embeddings yield competitive re-id accuracy. On larger datasets, such as PRW and MARS, the classification model achieves good performance without careful training sample selection <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b76">[77]</ref>. Yet the application of the identification loss requires more training instances per ID for model convergence. For comparison, this survey presents some baseline results for both types of models. In <ref type="table" target="#tab_4">Table 2</ref>, we implement the identification and verification models on the Market-1501 dataset <ref type="bibr" target="#b43">[44]</ref>. All the networks use the default parameter settings, and are fine-tuned from the ImageNet <ref type="bibr" target="#b77">[78]</ref> pre-trained models. Images are resized to 224 × 224 before being fed into the network. The initial learning rate is set to 0.001 and reduced by a factor of 0.1 after each epoch. Training is done after 36 epochs. We can clearly observe that the identification model outperforms the verification model, and that the residual-50 model <ref type="bibr" target="#b67">[68]</ref> yields state-of-the-art re-ID accuracy on Market-1501 compared with recent results <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b74">[75]</ref>.</p><p>The above-mentioned works learn deep features in an end-to-end manner, and there are alternatives that take low-level features as input. In <ref type="bibr" target="#b78">[79]</ref>, low-level descriptors including SIFT and color histograms are aggregated into a single Fisher Vector <ref type="bibr" target="#b79">[80]</ref> for each image. The hybrid network builds fully connected layers on the input Fisher vectors and enforces the linear discriminative analysis (LDA) as an objective function to produce embeddings that have low intra-class variance and high inter-class variance. Wu et al. <ref type="bibr" target="#b80">[81]</ref> propose concatenating the FC feature and a low-level feature vector, which is followed by another FC layer before the softmax loss layer. This method constrains the FC features using the hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Datasets and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Datasets</head><p>A number of datasets for image-based re-ID have been released, and some commonly used datasets are summarized in <ref type="table" target="#tab_6">Table 3</ref>. The most tested benchmark is VIPeR. It contains 632 identities, and two images for each identity. 10 random train/test splits are used for stable performance, and each split has 316 different identities in both the training <ref type="table" target="#tab_2">Dataset  time #ID #image  #camera  label  evaluation  VIPeR  2007 632  1,264  2  hand  CMC  iLIDS  2009 119  476  2  hand  CMC  GRID  2009 250  1,275  8  hand  CMC  CAVIAR  2011  72  610  2  hand  CMC  PRID2011  2011 200  1,134  2  hand  CMC  WARD  2012  70  4,786  3  hand  CMC  CUHK01  2012 971  3,884  2  hand  CMC  CUHK02  2013 1,816  7,264  10 (5 pairs)  hand  CMC  CUHK03  2014 1,467 13,164  2  hand/DPM  CMC  RAiD  2014  43  1,264  4  hand  CMC  PRID 450S  2014 450  900  2  hand  CMC  Market-1501 2015 1,501 32,668  6</ref> hand/DPM CMC/mAP  <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref> for image-based re-ID. and testing sets. These datasets reflect various scenarios. For example, the GRID dataset <ref type="bibr" target="#b83">[84]</ref> was collected in an underground station, iLIDS <ref type="bibr" target="#b82">[83]</ref> was captured at an airport arrival hall, and CUHK01 <ref type="bibr" target="#b87">[88]</ref>, CUHK02 <ref type="bibr" target="#b88">[89]</ref>, CUHK03 <ref type="bibr" target="#b15">[16]</ref> and Market-1501 <ref type="bibr" target="#b43">[44]</ref> were collected in a university campus. Over recent years, progress can observed in several aspects. First, the dataset scale is increasing. Many of these datasets are relatively small in size, especially those of the early days, but recent datasets, such as CUHK03 and Market-1501, are larger. Both have over 1,000 IDs and over 10,000 bounding boxes, and both datasets provide good amount of data for training deep learning models. That said, we must admit that the current data volume is still far from satisfactory. The community is in great need of larger datasets.</p><p>Second, the bounding boxes tend to be produced by pedestrian detectors (such as DPM <ref type="bibr" target="#b90">[91]</ref> and ACF <ref type="bibr" target="#b91">[92]</ref>) instead of being hand-drawn. For practical applications, it is infeasible to draw gallery bounding boxes using human labor, so detectors must be used. This may cause the bounding boxes to deviate from ideal ones. It is shown in <ref type="bibr" target="#b15">[16]</ref> that using detected bounding boxes usually leads to compromised re-ID accuracy compared to hand-drawn ones due to detector errors such as misalignment. In <ref type="bibr" target="#b43">[44]</ref>, a number of false detection results (on the background) are included in the gallery, which is inevitable when detectors are used. The experiments in <ref type="bibr" target="#b43">[44]</ref> show that re-ID accuracy drops as more distractors are added to the gallery. As a consequence, it is beneficial for the community to study datasets with practical imperfections such as false detection and misalignment.</p><p>Third, more cameras are used during collection. For example, each identity in Market-1501 can be captured by up to 6 cameras. This design calls for metric learning methods that have good generalization ability, instead of being carefully tuned between a certain camera pair. In fact, in a city-scale camera network with n, the number of camera pairs is C 2 n , so it is prohibitive to collect annotated data from each camera and train C 2 n distance metrics. For more detailed descriptions of these datasets, we refer to survey <ref type="bibr" target="#b4">[5]</ref> and website 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Evaluation Metrics</head><p>When evaluating re-ID algorithms, the cumulative matching characteristics (cmc) curve is usually used. CMC represents 1. http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu. edu/files/systems/projectpages/reiddataset.html the probability that a query identity appears in differentsized candidate lists. No matter how many ground truth matches there are in the gallery, only the first match is counted in the CMC calculation. So basically, CMC is accurate as an evaluation method only when one ground truth for each query exists. This measurement is acceptable, in practice, when people care more about returning the ground truth match in the top positions of the rank list.</p><p>For research integrity, however, when multiple ground truths exist in the gallery, Zheng et al. <ref type="bibr" target="#b43">[44]</ref> propose using the mean average precision (mAP) for evaluation. The motivation is that a perfect re-ID system should be able to return all true matches to the user. The case might be that two systems are equally competent at spotting the first ground truth, but have different retrieval recall ability. In this scenario, CMC does not have enough discriminative ability but mAP does. Therefore, mAP is used together with CMC for the Market-1501 dataset where multiple ground truths from multiple cameras exist for each query. Later, in <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b92">[93]</ref>, mAP results are also reported for datasets with multiple ground truths per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Re-ID Accuracy Over the Years</head><p>In this section, we summarize re-ID accuracy on several representative datasets over the years in <ref type="figure" target="#fig_1">Fig. 3</ref>. The presented datasets are VIPeR <ref type="bibr" target="#b81">[82]</ref>, CUHK01 <ref type="bibr" target="#b87">[88]</ref>, iLIDS <ref type="bibr" target="#b82">[83]</ref>, PRID 450S <ref type="bibr" target="#b89">[90]</ref>, CUHK03 <ref type="bibr" target="#b15">[16]</ref>, and Market-1501 <ref type="bibr" target="#b43">[44]</ref>. We broadly classify the current methods into two types, i.e., hand crafted and deeply learned. For each dataset, representative methods that report the highest re-ID accuracy in the corresponding year are shown. From these results, three major insights can be drawn.</p><p>First, a clear trend of performance improvement can be observed from the six datasets over the years. On VIPeR, CUHK01, i-LIDS, PRID 450S, CUHK03, and Market-1501, we observe a performance increase of +51.9%, +56.7%, +35.0%, +42.6%, +57.2%, and +31.62%, respectively. For example, on the most studied dataset VIPeR <ref type="bibr" target="#b81">[82]</ref>, from the year 2008 to 2016, representative works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref> witness a rank-1 accuracy from 12.0% in 2008 <ref type="bibr" target="#b23">[24]</ref> to 63.9% in 2015 <ref type="bibr" target="#b93">[94]</ref>, an improvement of +51.9%. For the Market-1501 dataset, since its release in 2015, the state-of-the-art results have increased from 44.42% <ref type="bibr" target="#b43">[44]</ref> to 76.04% <ref type="bibr" target="#b71">[72]</ref>, an improvement of 31.62%.</p><p>Second, with the exception of VIPeR, deep learning methods yield a new state of the art on the remaining 5 datasets. On these 5 datasets (CUHK01, i-LIDS, PRID 450S, CUHK03, and Market-1501), the performance of deep learning is superior to hand-crafted systems. On CUHK03 and Market-1501, the two largest datasets so far, we observe overwhelming advantage for deep learning <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b75">[76]</ref> compared to the (also extensive) tests of hand-crafted methods. Since VIPeR is relatively small, the advantage of deep learning cannot be tested to the full; instead, a hand-crafted metric learning may be more advantageous in this setting. Considering the cases in image classification and object detection, it is highly possible that deeply learned systems will continue dominating the re-ID community over the next few years.</p><p>Third, we speculate that there is still much room for further improvement, especially when larger datasets are to be released. For example, on the Market-1501 dataset, while the best rank-1 accuracy is 65.88% without using multiple queries <ref type="bibr" target="#b71">[72]</ref>, mAP is quite low (39.55%). This indicates that although it is relative easy to find the first true match (rank-1 accuracy) among a pool of 6 cameras, it is not trivial to locate the hard positives and thus achieve a high recall (mAP). On the other hand, although we seem to be able to achieve 60% to 70% rank-1 accuracy on these datasets, we must keep in mind that these datasets receive a very small proportion of practical usage. In fact, apart from <ref type="bibr" target="#b43">[44]</ref>, it is also reported in [96] a 10-fold gallery size increase leads to a 10-fold decrease in rank-1 accuracy, resulting in a single-digit rank-1 score even for the best-performing methods. As a consequence, considering the low mAP (re-ID recall) and the small scale of current datasets, we are more than optimistic that important breakthroughs are to be expected in image-based re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO-BASED PERSON RE-ID</head><p>In literature, person re-ID is mostly explored with single images (single shot). In recent years, video-based re-ID has become popular due to the increased data richness which induces more research possibilities. It shares a similar formulation to image-based re-ID as Eq. 1. Video-based re-ID replaces images q and g with two sets of bounding boxes</p><formula xml:id="formula_3">{q i } nq i=1 and {g j } ng j=1</formula><p>, where n q and n g are the number of bounding boxes within each video sequence, respectively. As important as the bounding box features are, video-based methods pay additional attention to multi-shot matching schemes and the integration of temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hand-crafted Systems</head><p>The first two trials <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> in 2010 were both hand-crafted systems. They basically use color-based descriptors and optionally employ foreground segmentation to detect the pedestrian. They use similar image features to image-based re-ID methods, where the major difference is the matching function. As mentioned in Section 1.2, both methods commonly calculate the minimum Euclidean distance between two sets of bounding box features as the set similarity. In essence, such methods should be classified into "multi-shot" person re-ID, where the similarity between two sets of frames plays a critical role. This multi-shot matching strategy is adopted by later works <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>. In <ref type="bibr" target="#b85">[86]</ref>, multiple shots are used to train a descriminative boosting model based on a set of covariance features. In <ref type="bibr" target="#b98">[99]</ref>, the SURF local feature is used to detect and describe interest points within short video sequences that are in turn indexed in the KD-tree to speed up matching. In <ref type="bibr" target="#b10">[11]</ref>, a spatial-temporal graph is generated to identify spatial-temporal stable regions for foreground segmentation. The the local descriptions are then calculated using a clustering method over time to improve matching performance. Cong et al. <ref type="bibr" target="#b99">[100]</ref> employ the manifold geometric structures from video sequences to construct more compact spatial descriptors with color-based features. Karaman et al. <ref type="bibr" target="#b100">[101]</ref> propose using the conditional random field (CRF) to incorporate constraints in the spatial and temporal domains. In <ref type="bibr" target="#b101">[102]</ref>, colors and selected face images are used to build a model over frames that capture the characteristic appearance as well as its variations over time. <ref type="bibr">Karanam et al. [103]</ref> make use of multi-shots for a person and propose that the probe feature be presented as a linear combination of the same person in the gallery. Multiple shots of an identity can also be employed to enhance body part alignment. In <ref type="bibr" target="#b84">[85]</ref>, in the effort to look for precise part-to-part correspondence, Cheng et al. propose an iterative algorithm in which the fitting of the pictorial structure becomes more accurate after each iteration due to the improvement of part detectors. In <ref type="bibr" target="#b103">[104]</ref>, pedestrian poses are estimated and frames with the same pose are matched with higher confidence.</p><p>The above methods typically build appearance models based on multiple shots, and a recent trend is to incorporate temporal cues in the model. Wang et al. <ref type="bibr" target="#b104">[105]</ref> propose using spatial-temporal descriptors to re-identify pedestrians. Its features include HOG3D <ref type="bibr" target="#b105">[106]</ref> and the gait energy image (GEI) <ref type="bibr" target="#b106">[107]</ref>. By designing a flow energy profile (FEP), walking cycles are detected so that frames around the local minimum/maximum are used to extract motion features. Finally, reliable spatial-temporal features are selected and matched through a discriminative video ranking model. In <ref type="bibr" target="#b107">[108]</ref>, Liu et al. propose de-composing a video sequence into a series of units that represent body-actions corresponding to certain action primitives, from which Fisher vectors are extracted for the final representation of the person. Gao et al. <ref type="bibr" target="#b108">[109]</ref> make use of the periodicity property of pedestrians and divide the walking cycle into several segments which are described by temporally aligned pooling. In <ref type="bibr" target="#b109">[110]</ref>, a new spatial-temporal descriptor is proposed based on densely computed multi-directional gradients and discarding noisy motion occurring over a short period.</p><p>Distance metric learning is also important when matching videos. In <ref type="bibr" target="#b110">[111]</ref>, a set verification method is proposed in which a transfer ranking is employed to tell whether the query matches one of the images belonging to the same identity. In <ref type="bibr" target="#b88">[89]</ref>, the multi-shot extension of the proposed local match model minimizes the distance of the best-matched pairs and reduces the number of cross-view transformations. In <ref type="bibr" target="#b111">[112]</ref>, Zhu et al. propose simultaneously learning intraand inter-video distance metrics to make video representation more compact and to discriminate videos of different identities. You et al. <ref type="bibr" target="#b112">[113]</ref> propose the top-push distance learning method which optimizes the top-rank matching in video re-ID by selecting discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deeply-learned Systems</head><p>In video-based re-ID, the data volume is typically larger than image-based datasets, because each tracklet contains a number of frames <ref type="table" target="#tab_8">(Table 4</ref>).</p><p>A basic difference between video-based and image-based re-ID is that with multiple images for each matching unit (video sequence), either a multi-match strategy or a singlematch strategy after video pooling should be employed. The multi-match strategy is used in older works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, which induces higher computational cost and may be problematic on large datasets. On the other hand, pooling-based methods aggregates frame-level features into a global vector, which has better scalability. As a consequence, current video-based re-ID methods typically involve the pooling step. This step can be max/average pooling as <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b113">[114]</ref>, or learned by a fully connected layer <ref type="bibr" target="#b114">[115]</ref>. In Zheng et al.'s system <ref type="bibr" target="#b20">[21]</ref>, temporal information is not explicitly captured; instead, frames of an identity are viewed as its training samples to train a classification CNN model with softmax loss. Frame features are aggregated by max pooling which yield competitive accuracy on three datasets. These methods are proven to be effective, and yet there is plenty of space for improvement. With respect to this point, the re-ID community can borrow ideas from the community of action/event recognition. For example, Xu et al. <ref type="bibr" target="#b115">[116]</ref> propose aggregating the column features in the 5th convolutional layer of CaffeNet into   <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b121">[122]</ref>, <ref type="bibr" target="#b122">[123]</ref> for video-based re-ID.</p><p>Fisher vectors <ref type="bibr" target="#b79">[80]</ref> or VLAD <ref type="bibr" target="#b116">[117]</ref>, in direct CNN feature transfer. Fernando et al. <ref type="bibr" target="#b117">[118]</ref> propose a learning-to-rank model to capture how frame features evolve over time in a video, which yields video descriptors of video-wide temporal dynamics. Wang et al. <ref type="bibr" target="#b118">[119]</ref> embed a multi-level encoding layer into the CNN model and produce video descriptors of varying sequence lengths.</p><p>Another good practice consists of injecting temporal information in the final representation. In hand-crafted systems, Wang et al. <ref type="bibr" target="#b104">[105]</ref> and Liu et al. <ref type="bibr" target="#b107">[108]</ref> use pure spatial-temporal features on the iLIDS-VID and PRID-2011 datasets and report competitive accuracy. In <ref type="bibr" target="#b20">[21]</ref>, however, it is shown that the spatial-temporal features are not sufficiently discriminative on the MARS dataset, because many pedestrians share similar waling motion under the same camera, and because motion feature of the same person can be distinct in different cameras. The point made in <ref type="bibr" target="#b20">[21]</ref> is that appearance features are critical in a large-scale video re-ID system. That said, this survey calls for attention to the recent works of <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>, <ref type="bibr" target="#b119">[120]</ref>, in which appearance features (e.g., CNN, color and LBP) are used as the starting point to be fed into RNN networks to capture the time flow between frames. In <ref type="bibr" target="#b113">[114]</ref>, features are extracted from consecutive video frames through a CNN model, and then fed through a recurrent final layer, so that information flow between time-steps is allowed. The features are then combined using max or average pooling to yield an appearance feature for the video. All these structures are incorporated into a siamese network. A similar architecture is used in <ref type="bibr" target="#b119">[120]</ref>. Their difference is twofold. First, a particular type of RNN, the Gated Recurrent Unit (GRU) is used in <ref type="bibr" target="#b119">[120]</ref>. Second, an identification loss is adopted in <ref type="bibr" target="#b113">[114]</ref>, which is beneficial for loss convergence and performance improvement. While the two works <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b119">[120]</ref> employ the siamese network for loss computation, Yan et al. <ref type="bibr" target="#b114">[115]</ref> and Zheng et al. <ref type="bibr" target="#b20">[21]</ref> use the identification model which classifies each input video into their respective identities. In <ref type="bibr" target="#b114">[115]</ref>, hand-crafted low-level features such as color and LBP are fed into several LSTMs and the LSTM outputs are connected to a softmax layer. In action recognition, Wu et al. <ref type="bibr" target="#b120">[121]</ref> propose extracting both appearance and spatialtemporal features from a video and build a hybrid network to fuse the two types of features. In this survey, we note that perhaps the discriminative combination of appearance and spatial-temporal models is an effective solution in future video re-ID research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets and Evaluation</head><p>Several multi-shot re-ID datasets exist, e.g., ETH <ref type="bibr" target="#b121">[122]</ref>, 3DPES <ref type="bibr" target="#b122">[123]</ref>, PRID-2011 <ref type="bibr" target="#b85">[86]</ref>, iLIDS-VID <ref type="bibr" target="#b104">[105]</ref>, and MARS <ref type="bibr" target="#b20">[21]</ref>. Some statistics of these datasets are summarized in <ref type="table" target="#tab_8">Table 4</ref>. The ETH dataset uses a single moving camera. It contains three sequences and multiple images from each sequence are provided. This dataset is relatively easy and the re-ID accuracy of the multi-shot scenario is nearly 100% <ref type="bibr" target="#b123">[124]</ref>. The 3DPeS dataset is collected with 8 non-overlapping outdoor cameras. Although the videos are released, this dataset is typically used for single-shot re-ID. PRID-2011 and iLIDS-VID are similar in that both datasets were captured by 2 cameras and each identity has 2 video sequences. iLIDS-VID has 300 identities captured under indoor scenes. PRID-2011 has 385 and 749 identities for each outdoor camera, respectively, and in this dataset 200 identities are observed in both cameras. During testing, 178 identities are used for PRID-2011 following the proposal by <ref type="bibr" target="#b104">[105]</ref>. It is generally believed that iLIDS-VID is more challenging than PRID-2011 due to extremely heavy occlusion. The MARS dataset <ref type="bibr" target="#b20">[21]</ref> was recently released which is a large-scale video re-ID dataset containing 1,261 identities in over 20,000 video sequences. It is produced using the DPM detector <ref type="bibr" target="#b90">[91]</ref> and the GMMCP tracker <ref type="bibr" target="#b124">[125]</ref>. Due to its recent release, we have not provided an extensive summary of results for the MARS dataset. <ref type="figure" target="#fig_2">Figure 4</ref> presents the evaluation of the stateof-the-art results on three representative video (multi-shot) re-ID datasets, i.e., ETHZ, iLIDS-VID, PRID-2011. Two major conclusions are drawn: First, the ETHZ dataset has reached its performance saturation. In 2015, Lisanti et al. <ref type="bibr" target="#b123">[124]</ref> and Martinel et al. <ref type="bibr" target="#b125">[126]</ref> report rank-1 accuracies approximating 100%. In <ref type="bibr" target="#b123">[124]</ref>, using 5 images per sequence, the rank-1 accuracy of ETHZ sequence 1, 2, and 3 is 99.8%, 99.7%, and 99.9%, respectively. Results with 10 frames per sequence is higher, achieving 100% <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b125">[126]</ref>. The primary reason is that the ETHZ dataset has relatively fewer identities, and the image variance is low due to the use of only one moving camera. This may be the first re-ID dataset to almost accomplish its initial objectives.</p><p>Second, active video re-ID research is still being conducted on the iLIDS-VID and PRID-2011 datasets. Since their introduction, we observe clear improvement of their rank-1 accuracy (including the ETHZ dataset). For iLIDS-VID, Wang et al. <ref type="bibr" target="#b104">[105]</ref> report a rank-1 accuracy of 23.3%, and an absolute improvement of 34.7% can be seen when compared to McLaughlin et al. <ref type="bibr" target="#b113">[114]</ref>. On PRID-2011, Wang et al. <ref type="bibr" target="#b104">[105]</ref> report a rank-1 accuracy = 19.0%, and two years later, Zheng et al. <ref type="bibr" target="#b20">[21]</ref> improve this score by 58.3% using the max pooling of CNN features fine-tuned on the MARS dataset.</p><p>Third, deep learning methods are producing overwhelmingly superior accuracy in video-based re-ID. On both the iLIDS-VID and PRID-2011 datasets, the best performing methods are based on the convolutional neural network with optional insertion of a recurrent neural network <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b113">[114]</ref>. Compared to image-based re-ID, the amount of training data is clearly larger in video re-ID. MARS provides over 500k training frames, compared to 13k in the Market-1501 dataset <ref type="bibr" target="#b43">[44]</ref>, from which MARS was extended. With these training data, it is feasible to train discriminative networks not only for video-based re-ID, but also for image-based datasets. We also note that, while the rank-1 accuracy on the MARS dataset reaches 68.3%, its mAP is still relatively low (49.3%), and when evaluating the performance of each camera pair, performance is further lowered. As a consequence, we believe that the research of video re-ID still has good potential for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FUTURE: DETECTION, TRACKING AND PERSON RE-ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Previous Works</head><p>Although person re-ID originates from multi-camera tracking, it is now studied as an independent research topic. In this survey, we view re-ID as an important future direction that will join pedestrian detection and tracking as a scenario, but in a more independent role. Specifically, we consider an end-to-end re-ID system 2 that takes raw videos as input and integrates pedestrian detection and tracking, along with re-identification.</p><p>Until recently, most re-ID works are based on two assumptions: first, that the gallery of pedestrian bounding boxes is given; second, that the bounding boxes are handdrawn, i.e., with perfect detection quality. However, in practice, these two assumptions do not hold. On the one hand, the gallery size varies with the detector threshold. A lower threshold produces more bounding boxes (a larger gallery, higher recall, and lower precision), and vice versa. When the detection recall/precision undergoes changes due to different thresholds, re-ID accuracy does not remain stable. On the other hand, when pedestrian detectors are used, detection errors typically exist with the bounding boxes, such as misalignment, miss-detection, and false alarms. Moreover, when pedestrian trackers are used, tracking errors may lead to outlier frames within a tracklet, i.e., background or pedestrians with different identities. So the quality of pedestrian detection and tracking may have direct influence 2. Here, "end-to-end" means spotting a query person from raw videos. on re-ID accuracy, which has been rarely discussed in the re-ID community. In the following, we will review the several works devoted to this direction.</p><p>In initial attempts to address the second problem, several datasets, i.e., CUHK03 <ref type="bibr" target="#b15">[16]</ref>, Market-1501 <ref type="bibr" target="#b43">[44]</ref>, and MARS <ref type="bibr" target="#b20">[21]</ref>, were introduced. These datasets do not assume perfect detection/tracking outputs and are a step closer to practical applications. For example, Li et al. <ref type="bibr" target="#b15">[16]</ref> show that on CUHK03, re-ID accuracy using the detected bounding boxes is lower than that obtained with hand-drawn bounding boxes. Later works also report this observation <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b126">[127]</ref>. These findings are closely related to practical applications. On MARS, tracking errors <ref type="figure">(Fig. 8)</ref> as well as detection errors are presented, but it remains unknown how tracking errors will affect re-ID accuracy.</p><p>Despite the fact that the datasets make progress by introducing detection/tracking errors, they do not evaluate explicitly how detection/tracking affects re-ID, which provides critical insights into how to select detectors/trackers among the vast number of existing works in an end-to-end re-ID system. To our knowledge, the first work on end-to-end person re-ID was proposed by Xu et al. <ref type="bibr" target="#b17">[18]</ref> in 2014. They use the term "commonness" to describe how an image bounding box resembles a pedestrian, and the term "uniqueness" to indicate the similarity between the gallery bounding box and the query. Commonness and uniqueness are fused by their product in an exponential function. This method works by eliminating the impact of false background detections. Although Xu et al. <ref type="bibr" target="#b17">[18]</ref> considers the impact of detection on re-ID, its limitation is a lack of comprehensive benchmarking and consideration of the dynamic issue of the gallery.</p><p>In 2016, Xiao et al. <ref type="bibr" target="#b127">[128]</ref> and Zheng et al. <ref type="bibr" target="#b76">[77]</ref> simultaneously introduce an end-to-end re-ID system based on large-scale datasets. The two works take raw video frames and a query bounding box as input ( <ref type="figure" target="#fig_3">Fig. 5</ref>). One is required to first perform pedestrian detection on the raw frames, and the resulting bounding boxes form the re-ID gallery. Then, classic re-ID is leveraged. This process, called "person search" in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b127">[128]</ref>, is no longer restricted to re-ID ( <ref type="figure" target="#fig_3">Fig. 5(b)</ref>): it pays equal attention to the detection module ( <ref type="figure" target="#fig_3">Fig. 5(a)</ref>). A very important aspect of this pipeline is that a better pedestrian detector tends to produce higher re-ID accuracy, given the same set of re-ID feature. In <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b127">[128]</ref>, extensive baselines are implemented on the person re-identification in the wild (PRW), and the large-scale person search (LSPS) datasets, respectively and this argument usually holds. Another interesting topic is whether pedestrian detection helps person re-ID. In <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b76">[77]</ref>, detection confidence is integrated in the final re-ID scores. In <ref type="bibr" target="#b127">[128]</ref>, pedestrian detection and re-ID are jointly considered in a CNN model which resembles faster R-CNN <ref type="bibr" target="#b128">[129]</ref>, while in <ref type="bibr" target="#b76">[77]</ref>, the ID-discriminative embedding (IDE) is shown to be superior when fined-tuned on a CNN model pre-trained on the R-CNN model <ref type="bibr" target="#b129">[130]</ref> for pedestrian detection. These methods provide initial insights on how weakly labeled detection data helps improve re-ID accuracy.</p><p>Nevertheless, in the so-called "end-to-end" systems <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b127">[128]</ref>, pedestrian tracking is not mentioned nor have we known any existing works/datasets addressing the influence of tracking on re-ID. This work views it as an "ultimate" goal to integrate detection, tracking, and retrieval into one framework, and evaluate the impact of each module on the   <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b127">[128]</ref>, <ref type="bibr" target="#b132">[133]</ref> for end-to-end person re-identification (search).</p><p>overall re-ID performance. This survey therefore calls for large-scale datasets that provide bounding box annotations to be used for the three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Future Issues</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">System Performance Evaluation</head><p>A proper evaluation methodology is a critical and sometimes tricky topic. Generally there is no single "correct" protocol, especially for the under-explored end-to-end re-ID task.</p><p>An end-to-end re-ID system departs from most current re-ID studies in dynamic galleries based on the specific detector/tracker used and their parameters. Moreover, it also remains mostly unknown how to evaluate detection/tracking performance in the scenario of person re-ID. As a consequence, this survey raises questions of system evaluation on two aspects. First, it is critical to use effective evaluation metrics for pedestrian detection and tracking in re-ID. The evaluation protocol should be able to quantify and rank detector/tracker performance in a realistic and unbiased manner and informative of re-ID accuracy. Pedestrian detection, for example, mostly employs the log-average miss rate (MR) which is averaged over the precision range of [10 −2 , 10 0 ] FPPI (false positives per image). Some also use average precision (AP) following the routine in PASCAL VOC <ref type="bibr" target="#b133">[134]</ref>. Dollár et al. <ref type="bibr" target="#b134">[135]</ref> argue that using the miss rate against FPPI is preferred to precision recall curves in certain tasks such as automotive applications, since there may be an upper limit on the acceptable FPPI. As opposed to the automotive applications of pedestrian detection, person re-ID aims to find a person which does not necessarily care about the false positive rates. So essentially we can employ both the miss rate and average precision to evaluate pedestrian detection for person re-ID.</p><p>An important parameter in the AP/MR computation is the intersect over union (IoU) score. A detected bounding box is considered correct if its IoU score with the ground truth bounding box is larger than a threshold. Typically the threshold is set to 0.5, and yet Zhang et al. <ref type="bibr" target="#b135">[136]</ref> study the difference between a "perfect single frame detector" and an automatic detector under various IoU scores. The KITTI benchmark <ref type="bibr" target="#b136">[137]</ref> requires an IoU of 0.7 for car detection, but 0.5 for pedestrians. For person re-identification, this problem is open to proposals. Some clues about it still exist and if we dive closer to the conclusions drawn in <ref type="bibr" target="#b76">[77]</ref>, we should be aware of the observation that using a larger IoU score (e.g., 0.7) is a better evaluation criteria than a low IoU (e.g., 0.5). <ref type="figure">Figure 6</ref> presents the relationship between detection accuracy (AP) and re-ID accuracy (rank-1 or mAP) on the PRW dataset. A linear relation is clearly presented between the two tasks under IoU = 0.7, while a scattered plot exists under IoU = 0.5. The correlation between detectors and recognizors is therefore more consistent with a larger IoU. Nevertheless, it is still far from satisfactory.</p><p>Given the consideration that bounding box localization quality is important for re-ID accuracy, it is a good idea to study IoU thresholds when assessing detector quality and see if it accords with re-ID accuracy. Our intuition is that a larger IoU criteria enforces better localization results, but there has to be some limit, because the difference in detector performance tends to diminish when IoU gets larger <ref type="bibr" target="#b135">[136]</ref>. It would also feasible to explore the usage of the average recall (AR) proposed in <ref type="bibr" target="#b137">[138]</ref> for IoU from 0.5 to 1 and plot the AR for a varying number of detector thresholds. Such an evaluation metric considers both recall and localization, and we speculate that it may be especially informative in re-ID where pedestrian detection recall and bounding box quality are of vital importance.</p><p>While there are at least some clues to guide the evaluation of pedestrian detection, how to evaluate tracking under person re-ID is largely unknown. In the multiple object tracking (MOT) benchmark <ref type="bibr" target="#b138">[139]</ref>, multiple evaluation metrics are used, including multiple object tracking precision (MOTP) <ref type="bibr" target="#b139">[140]</ref>, mostly track (MT) targets (percentage of ground truth persons whose trajectories are covered by the tracking results for at least 80%), the total number of false positives (FP), the total number of ID switches (IDS), the total number of times a trajectory is fragmented (Frag), the number of frames processed per second (Hz), etc. It might be possible that some of the metrics are of limited indication ability such as the processing speed, because tracking is an off-line step. For re-ID, we envision that tracking precision is critical as it is undesirable to have outlier images in the tracklets which compromise the effectiveness of pooling. We also speculate that 80% might not be an optimal threshold for evaluating MT under re-ID. As suggested by <ref type="bibr" target="#b104">[105]</ref>, extracting features within a walking cycle is a good practise, so generating long tracking sequences may not bring much re-ID improvement. In the future, once datasets are released to evaluate tracking and re-ID, an urgent problem is thus to design proper metrics to evaluate different trackers.</p><p>The second question w.r.t the evaluation procedure concerns the re-ID accuracy of the entire system. In contrast to traditional re-ID in which the gallery is fixed, in an end-to-end re-ID system, the gallery varies with the detection/tracking threshold. A stricter threshold indicates higher detection/tracking confidence, so the gallery is smaller and background detections are fewer and vice versa. Furthermore, the gallery size has a direct impact on re-ID accuracy. Let us take an extreme case as an example. When the detection/tracking threshold is very strict, the gallery can be very small, and it is even possible that the ground truth matches are excluded. At the other extreme, when the detection/tracking threshold is set to a very loose value, the gallery would be very large and contain a number of background detections which may exert a negative effect on re-ID, as shown in <ref type="bibr" target="#b43">[44]</ref>. Therefore, it is predictable that too strict or too loose a threshold leads to inferior galleries, and it is preferred that the re-ID evaluation protocol reflect how the re-ID accuracy changes with the gallery dynamics. In <ref type="bibr" target="#b76">[77]</ref>, Zheng et al. plot rank-1 accuracy and mAP against a different number of detections per image. It is observed that the curves first rise and then drop after they peak. In the PRW dataset, the peak is positioned at 4-5 detections per images, which can serve as an estimation of the average number of pedestrians per image. In <ref type="bibr" target="#b127">[128]</ref>, a similar protocol is employed, i.e., the rank-1 matching rate is plotted against detection recall, and reaches its maximum value when recall = 70%. When recall further increases, the prevalence of false detections will compromise the re-ID accuracy. Some other ideas could be explored, e.g., plotting re-ID accuracy against FPPI. Keeping in mind that the gallery size depends on the detector threshold, other new evaluation metrics that are informative and unbiased could be designed in the future.</p><p>We also point out another re-ID evaluation protocol in end-to-end systems. In practice, when being presented with a query bounding box/video sequence, while it is good to locate the identity in a certain frame and tell its coordinates  <ref type="figure">Fig. 6</ref>: Person re-ID accuracy (mAP and rank-1) versus pedestrian detection accuracy (AP) on the PRW dataset <ref type="bibr" target="#b76">[77]</ref>. Three re-ID methods are evaluated, i.e., BoW <ref type="bibr" target="#b43">[44]</ref>, LOMO + XQDA <ref type="bibr" target="#b19">[20]</ref>, and CNN <ref type="bibr" target="#b76">[77]</ref>. 9 detectors are evaluated, i.e., 1) DPM <ref type="bibr" target="#b90">[91]</ref> + RCNN (AlexNet), 2) DPM pre-trained on INRIA <ref type="bibr" target="#b130">[131]</ref>, 3) DPM re-trained on PRW, 4) ACF <ref type="bibr" target="#b91">[92]</ref> pre-trained on INRIA, 5) ACF + RCNN (AlexNet), 6) ACF + RCNN (ResidualNet), 7) ACF re-trained on PRW, 8) LDCF <ref type="bibr" target="#b131">[132]</ref> re-trained on PRW, and 9) LDCF pre-trained on INRIA. We can observe clearly the linear relation between re-ID and detection accuracy under IoU = 0.7 instead of IoU = 0.5. by pedestrian detection/tracking, it is also acceptable that the system only knows which frame(s) the identity re-appears in. The specific location of the query person can then be found by human labor which is efficient. In essence, determining the exact frame(s) where the queried person appears is a relatively easier task than a "detection/tracking+reidentification" task, because detection/tracking errors may not exert a large influence. In this scenario, re-ID accuracy should be higher than the standard re-ID task. Also, mean average precision can be used w.r.t the retrieved video frames. Since this task does not require locating persons very precisely, we can thus use relaxed detectors/proposals or trackers aiming at improving frame-level recall. Detectors/proposals can be learned to locate a rough region of pedestrians with a loose IoU restriction, and put more emphasis on matching, i.e., finding a particular person from a larger bounding box/spatial-temporal tube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The Influence of Detector/Tracker on Re-ID</head><p>Person re-ID originates from pedestrian tracking <ref type="bibr" target="#b8">[9]</ref>, in which tracklets from multiple cameras are associated if they are determined to be of the same identity. This line of research treats re-ID as a part of the tracking system, and does not evaluate the impact of localization/tracking accuracy on re-ID accuracy. However, even since the independence of re-ID, most studies have been conducted on hand-drawn image bounding boxes which is an idealized situation that hardly meets reality. Therefore, in an end-to-end re-ID system, it is critical that the impact of detection/tracking on re-ID be understood and that methods be proposed that detection/tracking methods/data can help re-ID.</p><p>First, pedestrian/tracking errors do affect re-ID accuracy, but the intrinsic mechanism and feasible solutions are still open to challenge. Detection errors <ref type="figure" target="#fig_5">(Fig. 7</ref>) may lead to pedestrian misalignment, scale changes, part missing and most importantly, false positives and miss detections, which compromise the re-ID performance and pose new challenges for the community <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b95">[96]</ref>.</p><p>A few re-ID works explicitly take the detection/tracking errors into account. In <ref type="bibr" target="#b28">[29]</ref>, Zheng et al. propose fusing local-local and global-local matches to address partial re-ID problems with severe occlusions or missing parts. In <ref type="bibr" target="#b17">[18]</ref>, Xu et al. compute a "commonness" score by matching the GMM encoded descriptor with a prior distribution. The score can be used to eliminate false positives which do not contain or provide good localization of a human body. In a similar way, Zheng et al. <ref type="bibr" target="#b76">[77]</ref> propose integrating detector confidence (after square root) into the re-ID similarity score, according to which the gallery bounding boxes are ranked. These works address detection errors after they happen. Nevertheless, there is a possibility that detection/tracking errors could be avoided at an earlier stage. For example, in the network designed by Xiao et al. <ref type="bibr" target="#b127">[128]</ref>, a localization loss is added in the fast R-CNN <ref type="bibr" target="#b140">[141]</ref> sub-module. It regulates localization quality which is critical for an effective re-ID system.</p><p>Future investigations are in need to reveal the dependence of person re-ID on detection/tracking quality. Since the idea to develop detector/trackers that are error-free is too idealistic, we advocate research into how detection confidence can be integrated into re-ID matching scores, i.e., how to correct errors by effectively identifying outliers, and how to train context models that do not rely solely on detected bounding boxes. For example, using clustering algorithms to filter out inconsistent frames within a tracklet can be effective in purifying tracking sequences. In another example, detected bounding boxes could be enlarged to include possibly missing body parts and learn discriminative visual features that in turn use or discard the enriched contextual information.</p><p>Secondly, we should be aware that detection and tracking, if appropriately designed, may be of help to re-ID. In <ref type="bibr" target="#b76">[77]</ref>, the IDE network fine-tuned on the R-CNN model <ref type="bibr" target="#b63">[64]</ref> is proved to be more effective than the one fine-tuned directly on an ImageNet pre-trained model. This illustrates the importance of using the excessive amount of labeled data in pedestrian detection, i.e., pedestrians with ID annotations and false positive detections. In <ref type="bibr" target="#b127">[128]</ref>, the end-to-end network integrates the loss of background detections, which is assumed to improve the discriminative ability of the learned embedding. The integration of detection scores into re-ID similarities <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b76">[77]</ref> can also be viewed as an alternative that detection helps <ref type="figure">Fig. 8</ref>: Tracking errors/artifacts in the MARS dataset <ref type="bibr" target="#b20">[21]</ref>. Each row represents a tracklet generated by the DPM detection + GMMCP tracker <ref type="bibr" target="#b124">[125]</ref>. First row: detection error and tracking error; second row: detection error; third row: occlusions in tracking; last row: tracking error.</p><p>re-ID.</p><p>It may seem not quite straightforward that pedestrian detection/tracking could help re-ID or the reverse, but if we consider the analogy of generic image classification and fine-grained classification, we may think of some clues. From example, fine-tuning the ImageNet pre-trained CNN model on the fine-grained datasets is an effective way for faster convergence and higher fine-grained recognition accuracy. It is also a good idea to jointly train a pedestrian detection and re-ID model by back-propagating the re-ID loss to the (fast) RCNN part. Being able to discriminate different identities may be beneficial to the task of discriminating pedestrians from the background. The latter could also be helpful to the former.</p><p>One of the ideas that can be explored is the use of unsupervised tracking data. In videos, tracking a pedestrian is not too difficult a task, though tracking errors are inevitable. Facial recognition, color, and non-background information are useful tools to improve tracking performance like in Harry Potter's Marauder's Map <ref type="bibr" target="#b141">[142]</ref>. Within a tracking sequence, the appearance of a person may undergo variances to some extent, but it can be expected that most of the bounding boxes are of the same person. In this scenario, each tracklet represents a person which contains a number of noisy but roughly usable training samples. We can therefore make use of racking results to train pedestrian verification/identification models, so as to alleviate the reliance on large-scale supervised data. As another promising idea, it is worth trying to pre-train CNN models using the detection/tracking data using the auto-encoder <ref type="bibr" target="#b142">[143]</ref> or the generative adversarial nets (GAN) <ref type="bibr" target="#b143">[144]</ref>. It would also be interesting to directly learn person descriptors using such unsupervised networks to help address the data issue in person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FUTURE: PERSON RE-ID IN VERY LARGE GAL-LERIES</head><p>The scale of data has increased significantly in the re-ID community in recent years, e.g., from several hundred gallery images in VIPeR <ref type="bibr" target="#b81">[82]</ref> and iLIDS <ref type="bibr" target="#b82">[83]</ref> to over 100k as in PRW <ref type="bibr" target="#b76">[77]</ref> and LSPS <ref type="bibr" target="#b127">[128]</ref>, which gives rise to the predominance of deep learning methods. However, it is apparent that current datasets are still far from a practical scale. Supposing that in a region-scale surveillance network with 100 cameras, if one video frame is used per second for pedestrian detection, and an average of 10 bounding boxes are produced from each frame, then, running the system for 12 hours will produce 3, 600 × 12 × 1 × 10 × 100 = 43.2 × 10 6 bounding boxes. But to our knowledge, previously no work has reported re-ID performance in such a large gallery. It seems that the largest gallery used in the literature is 500k <ref type="bibr" target="#b43">[44]</ref>, and evidence suggests that mAP drops over 7% compared to Market-1501 with a 19k gallery. Moreover, in <ref type="bibr" target="#b43">[44]</ref>, approximate nearest neighbor search <ref type="bibr" target="#b144">[145]</ref> is employed for fast retrieval but at the cost of compromised accuracy.</p><p>From both a research and an application perspective, person re-ID in very large galleries should be a critical direction in the future. Attempts to improve both the accuracy and efficiency issues should be made.</p><p>On the one hand, robust and large-scale learning of descriptors and distance metrics is much more important. This coincides with current research <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b80">[81]</ref>. Following large-scale image recognition <ref type="bibr" target="#b77">[78]</ref>, person re-ID will progress to large-scale evaluations. Although current methods address the re-ID problem between one or several pairs of cameras in a very limited time window, robustness in a camera networks over a long time period has not been well considered. In <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b145">[146]</ref>, the re-ID consistency within a camera network is jointly optimized with pairwise matching accuracy, but the testing datasets (WARD <ref type="bibr" target="#b86">[87]</ref> and RAiD <ref type="bibr" target="#b35">[36]</ref> ) only have 3 and 4 cameras and less than 100 identities. In a network with n cameras, the number of camera pairs is O(n 2 ). Considering the long recording time and lack of annotated data, it is typically prohibitive to train distance metrics or CNN descriptors in a pair-wise manner. As a consequence, training a global re-ID model with adaptation to various illumination condition and camera location is a priority. Toward this goal, an option is to design unsupervised descriptors <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b96">[97]</ref> which aim to find visually similar persons and treat visually dissimilar ones as false matches. But unsupervised methods may be prone to lighting changes.</p><p>On the other hand, efficiency is another important issue in such a large-scale setting. Although computation time could almost be omitted in small datasets <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, in our experiment using MATLAB 2014 on a server with 3.1GHz Intel Xeon E5-2687w v3 (10 cores), 64GB memory, it takes 8.50s to compute the distance between a 100-dim floating vector with a number of 10 million 100-dim vectors. If we use a 4,096-dim floating-point vector extracted from the CaffeNet <ref type="bibr" target="#b13">[14]</ref> and C++ programming, the time used increases dramatically to 60.7s including 33.2s for the distance calculation and 26.8s for the data to load from the disk. It is clear that the query time increases dramatically according to the feature dimensions and gallery size, which is not desirable for practical use. To our knowledge, previous works in person re-ID rarely focus on efficiency issues, and therefore effective solutions are lacking, but fortunately, we can resort to the image retrieval community for answers, and this survey provides two possible directions.</p><p>Inverted index-based. The inverted index is a de facto data structure in the Bag-of-Words (BoW) based retrieval methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b146">[147]</ref>, <ref type="bibr" target="#b147">[148]</ref>. Based on the quantization result of local descriptors, the inverted index has k entries where k denotes the codebook size. The indexing structure thus has k entries and each is attached to an inverted list, in which the local descriptors are indexed. The structure of the baseline inverted index is shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. A posting stores the image ID and the term frequency (TF) of the indexed descriptor and in a series of works, a number of other meta data can be stored, such as binary signature <ref type="bibr" target="#b147">[148]</ref>, feature coordinates <ref type="bibr" target="#b148">[149]</ref>, etc. For basic knowledge and state-of-the-art advances of the inverted index in instance retrieval, we refer readers to a recent survey <ref type="bibr" target="#b18">[19]</ref>.</p><p>In person re-ID, the use of local descriptors is popular <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The color and texture features are typically extracted from local patches. While some previous works use sophisticated matching algorithms <ref type="bibr" target="#b29">[30]</ref>, it is preferred that the matching procedure can be accelerated using the inverted index under a large gallery. A codebook is usually needed to quantize a local descriptor to visual words, and since the local descriptors are high-dimensional, a large codebook is needed to reduce quantization error. Under these circumstances, the inverted index is ready for use which saves memory costs to a large extent and, if properly employed, can have approximately the same accuracy compared to quantizationfree cases.</p><p>Hashing-based. Hashing has been an extensively studied solution to approximate nearest neighbor search, which aims to reduce the cost of finding exact nearest neighbors when the gallery is large or distance computation is costly <ref type="bibr" target="#b22">[23]</ref>. Learning to hash is popular in the community following the milestone work Spectral Hashing <ref type="bibr" target="#b149">[150]</ref>. It refers to learning hash functions, y = h(x), mapping a vector x to a compact y, and aims at finding the true nearest neighbor at high-ranks in the rank list while keeping the efficiency of the search process. Some classic learning to hash methods include product quantization (PQ) <ref type="bibr" target="#b116">[117]</ref>, iterative quantization (ITQ) <ref type="bibr" target="#b150">[151]</ref>, etc. Both methods are efficient in training and have fair retrieval accuracy. They do not require labeled data, so are applicable for re-ID tasks when large amount of training data may not be available.</p><p>Another application of supervised hashing consists of image retrieval <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b152">[153]</ref>, <ref type="bibr" target="#b153">[154]</ref>, <ref type="bibr" target="#b154">[155]</ref>, which is the interest of this section. The hash function is learned end-to-end through a deep learning network which outputs a binary vector given an input image. This line of works focus on several image classification datasets such as CIFAR-10 <ref type="bibr" target="#b155">[156]</ref> and NUS-WIDE <ref type="bibr" target="#b156">[157]</ref>, in order to leverage the training data that is lacking in generic instance retrieval datasets <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b147">[148]</ref>. In person re-ID, the application scenario fits well with deep hashing for image retrieval. In large galleries, efficient yet accurate hash methods are greatly needed, which is a less-explored direction in re-ID. As shown in <ref type="table" target="#tab_2">Table 1</ref>, training classes are available in re-ID datasets, and the testing procedure is a standard retrieval task, so the current arts in supervised hashing are readily to be adopted in re-ID in the light of the increasing size of the datasets <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The only relevant work we find is <ref type="bibr" target="#b157">[158]</ref> which learns hash functions in a triplet-loss CNN network with regularizations to enforce adjacency consistency. This method is tested on the CUHK03 dataset which contains 100 identities in each test split, so in this sense, performance evaluation on very large galleries is still lacking. As a consequence, this survey calls for very large re-ID datasets that will evaluate the scalability of re-ID methods and scalable algorithms especially those using hash codes to further push this task to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OTHER IMPORTANT YET UNDER-DEVELOPED OPEN ISSUES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Battle Against Data Volumn</head><p>Annotating large-scale datasets has always been a focus in the vision community. This problem is even more challenging in person re-ID, because apart from drawing a bounding box of a pedestrian, one has to assign him an ID. ID assignment is not trivial since a pedestrian may re-enter the fields of view (FOV) or enter another observation camera a long time after the pedestrian's first appearance. This makes collaborative annotation difficult, as it is costly for two collaborators to communicate on the annotated IDs. These difficulties partially explain why current datasets typically have a very limited number of images for each ID. The last two years have witnessed the release of several largescale datasets, e.g., Market-1501 <ref type="bibr" target="#b43">[44]</ref>, PRW <ref type="bibr" target="#b76">[77]</ref>, LSPS <ref type="bibr" target="#b127">[128]</ref>, and MARS <ref type="bibr" target="#b20">[21]</ref>, but they are still far from satisfaction in views of practical applications. In this survey, we believe two alternative strategies can help bypass the data issue.</p><p>First, how to use annotations from tracking and detection datasets remains under-explored. Compared to re-ID, tracking and detection annotations do not require ID assignment when a person re-enters FOV: the majority of effort has been spent on bounding box drawing. In <ref type="bibr" target="#b76">[77]</ref>, it is shown that adding more pedestrian and background training data in the R-CNN stage benefits the following training of the IDE descriptor. In <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b74">[75]</ref>, attribute annotations from independent datasets are employed to represent the re-ID images. Since the attributes can be annotated through collaboration among workers and have good generalization ability, they are also good alternatives to the deficiency of re-ID data. As a consequence, external resources are valuable for training re-ID systems when training data is lacking.</p><p>Apart from the pre-training/unsupervised strategies as mentioned in Section 4.2.2, a novel solution is to retrieve hard negatives from the unlabeled data which can be viewed as "true positives" in metric learning/CNN training. This strategy has been evaluated in object classification where a small portion of labels are disturbed before training <ref type="bibr" target="#b158">[159]</ref>. It can efficiently enlarge the training set, and at the same time reduce the risk of model over-fitting. Our preliminary experiments show that this direction yields decent improvement over the baselines. initial rank list <ref type="figure">Fig. 10</ref>: An example of re-ranking in re-ID. Given a query image, an initial rank list is obtained, in which an easy match (a) is ranked top, while two hard matches (b) and (c) have low ranks. The detection error in (b) can be corrected by retrieving the corresponding video frame and performing a finer search for the best bounding box within a local neighborhood. In this example, (c) is visually similar to (b) but not the query, so after (b) is retrieved, (c) can be found by similarity propagation.</p><p>The second strategy is transfer learning that transfers a trained model from the source to the target domain. Previously, supervised learning require large numbers of labeled data which limits the re-ID system to scale to other cameras. In <ref type="bibr" target="#b159">[160]</ref>, an unsupervised topic model is proposed to discover saliant image patches for re-ID matching and simultaneously remove background clusters. In <ref type="bibr" target="#b160">[161]</ref>, a weakly supervised method is proposed which requires full annotations from other re-ID dataset and a few samples captured in the target scenario. In <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref>, unsupervised transfer learning is proposed in which the target dataset is unlabeled. Ma et al. <ref type="bibr" target="#b161">[162]</ref> employ a cross-domain ranking SVM, while Peng et al. <ref type="bibr" target="#b162">[163]</ref> formulate the transfer problem as a dictionary learning task, which learns the shared invariant latent variables and is biased towards the target dataset. These methods indicate that it is feasible to learn a fair re-ID model from the source, and that it is beneficial to mine discriminative cues from the unsupervised data. Transfering CNN models to other re-ID datasets can be more difficult because the deep model provides a good fit to the source. Xiao et al. <ref type="bibr" target="#b75">[76]</ref> gather a number of source re-ID datasets and jointly train a recognition model for the target dataset. According to our experience, the usage of off-the-shelf metric learning methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b54">[55]</ref> can also correct the transfer effect to some extent, but unsupervised transfer learning is still an open issue for the deeply learned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Re-ranking Re-ID Results</head><p>The re-identification process ( <ref type="figure" target="#fig_3">Fig. 5(b)</ref>) can be viewed as a retrieval task, in which re-ranking is an important step to improve the retrieval accuracy. It refers to the re-ordering of the initial ranking result from which re-ranking knowledge can be discovered. For a detailed survey of search re-ranking methods, we refer the readers to <ref type="bibr" target="#b163">[164]</ref>.</p><p>A few previous works exist on this topic. Re-ranking can be performed either with human in the loop or fully automatically. When online human labeling is involved, Liu et al. <ref type="bibr" target="#b164">[165]</ref> propose the post-rank optimisation (POP) method which allows a user to provide an easy negative and, optionally, a few hard negatives from the initial rank list. The sparse human feedback enables on-the-fly automatic discriminative feature selection of the query person. In an improvement, Wang et al. <ref type="bibr" target="#b95">[96]</ref> design the human verification incremental learning (HVIL) model which does not require any pre-labelled training data and learns cumulatively from human feedback to provide instance model update. A number of incrementally learned HVIL models are combined into a single ensemble model for use when human feedback is no longer available. In a similary nature, Martinel et al. <ref type="bibr" target="#b165">[166]</ref> propose finding the most relevant gallery images for a query, sending them to the human labeler, and finally using the labels to update the re-ID model. Automatic reranking methods have also been studied in several works. Zheng et al. <ref type="bibr" target="#b166">[167]</ref> propose a query-adaptive fusion method to combine rank results of several re-ID systems. Specifically, the shape of the initial score curves is used and it is argued that the curve exhibits an "L" shape for a good feature. In <ref type="bibr" target="#b94">[95]</ref>, various metrics are ensembled based on the direct optimization of the CMC curve. García et al. <ref type="bibr" target="#b93">[94]</ref> analyze the unsupervised discriminant context information in the rank list. This is further combined with a re-ranking metric learned in the offline. Leng et al. <ref type="bibr" target="#b167">[168]</ref> use the idea of reciprocal knearest neighbors <ref type="bibr" target="#b168">[169]</ref> to refine the initial rank list based constructing images relations in the offline steps.</p><p>Re-ranking is still an open direction in person re-ID, while it has been extensively studied in instance retrieval. The application scenario can be depicted as follows. When searching for a person-of-interest, it is likely that its images under certain cameras are very difficult to find due to intensive image variations. But we may be able to find the true matches under some cameras which are more similar to the hard positives. So in this manner, hard positives can be found once the easy ones are returned. Re-ranking methods in instance retrieval can be readily adopted in person re-ID <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b169">[170]</ref>, <ref type="bibr" target="#b170">[171]</ref>. Since training data is available in re-ID <ref type="table" target="#tab_2">(Table 1)</ref>, it is possible to design re-ranking methods based on training distribution. For example, when doing k-NN re-ranking <ref type="bibr" target="#b169">[170]</ref>, the validity of the returned results can be determined from the training set according to the scores. Since re-ID is focused on pedestrians, re-ranking methods can be specifically designed. For example, after obtaining the initial rank list, a subset of the top-ranked images can be selected, and the video frames containing them can be retrieved. We can subsequently find the best localization through expensive sliding window method without incurring much computation burdens, so as to allieviate the impact of detector misalignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Open-World Person Re-ID</head><p>Most existing re-ID works can be viewed as an identification task (Eq. 1). Query identities are assumed to exist in the gallery and the tasks aim to determine the ID of the query.</p><p>By contrast, open-world re-ID systems study the person verification problem. That is, based on identification tasks, the open-world problem adds another condition to Eq. 1,</p><formula xml:id="formula_4">sim(q, g i * ) &gt; h,<label>(4)</label></formula><p>where h is the threshold above which we can assert that query q belongs to identity i * ; otherwise, q is determined an outlier identity which is not contained in the gallery, although i * is the first ranked identity in the identification process.</p><p>In literature, open-world person re-ID is still at its early stage, and several works are proposed to help define this task. In <ref type="bibr" target="#b171">[172]</ref>, Zheng et al. design a system consisting of a watch list (gallery) of several known identities and a number of probes including target and non-target ones. Their work aims to achieve high true target recognition (TTR) and low false target recognition (FTR) rate which calculate rate of the number of queries that are verified as the target identities to the total number of queries. In <ref type="bibr" target="#b172">[173]</ref>, Liao et al. divide open-world re-ID into two sub-tasks, i.e., detection and identification; the former decides whether a probe identity is present in the gallery and the latter assigns an ID to the accepted probe. Consequently two different evaluation metrics, the detection and identification rate (DIR) and the false accept rate (FAR) are proposed, based on which a receiver operating characteristic (ROC) curve can be drawn.</p><p>Open-world re-ID still remains a challenging task as evidenced by the low recognition rate under low false accept rate, as shown in <ref type="bibr" target="#b171">[172]</ref>, <ref type="bibr" target="#b172">[173]</ref>. The challenge mainly lies in two aspects i.e., detection and recognition, both of which are limited to the unsatisfying matching accuracya research focus in standard re-ID tasks. As indicated in <ref type="bibr" target="#b172">[173]</ref>, a 100% FAR corresponds to the standard close-set re-ID and its accuracy is limited by the current state of the art; a lower FAR is accompanied by lower re-ID accuracy due to the low recall of the true matches. As a consequence, from a technical perspective, the critical goal is to improve matching accuracy, based on which probabilistic models can be designed for novelty detection (verification) methods. Moreover, when focusing on re-ID accuracy, open-world re-ID should also consider the dynamics of the gallery <ref type="bibr" target="#b173">[174]</ref>. In a dynamic system with constantly incoming bounding boxes, a new identity will be added to the "watch list" if it is determined to not belong to any existing gallery identities, and vice versa. Enrolling new identities dynamically enables automatic database construction and facilitates the re-ID process with a pre-organized gallery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUDING REMARKS</head><p>Person re-identification, foretold in the oldest stories, is gaining extensive interest in the modern scientific community. In this paper, a survey of person re-identification is presented. First, a brief history of person re-ID is introduced and its similarities and differences to image classification and instance retrieval are described. Then, existing image and video-based methods are reviewed, which are categorized into hand-crafted and deeply-learned systems. Positioned inbetween image classification and instance retrieval, person re-ID has a long way from becoming an accurate and efficient application. Therefore, departing from previous surveys, this paper places more emphasis on the under-developed but critical future possibilities, such as the end-to-end re-ID systems that integrate pedestrian detection and tracking, and person re-ID in very large galleries, which we believe are necessary steps toward practical systems. We also highlight some important open issues that may attract further attention from the community. They include solving the data volume issue, re-ID re-ranking methods, and open re-ID systems. All in all, the integration of discriminative feature learning, detector/tracking optimization, and efficient data structures will lead to a successful person re-identification system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 1 :</head><label>21</label><figDesc>Milestones in the person re-ID history. Percentage of person re-ID papers on top conferences over the years. Numbers above the markers indicate the number of re-ID papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>et al. W. Li et al. R. Zhao et al. Z. Zhang et al. Z. Zhang et al. T. Xiao et al. &amp; H. Tao W. Schwartz et al. M. Farenzena et al. T. Xiao et al. W-S. Zheng et al. R. Layne et al. C. C. Loy et al. H. Wang et al. S. Bak et al. et al. J. Davis et al. K. Weinberger et al. M. Hirzer et al. Y. Yang et al. Y-C. Chen et al. S. Wu et al. et al. E. Ustinova et al. F. Xiong et al. S. Liao et al. (f) Market-1501 Person re-ID accuracy on (a) VIPeR [82] (b) iLIDS [83] (c) GRID [84] (d) CUHK01 [88] (e) CUHK03 [16] and (f) Market-1501 [44] over the years. Results from top venues using hand-crafted or deeply learned systems are presented.For CUHK03, we record results on the detected data, and for Market-1501, results using single queries are used. Since Market-1501 was released recently, results on this dataset are plotted according to their publication (or ArXiv) time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Video-based person re-ID accuracy on (a) ETH sequence 1<ref type="bibr" target="#b81">[82]</ref> (b) PRID-2011<ref type="bibr" target="#b85">[86]</ref> and (c) iLIDS-VID<ref type="bibr" target="#b104">[105]</ref> over the years. Results from top venues using hand-crafted or deeply learned systems are presented. For ETHZ, we report results obtained by 5 images per video sequence, and state-of-the-art results on SEQ. #1, SEQ. #2, and SEQ. #3 are drawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>An end-to-end person re-ID system that includes pedestrian detection and re-identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>%) of pedestrian detection mean Average Precision (%) of re−ID CNN descriptor (k) CNN, mAP, IoU = 0.5 %) of pedestrian detection mean Average Precision (%) of re−ID CNN descriptor (l) CNN, mAP, IoU = 0.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Detection errors in the Market-1501 dataset [44]. (a) misalignment and scale variances; (b) part missing; (c) false positives. In (a) and (b), the first and second rows represent DPM-detected and hand-drawn bounding boxes which have an IoU &gt; 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>An illustration of the inverted index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Y. Yang are with the Centre for Quantum Computation and Intelligent Systems, University of Technology at Sydney, NSW, Australia. E-mail: liangzheng06@gmail.com, yee.i.yang@gmail.com • A. Hauphtmann is with the School of Computer Science at Carnegie Mellon University, with a joint appointment in the Language Technologies Institute.</figDesc><table /><note>E-mail: alex@cs.cmu.edu</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Video/multi-shot re-ID Farenzena et al.Bazzani et al.    </figDesc><table><row><cell>Deep learning for re-ID</cell></row><row><cell>Yi et al.</cell></row><row><cell>Li et al.</cell></row><row><cell>Detection &amp; re-ID</cell></row><row><cell>Xu et al.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Comparing re-ID with classification and retrieval an additional cost function in the network, while<ref type="bibr" target="#b15">[16]</ref> uses a finer body partitioning. The experimental datasets do not overlap in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison of the identification and verification (siamese) models on the Market-1501 dataset (single query).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table /><note>Statistics of some commonly used datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Statistics of some currently available datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note>Datasets</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the pioneer researchers in person re-identification and other related fields.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Things and persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plantinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Metaphysics</title>
		<imprint>
			<biblScope unit="page" from="493" to="519" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The transformations of persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Rorty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">185</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sortals, natural kinds and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Cocchiarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Logique et analyse</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="439" to="474" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">People re-identification and tracking from multiple cameras: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cicirelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 19th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1601" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of approaches and trends in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bedagkar-Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="270" to="286" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Appearance descriptors for person re-identification: a comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Satta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.5748</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intelligent multi-camera video surveillance: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object identification in a bayesian context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1276" to="1282" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keeping track of humans: Have i seen this person before</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zajdel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zivkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 2005 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2081" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple-shot person re-identification by hpe signature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1413" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person search in a scene by jointly modeling people commonness and person uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="937" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sift meets cnn: A decade survey of instance retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01807</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A survey on learning to hash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00185</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="653" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3567" to="3574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2666" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4678" to="4686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3610" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification with correspondence structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3200" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Consistent re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical gaussianization for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1971" to="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1565" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local fisher discriminant analysis for pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boghossian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3318" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3550" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sample-specific svm learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning color names for real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1512" to="1523" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1363" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Person reidentification by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attributerestricted latent topic model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4204" to="4213" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Person re-identification: What features are important</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multitask learning with low rank attribute embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3739" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4184" to="4193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A richly annotated dataset for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07054</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Distance metric learning: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">78</biblScope>
		</imprint>
		<respStmt>
			<orgName>Michigan State Universiy</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Informationtheoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="780" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Large scale similarity learning using similar pairs for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Mullert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks for signal processing IX</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An ensemble color model for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="868" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Personnet: Person reidentification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04404</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01595</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">An enhanced deep feature representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Multi-camera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1988" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Re-identify people in wide area camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3594" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Mahalanobis distance learning for person re-identification,&quot; in Person Re-Identification, ser. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<editor>M. Cristani, S. Yan, and C. C. Loy</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="247" to="267" />
			<pubPlace>London, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Camera network based person re-identification by leveraging spatial-temporal constraint and multiple cameras relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="174" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Person re-identification ranking optimisation by discriminant context information analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machive Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vectors for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Person re-identification in multi-camera system by signature based on interest point descriptors collected on short video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamdoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Distributed Smart Cameras</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Video sequences association for people re-identification across multiple non-overlapping cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N T</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Douadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Identity inference: generalizing person re-identification scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Part-based spatio-temporal model for multi-person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bedagkar-Gala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1908" to="1915" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sparse re-id: Block sparsity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Improving person re-identification via pose-aware multi-shot matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-19th British Machine Vision Conference. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A spatiotemporal appearance representation for viceo-based pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3810" to="3818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Temporally aligned pooling representation for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4284" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A fast adaptive spatio-temporal 3d feature for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4294" to="4298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Transfer re-identification: From person to set-based verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2650" to="2657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Video-based person re-identification by simultaneously learning intra-video and intervideo distance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Top-push video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Person re-identification via recurrent feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Temporal pyramid pooling based convolutional neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01224</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01595</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Depth and appearance for mobile scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding</title>
		<meeting>the 2011 joint ACM workshop on Human gesture and behavior understanding</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Modiri</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4091" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Re-identification in the function space of feature warps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1656" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Sample-specific svm learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">What makes for effective detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Harry potter&apos;s marauder&apos;s map: Localizing and tracking multiple persons-of-interest by nonnegative discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3714" to="3720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Binary coding of speech spectrograms using a deep auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech. Citeseer</title>
		<imprint>
			<biblScope unit="page" from="1692" to="1695" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Query-driven iterated neighborhood graph search for large scale indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
		<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Network consistent data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Image retrieval with geometrypreserving visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Multilinear hyperplane hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5119" to="5127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Efficient training of very deep neural networks for supervised hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Nuswide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4766" to="4779" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Cross-scenario transfer person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Cross-domain person reidentification using domain adaptation ranking svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1599" to="1613" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Multimedia search reranking: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Pop: Person reidentification post-rank optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Temporal model adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Queryadaptive late fusion for image search and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Person reidentification with content and context re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6989" to="7014" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3013" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Towards open-world person re-identification by one-shot group-based verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="591" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.0872</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Modelling errors in a biometric reidentification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Decann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="209" to="219" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
