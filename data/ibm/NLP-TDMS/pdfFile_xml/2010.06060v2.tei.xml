<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BioMegatron: Larger Biomedical Domain Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoo-Chang</forename><surname>Shin</surname></persName>
							<email>hshin@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evelina</forename><surname>Bakhturina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Mani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA / Santa Clara</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BioMegatron: Larger Biomedical Domain Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of named entity recognition, relation extraction, and question answering. Model checkpoints and code are available at ngc.nvidia.com and github.com/ NVIDIA/NeMo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Effectively transferring the success of BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> to the biomedical domain, most notably  (BioBERT) and <ref type="bibr" target="#b1">Beltagy et al. (2019)</ref> (SciBERT) inspired a large number of similar works last year. For example, <ref type="bibr" target="#b16">Peng et al. (2019)</ref>; <ref type="bibr" target="#b0">Alsentzer et al. (2019)</ref>; <ref type="bibr" target="#b9">Huang et al. (2019)</ref> added clinical text to the PubMed biomedical pretraining corpus and tested on standard biomedical and clinical NLP benchmarks. Many other similar works appeared at the ACL BioNLP Workshop <ref type="bibr" target="#b4">(Demner-Fushman et al., 2019)</ref>.</p><p>More recently, <ref type="bibr">Gu et al. (2020)</ref> performed a comprehensive study on the pre-training corpus domain, language model masking method, and adversarial training, benchmarking on a number of different datasets for token classification, sequence classification, and sequence regression.</p><p>Compared to the previous works, we perform a more detailed study on (1) subword vocabulary, (2) labeling method, (2) model size, and (3) domain transfer, showing gains in token classification, sequence classification, and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>A prime example of Language Models (LMs) in the biomedical domain is BioBERT . It is a transformer LM pre-trained on the PubMed (www.ncbi.nlm.nih.gov/pubmed) biomedical text corpus comprised of biomedical literature abstracts. Their pre-training started from the checkpoint of <ref type="bibr" target="#b5">Devlin et al. (2018)</ref> trained on Wikipedia and Books-Corpus. Independently, <ref type="bibr" target="#b1">Beltagy et al. (2019)</ref> (SciBERT) pre-trained BERT from scratch using their vocabulary set on scientific text corpora, including PubMed abstracts and computer science papers. Both demonstrated increased performance over the previous non-BERT SOTA on biomedical benchmarks, including Named Entity Recognition (NER), Relation Extraction (RE), and Question Answering (QA). BioBERT and SciB-ERT report similar results on NER and RE, while only BioBERT report QA results.</p><p>They inspired other follow-up works <ref type="bibr" target="#b0">(Alsentzer et al., 2019;</ref><ref type="bibr" target="#b9">Huang et al., 2019;</ref><ref type="bibr" target="#b16">Peng et al., 2019)</ref>  <ref type="bibr" target="#b16">Peng et al. (2019)</ref> report slightly improved performance on RE using BERT Large while reporting worse results on NER, compared to BERT Base . These results on biomedical tasks do not benefit from scaling model size to the same degree as standard NLP benchmarks such as GLUE or SQuAD <ref type="bibr" target="#b22">(Shoeybi et al., 2019;</ref><ref type="bibr" target="#b18">Raffel et al., 2019)</ref>.</p><p>3 Language Model Pre-training BERT Base &amp; Large We compare our models to the pre-trained BERT Base &amp; Large models of BioBERT  and PubMedBERT <ref type="bibr">(Gu et al., 2020</ref>) (BERT Base ) for fine-tuning and evaluation. For QA we use the BERT Large variant of BioBERT following the authors' recommendation.</p><p>BioMegatron Megatron-LM <ref type="bibr" target="#b22">(Shoeybi et al., 2019)</ref> was introduced for efficient model parallel training of large LMs, with up to 8.3B parameters. <ref type="bibr" target="#b22">Shoeybi et al. (2019)</ref> showed that rearranging the order of the layer normalization and the residual connections is critical to enabling the scaling of the BERT-style models beyond 336m parameters, and we use the same architecture.</p><p>Megatron-LM also used a larger pre-training text corpus, comprised of Wikipedia <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, CC-Stories (Trinh and Le, 2018), Real-News <ref type="bibr" target="#b25">(Zellers et al., 2019)</ref>, and OpenWebtext <ref type="bibr" target="#b17">(Radford et al., 2019)</ref>. For our LM training, we use the 4.5 billion-word PubMed abstract set and the 1.6 billion-word CC0-licensed Commercial Use Collection of the PMC full-text corpus (www.ncbi.nlm.nih.gov/pmc).</p><p>We train three sizes of BioMegatron: with 345 million, 800 million, and 1.2 billion number of parameters <ref type="table" target="#tab_2">(Table 1)</ref>.</p><p>We compare four pre-training scenarios in the smallest 345m model -using BERT-cased/uncased vocabularies, each pre-trained from scratch and finetuned from general domain LM. We also compare two sets of domain vocabularies learned on PubMed text corpus using SentencePiece (github.com/google/sentencepiece) library, each containing 30k and 50k subword units.</p><p>We train the larger BioMegatron models with less variation: 800m models from scratch on PubMed with BERT -cased/-uncased vocabularies; and 1.2b model starting from general domain LM checkpoint using BERT-uncased vocabulary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Downstream Benchmark Tasks</head><p>We use the most widely used downstream biomedical benchmark datasets for NER, RE, and QA.</p><p>Named Entity Recognition The BC5CDR  NER dataset annotated disease and chemical terms with IOB tagging <ref type="bibr" target="#b20">(Ramshaw and Marcus, 1999)</ref>. In NCBI-disease <ref type="bibr" target="#b6">(Dogan et al., 2014)</ref>, only disease entities are IOB-tagged.</p><p>Relation Extraction The ChemProt <ref type="bibr" target="#b11">(Krallinger et al., 2015)</ref> dataset contains sentences from PubMed abstracts, where chemical-protein interaction types are annotated as five categories. Relation Extraction is essentially a sequence classification task, classifying a set of sentences into a category.</p><p>Question Answering The BioASQ-7b factoid task <ref type="bibr" target="#b24">(Tsatsaronis et al., 2015)</ref> is a biomedical QA dataset whose format is similar to the SQuAD dataset <ref type="bibr" target="#b19">(Rajpurkar et al., 2016)</ref>. In this task, context-snippet, question and answer triplets, and factoid question/answers are evaluated with strict accuracy (SAcc), lenient accuracy (LAcc), and mean reciprocal rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>The evaluation results on NER and RE are shown in <ref type="table" target="#tab_4">Table 2</ref>, and QA are shown in <ref type="table">Table 3</ref>. We perform entity-level F1 NER using the official CoNLL evaluation script translated into Python (github.com/ spyysalo/conlleval.py). RE uses micro-level F1, and QA uses the BioASQ evaluation script (github.com/BioASQ/Evaluation-Measures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Named Entity Recognition</head><p>While the NER benchmark datasets appear saturated due to the small sample size, we find that the subword vocabulary is the most critical factor. Examples of tokenization with different vocabularies are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Representing named entities as single terms is more helpful than breaking them into several subtokens.   <ref type="table">Table 3</ref>: Evaluation results on QA after fine-tuning for 30 epochs on checkpoints fine-tuned on SQuAD dataset with fixed hyper-parameter settings as num-fc-layers: 2; fc-hidden-size: 2048; fc-dropout: 0.1; max-seq-length: 512; learning-rate: 3e-5; cross-entropy loss, using Adam optimizer. BioMegatron models are pre-trained from scratch on PubMed, except 1.2b model which is fine-tuned from a general domain model checkpoint. break-out rate while being smaller in size than our 50k-size vocabulary. A lower break-out rate with smaller vocabulary size probably helps achieve bet-  ter NER performance despite smaller model size.</p><p>We can label the entities for NER training as: (1) marking the whole entity as a single label, and (2) labeling sub-tokens separately. <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of the labeling methods. We find these different schemes can result in as much as ∼2% difference in the F1-score on NER evaluation, possibly indicating that the datasets are too small. We report NER results by labeling sub-tokens separately, except for NCBI-disease dataset, which results in better whole-entity labeling across models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relation Extraction</head><p>Since RE is a classification task, albeit on sequences rather than on tokens, the choice of subword vocabulary has a notable effect.</p><p>We can also observe that larger models result in higher precision for lower recall, both for NER and RE. More hyper-parameter tuning could achieve higher F1-scores, even the generalization ability of such result may be questionable. <ref type="table">Table 3</ref> show evaluation results after fine-tuning on SQuAD for 10 epochs and BioASQ for 30 epochs each, following the recipe found to work best by . We found large batch size to be beneficial, as Q&amp;A pairs repeat up to 88 times. We use batch size of 64 per GPU with data parallelism on 16 GPUs. Using biomedical vocabularies result in much worse results, possibly due to its low relevance in the first SQuAD fine-tuning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Question Answering</head><p>Larger models tend to perform better in QA, though it levels off after 345m parameters. The larger model size effect is more evident when finetuning on BioASQ directly, as shown in <ref type="table" target="#tab_8">Table 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Domain Transfer and Generalization</head><p>We examine how well a general-or domain-specific LM generalizes across domains related to the model size. <ref type="bibr">Gu et al. (2020)</ref> studied the effect of "domain-specific" vs. "mixed-domain" pre-training, i.e., pre-training on PubMed from scratch vs. pretraining starting from a general domain LM (finetuning). They found that pre-training on PubMed from scratch is better for biomedical NLP benchmarks, but we analyze its effect with further pre-training (fine-tuning) steps. In other words, if starting from a general domain LM, does sufficient finetuning make it as good as a fully domain-specific model? Can such model have any advantage for cross-domain or cross-discipline generalization?</p><p>Benchmark Fine-tuning steps F1 NER BC5CDR-chem 10 3 steps 63.2 10 4 steps 74.3 10 5 steps 89.7 2 · 10 5 steps 89.37 3 · 10 5 steps 91.8 4 · 10 5 steps 92.1 5 · 10 5 steps 91.2 BC5CDR-disease 10 3 steps 39.4 10 4 steps 63.6 10 5 steps 79.8 2 · 10 5 steps 81.2 3 · 10 5 steps 79.2 4 · 10 5 steps 81.9 5 · 10 5 steps 81.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RE ChemProt</head><p>10 3 steps 0.00 10 4 steps 34.1 10 5 steps 63.4 2 · 10 5 steps 71.1 3 · 10 5 steps 70.4 4 · 10 5 steps 69.7 5 · 10 5 steps 68.3  <ref type="table" target="#tab_9">Table 6</ref> shows F1-score evaluation on NER and RE benchmarks using a general-domain BioMegatron-1.2b with additional fine tuning. It shows that even for a large LM that was pre-trained on a large text corpus, it needs sufficient further pretraining on domain text (PubMed). After sufficient pre-training on domain text, it can be as good as an LM pre-trained on domain-text only, except that vocabulary has more significant effect on NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>SAcc LAcc MRR Megatron-345m (general LM) 38.5 52.6 43.7 Megatron-1.2b (general LM) 29.3 39.7 32.7  <ref type="table" target="#tab_10">Table 7</ref> shows the results of general-domain LMs fine-tuned on BioASQ-7b-factoid. Larger models do not perform better, which may indicate overfitting is occuring on the small training set. <ref type="table" target="#tab_12">Table 8</ref> shows the generalization ability of   , Megatron-LM <ref type="bibr" target="#b22">(Shoeybi et al., 2019)</ref>.</p><p>BioMegatron models on SQuAD datasets. Here, a large biomedical LM pre-trained on large text corpus performs better than smaller general domain LMs such as BERT LARGE , even when pre-trained on the biomedical text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Other Domain-Specific Factors</head><p>Size and Bias in Biomedical Datasets Annotating biomedical data requires in-depth domain knowledge. Besides, data often have substantial label bias as the occurrences of "abnormal" or "findings" are rare by nature. As a result, biomedical benchmark data tend to be smaller and highly biased than their general domain counterparts.   <ref type="table" target="#tab_14">Table 9</ref> shows a comparison of benchmark datasets for NER, RE (CLS), and QA in the biomedical domain and their general-domain counterparts. The SQuAD Q&amp;A set is 15 times larger than the BioASQ data, where the same question-answer combinations appear up to 88 times in BioASQ.</p><p>Question-answer pairs are seldom repeated in SQuAD data, at most twice. The BC5CDR NER dataset is 1/3 size of CONLL-2003 and the ratio of I/O to O tags 0.08, compared to 0.18 for CONLL.</p><p>Methods to circumvent data imbalance issues such as oversampling the minority classes <ref type="bibr" target="#b2">(Chawla et al., 2002;</ref><ref type="bibr" target="#b3">Chen et al., 2010</ref>) and using weighted cross-entropy gave minor effects on our NER and RE benchmarks. Recently,  proposed dice-loss for data-imbalance issues in NLP, with SOTA results on NER and QA, which could be a future avenue to explore for domain LMs. Transfer learning showed effectiveness in the biomedical QA task. However, it is somewhat unclear how to apply it to NER and RE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PubMed Corpus #Words BioBERT abstracts 4.5 billion PubMedBERT abstracts + full-text 16.8 billion BioMegatron abstracts + full-text-CC 6.1 billion <ref type="table" target="#tab_2">Table 10</ref>: Pre-training text corpus of each biomedical LM. We pre-train on PubMed abstracts and full-text commercial-collection (CC) that are free of copyrights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Corpus and Duration</head><p>PubMed-BERT is pre-trained on a much larger text corpus, as shown in <ref type="table" target="#tab_2">Table 10</ref>. It is a performant domain-LM with a larger pre-training corpus and adequate domain vocabulary compared to its model size. We pre-train our LMs for about one epoch, reaching a masked-LM loss of about 1.2 <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>. Further pre-training may be helpful, but it is challenging to have strictly controlled experiments with many different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We review and test several factors that can affect the performance of domain language models. We find that a language model targeted for a domain and application performs best. For example, model size is a secondary factor to vocabulary set for token classification task. Larger model size does not necessarily translate to better performance on a cross-domain benchmark task. This probably indicates that there is no master model that can "do it all", at least well enough as a targeted one. The model size is a secondary factor; larger model size can probably further improve the performance of a a domain-and applicationspecific language model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of tokenization with different subword vocabularies. Blue and purple text show wordlevel and subtoken-level entity labeling, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model configurations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>Benchmark</cell><cell>Model</cell><cell cols="2">#Parameters Vocabulary</cell><cell>Prec Rec F1</cell></row><row><cell></cell><cell></cell><cell>BioBERT</cell><cell>110m</cell><cell>BERT-cased</cell><cell>90.0 93.4 91.7</cell></row><row><cell></cell><cell></cell><cell>PubMedBERT</cell><cell>110m</cell><cell>PubMedBERT-vocab (30k)</cell><cell>92.1 93.2 92.6</cell></row><row><cell></cell><cell>BC5CDR-chem</cell><cell>BioMegatron BioMegatron</cell><cell>345m 345m</cell><cell>Bio-vocab-30k Bio-vocab-50k</cell><cell>92.1 93.6 92.9 92.9 92.0 92.5</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>800m</cell><cell>BERT-cased</cell><cell>91.3 92.9 92.1</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>1.2b</cell><cell>BERT-uncased</cell><cell>92.0 90.5 91.3</cell></row><row><cell></cell><cell></cell><cell>BioBERT</cell><cell>110m</cell><cell>BERT-cased</cell><cell>85.0 89.4 87.2</cell></row><row><cell></cell><cell></cell><cell>PubMedBERT</cell><cell>110m</cell><cell cols="2">PubMedBERT-uncased (30k) 86.2 88.4 87.3</cell></row><row><cell>NER</cell><cell>BC5CDR-disease</cell><cell>BioMegatron BioMegatron</cell><cell>345m 345m</cell><cell>Bio-vocab-30k Bio-vocab-50k</cell><cell>85.2 88.8 87.0 86.1 91.0 88.5</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>800m</cell><cell>BERT-cased</cell><cell>85.8 90.1 87.9</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>1.2b</cell><cell>BERT-uncased</cell><cell>83.8 89.2 86.4</cell></row><row><cell></cell><cell></cell><cell>BioBERT</cell><cell>110m</cell><cell>BERT-cased</cell><cell>85.0 90.0 87.5</cell></row><row><cell></cell><cell></cell><cell>PubMedBERT</cell><cell>110m</cell><cell cols="2">PubMedBERT-uncased (30k) 85.9 87.7 86.8</cell></row><row><cell></cell><cell>NCBI-disease</cell><cell>BioMegatron BioMegatron</cell><cell>345m 345m</cell><cell>Bio-vocab-30k Bio-vocab-50k</cell><cell>85.6 88.6 87.1 83.7 90.4 87.0</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>800m</cell><cell>BERT-cased</cell><cell>87.0 88.8 87.8</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>1.2b</cell><cell>BERT-uncased</cell><cell>83.5 90.1 86.7</cell></row><row><cell></cell><cell></cell><cell>BioBERT</cell><cell>110m</cell><cell>BERT-cased</cell><cell>76.5 73.3 74.8</cell></row><row><cell></cell><cell></cell><cell>PubMedBERT</cell><cell>110m</cell><cell cols="2">PubMedBERT-uncased (30k) 73.6 77.7 75.6</cell></row><row><cell>RE</cell><cell>ChemProt</cell><cell>BioMegatron BioMegatron</cell><cell>345m 345m</cell><cell>Bio-vocab-30k Bio-vocab-50k</cell><cell>77.8 72.5 75.1 74.5 79.7 77.0</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>800m</cell><cell>BERT-cased</cell><cell>80.4 68.9 74.3</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>1.2b</cell><cell>BERT-uncased</cell><cell>82.0 65.6 72.9</cell></row></table><note>shows the rate named entities break into sub-tokens for each benchmark training set with different sub-word vo- cabularies. PubMedBERT vocabulary set has a low</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Benchmark</cell><cell>Model</cell><cell cols="2">#Parameters Vocabulary</cell><cell cols="3">SAcc LAcc MRR</cell></row><row><cell></cell><cell></cell><cell>BioBERT-Base</cell><cell>110m</cell><cell>BERT-cased</cell><cell>30.8</cell><cell>64.1</cell><cell>41.1</cell></row><row><cell>QA</cell><cell>BioASQ-7b-factoid</cell><cell>BioBERT-Large BioMegatron</cell><cell>345m 345m</cell><cell cols="2">BERT-cased BERT-uncased 46.2 42.8</cell><cell>62.8 62.6</cell><cell>50.1 52.5</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>800m</cell><cell>BERT-uncased</cell><cell>45.2</cell><cell>58.6</cell><cell>50.4</cell></row><row><cell></cell><cell></cell><cell>BioMegatron</cell><cell>1.2b</cell><cell>BERT-uncased</cell><cell>47.4</cell><cell>60.9</cell><cell>52.4</cell></row></table><note>Evaluation results on NER and RE after fine-tuning for 30 epochs with hyper-parameter settings of: num-fc-layers: {1, 2}; fc-hidden-size: {512, 1024}; fc-dropout: 0.5; max-seq-length: 128; learning-rate: 5e-5; cross-entropy loss, with Adam optimizer. BioMegatron models are pre-trained from scratch on PubMed, except 1.2b model which is fine-tuned from a general domain model checkpoint.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The rate of named entities breaking into subtokens (#tokens/#words) in NER training sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on BioASQ-7b factoid, without finetuning on SQuAD dataset first. The other models, including those using domain vocabularies, could not achieve any comparable results. A consistent pattern of improvement over model size noticeable on par with findings in general domain LM on SQuAD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Comparison of fine-tuning steps for NER</cell></row><row><cell>and RE benchmark when pre-training general-domain</cell></row><row><cell>Megatron-1.2b model on PubMed. Cross-domain LMs</cell></row><row><cell>should be trained sufficiently long on domain text to</cell></row><row><cell>achieve comparable performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Fine-tuning and evaluating on BioASQ-7b us-</cell></row><row><cell>ing general domain LMs not trained on PubMed corpus.</cell></row><row><cell>Larger model does not perform better.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Fine-tuning on SQuAD -v1.1/-v2.0 using</cell></row><row><cell>BioMegatron and evaluating on F1-score on dev-set.</cell></row><row><cell>BioMegatron with '-ft' are pre-trained from general do-</cell></row><row><cell>main checkpoints (fine-tuned). Results of other gen-</cell></row><row><cell>eral domain LMs are compared: RoBERTa</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Label bias in general and biomedical benchmark dataset.CONLL-2003 (Sang and<ref type="bibr" target="#b21">De Meulder, 2003)</ref>, MRPC<ref type="bibr" target="#b7">(Dolan et al., 2005)</ref>, and SQuAD (Rajpurkar et al., 2016) are general domain dataset for NER, CLS (RE), and QA, respectively, for comparison against biomedical domain dataset. Label bias is computed as [sum of the #samples of minority labels]/[#samples of majority label], for NER and RE (CLS), and [#minimum repeat of the same answer]/[#maximum repeat of the same answer] for QA.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Sun Kim at NIH/NCBI (now at Amazon Alexa AI) for helpful discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hung</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdermott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">Scibert: Pretrained contextualized embeddings for scientific text</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ramoboost: ranked minority oversampling in boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1624" to="1642" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Bretonnel Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii</surname></persName>
		</author>
		<title level="m">Proceedings of the 18th bionlp workshop and shared task</title>
		<meeting>the 18th bionlp workshop and shared task</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Proceedings of the 18th BioNLP Workshop and Shared Task</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Rezarta Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Microsoft research paraphrase corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-03" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<title level="m">Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mimiciii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The chemdner corpus of chemicals and drugs and its annotation principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02855</idno>
		<title level="m">Dice loss for data-imbalanced nlp tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Better language models and their implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing using very large corpora</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="157" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Defending against neural fake news. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
