<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation CCS CONCEPTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 4-8, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang@u.nus.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
							<email>caoyixin2011@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation CCS CONCEPTS</title>
					</analytic>
					<monogr>
						<title level="m">KDD &apos;19</title>
						<meeting> <address><addrLine>Anchorage, AK, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 4-8, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330989</idno>
					<note>/xiangwang1223/knowledge_graph_attention_network. * Xiangnan He is the corresponding author. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00 • Information systems → Recommender systems. KEYWORDS Collaborative Filtering, Recommendation, Graph Neural Network, Higher-order Connectivity, Embedding Propagation, Knowledge Graph ACM Reference Format: Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT: Knowledge Graph Attention Network for Recommendation. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users.</p><p>In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations -which connect two items with one or multiple linked attributes -are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit highorder relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM <ref type="bibr" target="#b11">[11]</ref> and RippleNet <ref type="bibr" target="#b29">[29]</ref>. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github. com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The success of recommendation system makes it prevalent in Web applications, ranging from search engines, E-commerce, to social media sites and news portals -without exaggeration, almost every service that provides content to users is equipped with a recommendation system. To predict user preference from the key (and widely available) source of user behavior data, much research effort has been devoted to collaborative filtering (CF) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b32">32]</ref>. Despite its effectiveness and universality, CF methods suffer from the inability of modeling side information <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>, such as item attributes, user profiles, and contexts, thus perform poorly in sparse situations where users and items have few interactions. To integrate such information, a common paradigm is to transform them into a generic feature vector, together with user ID and item ID, and feed them into a supervised learning (SL) model to predict the score. Such a SL paradigm for recommendation has been widely deployed in industry <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b41">41]</ref>, and some representative models include factorization machine (FM) <ref type="bibr" target="#b23">[23]</ref>, NFM (neural FM) <ref type="bibr" target="#b11">[11]</ref>, Wide&amp;Deep <ref type="bibr" target="#b6">[7]</ref>, and xDeepFM <ref type="bibr" target="#b18">[18]</ref>, etc.</p><p>Although these methods have provided strong performance, a deficiency is that they model each interaction as an independent data instance and do not consider their relations. This makes them insufficient to distill attribute-based collaborative signal from the collective behaviors of users. As shown in <ref type="figure">Figure 1</ref>, there is an interaction between user u 1 and movie i 1 , which is directed by the person e 1 . CF methods focus on the histories of similar users who also watched i 1 , i.e., u 4 and u 5 ; while SL methods emphasize the similar items with the attribute e 1 , i.e., i 2 . Obviously, these two types of information not only are complementary for recommendation, <ref type="figure">Figure 1</ref>: A toy example of collaborative knowledge graph. u 1 is the target user to provide recommendation for. The yellow circle and grey circle denote the important users and items discovered by high-order relations but are overlooked by traditional methods. Best view in color.</p><p>but also form a high-order relationship between a target user and item together. However, existing SL methods fail to unify them and cannot take into account the high-order connectivity, such as the users in the yellow circle who watched other movies directed by the same person e 1 , or the items in the grey circle that share other common relations with e 1 .</p><p>To address the limitation of feature-based SL models, a solution is to take the graph of item side information, aka. knowledge graph 1 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, into account to construct the predictive model. We term the hybrid structure of knowledge graph and user-item graph as collaborative knowledge graph (CKG). As illustrated in <ref type="figure">Figure  1</ref>, the key to successful recommendation is to fully exploit the high-order relations in CKG, e.g., the long-range connectivities:</p><formula xml:id="formula_0">• u 1 r 1 − → i 1 −r 2 − −− → e 1 r 2 − → i 2 −r 1 − −− → {u 2 , u 3 }, • u 1 r 1 − → i 1 −r 2 − −− → e 1 r 3 − → {i 3 , i 4 },</formula><p>which represent the way to the yellow and grey circle, respectively. Nevertheless, to exploit such high-order information the challenges are non-negligible: 1) the nodes that have high-order relations with the target user increase dramatically with the order size, which imposes computational overload to the model, and 2) the high-order relations contribute unequally to a prediction, which requires the model to carefully weight (or select) them.</p><p>Several recent efforts have attempted to leverage the CKG structure for recommendation, which can be roughly categorized into two types, path-based <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref> and regularizationbased <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38]</ref>:</p><p>• Path-based methods extract paths that carry the high-order information and feed them into predictive model. To handle the large number of paths between two nodes, they have either applied path selection algorithm to select prominent paths <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b33">33]</ref>, or defined meta-path patterns to constrain the paths <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>One issue with such two-stage methods is that the first stage of path selection has a large impact on the final performance, but it is not optimized for the recommendation objective. Moreover, defining effective meta-paths requires domain knowledge, which can be rather labor-intensive for complicated KG with diverse types of relations and entities, since many meta-paths have to be defined to retain model fidelity. • Regularization-based methods devise additional loss terms that capture the KG structure to regularize the recommender model learning. For example, KTUP <ref type="bibr" target="#b4">[5]</ref> and CFKG <ref type="bibr" target="#b0">[1]</ref> jointly train the two tasks of recommendation and KG completion with shared item embeddings. Instead of directly plugging high-order relations into the model optimized for recommendation, these methods only encode them in an implicit manner. Due to the lack of an explicit modeling, neither the long-range connectivities are guaranteed to be captured, nor the results of high-order modeling are interpretable.</p><p>Considering the limitations of existing solutions, we believe it is of critical importance to develop a model that can exploit high-order information in KG in an efficient, explicit, and end-to-end manner. Towards this end, we take inspiration from the recent developments of graph neural networks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b28">28]</ref>, which have the potential of achieving the goal but have not been explored much for KG-based recommendation. Specifically, we propose a new method named Knowledge Graph Attention Network (KGAT), which is equipped with two designs to correspondingly address the challenges in high-order relation modeling: 1) recursive embedding propagation, which updates a node's embedding based on the embeddings of its neighbors, and recursively performs such embedding propagation to capture high-order connectivities in a linear time complexity; and 2) attention-based aggregation, which employs the neural attention mechanism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">27]</ref> to learn the weight of each neighbor during a propagation, such that the attention weights of cascaded propagations can reveal the importance of a high-order connectivity. Our KGAT is conceptually advantageous to existing methods in that: 1) compared with path-based methods, it avoids the laborintensive process of materializing paths, thus is more efficient and convenient to use, and 2) compared with regularization-based methods, it directly factors high-order relations into the predictive model, thus all related parameters are tailored for optimizing the recommendation objective.</p><p>The contributions of this work are summarized as follows:</p><p>• We highlight the importance of explicitly modeling the highorder relations in collaborative knowledge graph to provide better recommendation with item side information. • We develop a new method KGAT, which achieves high-order relation modeling in an explicit and end-to-end manner under the graph neural network framework. • We conduct extensive experiments on three public benchmarks, demonstrating the effectiveness of KGAT and its interpretability in understanding the importance of high-order relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK FORMULATION</head><p>We first introduce the concept of CKG and highlight the high-order connectivity among nodes, as well as the compositional relations.</p><p>User-Item Bipartite Graph: In a recommendation scenario, we typically have historical user-item interactions (e.g., purchases and clicks). Here we represent interaction data as a user-item bipartite graph G 1 , which is defined as {(u, y ui , i)|u ∈ U, i ∈ I)}, where U and I separately denote the user and item sets, and a link y ui = 1 indicates that there is an observed interaction between user u and item i; otherwise y ui = 0.</p><p>Knowledge Graph. In addition to the interactions, we have side information for items (e.g., item attributes and external knowledge). Typically, such auxiliary data consists of real-world entities and relationships among them to profile an item. For example, a movie can be described by its director, cast, and genres. We organize the side information in the form of knowledge graph G 2 , which is a directed graph composed of subject-property-object triple facts <ref type="bibr" target="#b4">[5]</ref>. Formally, it is presented as {(h, r, t)|h, t ∈ E, r ∈ R}, where each triplet describes that there is a relationship r from head entity h to tail entity t. For example, (Hugh Jackman, ActorOf, Logan) states the fact that Hugh Jackman is an actor of the movie Logan. Note that R contains relations in both canonical direction (e.g., ActorOf ) and inverse direction (e.g., ActedBy). Moreover, we establish a set of item-entity alignments A = {(i, e)|i ∈ I, e ∈ E}, where (i, e) indicates that item i can be aligned with an entity e in the KG.</p><p>Collaborative Knowledge Graph. Here we define the concept of CKG, which encodes user behaviors and item knowledge as a unified relational graph. We first represent each user behavior as a triplet, (u, Interact, i), where y ui = 1 is represented as an additional relation Interact between user u and item i. Then based on the item-entity alignment set, the user-item graph can be seamlessly integrated with KG as a unified graph G = {(h, r , t)|h, t ∈ E ′ , r ∈ R ′ }, where E ′ = E ∪ U and R ′ = R ∪ {Interact}.</p><p>Task Description We now formulate the recommendation task to be addressed in this paper:</p><p>• Input: collaborative knowledge graph G that includes the useritem bipartite graph G 1 and knowledge graph G 2 . • Output: a prediction function that predicts the probabilityŷ ui that user u would adopt item i.</p><p>High-Order Connectivity. Exploiting high-order connectivity is of importance to perform high-quality recommendation. Formally, we define the L-order connectivity between nodes as a multi-hop relation path: e 0</p><formula xml:id="formula_1">r 1 − → e 1 r 2 − → · · · r L − − → e L ,</formula><p>where e l ∈ E ′ and r l ∈ R ′ ; (e l −1 , r l , e l ) is the l-th triplet, and L is the length of the sequence. To infer user preference, CF methods build upon behavior similarity among users -more specifically similar users would exhibit similar preferences on items. Such intuition can be represented as behaviorbased connectivity like u 1</p><formula xml:id="formula_2">r 1 − → i 1 −r 1 − −− → u 2 r 1</formula><p>− → i 2 , which suggests that u 1 would exhibit preference on i 2 , since her similar user u 2 has adopted i 2 before. Distinct from CF methods, SL models like FM and NFM focus on attributed-based connectivity, assuming that users tend to adopt items that share similar properties. For example,</p><formula xml:id="formula_3">u 1 r 1 − → i 1 r 2 − → e 1 −r 2</formula><p>− −− → i 2 suggests that u 1 would adopt i 2 since it has the same director e 1 with i 1 she liked before. However, FM and NFM treat entities as the values of individual feature fields, failing to reveal relatedness across fields and related instances. For instance, it is hard to model u 1</p><formula xml:id="formula_4">r 1 − → i 1 r 2 − → e 1 −r 3 − −− → i 2 ,</formula><p>although e 1 serves as the bridge connecting director and actor fields. We therefore argue that these methods do not fully explore the high-order connectivity and leave compositional high-order relations untouched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We now present the proposed KGAT model, which exploits highorder relations in an end-to-end fashion. <ref type="figure" target="#fig_0">Figure 2</ref> shows the model framework, which consists of three main components: 1) embedding layer, which parameterizes each node as a vector by preserving the structure of CKG; 2) attentive embedding propagation layers, which recursively propagate embeddings from a node's neighbors to update its representation, and employ knowledge-aware attention mechanism to learn the weight of each neighbor during a propagation; and 3) prediction layer, which aggregates the representations of a user and an item from all propagation layers, and outputs the predicted matching score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>Knowledge graph embedding is an effective way to parameterize entities and relations as vector representations, while preserving the graph structure. Here we employ TransR <ref type="bibr" target="#b19">[19]</ref>, a widely used method, on CKG. To be more specific, it learns embeds each entity and relation by optimizing the translation principle e r h + e r ≈ e r t , if a triplet (h, r , t) exists in the graph. Herein, e h , e t ∈ R d and e r ∈ R k are the embedding for h, t, and r , respectively; and e r h , e r t are the projected representations of e h and e t in the relation r 's space. Hence, for a given triplet (h, r , t), its plausibility score (or energy score) is formulated as follows:</p><formula xml:id="formula_5">д(h, r , t) = ∥W r e h + e r − W r e t ∥ 2 2 ,<label>(1)</label></formula><p>where W r ∈ R k ×d is the transformation matrix of relation r , which projects entities from the d-dimension entity space into the kdimension relation space. A lower score of д(h, r , t) suggests that the triplet is more likely to be true true, and vice versa. The training of TransR considers the relative order between valid triplets and broken ones, and encourages their discrimination through a pairwise ranking loss:</p><formula xml:id="formula_6">L KG = (h,r,t,t ′ )∈ T − ln σ д(h, r, t ′ ) − д(h, r, t) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_7">T = {(h, r, t, t ′ )|(h, r , t) ∈ G, (h, r , t ′ ) ̸ ∈ G}, and (h, r , t ′ )</formula><p>is a broken triplet constructed by replacing one entity in a valid triplet randomly; σ (·) is the sigmoid function. This layer models the entities and relations on the granularity of triples, working as a regularizer and injecting the direct connections into representations, and thus increases the model representation ability (evidences in Section 4.4.3.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attentive Embedding Propagation Layers</head><p>Next we build upon the architecture of graph convolution network <ref type="bibr" target="#b17">[17]</ref> to recursively propagate embeddings along highorder connectivity; moreover, by exploiting the idea of graph attention network <ref type="bibr" target="#b28">[28]</ref>, we generate attentive weights of cascaded propagations to reveal the importance of such connectivity. Here we start by describing a single layer, which consists of three components: information propagation, knowledge-aware attention, and information aggregation, and then discuss how to generalize it to multiple layers.</p><p>Information Propagation: One entity can be involved in multiple triplets, serving as the bridge connecting two triplets and propagating information. Taking e 1</p><formula xml:id="formula_8">r 2 − → i 2 −r 1 − −− → u 2 and e 2 r 3 − → i 2 −r 1</formula><p>− −− → u 2 as an example, item i 2 takes attributes e 1 and e 2 as inputs to enrich its own features, and then contributes user u 2 's preferences, which can be simulated by propagating information from e 1 to u 2 . We build upon this intuition to perform information propagation between an entity and its neighbors.</p><p>Considering an entity h, we use N h = {(h, r, t)|(h, r , t) ∈ G} to denote the set of triplets where h is the head entity, termed egonetwork <ref type="bibr" target="#b21">[21]</ref>. To characterize the first-order connectivity structure of entity h, we compute the linear combination of h's ego-network:</p><formula xml:id="formula_9">e N h = (h,r,t )∈N h π (h, r , t)e t ,<label>(3)</label></formula><p>where π (h, r , t) controls the decay factor on each propagation on edge (h, r , t), indicating how much information being propagated from t to h conditioned to relation r .</p><p>Knowledge-aware Attention: We implement π (h, r , t) via relational attention mechanism, which is formulated as follows:</p><formula xml:id="formula_10">π (h, r , t) = (W r e t ) ⊤ tanh (W r e h + e r ) ,<label>(4)</label></formula><p>where we select tanh <ref type="bibr" target="#b28">[28]</ref> as the nonlinear activation function. This makes the attention score dependent on the distance between e h and e t in the relation r 's space, e.g., propagating more information for closer entities. Note that, we employ only inner product on these representations for simplicity, and leave the further exploration of the attention module as the future work. Hereafter, we normalize the coefficients across all triplets connected with h by adopting the softmax function:</p><formula xml:id="formula_11">π (h, r, t) = exp(π (h, r, t)) (h,r ′ ,t ′ )∈N h exp(π (h, r ′ , t ′ )) .<label>(5)</label></formula><p>As a result, the final attention score is capable of suggesting which neighbor nodes should be given more attention to capture collaborative signals. When performing propagation forward, the attention flow suggests parts of the data to focus on, which can be treated as explanations behind the recommendation. Distinct from the information propagation in GCN <ref type="bibr" target="#b17">[17]</ref> and GraphSage <ref type="bibr" target="#b9">[9]</ref> which set the discount factor between two nodes as 1/ |N h ||N t | or 1/|N t |, our model not only exploits the proximity structure of graph, but also specify varying importance of neighbors. Moreover, distinct from graph attention network <ref type="bibr" target="#b28">[28]</ref> which only takes node representations as inputs, we model the relation e r between e h and e t , encoding more information during propagation.</p><p>We perform experiments to verify the effectiveness of the attention mechanism and visualize the attention flow in Section 4.4.3 and Section 4.5, respectively.</p><p>Information Aggregation: The final phase is to aggregate the entity representation e h and its ego-network representations e N h as the new representation of entity h -more formally, e</p><formula xml:id="formula_12">(1) h = f (e h , e N h )</formula><p>. We implement f (·) using three types of aggregators:</p><p>• GCN Aggregator <ref type="bibr" target="#b17">[17]</ref> sums two representations up and applies a nonlinear transformation, as follows:</p><formula xml:id="formula_13">f GCN = LeakyReLU W(e h + e N h ) ,<label>(6)</label></formula><p>where we set the activation function set as LeakyReLU <ref type="bibr" target="#b20">[20]</ref>; W ∈ R d ′ ×d are the trainable weight matrices to distill useful information for propagation, and d ′ is the transformation size. • GraphSage Aggregator <ref type="bibr" target="#b9">[9]</ref> concatenates two representations, followed by a nonlinear transformation:</p><formula xml:id="formula_14">f GraphSage = LeakyReLU W(e h ||e N h ) ,<label>(7)</label></formula><p>where || is the concatenation operation. • Bi-Interaction Aggregator is carefully designed by us to consider two kinds of feature interactions between e h and e N h , as follows:</p><formula xml:id="formula_15">f Bi-Interaction =LeakyReLU W 1 (e h + e N h ) + LeakyReLU W 2 (e h ⊙ e N h ) ,<label>(8)</label></formula><p>where W 1 , W 2 ∈ R d ′ ×d are the trainable weight matrices, and ⊙ denotes the element-wise product. Distinct from GCN and GraphSage aggregators, we additionally encode the feature interaction between e h and e N h . This term makes the information being propagated sensitive to the affinity between e h and e N h , e.g., passing more messages from similar entities. To summarize, the advantage of the embedding propagation layer lies in explicitly exploiting the first-order connectivity information to relate user, item, and knowledge entity representations. We empirically compare the three aggregators in Section 4.4.2.</p><p>High-order Propagation: We can further stack more propagation layers to explore the high-order connectivity information, gathering the information propagated from the higher-hop neighbors. More formally, in the l-th steps, we recursively formulate the representation of an entity as:</p><formula xml:id="formula_16">e (l ) h = f e (l −1) h , e (l −1) N h ,<label>(9)</label></formula><p>wherein the information propagated within l-ego network for the entity h is defined as follows,</p><formula xml:id="formula_17">e (l −1) N h = (h,r,t )∈N h π (h, r , t)e (l −1) t ,<label>(10)</label></formula><p>e (l −1) t is the representation of entity t generated from the previous information propagation steps, memorizing the information from its (l − 1)-hop neighbors; e (0) h is set as e h at the initial informationpropagation iteration. It further contributes to the representation of entity h at layer l. As a result, high-order connectivity like u 2</p><formula xml:id="formula_18">r 1 − → i 2 −r 2 − −− → e 1 r 2 − → i 1 −r 1</formula><p>− −− → u 1 can be captured in the embedding propagation process. Furthermore, the information from u 2 is explicitly encoded in e (3) u 1 . Clearly, the high-order embedding propagation seamlessly injects the attribute-based collaborative signal into the representation learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Prediction</head><p>After performing L layers, we obtain multiple representations for user node u, namely {e</p><formula xml:id="formula_19">(1) u , · · · , e (L) u }; analogous to item node i, {e (1) i , · · · , e (L)</formula><p>i } are obtained. As the output of the l-th layer is the message aggregation of the tree structure depth of l rooted at u (or i) as shown in <ref type="figure">Figure 1</ref>, the outputs in different layers emphasize the connectivity information of different orders. We hence adopt the layer-aggregation mechanism <ref type="bibr" target="#b34">[34]</ref> to concatenate the representations at each step into a single vector, as follows:</p><formula xml:id="formula_20">e * u = e (0) u ∥· · · ∥e (L) u , e * i = e (0) i ∥· · · ∥e (L) i ,<label>(11)</label></formula><p>where ∥ is the concatenation operation. By doing so, we not only enrich the initial embeddings by performing the embedding propagation operations, but also allow controlling the strength of propagation by adjusting L. Finally, we conduct inner product of user and item representations, so as to predict their matching score:</p><formula xml:id="formula_21">y(u, i) = e * u ⊤ e * i .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>To optimize the recommendation model, we opt for the BPR loss <ref type="bibr" target="#b22">[22]</ref>. Specifically, it assumes that the observed interactions, which indicate more user preferences, should be assigned higher prediction values than unobserved ones:</p><formula xml:id="formula_22">L CF = (u,i, j)∈ O − ln σ ŷ(u, i) −ŷ(u, j)<label>(13)</label></formula><p>where O = {(u, i, j)|(u, i) ∈ R + , (u, j) ∈ R − } denotes the training set, R + indicates the observed (positive) interactions between user u and item j while R − is the sampled unobserved (negative) interaction set; σ (·) is the sigmoid function. Finally, we have the objective function to learn Equations (2) and (13) jointly, as follows:</p><formula xml:id="formula_23">L KGAT = L KG + L CF + λ ∥Θ∥ 2 2 ,<label>(14)</label></formula><p>where Θ = {E, W r , ∀l ∈ R, W</p><p>1 , W</p><p>2 , ∀l ∈ {1, · · · , L}} is the model parameter set, and E is the embedding table for all entities and relations; L 2 regularization parameterized by λ on Θ is conducted to prevent overfitting. It is worth pointing out that in terms of model size, the majority of model parameters comes from the entity embeddings (e.g., 6.5 million on experimented Amazon dataset), which is almost identical to that of FM; the propagation layer weights are lightweight (e.g., 5.4 thousand for the tower structure of three layers, i.e., 64 − 32 − 16 − 8, on the Amazon dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Training:</head><p>We optimize L KG and L C F alternatively, where mini-batch Adam <ref type="bibr" target="#b16">[16]</ref> is adopted to optimize the embedding loss and the prediction loss. Adam is a widely used optimizer, which is able to adaptively control the learning rate w.r.t. the absolute value of gradient. In particular, for a batch of randomly sampled (h, r , t, t ′ ), we update the embeddings for all nodes; hereafter, we sample a batch of (u, i, j) randomly, retrieve their representations after L steps of propagation, and then update model parameters by using the gradients of the prediction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Time Complexity Analysis:</head><p>As we adopt the alternative optimization strategy, the time cost mainly comes from two parts. For the knowledge graph embedding (cf. Equation <ref type="formula" target="#formula_6">(2)</ref>), the translation principle has computational complexity O(|G 2 |d 2 ). For the attention embedding propagation part, the matrix multiplication of the l-th layer has computational complexity O(|G|d l d l −1 ); and d l and d l −1 are the current and previous transformation size. For the final prediction layer, only the inner product is conducted, for which the time cost of the whole training epoch is O( L l =1 |G|d l ). Finally, the overall training complexity of KGAT is O(|G 2 |d 2 + L l =1 |G|d l d l −1 + |G|d l ). As online services usually require real-time recommendation, the computational cost during inference is more important that that of training phase. Empirically, FM, NFM, CFKG, CKE, GC-MC, KGAT, MCRec, and RippleNet cost around 700s, 780s, 800s, 420s, 500s, 560s, 20 hours, and 2 hours for all testing instances on Amazon-Book dataset, respectively. As we can see, KGAT achieves comparable computation complexity to SL models (FM and NFM) and regularization-based methods (CFKG and CKE), being much efficient that path-based methods (MCRec and RippleNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our proposed method, especially the embedding propagation layer, on three real-world datasets. We aim to answer the following research questions:</p><p>• RQ1: How does KGAT perform compared with state-of-the-art knowledge-aware recommendation methods? • RQ2: How do different components (i.e., knowledge graph embedding, attention mechanism, and aggregator selection) affect KGAT? • RQ3: Can KGAT provide reasonable explanations about user preferences towards items?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description</head><p>To evaluate the effectiveness of KGAT, we utilize three benchmark datasets: Amazon-book, Last-FM, and Yelp2018, which are publicly accessible and vary in terms of domain, size, and sparsity. Amazon-book 2 : Amazon-review is a widely used dataset for product recommendation <ref type="bibr" target="#b10">[10]</ref>. We select Amazon-book from this collection. To ensure the quality of the dataset, we use the 10-core setting, i.e., retaining users and items with at least ten interactions. Last-FM 3 : This is the music listening dataset collected from Last.fm online music systems. Wherein, the tracks are viewed as the items.</p><p>In particular, we take the subset of the dataset where the timestamp is from Jan, 2015 to June, 2015. We use the same 10-core setting in order to ensure data quality. Yelp2018 4 : This dataset is adopted from the 2018 edition of the Yelp challenge. Here we view the local businesses like restaurants and bars as the items. Similarly, we use the 10-core setting to ensure that each user and item have at least ten interactions.</p><p>Besides the user-item interactions, we need to construct item knowledge for each dataset. For Amazon-book and Last-FM, we follow the way in <ref type="bibr" target="#b40">[40]</ref> to map items into Freebase entities via title matching if there is a mapping available. In particular, we consider the triplets that are directly related to the entities aligned with items, no matter which role (i.e., subject or object) it serves as. Distinct from existing knowledge-aware datasets that provide only one-hop entities of items, we also take the triplets that involve two-hop neighbor entities of items into consideration. For Yelp2018, we extract item knowledge from the local business information network (e.g., category, location, and attribute) as KG data. To ensure the KG quality, we then preprocess the three KG parts by filtering out infrequent entities (i.e., lowever than 10 in both datasets) and retaining the relations appearing in at least 50 triplets. We summarize the statistics of three datasets in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For each dataset, we randomly select 80% of interaction history of each user to constitute the training set, and treat the remaining as the test set. From the training set, we randomly select 10% of interactions as validation set to tune hyper-parameters. For each observed user-item interaction, we treat it as a positive instance, and then conduct the negative sampling strategy to pair it with one negative item that the user did not consume before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation Metrics.</head><p>For each user in the test set, we treat all the items that the user has not interacted with as the negative items. Then each method outputs the user's preference scores over all the items, except the positive ones in the training set. To evaluate the effectiveness of top-K recommendation and preference ranking, we adopt two widely-used evaluation protocols <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b35">35]</ref>: recall@K and ndcg@K. By default, we set K = 20. We report the average metrics for all users in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Baselines.</head><p>To demonstrate the effectiveness, we compare our proposed KGAT with SL (FM and NFM), regularization-based (CFKG and CKE), path-based (MCRec and RippleNet), and graph neural network-based (GC-MC) methods, as follows:</p><p>• FM [23]: This is a bechmark factorization model, where considers the second-order feature interactions between inputs. Here we treat IDs of a user, an item, and its knowledge (i.e., entities connected to it) as input features. • NFM <ref type="bibr" target="#b11">[11]</ref>: The method is a state-of-the-art factorization model, which subsumes FM under neural network. Specially, we employed one hidden layer on input features as suggested in <ref type="bibr" target="#b11">[11]</ref>. • CKE <ref type="bibr" target="#b38">[38]</ref>: This is a representative regularization-based method, which exploits semantic embeddings derived from TransR <ref type="bibr" target="#b19">[19]</ref> to enhance matrix factorization <ref type="bibr" target="#b22">[22]</ref>. • CFKG <ref type="bibr" target="#b0">[1]</ref>: The model applies TransE <ref type="bibr" target="#b1">[2]</ref> on the unified graph including users, items, entities, and relations, casting the recommendation task as the plausibility prediction of (u, Interact, i) triplets. • MCRec <ref type="bibr" target="#b14">[14]</ref>: This is a path-based model, which extracts qualified meta-paths as connectivity between a user and an item. • RippleNet <ref type="bibr" target="#b29">[29]</ref>: Such model combines regularization-and pathbased methods, which enrich user representations by adding that of items within paths rooted at each user. • GC-MC <ref type="bibr" target="#b26">[26]</ref>: Such model is designed to employ GCN <ref type="bibr" target="#b17">[17]</ref> encoder on graph-structured data, especially for the user-item bipartite graph. Here we apply it on the user-item knowledge graph. Especially, we employ one graph convolution layers as suggested in <ref type="bibr" target="#b26">[26]</ref>, where the hidden dimension is set equal to the embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Settings.</head><p>We implement our KGAT model in Tensorflow. The embedding size is fixed to 64 for all models, except RippleNet 16 due to its high computational cost. We optimize all models with Adam optimizer, where the batch size is fixed at 1024. The default Xavier initializer <ref type="bibr" target="#b8">[8]</ref> to initialize the model parameters.</p><p>We apply a grid search for hyper-parameters: the learning rate is tuned amongst {0.05, 0.01, 0.005, 0.001}, the coefficient of L 2 normalization is searched in {10 −5 , 10 −4 , · · · , 10 1 , 10 2 }, and the dropout ratio is tuned in {0.0, 0.1, · · · , 0.8} for NFM, GC-MC, and KGAT. Besides, we employ the node dropout technique for GC-MC and KGAT, where the ratio is searched in {0.0, 0.1, · · · , 0.8}. For MCRec, we manually define several types of user-item-attributeitem meta-paths, such as user-book-author-user and user-book-genreuser for Amazon-book dataset; we set the hidden layers as suggested in <ref type="bibr" target="#b14">[14]</ref>, which is a tower structure with 512, 256, 128, 64 dimensions. For RippleNet, we set the number of hops and the memory size as 2 and 8, respectively. Moreover, early stopping strategy is performed, i.e., premature stopping if recall@20 on the validation set does not increase for 50 successive epochs. To model the third-order connectivity, we set the depth of KGAT L as three with hidden dimension 64, 32, and 16, respectively; we also report the effect of layer depth in Section 4.4.1. For each layer, we conduct the Bi-Interaction aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison (RQ1)</head><p>We first report the performance of all the methods, and then investigate how the modeling of high-order connectivity alleviate the sparsity issues.    <ref type="table" target="#tab_1">Table 2</ref>. We have the following observations:</p><p>• KGAT consistently yields the best performance on all the datasets. In particular, KGAT improves over the strongest baselines w.r.t. recall@20 by 8.95%, 4.93%, and 7.18% in Amazon-book, Last-FM, and Yelp2018, respectively. By stacking multiple attentive embedding propagation layers, KGAT is capable of exploring the high-order connectivity in an explicit way, so as to capture collaborative signal effectively. This verifies the significance of capturing collaborative signal to transfer knowledge. Moreover, compared with GC-MC, KGAT justifies the effectiveness of the attention mechanism, specifying the attentive weights w.r.t. compositional semantic relations, rather than the fixed weights used in GC-MC. • SL methods (i.e., FM and NFM) achieve better performance than the CFKG and CKE in most cases, indicating that regularizationbased methods might not make full use of item knowledge. In particular, to enrich the representation of an item, FM and NFM exploit the embeddings of its connected entities, while CFKG and CKE only use that of its aligned entities. Furthermore, the cross features in FM and NFM actually serve as the second-order connectivity between users and entities, whereas CFKG and CKE model connectivity on the granularity of triples, leaving highorder connectivity untouched. • Compared to FM, the performance of RippleNet verifies that incorporating two-hop neighboring items is of importance to enrich user representations. It therefore points to the positive effect of modeling the high-order connectivity or neighbors. However, RippleNet slightly underperforms NFM in Amazonbook and Last-FM, while performing better in Yelp2018. One possible reason is that NFM has stronger expressiveness, since the hidden layer allows NFM to capture the nonlinear and complex feature interactions between user, item, and entity embeddings. • RippleNet outperforms MCRec by a large margin in Amazonbook. One possible reason is that MCRec depends heavily on the quality of meta-paths, which require extensive domain knowledge to define. The observation is consist with <ref type="bibr" target="#b29">[29]</ref>. • GC-MC achieves comparable performance to RippleNet in Last-FM and Yelp2018 datasets. While introducing the high-order connectivity into user and item representations, GC-MC forgoes the semantic relations between nodes; whereas RippleNet utilizes relations to guide the exploration of user preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>Performance Comparison w.r.t. Interaction Sparsity Levels. One motivation to exploiting KG is to alleviate the sparsity issue, which usually limits the expressiveness of recommender systems. It is hard to establish optimal representations for inactive users with few interactions. Here we investigate whether exploiting connectivity information helps alleviate this issue. Towards this end, we perform experiments over user groups of different sparsity levels. In particular, we divide the test set into four groups based on interaction number per user, meanwhile try to keep different groups have the same total interactions. Taking Amazon-book dataset as an example, the interaction numbers per user are less than 7, 15, 48, and 4475 respectively. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the results w.r.t. ndcg@20 on different user groups in Amazon-book, Last-FM, and Yelp2018. We can see that:</p><p>• KGAT outperforms the other models in most cases, especially on the two sparsest user groups in Amazon-Book and Yelp2018. It again verifies the significance of high-order connectivity modeling, which 1) contains the lower-order connectivity used in baselines, and 2) enriches the representations of inactive users via recursive embedding propagation. • It is worthwhile pointing out that KGAT slightly outperforms some baselines in the densest user group (e.g., the &lt; 2057 group of Yelp2018). One possible reason is that the preferences of users with too many interactions are too general to capture. High-order connectivity could introduce more noise into the user preferences, thus leading to the negative effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study of KGAT (RQ2)</head><p>To get deep insights on the attentive embedding propagation layer of KGAT, we investigate its impact. We first study the influence of layer numbers. In what follows, we explore how different  aggregators affect the performance. We then examine the influence of knowledge graph embedding and attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effect of Model Depth.</head><p>We vary the depth of KGAT (e.g., L) to investigate the efficiency of usage of multiple embedding propagation layers. In particular, the layer number is searched in the range of {1, 2, 3, 4}; we use KGAT-1 to indicate the model with one layer, and similar notations for others. We summarize the results in <ref type="table" target="#tab_2">Table 3</ref>, and have the following observations:</p><p>• Increasing the depth of KGAT is capable of boosting the performance substantially. Clearly, KGAT-2 and KGAT-3 achieve consistent improvement over KGAT-1 across all the board. We attribute the improvements to the effective modeling of highorder relation between users, items, and entities, which is carried by the second-and third-order connectivities, respectively. • Further stacking one more layer over KGAT-3, we observe that KGAT-4 only achieve marginal improvements. It suggests that considering third-order relations among entities could be sufficient to capture the collaborative signal, which is consistent to the findings in <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b33">33]</ref>. <ref type="table" target="#tab_0">• Jointly analyzing Tables 2 and 3, KGAT-1 consistently outperforms</ref> other baselines in most cases. It again verifies the effectiveness of that attentive embedding propagation, empirically showing that it models the first-order relation better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effect of Aggregators.</head><p>To explore the impact of aggregators, we consider the variants of KGAT-1 that uses different settings -more specifically GCN, GraphSage, and Bi-Interaction (cf. Section 3.1), termed KGAT-1 GCN , KGAT-1 GraphSage , and KGAT-1 Bi , respectively. <ref type="table" target="#tab_3">Table 4</ref> summarizes the experimental results. We have the following findings:</p><p>• KGAT-1 GCN is consistently superior to KGAT-1 GraphSage . One possible reason is that GraphSage forgoes the interaction between the entity representation e h and its ego-network representation e N h . It hence illustrates the importance of feature interaction when performing information aggregation and propagation. • Compared to KGAT-1 GCN , the performance of KGAT-1 Bi verifies that incorporating additional feature interaction can improve the representation learning. It again illustrates the rationality and effectiveness of Bi-Interaction aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Effect of Knowledge Graph Embedding and Attention</head><p>Mechanism. To verify the impact of knowledge graph embedding and attention mechanism, we do ablation study by considering three  variants of KGAT-1. In particular, we disable the TransR embedding component (cf. Equation <ref type="formula" target="#formula_6">(2)</ref>) of KGAT, termed KGAT-1 w/o KGE ; we disable the attention mechanism (cf. Equation <ref type="formula" target="#formula_10">(4)</ref>) and set π (h, r , t) as 1/|N h |, termed KGAT-1 w/o Att . Moreover, we obtain another variant by removing both components, named KGAT-1 w/o K&amp;A . We summarize the experimental results in <ref type="table" target="#tab_4">Table 5</ref> and have the following findings: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study (RQ3)</head><p>Benefiting from the attention mechanism, we can reason on highorder connectivity to infer the user preferences on the target item, offering explanations. Towards this end, we randomly selected one user u 208 from Amazon-Book, and one relevant item i 4293 (from the test, unseen in the training phase). We extract behavior-based and attribute-based high-order connectivity connecting the user-item pair, based on the attention scores. <ref type="figure" target="#fig_3">Figure 4</ref> shows the visualization of high-order connectivity. There are two key observations:</p><p>• KGAT captures the behavior-based and attribute-based highorder connectivity, which play a key role to infer user preferences. The retrieved paths can be viewed as the evidence why the item meets the user's preference. As we can see, the connectivity u 208 − −−→ i 4293 has the highest attention score, labeled with the solid line in the left subfigure. Hence, we can generate the explanation as The Last Colony is recommended since you have watched Old Man's War written by the same author John Scalzi.</p><p>• The quality of item knowledge is of crucial importance. As we can see, entity English with relation Original Language is involved in one path, which is too general to provide high-quality explanations. This inspires us to perform hard attention to filter less informative entities out in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this work, we explore high-order connectivity with semantic relations in CKG for knowledge-aware recommendation. We devised a new framework KGAT, which explicitly models the highorder connectivities in CKG in an end-to-end fashion. At it core is the attentive embedding propagation layer, which adaptively propagates the embeddings from a node's neighbors to update the node's representation. Extensive experiments on three real-world datasets demonstrate the rationality and effectiveness of KGAT. This work explores the potential of graph neural networks in recommendation, and represents an initial attempt to exploit structural knowledge with information propagation mechanism. Besides knowledge graph, many other structural information indeed exists in real-world scenarios, such as social networks and item contexts. For example, by integrating social network with CKG, we can investigate how social influence affects the recommendation. Another exciting direction is the integration of information propagation and decision process, which opens up research possibilities of explainable recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed KGAT model. The left subfigure shows model framework of KGAT, and the right subfigure presents the attentive embedding propagation layer of KGAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) ndcg on Amazon-Book (b) ndcg on Last-FM (c) ndcg on Yelp2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance comparison over the sparsity distribution of user groups on different datasets. The background histograms indicate the density of each user group; meanwhile, the lines demonstrate the performance w.r.t. ndcg@20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Real Example from Amazon-Book.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>Removing knowledge graph embedding and attention components degrades the model's performance. KGAT-1 w/o K&amp;A consistently underperforms KGAT-1 w/o KGE and KGAT-1 w/o Att . It makes sense since KGAT w/o K&amp;A fails to explicitly model the representation relatedness on the granularity of triplets. • Compared with KGAT-1 w/o Att , KGAT-1 w/o KGE performs better in most cases. One possible reason is that treating all neighbors equally (i.e., KGAT-1 w/o Att ) might introduce noises and mislead the embedding propagation process. It verifies the substantial influence of graph attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Amazon-book Last-FM</cell><cell>Yelp2018</cell></row><row><cell>User-Item Interaction</cell><cell>#Users #Items #Interactions</cell><cell cols="3">70, 679 24, 915 847, 733 3, 034, 796 1, 185, 068 23, 566 45, 919 48, 123 45, 538</cell></row><row><cell>Knowledge Graph</cell><cell>#Entities #Relations #Triplets</cell><cell>88, 572 39 2, 557, 746</cell><cell cols="2">58, 266 9 464, 567 1, 853, 704 90, 961 42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall Performance Comparison.</figDesc><table><row><cell></cell><cell cols="2">Amazon-Book</cell><cell cols="2">Last-FM</cell><cell cols="2">Yelp2018</cell></row><row><cell></cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell>FM</cell><cell>0.1345</cell><cell>0.0886</cell><cell>0.0778</cell><cell>0.1181</cell><cell>0.0627</cell><cell>0.0768</cell></row><row><cell>NFM</cell><cell>0.1366</cell><cell>0.0913</cell><cell>0.0829</cell><cell>0.1214</cell><cell>0.0660</cell><cell>0.0810</cell></row><row><cell>CKE</cell><cell>0.1343</cell><cell>0.0885</cell><cell>0.0736</cell><cell>0.1184</cell><cell>0.0657</cell><cell>0.0805</cell></row><row><cell>CFKG</cell><cell>0.1142</cell><cell>0.0770</cell><cell>0.0723</cell><cell>0.1143</cell><cell>0.0522</cell><cell>0.0644</cell></row><row><cell>MCRec</cell><cell>0.1113</cell><cell>0.0783</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">RippleNet 0.1336</cell><cell>0.0910</cell><cell>0.0791</cell><cell>0.1238</cell><cell>0.0664</cell><cell>0.0822</cell></row><row><cell>GC-MC</cell><cell>0.1316</cell><cell>0.0874</cell><cell>0.0818</cell><cell>0.1253</cell><cell>0.0659</cell><cell>0.0790</cell></row><row><cell cols="2">KGAT 0.1489  %Improv. 8.95%</cell><cell>10.05%</cell><cell>4.93%</cell><cell>5.77%</cell><cell>7.18%</cell><cell>5.54%</cell></row></table><note>* 0.1006* 0.0870* 0.1325* 0.0712* 0.0867*4.3.1 Overall Comparison. The performance comparison results are presented in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of embedding propagation layer numbers (L).</figDesc><table><row><cell cols="2">Amazon-Book</cell><cell cols="2">Last-FM</cell><cell cols="2">Yelp2018</cell></row><row><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell cols="6">KGAT-1 0.1393 0.0948 0.0834 0.1286 0.0693 0.0848</cell></row><row><cell cols="6">KGAT-2 0.1464 0.1002 0.0863 0.1318 0.0714 0.0872</cell></row><row><cell cols="6">KGAT-3 0.1489 0.1006 0.0870 0.1325 0.0712 0.0867</cell></row><row><cell cols="6">KGAT-4 0.1503 0.1015 0.0871 0.1329 0.0722 0.0871</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of aggregators.</figDesc><table><row><cell></cell><cell cols="2">Amazon-Book</cell><cell cols="2">Last-FM</cell><cell cols="2">Yelp2018</cell></row><row><cell>Aggregator</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell>GCN</cell><cell cols="6">0.1381 0.0931 0.0824 0.1278 0.0688 0.0847</cell></row><row><cell>GraphSage</cell><cell cols="6">0.1372 0.0929 0.0822 0.1268 0.0666 0.0831</cell></row><row><cell cols="7">Bi-Interaction 0.1393 0.0948 0.0834 0.1286 0.0693 0.0848</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of knowledge graph embedding and attention mechanism.</figDesc><table><row><cell></cell><cell cols="2">Amazon-Book</cell><cell cols="2">Last-FM</cell><cell cols="2">Yelp2018</cell></row><row><cell></cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell><cell>recall</cell><cell>ndcg</cell></row><row><cell cols="7">w/o K&amp;A 0.1367 0.0928 0.0819 0.1252 0.0654 0.0808</cell></row><row><cell cols="7">w/o KGE 0.1380 0.0933 0.0826 0.1273 0.0664 0.0824</cell></row><row><cell>w/o Att</cell><cell cols="6">0.1377 0.0930 0.0826 0.1270 0.0657 0.0815</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A KG is typically described as a heterogeneous network consisting of entity-relationentity triplets, where the entity can be an item or an attribute.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://jmcauley.ucsd.edu/data/amazon.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://grouplens.org/datasets/hetrec-2011/. 4 https://www.yelp.com/dataset/challenge.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Collective Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiansi</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Item-and Component-Level Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemal</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
	</analytic>
	<monogr>
		<title level="m">DLRS@RecSys</title>
		<imprint>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAIS: Neural Attentive Item Similarity Model for Recommendation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural Collaborative Filtering. In WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging Metapath based Context for Top-N Recommendation with A Neural Co-Attention Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1531" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In KDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent knowledge graph embedding for effective recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bozzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6000" to="6010" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">TEM: Tree-enhanced Embedding Model for Explainable Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1543" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Item Silk Road: Recommending Items from Information Domains to Social Users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explainable Reasoning over Knowledge Graphs for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Yixin Cao, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collaborative filtering with entity similarity regularization in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: a heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Collaborative Knowledge Base Embedding for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">KB4Rec: A Dataset for Linking Knowledge Bases with Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
		<idno>abs/1807.11141</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep Interest Network for Click-Through Rate Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
