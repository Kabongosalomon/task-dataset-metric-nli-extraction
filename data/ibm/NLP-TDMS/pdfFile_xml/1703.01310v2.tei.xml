<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Count-Based Exploration with Neural Density Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
						</author>
						<title level="a" type="main">Count-Based Exploration with Neural Density Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref> <p>introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to nontabular reinforcement learning. This pseudocount was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Exploration is the process by which an agent learns about its environment. In the reinforcement learning framework, this involves reducing the agent's uncertainty about the environment's transition dynamics and attainable rewards. From a theoretical perspective, exploration is now <ref type="bibr">wellunderstood (e.g. Strehl &amp; Littman, 2008;</ref><ref type="bibr" target="#b10">Jaksch et al., 2010;</ref><ref type="bibr" target="#b15">Osband et al., 2016)</ref>, and Bayesian methods have been successfully demonstrated in a number of settings <ref type="bibr" target="#b3">(Deisenroth &amp; Rasmussen, 2011;</ref><ref type="bibr" target="#b8">Guez et al., 2012)</ref>. On the other hand, practical algorithms for the general case remain scarce; fully Bayesian approaches are usually intractable in large state spaces, and the count-based method typical of theoretical results is not applicable in the presence of value function approximation.</p><p>Recently, <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref> proposed the notion of pseudo-count as a reasonable generalization of the tabular setting considered in the theory literature. The pseudocount is defined in terms of a density model ρ trained on the sequence of states experienced by an agent:</p><formula xml:id="formula_0">N(x) = ρ(x)n(x),</formula><p>wheren(x) can be thought of as a total pseudo-count computed from the model's recoding probability ρ (x), the probability of x computed immediately after training on x. As a practical application the authors used the pseudocounts derived from the simple CTS density model <ref type="bibr" target="#b0">(Bellemare et al., 2014)</ref> to incentivize exploration in Atari 2600 agents. One of the main outcomes of their work was substantial empirical progress on the infamously hard game MONTEZUMA'S REVENGE.</p><p>Their method critically hinged on several assumptions regarding the density model: 1) the model should be learning-positive, i.e. the probability assigned to a state x should increase with training; 2) it should be trained online, using each sample exactly once; and 3) the effective model step-size should decay at a rate of n −1 . Part of their empirical success also relied on a mixed Monte Carlo/Q-Learning update rule, which permitted fast propagation of the exploration bonuses.</p><p>In this paper, we set out to answer several research questions related to these modelling choices and assumptions: In particular, we explore the use of PixelCNN (van den <ref type="bibr">Oord et al., 2016b;</ref><ref type="bibr">a)</ref>, a state-of-the-art neural density model. We examine the challenges posed by this approach:</p><p>Model choice. Performing two evaluations and one model update at each agent step (to compute ρ(x) and ρ (x)) can be prohibitively expensive. This requires the design of a simplified -yet sufficiently expressive and accurate -Pix-elCNN architecture.</p><p>Model training. A CTS model can naturally be trained from sequentially presented, correlated data samples. Training a neural model in this online fashion requires more careful attention to the optimization procedure to prevent overfitting and catastrophic forgetting <ref type="bibr" target="#b4">(French, 1999)</ref>.</p><p>Model use. The theory of pseudo-counts requires the density model's rate of learning to decay over time. Optimization of a neural model, however, imposes constraints on the step-size regime which cannot be violated without deteriorating effectiveness and stability of training.</p><p>The concept of intrinsic motivation has made a recent resurgence in reinforcement learning research, in great part due to a dissatisfaction with -greedy and Boltzmann policies. Of note, <ref type="bibr">Tang et al. (2016)</ref> maintain an approximate count by means of hash tables over features, which in the pseudo-count framework corresponds to a hash-based density model. <ref type="bibr" target="#b9">Houthooft et al. (2016)</ref> used a second-order Taylor approximation of the prediction gain to drive exploration in continuous control. As research moves towards ever more complex environments, we expect the trend towards more intrinsically motivated solutions to continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pseudo-Count and Prediction Gain</head><p>Here we briefly introduce notation and results, referring the reader to  for technical details.</p><p>Let ρ be a density model on a finite space X , and ρ n (x) the probability assigned by the model to x after being trained on a sequence of states x 1 , . . . , x n . Assume ρ n (x) &gt; 0 for all x, n. The recoding probability ρ n (x) is then the probability the model would assign to x if it were trained on that same x one more time. We call ρ learning-positive if ρ n (x) ≥ ρ n (x) for all x 1 , . . . , x n , x ∈ X . The prediction gain (PG) of ρ is PG n (x) = log ρ n (x) − log ρ n (x).</p><p>(1)</p><p>A learning-positive ρ implies PG n (x) ≥ 0 for all x ∈ X . For learning-positive ρ, we define the pseudo-count aŝ</p><formula xml:id="formula_1">N n (x) = ρ n (x)(1 − ρ n (x)) ρ n (x) − ρ n (x) ,</formula><p>derived from postulating that a single observation of x ∈ X should lead to a unit increase in pseudo-count:</p><formula xml:id="formula_2">ρ n (x) =N n (x) n , ρ n (x) =N n (x) + 1 n + 1 ,</formula><p>wheren is the pseudo-count total. The pseudo-count generalizes the usual state visitation count function N n (x). Under certain assumptions on ρ n , pseudo-counts grow approximately linearly with real counts. Crucially, the pseudo-count can be approximated using the prediction gain of the density model:</p><formula xml:id="formula_3">N n (x) ≈ e PGn(x) − 1 −1 .</formula><p>Its main use is to define an exploration bonus. We consider a reinforcement learning (RL) agent interacting with an environment that provides observations and extrinsic rewards (see <ref type="bibr">Sutton &amp; Barto, 1998</ref>, for a thorough exposition of the RL framework). To the reward at step n we add the bonus r + (x) := (N n (x)) −1/2 , which incentivizes the agent to try to re-experience surprising situations. Quantities related to prediction gain have been used for similar purposes in the intrinsic motivation literature <ref type="bibr" target="#b12">(Lopes et al., 2012)</ref>, where they measure an agent's learning progress <ref type="bibr" target="#b16">(Oudeyer et al., 2007)</ref>. Although the pseudo-count bonus is close to the prediction gain, it is asymptotically more conservative and supported by stronger theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Density Models for Images</head><p>The CTS density model <ref type="bibr" target="#b0">(Bellemare et al., 2014)</ref> is based on the namesake algorithm, Context Tree Switching (Veness et al., 2012), a Bayesian variable-order Markov model. In its simplest form, the model takes as input a 2D image and assigns to it a probability according to the product of location-dependent L-shaped filters, where the prediction of each filter is given by a CTS algorithm trained on past images. In <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref>, this model was applied to 3-bit greyscale, 42 × 42 downsampled Atari 2600 frames <ref type="figure">(Fig. 1</ref>). The CTS model presents advantages in terms of simplicity and performance but is limited in expressiveness, scalability, and data efficiency.</p><p>In recent years, neural generative models for images have achieved impressive successes in their ability to generate diverse images in various domains <ref type="bibr" target="#b11">(Kingma &amp; Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014;</ref><ref type="bibr" target="#b6">Gregor et al., 2015;</ref><ref type="bibr" target="#b5">Goodfellow et al., 2014)</ref>. In particular, <ref type="bibr">van den Oord et al. (2016b;</ref><ref type="bibr">a)</ref> introduced PixelCNN, a fully convolutional neural network composed of residual blocks with multiplicative gating units, which models pixel probabilities conditional on previous pixels (in the usual top-left to bottomright raster-scan order) by using masked convolution fil-Original Frame (160x210) 3-bit Greyscale (42x42) <ref type="figure">Figure 1</ref>. Atari frame preprocessing  ters. This model achieved state-of-the-art modelling performance on standard datasets, paired with the computational efficiency of a convolutional feed-forward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-Step RL Methods</head><p>A distinguishing feature of reinforcement learning is that the agent "learns on the basis of interim estimates" <ref type="bibr">(Sutton, 1996)</ref>. For example, the Q-Learning update rule is</p><formula xml:id="formula_4">Q(x, a) ← Q(x, a) + α [r(x, a) + γ max a Q(x , a ) − Q(x, a)] δ(x,a)</formula><p>, linking the reward r and next-state value function Q(x , a ) to the current state value function Q(x, a). This particular form is the stochastic update rule with step-size α and involves the TD-error δ. In the approximate reinforcement learning setting, such as when Q(x, a) is represented by a neural network, this update is converted into a loss to be minimized, most commonly the squared loss δ 2 (x, a).</p><p>It is well known that better performance, both in terms of learning efficiency and approximation error, is attained by multi-step methods <ref type="bibr">(Sutton, 1996;</ref><ref type="bibr">Tsitsiklis &amp; van Roy, 1997)</ref>. These methods interpolate between one-step methods (Q-Learning) and the Monte-Carlo update</p><formula xml:id="formula_5">Q(x, a) ← Q(x, a) + α ∞ t=0 γ t r(x t , a t ) − Q(x, a) δMC(x,a)</formula><p>, where x 0 , a 0 , x 1 , a 1 , . . . is a sample path through the environment beginning in (x, a). To achieve their success on the hardest Atari 2600 games, <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref> used the mixed Monte-Carlo update (MMC)</p><formula xml:id="formula_6">Q(x, a) ← Q(x, a) + α [(1 − β)δ(x, a) + βδ MC (x, a)] ,</formula><p>with β ∈ [0, 1]. This choice was made for "computational and implementational simplicity", and is a particularly coarse multi-step method. A better multi-step method is the recent Retrace(λ) algorithm . Retrace(λ) uses a product of truncated importance sam-pling ratios c 1 , c 2 , . . . to replace δ with the error term</p><formula xml:id="formula_7">δ RETRACE (x, a) := ∞ t=0 γ t t s=1 c s δ(x t , a t ),</formula><p>effectively mixing in TD-errors from all future time steps. <ref type="bibr">Munos et al.</ref> showed that Retrace(λ) is safe (does not diverge when trained on data from an arbitrary behaviour policy), and efficient (makes the most of multi-step returns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Using PixelCNN for Exploration</head><p>As mentioned in the Introduction, the theory of using density models for exploration makes several assumptions that translate into concrete requirements for an implementation:</p><p>(a) The density model should be trained completely online, i.e. exactly once on each state experienced by the agent, in the given sequential order.</p><p>(b) The prediction gain (PG) should decay at a rate n −1 to ensure that pseudo-counts grow approximately linearly with real counts.</p><p>(c) The density model should be learning-positive.</p><p>Simultaneously, a partly competing set of requirements are posed by the practicalities of training a neural density model and using it as part of an RL agent:</p><p>(d) For stability, efficiency, and to avoid catastrophic forgetting in the context of a drifting data distribution, it is advantageous to train a neural model in minibatches, drawn randomly from a diverse dataset.</p><p>(e) For effective training, a certain optimization regime (e.g. a fixed learning rate schedule) has to be followed.</p><p>(f) The density model must be computationally lightweight, to allow computing the PG (two model evaluations and one update) as part of every training step of an RL agent.</p><p>We investigate how to best resolve these tensions in the context of the Arcade Learning Environment <ref type="bibr" target="#b1">(Bellemare et al., 2013)</ref>, a suite of benchmark Atari 2600 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Designing a Suitable Density Model</head><p>Driven by (f) and aiming for an agent with computational performance comparable to DQN, we design a slim variant of the PixelCNN network. Its core is a stack of 2 gated residual blocks with 16 feature maps (compared to 15 residual blocks with 128 feature maps in vanilla PixelCNN). As was done with the CTS model, images are downsampled to 42 × 42 and quantized to 3-bit greyscale. See Appendix A for technical details. Average Score Montezuma's Revenge LR: 0. 001, PG: 0. 1 · n −1/2 · PGn LR: 0. 001, PG: 0. 01 · PGn LR: 0. 1 · n −1/2 , PG: 0. 1 · n −1/2 · PGn LR: 0. 1 · n −1/2 , PG: 0. 01 · PGn <ref type="figure">Figure 2</ref>. Left: PixelCNN log loss on FREEWAY, when trained online, on a random permutation (single use of each frame) or on randomly drawn samples (with replacement, potentially using same frame multiple times) from the state sequence.  Montezuma's Revenge </p><formula xml:id="formula_8">c · n −1 c · n −1/2 c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training the Density Model</head><p>Instead of using randomized mini-batches, we train the density model completely online on the sequence of experienced states. Empirically we found that with minor tuning of optimization hyper-parameters we could train the model as robustly on a temporally correlated sequence of states as on a sequence with randomized order <ref type="figure">(Fig. 2(left)</ref>).</p><p>Besides satisfying the theoretical requirement (a), completely online training of the density model has the advantage that ρ n = ρ n+1 , so that the model update performed for computing the PG need not be reverted 1 . acteristics (e.g. different gradient magnitudes), invalidating the assumptions underlying the optimization algorithm and leading to slower or unstable training.</p><p>To determine a suitable online learning rate schedule, we train the model on a sequence of 1M frames of experience of a random-policy agent. We compare the loss achieved by training procedures following constant or decaying learning rate schedules, see <ref type="figure" target="#fig_3">Fig. 3</ref>. The lowest final training loss is achieved by a constant learning rate of 0.001 or a decaying learning rate of 0.1 · n −1/2 . We settled our choice on the constant learning rate schedule as it showed greater robustness with respect to the choice of initial learning rate.</p><p>PixelCNN rapidly learns a sensible distribution over state space. <ref type="figure">Fig. 2</ref>(left) shows the model's loss decaying as it learns to exploit image regularities. Spikes in its loss function quickly start to correspond to visually meaningful events, such as the starts of episodes ( <ref type="figure">Fig. 2(middle)</ref>). A video of early density model training is provided in http://youtu.be/T6iaa8Z4eyE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computing the Pseudo-Count</head><p>From the previous section we obtain a particular learning rate schedule that cannot be arbitrarily modified without deteriorating the model's training performance or stability. To achieve the required PG decay (b), we instead replace PG n by c n · PG n with a suitably decaying sequence c n .</p><p>In experiments comparing actual agent performance we empirically determined that in fact the constant learning rate 0.001, paired with a PG decay c n = c · n −1/2 , obtains the best exploration results on hard exploration games like MONTEZUMA'S REVENGE, see <ref type="bibr">Fig. 2(right)</ref>. We find the model to be robust across 1-2 orders of magnitude for the value of c, and informally determine c = 0.1 to be a sensible configuration for achieving good results on a broad range of Atari 2600 games (see also Section 7).</p><p>Regarding (c), it is hard to ensure learning-positiveness for a deep neural model, and a negative PG can occur whenever the optimizer 'overshoots' a local loss minimum. As a workaround, we threshold the PG value at 0. To summarize, the computed pseudo-count iŝ</p><formula xml:id="formula_9">N n (x) = exp c · n −1/2 · (PG n (x)) + − 1 −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Exploration in Atari 2600 Games</head><p>Having described our pseudo-count friendly adaptation of PixelCNN, we now study its performance on Atari games. To this end we augment the environment reward with a pseudo-count exploration bonus, yielding the combined reward r(x, a) + (N n (x)) −1/2 . As usual for neural networkbased agents, we ensure the total reward lies in [−1, 1] by clipping larger values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DQN with PixelCNN Exploration Bonus</head><p>Our first set of experiments provides the PixelCNN exploration bonus to a DQN agent <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref> 2 . At each agent step, the density model receives a single frame, with which it simultaneously updates its parameters and outputs the PG. We refer to this agent as DQN-PixelCNN.</p><p>The DQN-CTS agent we compare against is derived from the one in . For better comparability, it is trained in the same online fashion as DQN-PixelCNN, i.e. the PG is computed whenever we train the density model. By contrast, the original DQN-CTS queried the PG at the end of each episode.</p><p>Unless stated otherwise, we always use the mixed Monte Carlo update (MMC) for the intrinsically motivated agents 3 , but regular Q-Learning for the baseline DQN. 3 The use of MMC in a replay-based agent poses a minor complication, as the MC return is not available for replay until the end of an episode. For simplicity, in our implementation we disregard this detail and set the MC return to 0 for transitions from the most recent episode.  Overall PixelCNN provides the DQN agent with a larger advantage than CTS, and often accelerates or stabilizes training even when not affecting peak performance. Out of 57 Atari games, DQN-PixelCNN outperforms DQN-CTS in 52 games by maximum achieved score, and 51 by AUC (methodology in Appendix B). See <ref type="figure">Fig. 6</ref> for a high level comparison (appendix <ref type="figure" target="#fig_5">Fig. 15</ref> for full training graphs). The greatest gains from using either exploration bonus are observed in games categorized as hard exploration games in the 'taxonomy of exploration' in <ref type="bibr">(Bellemare et al., 2016, reproduced in Appendix D)</ref>, specifically in the most challenging sparse reward games (e.g. MONTEZUMA'S RE-VENGE, PRIVATE EYE, VENTURE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A Multi-Step RL Agent with PixelCNN</head><p>Empirical practitioners know that techniques beneficial for one agent architecture often can be detrimental for a different algorithm. To demonstrate the wide applicability of the PixelCNN exploration bonus, we also evaluate it with the more recent Reactor agent 4 <ref type="bibr" target="#b7">(Gruslys et al., 2017)</ref>. This replay-based actor-critic agent represents its policy and value function by a recurrent neural network and, crucially, uses the multi-step Retrace(λ) algorithm for policy evaluation, replacing the MMC we use in DQN-PixelCNN.</p><p>To reduce impact on computational efficiency of this agent, we sub-sample intrinsic rewards: we perform updates of the PixelCNN model and compute the reward bonus on (randomly chosen) 25% of all steps, leaving the agent's reward unchanged on other steps. We use the same PG decay schedule of 0.1n −1/2 , with n the number of model updates.  Training curves for the Reactor/Reactor-PixelCNN agent compared to DQN/DQN-PixelCNN are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. The baseline Reactor agent is superior to the DQN agent, obtaining higher scores and learning faster in about 50 out of 57 games. It is further improved on a large fraction of games by the PixelCNN exploration reward, see <ref type="figure">Fig. 8</ref> (full training graphs in appendix <ref type="figure">Fig. 16</ref>).</p><p>The effect of the exploration bonus is rather uniform, yielding improvements on a broad range of games. In particu- <ref type="bibr">4</ref> The exact agent variant is referred to as 'β-LOO' with β = 1.</p><p>lar, Reactor-PixelCNN enjoys better sample efficiency (in terms of area under the curve, AUC) than vanilla Reactor. We hypothesize that, like other policy gradient algorithms, Reactor generally suffers from weaker exploration than its value-based counterpart DQN. This aspect is much helped by the exploration bonus, boosting the agent's sample efficiency in many environments. However, on hard exploration games with sparse rewards, Reactor seems unable to make full use of the exploration bonus. We believe this is because, in very sparse settings, the propagation of reward information across long horizons becomes crucial. The MMC takes one extreme of this view, directly learning from the observed returns. The Retrace(λ) algorithm, on the other hand, has an effective horizon which depends on λ and, critically, the truncated importance sampling ratio. This ratio results in the discarding of trajectories which are off-policy, i.e. unlikely under the current policy. We hypothesize that the very goal of the Retrace(λ) algorithm to learn cautiously is what prevents it from taking full advantage of the exploration bonus!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Quality of the Density Model</head><p>PixelCNN can be expected to be more expressive and accurate than the less advanced CTS model, and indeed, samples generated after training are somewhat higher quality <ref type="figure" target="#fig_4">(Fig. 4)</ref>. However, we are not using the generative function of the models when computing an exploration bonus, and a better generative model does not necessarily give rise to better probability estimates <ref type="bibr">(Theis et al., 2016)</ref>. In <ref type="figure" target="#fig_9">Fig. 9</ref> we compare the PG produced by the two models throughout 5K training steps. PixelCNN consistently produces PGs lower than CTS. More importantly, its PGs are smoother, exhibiting less variance between successive states, while showing more pronounced peaks at certain infrequent events. This yields a reward bonus that is less harmful in easy exploration games, while providing a strong signal in the case of novel or rare events.</p><p>Another distinguishing feature of PixelCNN is its nondecaying step-size. The per-step PG never completely vanishes, as the model tracks the most recent data. This provides an unexpected benefit: the agent remains mildly surprised by significant state changes, e.g. switching rooms in MONTEZUMA'S REVENGE. These persistent rewards act as milestones that the agent learns to return to. This is illustrated in <ref type="figure">Fig. 10</ref>, depicting the intrinsic reward over the course of an episode. The agent routinely revisits the righthand side of the torch room, not because it leads to reward but just to "take in the sights". A video of the episode is provided at http://youtu.be/232tOUPKPoQ. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Steps</head><p>Intrinsic Reward <ref type="figure">Figure 10</ref>. Intrinsic reward in MONTEZUMA'S REVENGE.</p><p>Lastly, PixelCNN's convolutional nature is expected to be beneficial for its sample efficiency. In Appendix C we compare to a convolutional CTS and confirm that this explains part, but not all of PixelCNN's advantage over vanilla CTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Importance of the Monte Carlo Return</head><p>Like for DQN-CTS, the success of DQN-PixelCNN hinges on the use of the mixed Monte Carlo update. The transient and vanishing nature of the exploration rewards requires the learning algorithm to latch on to these rapidly. The MMC serves this end as a simple multi-step method, helping to propagate reward information faster. An additional benefit lies in the fact that the Monte Carlo return helps bridging long horizons in environments where rewards are far apart and encountered rarely. On the other hand, it is 5 Another agent video on the game PRIVATE EYE can be found at http://youtu.be/kNyFygeUa2E. important to note that the Monte Carlo return's on-policy nature increases variance in the learning algorithm, and can prevent the algorithm's convergence to the optimal policy when training off-policy. It can therefore be expected to adversely affect training performance in some games.</p><p>To distill the effect of the MMC on performance, we compare all four combinations of DQN with/without PixelCNN exploration bonus and with/without MMC. <ref type="figure" target="#fig_10">Fig. 11</ref> shows the performance of these four agent variants (graphs for all games are shown in <ref type="figure" target="#fig_7">Fig. 17</ref>). These games were picked to illustrate several commonly occurring cases:</p><p>• Most importantly, the situation is rather different when we restrict our attention to the hardest exploration games with sparse rewards. Here the baseline DQN agent fails to make any training progress, and neither Monte Carlo return nor the exploration bonus alone provide any significant benefit. Their combination however grants the agent rapid training progress and allows it to achieve high performance.</p><p>One effect of the exploration bonus in these games is to provide a denser reward landscape, enabling the agent to learn meaningful policies. Due to the transient nature of the exploration bonus, the agent needs to be able to learn from this reward signal faster than regular one-step methods allow, and MMC proves to be an effective solution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Pushing the Limits of Intrinsic Motivation</head><p>In this section we explore the idea of a 'maximally curious' agent, whose reward function is dominated by the exploration bonus. For that we increase the PG scale, previously chosen conservatively to avoid adverse effects on easy exploration games. <ref type="figure" target="#fig_12">Fig. 12</ref> shows DQN-PixelCNN performance on the hardest exploration games when the PG scale is increased by 1-2 orders of magnitude. The algorithm seems fairly robust across a wide range of scales: the main effect of increasing this parameter is to trade off exploration (seeking maximal reward) with exploitation (optimizing the current policy).</p><p>As expected, a higher PG scale translates to stronger exploration: several runs obtain record peak scores (900 in GRAVITAR, 6,600 in MONTEZUMA'S REVENGE, 39,000 in PRIVATE EYE, 1,500 in VENTURE) surpassing the state of the art by a substantial margin (for previously published results, see Appendix D). Aggressive scaling speeds up the agent's exploration and achieves peak performance rapidly, but can also deteriorate its stability and long-term performance. Note that in practice, because of the non-decaying step-size the PG does not vanish. After reward clipping, an overly inflated exploration bonus can therefore become essentially constant, no longer providing a useful intrinsic motivation signal to the agent.</p><p>Another way of creating an entirely curiosity-driven agent is to ignore the environment reward altogether and train based on the exploration reward only, see <ref type="figure" target="#fig_3">Fig. 13</ref>. Remarkably, the curiosity signal alone is sufficient to train a highperforming agent (measured by environment reward!).</p><p>It is worth noting that agents with exploration bonus seem to 'never stop exploring': for different seeds, the agents make learning progress at very different times during training, a qualitative difference to vanilla DQN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We demonstrated the use of PixelCNN for exploration and showed that its greater accuracy and expressiveness translate into a more useful exploration bonus than that obtained from previous models. While the current theory of pseudo- counts puts stringent requirements on the density model, we have shown that PixelCNN can be used in a simpler and more general setup, and can be trained completely online. It also proves to be widely compatible with both valuefunction and policy-based RL algorithms.</p><p>In addition to pushing the state of the art on the hardest exploration problems among the Atari 2600 games, Pixel-CNN improves speed of learning and stability of baseline RL agents across a wide range of games. The quality of its reward bonus is evidenced by the fact that on sparse reward games, this signal alone suffices to learn to achieve significant scores, creating a truly intrinsically motivated agent.</p><p>Our analysis also reveals the importance of the Monte Carlo return for effective exploration. The comparison with more sophisticated but fixed-horizon multi-step methods shows that its significance lies both in faster learning in the context of a useful but transient reward function, as well as bridging reward gaps in environments where extrinsic and intrinsic rewards are, or quickly become, extremely sparse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PixelCNN Hyper-parameters</head><p>The PixelCNN model used in this paper is a lightweight variant of the Gated PixelCNN introduced in (van den <ref type="bibr">Oord et al., 2016a)</ref>. It consists of a 7 × 7 masked convolution, followed by two residual blocks with 1×1 masked convolutions with 16 feature planes, and another 1×1 masked convolution producing 64 features planes, which are mapped by a final masked convolution to the output logits. Inputs are 42 × 42 greyscale images, with pixel values quantized to 8 bins.</p><p>The model is trained completely online, from the stream of Atari frames experienced by an agent. Optimization is performed with the (uncentered) RMSProp optimizer (Tieleman &amp; Hinton, 2012) with momentum 0.9, decay 0.95 and epsilon 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methodology</head><p>Unless otherwise stated, all agent performance graphs in this paper show the agent's training performance, measured as the undiscounted per-episode return, averaged over 1M environment frames per data point.</p><p>The algorithm-comparison graphs <ref type="figure">Fig. 6 and Fig. 8</ref> show the relative improvement of one algorithm over another in terms of area-under-the-curve (AUC). A comparison by maximum achieved score would yield similar overall results, but underestimate the advantage in terms of learning speed (sample efficiency) and stability that the intrinsically motivated and MMC-based agents show over the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convolutional CTS</head><p>In Section 4 we have seen that DQN-PixelCNN outperforms DQN-CTS in most of the 57 Atari games, by providing a more impactful exploration bonus in hard exploration games, as well as a more graceful (less harmful) one in games where the learning algorithm does not benefit from the additional curiosity signal. One may wonder whether this improvement is due to the generally more expressive and accurate density model PixelCNN, or simply its convolutional nature, which gives it an advantage in generalization and sample efficiency over a model that represents pixel probabilities in a completely location-dependent way.</p><p>To answer this question, we developed a convolutional variant of the CTS model. This model has a single set of parameters conditioning a pixel's value on its predecessors shared across all pixel locations, instead of the locationdependent parameters in the regular CTS. In <ref type="figure" target="#fig_4">Fig. 14</ref>  We first consider dense reward games like Q*BERT and ZAXXON, where most improvement comes from the use of the MMC, and the exploration bonus hurts performance. We find that in fact convolutional CTS behaves fairly similarly to PixelCNN, leaving agent performance unaffected, whereas regular CTS causes the agent to train more slowly or reach an earlier performance plateau. On the sparse reward games (GRAVITAR, PRIVATE EYE, VENTURE) however, convolutional CTS shows to be as inferior to Pixel-CNN as the vanilla CTS variant, failing to achieve the significant improvements over the baseline agents presented in this paper.</p><p>We conclude that while the convolutional aspect plays a role in the 'softer' nature of the PixelCNN model compared to its CTS counterpart, it alone is insufficient to explain the massive exploration boost that the PixelCNNderived reward provides to the DQN agent. The more advanced model's accuracy advantage translates into a more targeted and useful curiosity signal for the agent, which distinguishes novel from well-explored states more clearly and allows for more effective exploration. <ref type="table">Table 1</ref> reproduces <ref type="bibr" target="#b2">Bellemare et al. (2016)</ref>'s taxonomy of games available through the ALE according to their exploration difficulty. "Human-Optimal" refers to games where DQN-like agents achieve human-level or higher performance; "Score Exploit" refers to games where agents find ways to achieve superhuman scores, without necessarily playing the game as a human would. "Sparse" and "Dense" rewards are qualitative descriptors of the game's reward structure. See the original source for additional details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Hardest Exploration Games</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Model loss averaged over 10K frames, after 1M training frames, for constant, n −1 , and n −1/2 learning rate schedules. The smallest loss is achieved by a constant learning rate of 10 −3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Samples after 25K steps. Left: CTS, right: PixelCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>shows training curves of DQN compared to DQN-2 Unlike Bellemare et al. we use regular Q-Learning instead of Double Q-Learning (van Hasselt et al., 2016), as our early experiments showed no significant advantage of DoubleDQN with the PixelCNN-based exploration reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>DQN, DQN-CTS and DQN-PixelCNN on hard exploration games (top) and easier ones (bottom). Improvements (in % of AUC) of DQN-PixelCNN and DQN-CTS over DQN in 57 Atari games. Annotations indicate the number of hard exploration games with positive (right) and negative (left) improvement, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Reactor/Reactor-PixelCNN and DQN/DQN-PixelCNN training performance (averaged over 3 seeds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>-PixelCNN over Reactor (by AUC) Easy exploration (40) Hard exploration, dense reward (10) Hard exploration, sparse reward (7) Figure 8. Improvements (in % of AUC) of Reactor-PixelCNN over Reactor in 57 Atari games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>PG on MONTEZUMA'S REVENGE (log scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Top: games where MMC completely explains the improved/decreased performance of DQN-PixelCNN compared to DQN. Bottom-left: MMC and PixelCNN show additive benefits. Bottom-right: hard exploration, sparse reward game -only combining MMC and PixelCNN bonus achieves training progress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>MMC speeds up training and improves final performance significantly (examples: BANK HEIST, TIME PILOT). In these games, MMC alone explains most or all of the improvement of DQN-PixelCNN over DQN. • MMC hurts performance (examples: MS. PAC-MAN, BREAKOUT). Here too, MMC alone explains most of the difference between DQN-PixelCNN and DQN. • MMC and PixelCNN reward bonus have a compounding effect (example: H.E.R.O.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>DQN-PixelCNN, hard exploration games, different PG scales c · n −1/2 · PGn (c = 0.1, 1, 10) (5 seeds each).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>DQN-PixelCNN trained from intrinsic reward only (3 seeds for each configuration).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .Figure 16 .</head><label>1516</label><figDesc>Training curves of DQN, DQN-CTS and DQN-PixelCNN across all 57 Atari games. Training curves of DQN, DQN-PixelCNN, Reactor and Reactor-PixelCNN across all 57 Atari games. (w/o MC) DQN-PixelCNN (with MC) Figure 17. Training curves of DQN and DQN-PixelCNN, each with and without MMC, across all 57 Atari games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1. To what extent does a better density model give rise to better exploration? 2. Can the above modelling assumptions be relaxed without sacrificing exploration performance? 3. What role does the mixed Monte Carlo update play in successfully incentivizing exploration?</figDesc><table /><note>arXiv:1703.01310v2 [cs.AI] 14 Jun 2017</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CTS and DQN-PixelCNN. On the famous MONTEZUMA'S REVENGE, both intrinsically motivated agents vastly outperform the baseline DQN. On other hard exploration games (PRIVATE EYE; or VENTURE, appendixFig. 15), DQN-PixelCNN achieves state of the art results, substantially outperforming DQN and DQN-CTS. The other two games shown (ASTEROIDS, BERZERK) pose easier exploration problems, where the reward bonus should not provide large improvements and may have a negative effect by skewing the reward landscape. Here, DQN-PixelCNN behaves more gracefully and still outperforms DQN-CTS. We hypothesize this is due to a qualitative difference between the models, see Section 5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Filip, and Abbeel, Pieter. #Exploration: A study of count-based exploration for deep reinforcement learning. arXiv preprint arXiv:1611.04717, 2016.Theis, Lucas, van den Oord, Aäron, and Bethge, Matthias.A note on the evaluation of generative models. In Proceedings of the International Conference on Learning Representations, 2016.</figDesc><table><row><cell cols="2">Tieleman, Tijmen and Hinton, Geoffrey. RMSProp: divide</cell></row><row><cell cols="2">the gradient by a running average of its recent magni-</cell></row><row><cell cols="2">tude. COURSERA. Lecture 6.5 of Neural Networks for</cell></row><row><cell>Machine Learning, 2012.</cell><cell></cell></row><row><cell cols="2">Tsitsiklis, John N. and van Roy, Benjamin. An analysis of</cell></row><row><cell cols="2">temporal-difference learning with function approxima-</cell></row><row><cell cols="2">tion. IEEE Transactions on Automatic Control, 42(5):</cell></row><row><cell>674-690, 1997.</cell><cell></cell></row><row><cell cols="2">van den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse,</cell></row><row><cell cols="2">Vinyals, Oriol, Graves, Alex, et al. Conditional image</cell></row><row><cell cols="2">generation with PixelCNN decoders. In Advances in</cell></row><row><cell cols="2">Neural Information Processing Systems, 2016a.</cell></row><row><cell cols="2">van den Oord, Aaron, Kalchbrenner, Nal, and</cell></row><row><cell>Kavukcuoglu, Koray.</cell><cell>Pixel recurrent neural net-</cell></row><row><cell cols="2">works. In Proceedings of the International Conference</cell></row><row><cell cols="2">on Machine Learning, 2016b.</cell></row><row><cell cols="2">van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep</cell></row><row><cell cols="2">reinforcement learning with Double Q-learning. In Pro-</cell></row><row><cell cols="2">ceedings of the AAAI Conference on Artificial Intelli-</cell></row><row><cell>gence, 2016.</cell><cell></cell></row><row><cell cols="2">Veness, Joel, Ng, Kee Siong, Hutter, Marcus, and Bowling,</cell></row><row><cell cols="2">Michael H. Context tree switching. In Proceedings of</cell></row><row><cell cols="2">the Data Compression Conference, 2012.</cell></row><row><cell cols="2">Wang, Ziyu, Schaul, Tom, Hessel, Matteo, van Hasselt, Hado, Lanctot, Marc, and de Freitas, Nando. Dueling network architectures for deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1995-2003, 2016.</cell><cell>Rezende, Danilo Jimenez, Mohamed, Shakir, and Wier-stra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The International Conference on Machine Learning, 2014.</cell></row><row><cell></cell><cell></cell><cell>Strehl, Alexander L. and Littman, Michael L. An analysis</cell></row><row><cell></cell><cell></cell><cell>of model-based interval estimation for Markov decision</cell></row><row><cell></cell><cell></cell><cell>processes. Journal of Computer and System Sciences, 74</cell></row><row><cell></cell><cell></cell><cell>(8):1309 -1331, 2008.</cell></row><row><cell></cell><cell></cell><cell>Sutton, Richard S. Generalization in reinforcement learn-</cell></row><row><cell></cell><cell></cell><cell>ing: Successful examples using sparse coarse coding.</cell></row><row><cell></cell><cell></cell><cell>In Advances in Neural Information Processing Systems,</cell></row><row><cell></cell><cell></cell><cell>1996.</cell></row></table><note>Sutton, Richard S. and Barto, Andrew G. Reinforcement learning: An introduction. MIT Press, 1998. Tang, Haoran, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, De Turck,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 Table 2 .</head><label>22</label><figDesc>Comparison with previously published results on hard exploration, sparse reward games. The compared agents are DQN (</figDesc><table><row><cell>compares previously published results on the 7 hard</cell></row><row><cell>exploration, sparse reward Atari 2600 games with results</cell></row><row><cell>obtained by DQN-CTS and DQN-PixelCNN.</cell></row></table><note>Mnih et al., 2015), A3C-CTS ("A3C+" in (Bellemare et al., 2016)), Prioritized Dueling DQN (Wang et al., 2016), and the basic versions of DQN-CTS and DQN-PixelCNN from Section 4. For our agents we report the maximum scores achieved over 150M frames of training, averaged over 3 seeds.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">DeepMind, London, UK. Correspondence to: Georg Ostrovski &lt;ostrovski@google.com&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Tom Schaul, Olivier Pietquin, Ian Osband, Sriram Srinivasan, Tejas Kulkarni, Alex Graves, Charles Blundell, and Shimon Whiteson for invaluable feedback on the ideas presented here, and Audrunas Gruslys especially for providing the Reactor agent.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skip context tree switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Talvitie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yavar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PILCO: A model-based and data-efficient approach to policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Reactor: A sample-efficient actor-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gheshlaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04651</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient bayes-adaptive reinforcement learning using samplebased search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nearoptimal regret bounds for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1563" to="1600" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploration in model-based reinforcement learning by empirically estimating learning progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intrinsic motivation systems for autonomous mental development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><forename type="middle">V</forename><surname>Hafner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
