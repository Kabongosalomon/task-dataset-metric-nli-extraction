<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Graph Representation Learning for Video Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>201X</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>El</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Qi</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Adaptive Graph Representation Learning for Video Person Re-identification</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="issue">1</biblScope>
							<date type="published">201X</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video Person Re-Identification</term>
					<term>Graph Neural Network</term>
					<term>Consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed the remarkable progress of applying deep learning models in video person reidentification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolutionaware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As an important and challenging problem in computer vision, person re-identification (Re-ID) aims at precisely retrieving the same identities from the gallery with a person of interest as a query given, and it has a wide range of applications in intelligent surveillance and video analysis <ref type="bibr" target="#b0">[1]</ref>. Typically, person Re-ID is carried out in the domain of individual images without capturing the temporal coherence information. More recently, several video person Re-ID approaches <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b17">[18]</ref>   <ref type="figure">Fig. 1</ref>: Overview of graph construction in our proposed method. 1) The feature map from each frame is processed by a pyramid pooling module, the extracted regional features are treated as the graph nodes.</p><p>2) The pose alignment adjacency graph A p (colorful solid line) is constructed by connecting the regions containing the same human part.</p><p>3) The feature affinity adjacency graph A f (yellow dotted line) is constructed by measuring the affinity of regional features. 4) The adaptive structure-aware adjacency graph is built by combining two graphs. Best viewed in color, some graph edges are omitted for clarity.</p><p>emerge to directly perform the person context modeling at the video level, which is more fit for practical use with more visual cues exploited for coping with complicated circumstances. In the literature, most existing methods for video person Re-ID first extract the feature vectors frame by frame and generate the video-level feature representation by temporal aggregation, then compare them in a particular metric space. Although recent deep learning based methods have made notable progress, re-ID problem remains challenging due to arXiv:1909.02240v2 [cs.CV] 12 Jun 2020 occlusion, viewpoints, illumination, and pose variation in the video. To address these issues, recent studies <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> concentrate on aggregating the features from image regions with attention mechanism. However, under the circumstances of complicated situations (e.g. occlusion and pose variations), these approaches are often incapable of effectively utilizing the intrinsic relations between person parts across frames, which play an important role in learning robust video representations. For instance, if the body parts are occluded in the first frame, the appearance cues and contextual information from the other frames are complementary. Hence, how to adaptively perform relation modeling and contextual information propagation among spatial regions is a key issue to solve in video person Re-ID.</p><p>Motivated by the aforementioned observations, we propose an adaptive graph learning scheme to model the contextual relations and propagate complementary information simultaneously. As shown in <ref type="figure">Figure 1</ref>, we construct two kinds of relations named pose alignment connection and feature affinity connection between the spatiotemporal regions. Specifically, 1) pose alignment connection: regions containing the same part are connected to align the spatial regions across the frames. With the pose alignment connection, we are capable of capturing the relations between human body parts; and 2) feature affinity connection: we define the edges by measuring the visual correlations of extracted regional features. With the feature affinity connection, we can model the visually semantic relationships between regional features precisely.</p><p>By combining these two complementary relation connections, we obtain an adaptive structure-aware adjacency graph. Then we capture the contextual interactions on the graph structure via graph neural network (GNN). The effective and discriminative messages aggregated from neighbors are used to refine the original regional features. With feature propagation, the discrimination of the informative regional features is enhanced and the noisy parts are weakened.</p><p>Based on the observations in <ref type="bibr" target="#b20">[21]</ref>, the visual cues in the video are rich yet possibly redundant, and some keyframes are sufficient to represent the long-range video. As a consequence, we propose a novel regularization, which enforces the consistency among different temporal resolutions, to learn the temporal resolution invariant representation. Specifically, the frame-level features are randomly selected as the subsequences, and input into the attention module, then the output video representations are enforced to be close to each other in the metric space.</p><p>Overall, the main contributions of this work are summarized as follows:</p><p>• We propose an adaptive structure-aware spatiotemporal graph representation based on two types of graph connections for relation modeling: pose alignment connection and feature affinity connection. By combining these two relation connections, the adaptive structure-aware graph representation is capable of well capturing the semantic relations between regions across frames. • We propose a novel regularization to learn the temporal resolution invariant representation, which is compact and captures the discriminative information in the sequence.</p><p>We conduct extensive experiments on four widely used benchmarks (i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID), and experimental results demonstrate the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Person Re-ID. Person Re-ID in still images is widely explored <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b31">[32]</ref>. Currently, the researchers start to focus on video-based person Re-ID <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Facilitated by deep learning techniques, impressive progress has been observed with video person Re-ID recently. McLaughlin et al. <ref type="bibr" target="#b1">[2]</ref> and Yan et al. <ref type="bibr" target="#b32">[33]</ref> employ RNN to model the inter-sequence dependency and aggregate the features extracted from the video frames with average pooling or max pooling. Zhou et al. <ref type="bibr" target="#b2">[3]</ref> separately model the spatial and temporal coherence with two RNN networks: the temporal model (TAM) focuses on discriminative frames and spatial model (SRM) integrates the contexture at different locations for better similarity evaluation. Wu et al. <ref type="bibr" target="#b19">[20]</ref> extend GRU with attention mechanism to selectively propagate relevant features and memorize their spatial dependencies through the network. Dai et al. <ref type="bibr" target="#b8">[9]</ref> proposes a S 2 T N network to address the pose alignment and combine the bi-directional LSTM with residual learning to perform temporal residual learning.</p><p>Recently, the attention networks are widely studied for temporal feature fusion. In <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, the discriminative frames are selected with attention temporal pooling, where each frame is assigned with a quality score and then fused to a final video representation. Similarly, Zhang et al. <ref type="bibr" target="#b20">[21]</ref> employ reinforcement learning to train an agent to verify whether the pair of images are the same or different, and the Q value is a good indicator of the difficulty of image pairs. In <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, the authors extend the temporal attention to spatiotemporal attention to select informative regions and achieve the impressive improvements. Chen et al. <ref type="bibr" target="#b11">[12]</ref> leverage the body joints to attend to the saliency parts of the person in the video to extract the discriminative local features in a siamese network. And in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, the video representation is generated by considering not only intra-sequence influence but also inter-sequence mutual information. Different from the previous 2D CNN based methods, 3D convolution neural network (3D CNN) is also adopted to address the video person Re-ID <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Wu et al. <ref type="bibr" target="#b9">[10]</ref> adopt 3D CNN and 3D pooling to aggregate the spatial and temporal cues simultaneously. Li et al. <ref type="bibr" target="#b13">[14]</ref> propose a variant of ResNet by inserting multi-scale 3D (M3D) layer and residual attention layer (RAL) into the ResNet. Similarly, Liu et al. <ref type="bibr" target="#b15">[16]</ref> incorporate nonlocal modules with ResNet50 as the Non-local Video Attention Network (NVAN) and propose a spatially and temporally efficient variant. Moreover, attributions are utilized to generate the confidence as the weight for sub-features extracted from video frames in <ref type="bibr" target="#b33">[34]</ref>. The generative models are adopted to address the occlusion and pose variant in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Graph Models. Graph models are utilized in several computer vision tasks, and Graph Neural Networks (GNN) is introduced in <ref type="bibr" target="#b34">[35]</ref> to model the relations between graph nodes, and a large number of the variants <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b40">[41]</ref> are proposed. In</p><formula xml:id="formula_0">_~4 _ _ × × 3 × × … ( −1) ( −1) (⋅) (⋅) ( ) + ( ) ( ) …. × × × × … … … … 2 c c Fig. 2:</formula><p>The overall architecture of our proposed method. 1) T frames are sampled from a long-range video with a restricted random sampling method. 2) In graph branch, for the output of each image, pyramid pooling is used to extract the N × ddimension feature, where N represents the number of regions, the feature vector for each region has d dimensions.</p><p>3) The extracted feature vectors are treated as the graph nodes, we then employ GNN to perform feature propagation on the graph iteratively in the graph feature propagation module. 4) We carry out the attention module to generate the discriminative video representation, the subsequences are randomly selected and forward into the attention module to learn a consistent video representation. 5) Feature vectors from graph branch and global branch are concatenated for testing.</p><p>recent, Re-ID methods <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b45">[46]</ref> combined with graph models are also explored. Cheng et al. <ref type="bibr" target="#b41">[42]</ref> formulate the structured distance relationships into the graph Laplacian form to take advantage of the relationships among training samples. In <ref type="bibr" target="#b42">[43]</ref>, an algorithm that maps the ranking process to a problem in graph theory is proposed. Shen et al. <ref type="bibr" target="#b43">[44]</ref> leverage the similarities between different probe-gallery pairs for updating the features extracted from images. Chen et al. <ref type="bibr" target="#b44">[45]</ref> involves multiple images to model the relationships among the local and global similarities in a unified CRF. Yan et al. <ref type="bibr" target="#b45">[46]</ref> formulate the person search as a graph matching problem, and solve it by considering the context information in the probegallery pairs. To address the unsupervised Re-ID problem, Ye et al. <ref type="bibr" target="#b46">[47]</ref> involves the graph matching into an iteratively updating procedure for a robust label estimation.</p><p>As a closely related problem, there are several graphbased methods <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref>. Connie et al. <ref type="bibr" target="#b48">[49]</ref> propose to learn Grassmannian graph embedding by constraining the geometry of the manifold. In <ref type="bibr" target="#b47">[48]</ref>, cross wavelet transform and bipartite graph model are used to extract the dynamic and static features respectively. Our proposed method is similar to <ref type="bibr" target="#b49">[50]</ref>, which combines the Graph Attention Network (GAT) with the feature extractor to discover the relationship between frames and the variation of a region in the temporal domain, while its region is based on the strong activated point in the feature map and the weight matrix is learned without prior knowledge.</p><p>In a nutshell, the graph model based methods in Re-ID usually build up a graph to represent the relationships among training samples, where the graph nodes are images or videos. While in our proposed method, the graph is dynamically learned with prior knowledge to model the intrinsic contextual relationships among the regions in an image sequence, the local, global, and structure information are propagated among the different regional features to learn the discriminative video feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Video person Re-ID aims to retrieve the identities from the gallery with the given queries. The overall architecture of our proposed method is illustrated in <ref type="figure">Figure 2</ref>. Given a longrange video for the specific identity, T frames are randomly sampled with a restricted sampling method <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and then grouped as an image sequence {I t } t=1,...,T . We first feed them into the ResNet50-based <ref type="bibr" target="#b51">[52]</ref> feature extractor module, in which the stride of the first residual block in conv5 is set to 1. In the global branch, 3D global average pooling is used for the feature maps and produces a video representation x gap ∈ R d . In the graph branch, we obtain regional features <ref type="bibr" target="#b23">[24]</ref>, where the feature maps are vertically partitioned into 1, 2, and 4 regions 1 in our experiments, and N = 7 is the number of regions for an individual frame. Then, we utilize the pose information and feature affinity to construct an adaptive structure-aware adjacency graph, which captures the intrinsic relations between these regional features. In the graph feature propagation module, the regional features are updated iteratively by aggregating the contextual information from neighbors on the graph. Next, we utilize the attention module to yield the video representations. The network is supervised by identification loss and triplet ranking loss together. We will discuss these modules in the following sections.</p><formula xml:id="formula_1">X = {x i } T ·N i=1 with pyramid pooling</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Graph Feature Propagation</head><p>As discussed in Section I, the relations between human parts are beneficial for mitigating the impact of complex situations such as occlusion and clutter background. So, how to describe the relationships between different human parts and propagate contextual messages is critical for learning the discriminative video representations. The graph is commonly used to model this kind of relations, and we adopt GNN to leverage the information from the neighborhood.</p><formula xml:id="formula_2">_~4 × × 3 × × ( −1) ( −1) ( −1) ( −1) (⋅) (⋅) (⋅) ( ) + ( ) ( ) …. × × × × … … … …</formula><p>Adaptive Structure-Aware Adjacency Graph. To depict the relations of human parts, we employ the pose information and feature affinity to construct an adaptive structure-aware</p><formula xml:id="formula_3">adjacency graph G = {V, A}. V = {v i } T ·N i=1</formula><p>is the vertex set containing T · N nodes, where each node v i corresponds to a spatial region in the frame. To define the edge A ∈ R (T ·N )×(T ·N ) on the graph, we introduce two types of relations: pose alignment connection and feature affinity connection.</p><p>The pose alignment connection is defined by leveraging the human body joints: two regions (nodes) are connected if they contain the same human parts. Formally, we define a set S i for each region v i where S i ⊆ {head, trunk, leg}. The pose alignment adjacency graph A p for the two nodes v i and v j is then calculated as follows:</p><formula xml:id="formula_4">A p ij = 1 i = j and |S i ∩ S j | = 0, 0 otherwise,<label>(1)</label></formula><p>where | · | means the cardinality of a set. We obtain S i with the following procedure, first, we locate the joints of the human body by making use of a human pose estimation algorithm, this is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. Then we separate these estimated keypoints into three parts: head part (consist of nose, neck, eyes, and ears), trunk part (consist of shoulders, bows, and wrists), and leg part (consist of hips, knees, and ankles). For the i-th spatial region, S i is constitutive of the parts located in this spatial region. We present an example in <ref type="figure" target="#fig_0">Figure 3</ref>, the feature maps are vertically partitioned into 4 regions B 1 , B 2 , B 3 , B 4 . In image I 1 and I 2 , the head part is in B 1 and B 2 respectively, so the pose alignment connection between these two nodes is set to 1. Then, we can create the pose alignment adjacency graph A p . Pose alignment adjacency connection reflects only the coarse relations between different spatial regions and the recent method <ref type="bibr" target="#b52">[53]</ref> shows the dynamic graph could learn better graph representations compared with the fixed graph structure.</p><formula xml:id="formula_5">( −1) ( −1) ( −1) ( −1) (⋅) (⋅) (⋅) ( ) + ( ) ( ) …. × × × × … … … …</formula><p>To describe the fine relations between the regions, we propose to learn an adaptive feature affinity adjacency graph A f , which aims to capture the affinity between the regions. For two nodes v i and v j , the node features are x i and x j respectively, then the entry of adjacency graph A f is formulated as follows:</p><formula xml:id="formula_6">A f ij =S(x i , x j ) = 2 e xi−xj 2 + 1 .<label>(2)</label></formula><p>We calculate the edge weight matrix A by combining the pose alignment adjacency matrix and feature affinity matrix:</p><formula xml:id="formula_7">A ij = 1 1 + γ ( A p ij j A p ij + γ A f ij j A f ij ),<label>(3)</label></formula><p>where γ is the weight parameter to balance the pose alignment adjacency matrix and the feature affinity matrix, and γ is set to 1 in our all experiments.</p><p>Graph Feature Propagation Module. After obtaining the graph, we perform contextual message propagation to update original spatial regional features iteratively. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, we employ Graph Neural Network (GNN) <ref type="bibr" target="#b34">[35]</ref> to aggregate the information from neighbors for each node. In the graph feature propagation module, we stack L graph feature propagation layers, in the l-th layer, the aggregation and updating operations are defined as follows:</p><formula xml:id="formula_8">x (l) i = (1 − α)x (l−1) i + α T ·N j=1 A (l) ij F (l) (x (l−1) j )<label>(4)</label></formula><p>where i ∈ {1, 2, . . . , T · N }, l ∈ {1, 2, . . . , L}, and x (l) i stands for the refined regional feature output from l-th feature propagation layer and x (0) i = x i is the original node feature, F (l) (·) is the combination of an FC-layer and batch normalization layer to encode the contextual messages from neighbors, A (l) refers to the adaptive structure-aware adjacency graph, and α is used to balance the aggregated feature and original feature, which is set as 0.1 in our experiments. The output from graph feature propagation module is denoted asX = [x 1 ,x 2 , . . . ,x T ·N ], wherex i ∈ R d is the updated regional feature vector. Temporal Attention Module Given the updated regional featuresX, we perform a simple yet effective spatio-temporal attention <ref type="bibr" target="#b14">[15]</ref> to obtain the video representation, which is calculated as:</p><formula xml:id="formula_9">x graph = T ·N i=1 x i 1 j x j 1x i .<label>(5)</label></formula><p>As discussed in Section I, in order to model the consistency of subsequences, we randomly select T − i frames from the image sequence, where i = 1, 2, . . . , T s . And then feed the feature vectors of these subsequence frames into the temporal attention module to obtain video representations x graph,1 , . . . , x graph,Ts . To keep the consistency of different subsequences, we enforce these video representations to be close to each other in the metric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Functions</head><p>We employ two kinds of losses to jointly supervise the training of parameters: cross entropy loss and soft hard triplet loss <ref type="bibr" target="#b53">[54]</ref>, the losses are formulated as follows:</p><formula xml:id="formula_10">L xent (x) = − 1 P · K P i=1 K a=1 log[ e Wy i,a xi,a P ·K c=1 e Wc xi,a ],<label>(6)</label></formula><formula xml:id="formula_11">L htri (x) = P i=1 K a=1 ln(1+exp( hardest positive max p=1,··· ,K D(x i,a , x i,p ) − min n=1,...,K j=1,...,P j =i D(x i,a , x j,n ) hardest negative )),<label>(7)</label></formula><p>where P and K are respectively the number of identities and sampled images of each identity. So there are P · K images in a mini-batch, x i,a , x i,p and x j,n are the features extracted from the anchor, positive and negative samples respectively, D(·) is the L2-norm distance for two feature vectors. For the output from global branch, we have two losses l global xent and l global htri :</p><formula xml:id="formula_12">l global xent = L xent ([BN (x (1) gap ), . . . , BN (x (P ·K) gap ])),<label>(8)</label></formula><formula xml:id="formula_13">l global htri = L htri ([x (1) gap , . . . , x (P ·K) gap ]),<label>(9)</label></formula><p>for the output from graph branch, we have two losses l graph xent and l graph htri similarily:</p><formula xml:id="formula_14">l graph xent = L xent ([BN (x (1) graph ), . . . , BN (x (1) graph,Ts ), . . . , BN (x (P ·K) graph ), . . . , BN (x (P ·K) graph,Ts )]),<label>(10)</label></formula><p>l graph htri = L htri ([x <ref type="bibr" target="#b0">(1)</ref> graph , x </p><p>where BN (·) is the BNNeck introduced in <ref type="bibr" target="#b54">[55]</ref>, [·] means concatenation. The total loss is the summation of the four losses:</p><p>l total = l global xent + l global htri + l graph xent + l graph htri <ref type="bibr" target="#b11">(12)</ref> IV. EXPERIMENTS A. Datasets PRID2011 <ref type="bibr" target="#b55">[56]</ref> dataset consists of person videos from two camera views, containing 385 and 749 identities, respectively. Only the first 200 people appear in both cameras. The length of the image sequence varies from 5 to 675 frames, but we use only the sequences whose frame number is larger than 21.</p><p>iLIDS-VID <ref type="bibr" target="#b56">[57]</ref> dataset consists of 600 image sequences of 300 persons. For each person, there are two videos with the sequence length ranging from 23 to 192 frames with an average duration of 73 frames.</p><p>MARS dataset <ref type="bibr" target="#b57">[58]</ref> is the largest video-based person reidentification benchmark with 1,261 identities and around 20,000 video sequences generated by DPM detector and GMMCP tracker. The dataset is captured by six cameras, each identity is captured by at least 2 cameras and has 13.2 sequences on average. There are 3,248 distracter sequences in the dataset, it increases the difficulty of Re-ID.</p><p>DukeMTMC-VideoReID dataset is another large scale benchmark dataset for video-based person Re-ID, which is derived from DukeMTMC dataset <ref type="bibr" target="#b58">[59]</ref> and re-organized by Wu et al. <ref type="bibr" target="#b59">[60]</ref>. DukeMTMC-VideoReID dataset contains a total of 4,832 tracklets and 1,812 identities, it is separated into 702, 702, and 408 identities for training, testing, and distraction. In total, it has 369,656 frames of 2,196 tracklets for training and 445,764 frames of 2,636 tracklets for testing and distraction. Each tracklet has 168 frames on average, and the bounding boxes are annotated manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>For evaluation, we employ the standard metrics used in person Re-ID literature: cumulative matching characteristic (CMC) curve and mean average precision (mAP). CMC curve judges the ranking capabilities of the Re-ID model, mAP reflects the true ranking results while multiple ground-truth sequences exist. For PRID2011 and iLIDS-VID datasets, we follow the evaluation protocol used in <ref type="bibr" target="#b55">[56]</ref>. Each dataset is divided into two parts for training and testing, the final accuracy is the average of "10-fold cross validation", only CMC accuracy is reported in PRID2011 and iILIDS-VID because of the equivalence of CMC and mAP on these two datasets. For MARS and DukeMTMC-VideoReID dataset, both CMC and mAP are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Settings. Our experiments are implemented with Pytorch and four TITAN X GPUs. ResNet50 <ref type="bibr" target="#b51">[52]</ref> is first pre-trained on ImageNet, and the input images are all resized to 256 × 128. In the training stage, we employ a restricted random sampling strategy <ref type="bibr" target="#b5">[6]</ref> to randomly sample T = 8 frames from every video and group them into a tracklet. We update the parameters by employing ADAM <ref type="bibr" target="#b60">[61]</ref> with a learning rate of 1×10 −4 and weight decay of 5×10 −4 . We train the network for 300 epochs, the learning rate decays to <ref type="bibr" target="#b0">1</ref> 10 every 100 epochs. For batch hard triplet loss, we set P = 4 and K = 4 in our experiments. In the temporal attention module, T s is set as 3. In the testing stage, cosine distance between the representations is calculated for ranking, a video containing T v frames is split into T chunks firstly, then we make use of two kinds of strategies: 1) the first image is collected as an image sequence to represent this video; 2) In each chunk, i-th frames are grouped as an image sequence, we can obtain Tv T image sequences, and the video representations are averaged as a single video representation. In our experiments, the first strategy is fast and the second strategy is more accurate.</p><p>Pose Estimation. In our implementation, the pose information is obtained by AlphaPose <ref type="bibr" target="#b31">[32]</ref> before the training. For the cases that multiple persons exist in the frame, we get the pose following the criterion: 1) The bounding box for the correct identity is bigger than the other persons;</p><p>2) The correct identity is in the center of the image, here we use the detected center point to represent the person;</p><p>3) The average confidence score of the keypoints for the correct identity is higher than the others. Then we define the body parts heuristically based on the pose information, and we construct the pose alignment graph which builds up the relations between the vertically partitioned regional feature vectors. Combine the pose alignment graph with the feature affinity graph, we refine the feature vectors in the feature propagation module and utilize the refined feature vectors for Re-ID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-art Methods</head><p>To validate the effectiveness of our proposed method, we compare our proposed method with several state-of-the-art methods on PRID2011, iLIDS-VID, MARS, and DukeMTMC-VideoReID, include RCN <ref type="bibr" target="#b1">[2]</ref>, IDE+XQDA <ref type="bibr" target="#b57">[58]</ref>, RFA-Net <ref type="bibr" target="#b32">[33]</ref>, SeeForest <ref type="bibr" target="#b2">[3]</ref>, QAN <ref type="bibr" target="#b4">[5]</ref>, AMOC+EF <ref type="bibr" target="#b61">[62]</ref>, ASTPN <ref type="bibr" target="#b3">[4]</ref>, Snippet <ref type="bibr" target="#b6">[7]</ref>, STAN <ref type="bibr" target="#b5">[6]</ref>, EUG <ref type="bibr" target="#b59">[60]</ref>, SDM <ref type="bibr" target="#b20">[21]</ref>, RQEN <ref type="bibr" target="#b18">[19]</ref>, PersonVLAD <ref type="bibr" target="#b9">[10]</ref>, M3D <ref type="bibr" target="#b13">[14]</ref>, STMP <ref type="bibr" target="#b12">[13]</ref>, STA <ref type="bibr" target="#b14">[15]</ref>, TRL+XQDA <ref type="bibr" target="#b8">[9]</ref>, SCAN <ref type="bibr" target="#b10">[11]</ref>, STAL <ref type="bibr" target="#b11">[12]</ref>, and STE-NVAN <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr">Method</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARS R1</head><p>R5 R20 mAP IDE+XQDA <ref type="bibr" target="#b57">[58]</ref> 65    <ref type="table" target="#tab_2">Table I and Table II</ref>, it is not difficult to find that our proposed method outperforms the existing approaches. On MARS dataset, our proposed method surpasses the previous best approach STE-NVAN <ref type="bibr" target="#b15">[16]</ref> by 0.6% and 0.7% in terms of Rank-1 and mAP. And on DukeMTMC-VideoReID, our method achieves the best performance with 97.0% and 95.4% at Rank-1 and mAP accuracy respectively. The results outperform the state-of-the-art STA <ref type="bibr" target="#b14">[15]</ref> by a large margin. These experimental results confirm the effectiveness and superiority of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MARS and DukeMTMC-VideoReID From</head><p>Results on iLIDS-VID and PRID2011 As shown in <ref type="table" target="#tab_2">Table III</ref>, results demonstrate the advantages of the proposed method over existing state-of-the-art approaches on  iLIDS-VID and PRID2011. Specifically, our proposed method achieves the Rank-1 accuracy of 84.5% and 94.6% on these two datasets, and surpass all the previous approaches without incorporating optical flow. On these two small-scale datasets, by comparing Snnipet and Snippet+OF, we can observe that the motion information provides more reliable features than appearance cues. While even compared with those methods utilizing optical flow, the results of our proposed method are also competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>To analyze the effectiveness of components in our proposed method, we conduct several experiments on MARS dataset. The experimental results are summarized in <ref type="table" target="#tab_2">Table V and  Table IV</ref>    <ref type="table" target="#tab_2">Table IV</ref>, we find out that the performance is consistent in these settings, where Rank-1 accuracies are all above 89.0% and mAP accuracies are all above 80.5%. In addition, the performance with K = 2 surpasses the other settings, so we stack 2 propagation layers in our experiments. Analysis on components. In <ref type="table" target="#tab_8">Table V</ref>, Baseline contains only the ResNet backbone and 3D global average pooling, and is supervised by l gap xent and l gap htri , the Rank-1 and mAP accuracy of baseline approach is 87.8% and 78.0% respectively. +Attention refers to adopt temporal attention module in the graph branch, the corresponding performance is 88.0% in Rank-1 and 79.4% in mAP. A p refers to only adopting the pose alignment adjacency graph in the feature propagation module, we stack two feature propagation layers in our experiments. Compared with +Attention, +A p improves Rank-1 and mAP accuracy by 0.7% and 0.4%. A f refers to utilizing feature affinity graph, similar to A p , we stack two feature propagation layers, and we can obtain 88.5% in Rank-1 and79.8% in mAP. Furthermore, we can achieve 89.3% and 80.4% on MARS dataset by combining A p and A f . With the consistency loss in the training stage, we can find the Rank-1 and mAP accuracy are improved by 0.5% and 0.7% respectively. With all these proposed components, we improve the Rank-1 and mAP accuracies from 87.8% and 78.0% to 89.8% and 81.1% respectively.</p><p>Qualitative results. We visualize the qualitative results in <ref type="figure">Figure 6</ref> with Grad-CAM <ref type="bibr" target="#b62">[63]</ref>, which is popularly used in computer vision problems for a visual explanation. We show the cases for clutter background in image sequence (a) and (b), when the distractions appear in the background, CAM of the model without graph highlights the distractions in red boxes, while for the models with graph, the influence of distractions is weakened. For occlusion cases shown in image sequence (c), the identity is partially occluded by the other person, the wrong attentive regions are highlighted in CAM for the model without the graph, and the models with the graph are not influenced by the occlusion. In general, our proposed graph-based methods are capable of alleviating the impact of clutter background and occlusion.</p><p>Retrieval Results As illustrated in <ref type="figure">Figure 7</ref>, we provide the retrieval results on MARS dataset for the baseline model and our proposed method. The illustration presents the improve-  <ref type="figure">Fig. 6</ref>: Qualitative results for the baseline model (w/o graph) and our proposed method (w/ A f , w/ A p , and w/ graphs). We employ Grad-CAM to visualize the regions focused by the Re-ID model. The first row is the image sequences sampled from videos. In the other rows, the class activation maps for the baseline model and our proposed method are provided. Compared with the model without graph learning, we find out that our proposed method is robust to the occlusion and clutter background. Best viewed in color and zoomed in.</p><p>ment of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes an innovative graph representation learning approach for video person Re-ID. The proposed method can learn an adaptive structure-aware adjacency graph over the spatial person regions. By aggregating the contextual messages from neighbors for each node, the intrinsic affinity structure information among person feature nodes is captured adaptively, and the complementary contextual information is further propagated to enrich the person feature representations. Furthermore, we propose a novel regularization to enforce the consistency among different temporal resolutions, and it is beneficial for learning the compact and discriminative representations. The experimental results on four standard benchmarks demonstrate the effectiveness of the proposed scheme, and extensive ablation studies validate the feasibility of components in the network. <ref type="figure">Fig. 7</ref>: Comparison of Rank-10 of our proposed method and baseline model. In each row, the images present the videos in the gallery. The images with the green box match the query, and the red box is the wrong matched result. Besides, the images with the yellow box are distracters, which is neglected while calculating the accuracy. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Construction of pose alignment adjacency graph for spatial regions. The estimated keypoints are separated into three parts: head part (consist of nose, neck, eyes, and ears), trunk part (consist of shoulders, bows, and wrists), and leg part (consist of hips, knees, and ankles). The spatial regions (nodes) containing the same part are connected in the pose alignment adjacency graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Graph feature propagation module. Given the adaptive pose alignment adjacency graph A, the original spatial regional features are updated iteratively through the feature propagation layers. (b) Feature propagation layer. The features from neighbors are processed by an fc layer F (·), and then aggregated by the weights from the adjacency graph. The operation is defined in Equation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Keypoint detection examples. The human body joints are marked by colorful dots, and linked by yellow lines. Best viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This work is in part supported bykey scientific technological innovation research project by Ministry of Education, Zhejiang Provincial Natural Science Foundation of China under Grant LR19F020004, the National Natural Science Foundationof China under Grants(61751209, 6162510, and 61972071), Baidu AI Frontier Technology Joint Research Program, ZhejiangUniversity K.P.Chao's High Technology Development Foundation, and Zhejiang Lab (No.2019KD0AB02). Y. Wu, Omar, F. Wu are with College of Computer Science, Zhejiang University, Hangzhou 310027, China (e-mail: ymw, obourahla@zju.edu.cn wufei@cs.zju.edu.cn). X. Li*(corresponding author) is with the College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China (e-mail: xilizju@zju.edu.cn). Q. Tian is with the Department of Computer Science, University of Texas, San Antonio, TX 78249-1604 USA (e-mail: qitian@cs.utsa.edu). X. Zhou is with the school of automation engineering, University of Electronic Science and Technology of China (e-mail: zhouxue@uestc.edu.cn).</figDesc><table><row><cell>pyramid pooling</cell><cell>pyramid pooling</cell><cell>pyramid pooling</cell></row><row><cell>pose alignment</cell><cell></cell><cell>feature affinity</cell></row><row><cell></cell><cell></cell><cell>( , )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">: Comparison with state-of-the-art methods on MARS</cell></row><row><cell cols="5">dataset, Rank-1, -5, -20 accuracies(%) and mAP are reported.  †</cell></row><row><cell cols="5">refers to optical flow, and Test Strategy 2 is the second strategy</cell></row><row><cell>introduced in Section IV-C.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">DukeMTMC-VideoReID R1 R5 R20 mAP</cell></row><row><cell>STA [15]</cell><cell>96.2</cell><cell>99.3</cell><cell>-</cell><cell>94.9</cell></row><row><cell>STE-NVAN [16]</cell><cell>95.2</cell><cell>-</cell><cell>-</cell><cell>93.5</cell></row><row><cell>EUG [60]</cell><cell>83.6</cell><cell>94.6</cell><cell>97.6</cell><cell>78.3</cell></row><row><cell>VRSTC [18]</cell><cell>95</cell><cell>99.1</cell><cell>-</cell><cell>93.5</cell></row><row><cell>Ours</cell><cell>96.7</cell><cell>99.2</cell><cell>99.7</cell><cell>94.2</cell></row><row><cell cols="2">+Test Strategy 2 97.0</cell><cell>99.3</cell><cell>99.9</cell><cell>95.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>: Comparison with state-of-the-art methods on</cell></row><row><cell>DukeMTMC-VideoReID dataset, Rank-1, -5, -20 accura-</cell></row><row><cell>cies(%) and mAP are reported.  † refers to optical flow, and Test</cell></row><row><cell>Strategy 2 is the second strategy introduced in Section IV-C.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>: Comparison with state-of-the-art methods on</cell></row><row><cell>PRID2011 and iLIDS-VID datasets, Rank-1, -5, -20 accura-</cell></row><row><cell>cies(%) are reported.  † refers to optical flow, and Test Strategy</cell></row><row><cell>2 is the second strategy introduced in Section IV-C. The</cell></row><row><cell>approaches utilizing optical flow are not directly compared.</cell></row><row><cell>The experimental results indicate that our proposed method</cell></row><row><cell>achieves the state-of-the-art performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.</figDesc><table><row><cell>K</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R20</cell><cell>mAP</cell></row><row><cell>1</cell><cell>89.3</cell><cell>96.0</cell><cell>97.1</cell><cell>97.6</cell><cell>80.8</cell></row><row><cell>2</cell><cell>89.8</cell><cell>96.1</cell><cell>97.0</cell><cell>97.6</cell><cell>81.1</cell></row><row><cell>3</cell><cell>89.5</cell><cell>96.1</cell><cell>97.1</cell><cell>97.0</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Analysis on feature propagation module. K is the number of feature propagation layers. We use the model trained with K = 2 in our experiments.Analysis on feature propagation module. We carry out experiments to investigate the effect of varying the number of feature propagation layers. We evaluate the results of stacking 1, 2, and 3 propagation layers based on the model combined with adaptive pose alignment adjacency graph and consistent</figDesc><table><row><cell>Model</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R20</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>87.8</cell><cell>95.3</cell><cell>96.3</cell><cell>97.3</cell><cell>78.0</cell></row><row><cell>+Attention</cell><cell>88.0</cell><cell>95.3</cell><cell>96.9</cell><cell>97.9</cell><cell>79.4</cell></row><row><cell>+A p</cell><cell>88.7</cell><cell>95.9</cell><cell>96.9</cell><cell>97.8</cell><cell>79.8</cell></row><row><cell>+A f</cell><cell>88.5</cell><cell>95.9</cell><cell>97.1</cell><cell>98.0</cell><cell>79.8</cell></row><row><cell>+A p +A f</cell><cell>89.3</cell><cell>96.1</cell><cell>96.9</cell><cell>97.8</cell><cell>80.4</cell></row><row><cell cols="2">+A p +A f +consistent 89.8</cell><cell>96.1</cell><cell>97.0</cell><cell>97.6</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Ablation study on MARS dataset, we present Rank-1, -5, -10, -20 accuracy(%) and mAP(%). A p , A f , A p +A f , and consistent represent pose alignment adjacency graph, feature affinity graph, combined adjacency graph, and consistent loss respectively. Baseline consists of feature extractor and temporal average pooling, the number of feature propagation layer is set to 2. loss, i.e. +A p + A f + consistent inTable V. As shown in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, regions and nodes are interchangeably for the same meaning.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Keeping track of humans: Have i seen this person before?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zajdel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zivkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Robot. and Auto</title>
		<meeting>IEEE Conf. Robot. and Auto</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2081" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly attentive spatial-temporal pooling networks for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video person reidentification with competitive snippet-similarity aggregation and coattentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video person re-identification by temporal residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1366" to="1377" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3-d personvlad: Learning deep global representations for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scan: Selfand-collaborative attention network for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial-temporal attention-aware learning for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial and temporal mutual promotion for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale 3d convolution network for video based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sta: Spatial-temporal attention for large-scale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatially and temporally efficient non-local attention network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gan-based poseaware regulation for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. IEEE</title>
		<meeting>IEEE Winter Conf. Appl. Comput. IEEE</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1175" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactionand-aggregation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit<address><addrLine>1, 2, 6, 7</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Region-based quality estimation network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Where-and-when to look: Deep siamese attention networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-shot pedestrian reidentification via sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput. Vis</title>
		<meeting>Eur. Conference Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hard-aware pointto-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput</title>
		<meeting>Eur. Conference Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient online local metric adaptation via negative samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2420" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput</title>
		<meeting>Eur. Conference Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person reidentification via recurrent feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput. Vis</title>
		<meeting>Eur. Conference Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attribute-driven feature disentangling and temporal aggregation for video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep feature learning via structured graph laplacian embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="94" to="104" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape: A novel graph theoretic algorithm for making consensus-based decisions in person re-identification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput</title>
		<meeting>Eur. Conference Comput</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8649" to="8658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning context graph for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5142" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gait recognition by cross wavelet transform and graph model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Deore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A grassmann graph embedding framework for gait analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Connie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K O</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B J</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial-temporal graph attention network for video-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="274" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput. Vis</title>
		<meeting>Eur. Conference Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshop</title>
		<meeting>IEEE Conf. Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput. Vis</title>
		<meeting>Eur. Conference Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scandinavian Conf. Image Anal</title>
		<meeting>Scandinavian Conf. Image Anal</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conference Comput. Vis</title>
		<meeting>Eur. Conference Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshop</title>
		<meeting>ECCV Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Videobased person re-identification with accumulative motion context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
