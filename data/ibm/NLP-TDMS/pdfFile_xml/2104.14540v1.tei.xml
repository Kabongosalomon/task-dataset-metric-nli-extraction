<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UCL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niantic</forename></persName>
						</author>
						<title level="a" type="main">The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised monocular depth estimation networks are trained to predict scene depth using nearby frames as a supervision signal during training. However, for many applications, sequence information in the form of video frames is also available at test time. The vast majority of monocular networks do not make use of this extra signal, thus ignoring valuable information that could be used to improve the predicted depth. Those that do, either use computationally expensive test-time refinement techniques or off-theshelf recurrent networks, which only indirectly make use of the geometric information that is inherently available.</p><p>We propose ManyDepth, an adaptive approach to dense depth estimation that can make use of sequence information at test time, when it is available. Taking inspiration from multi-view stereo, we propose a deep end-to-end cost volume based approach that is trained using self-supervision only. We present a novel consistency loss that encourages the network to ignore the cost volume when it is deemed unreliable, e.g. in the case of moving objects, and an augmentation scheme to cope with static cameras. Our detailed experiments on both KITTI and Cityscapes show that we outperform all published self-supervised baselines, including those that use single or multiple frames at test time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowing the depth to each pixel in an image has proved to be a useful and versatile tool, with applications ranging from augmented reality <ref type="bibr" target="#b58">[59]</ref>, autonomous driving <ref type="bibr" target="#b21">[22]</ref>, through to 3D reconstruction <ref type="bibr" target="#b63">[64]</ref>. While specialist hardware can give per-pixel depth, e.g. from structured light or Lidar sensors, a more attractive approach is to only require a single RGB camera. Many recent monocular depth from RGB methods are trained using only self-supervision, which removes the need for expensive hardware to capture training depth data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. While these approaches appear to be very promising, their test-time depth estimation performance is not yet on a par with specialist depth hardware or deep multi-view methods <ref type="bibr" target="#b72">[73]</ref>. In an attempt to close this performance gap, we observe that in most practical scenarios more than one frame is available at test time e.g. from a camera on a moving vehicle or micro-baseline frames from a phone camera <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref>. Yet these additional frames are typically not exploited by current monocular methods. In this work, we use these additional frames at both training and test time, when they are available, to self-supervise a multi-frame depth estimation system. We show that a straightforward application of self-supervised training to a multi-view plane-sweep stereo architecture produces poor results, significantly worse than self-supervised single frame networks. To overcome this, we introduce several innovations to address issues caused by moving objects, scale ambiguity, and static cameras. We call our resultant multi-frame system ManyDepth.</p><p>We make the following three contributions: 1. A novel self-supervised multi-frame depth estimation model that combines the strengths of monocular and multi-view depth estimation by making use of multiple frames at test time, when they are available. 2. We show that moving objects and static scenes significantly impact self-supervised multi-view matching approaches, and we introduce efficient losses and train-ing solutions to alleviate this problem. 3. We propose an adaptive cost volume to overcome the scale ambiguity arising from self-supervised training on monocular sequences. To the best of our knowledge, this is the first time cost volume extents have been learned from data rather than set as parameters.</p><p>Our ManyDepth model outperforms existing single and multi-frame approaches on the KITTI and Cityscapes datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Monocular depth estimation</head><p>The goal of monocular depth estimation is to predict the depth of each pixel in a single input image. Supervised approaches either make use of dense supervision from depth sensors e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> or sparse supervision from human annotations e.g. <ref type="bibr" target="#b7">[8]</ref>. Self-supervised methods remove the limitation of requiring ground truth depth supervision, instead training with image-reconstruction losses using stereo pairs <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> or monocular video sequences <ref type="bibr" target="#b98">[99]</ref>.</p><p>Recent advances in self-supervised training have focused on addressing various challenges resulting from learning from images alone e.g. more robust image reconstruction losses <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b71">72]</ref>, discrete rather than continuous depth predictions <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>, feature space reconstruction losses <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b71">72]</ref>, sparse automatically generated depth supervision <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b82">83]</ref>, occlusion handling <ref type="bibr" target="#b25">[26]</ref>, improved network architectures <ref type="bibr" target="#b27">[28]</ref>, and moving objects during training <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53]</ref> and at test time <ref type="bibr" target="#b53">[54]</ref>. Our underlying monocular architecture is based on <ref type="bibr" target="#b23">[24]</ref>, and could similarly benefit from many of the above enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-frame monocular depth estimation</head><p>There is a growing number of works that extend existing self-supervised monocular models so that they can leverage temporal information at test time to improve the quality of the predicted depth. It is worth noting that there are also several non-deep-learning methods that also aim to produce consistent sequential depth estimates e.g. <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b43">44]</ref>, in addition to conventional SLAM based methods <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b88">89]</ref>, and SLAM methods that integrate a monocular depth estimation network <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52]</ref>. However, here we focus on state-of-the-art neural network based depth estimation.</p><p>Test-time refinement approaches adapt monocular methods to use sequence information at test time e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b50">51]</ref>. As self-supervised training does not require any ground truth depth supervision, the same losses used during training can be applied to the test frames to update the network's parameters. The downside is that this necessitates the additional computation of multiple forward and backward model passes for a set of test frames, potentially taking several seconds to perform per set <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b58">59]</ref>. A second broad group of approaches combine traditional monocular networks with recurrent layers to process sequences of frames e.g. <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b96">97]</ref>. A related approach uses pairs of sequential frames at test time, sharing features between the pose and depth modules <ref type="bibr" target="#b78">[79]</ref> or computing depth-from-flow <ref type="bibr" target="#b87">[88]</ref>. These approaches are much more efficient compared to test-time refinement, but they can be more computationally demanding during training due to the need to extract features from multiple frames in a sequence. A further limitation of these methods is that they do not explicitly reason about geometry during inference; they simply rely on the network having learned how to extract meaningful temporal representations.</p><p>Our experiments show that our approach often outperforms test-time refinement in terms of accuracy while retaining the efficiency of recurrent methods at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep multi-view depth estimation</head><p>Our problem of predicting depth from multiple frames is related to multi-view depth estimation. While early deep stereo methods used mostly convolutional layers to map from images pairs to depth using ground-truth supervision e.g. <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b54">55]</ref>, <ref type="bibr" target="#b44">[45]</ref> showed that integrating a plane-sweep stereo cost volume significantly improved results. Recent approaches improved the underlying architectures and contributed more effective ways of regularizing the cost volume <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b9">10]</ref>. It is also possible to train stereo networks without ground truth supervision <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36]</ref>, but these models are typically outperformed by supervised variants. Some works fuse conventional matching-based stereo estimation with monocular depth cues <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast, we do not require stereo pairs during training or testing.</p><p>A more general version of the stereo-matching problem is multi-view stereo (MVS), which operates on unordered image collections. Early deep MVS methods used memoryexpensive 3D grid representations e.g. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref>. Current supervised approaches, e.g. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57]</ref>, utilize cost volumes but assume ground truth depth and camera poses are available for training. They often require camera poses at test time too; which can be refined from an initial estimate <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b55">56]</ref>. Some methods can predict pose at test time, but they still need to be trained with supervision e.g. <ref type="bibr" target="#b76">[77]</ref>.</p><p>Similar to our approach, <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35]</ref> process sequences of frames at test time using cost volumes. However, by using ground-truth depth supervision and provided camera poses they side-step the challenges associated with training from self-supervision alone. <ref type="bibr" target="#b85">[86]</ref> predict depth from triplets of frames without requiring pose information, but their method cannot deal with variable numbers of frames at test time and is trained with ground truth depth. Concurrent with our work, <ref type="bibr" target="#b84">[85]</ref> learn depth from a cost volume, but they use stereo pairs and sparse supervised depth at training time and long sequences for pose estimation.</p><p>Most related to us are self-supervised MVS methods that also do not require any ground truth depth i.e. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b12">13]</ref>. However, there are several reasons why these existing selfsupervised and supervised MVS methods aren't applicable in many scenarios: (i) they need more than one input image at test time, (ii) they assume that the camera is not static, (iii) they typically require camera poses to be provided during training and sometimes also at test time, and (iv) they assume that there are no moving objects in the scene. Our approach leverages the best of monocular and multi-view methods by making use of sequence information at test time, when it is available, while also being robust to scene motion -see <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem setup</head><p>The aim of depth estimation is to predict a depth map D t , pixel-aligned with an input image I t . Conventional singleimage depth estimation methods, e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b98">99]</ref>, train a deep network θ depth to map from I t to D t ,</p><formula xml:id="formula_0">D t = θ depth (I t ).<label>(1)</label></formula><p>In contrast, like other multi-frame methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b80">81]</ref>, our model accepts as input N previous temporal video frames,</p><formula xml:id="formula_1">D t = θ depth (I t , I t−1 , . . . , I t−N ).<label>(2)</label></formula><p>While our model makes use of information from multiple frames, it also can operate in the regime where only one frame is available at test time. Unlike similar works e.g. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b4">5]</ref>, we do not use future frames at test time e.g. I t+1 , the use of which would preclude online applications. At training time, we exploit previous and future frames as a supervisory signal, and do not require stereo supervision. We do not assume access to the true relative camera pose between I t and the preceding frames; instead we learn to predict these poses {T n } N n=1 at training and test time with a differentiable pose network θ pose , following <ref type="bibr" target="#b98">[99]</ref>. We also do not make use of any trained semantic models to mask moving objects e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. We do, however, assume known fixed camera intrinsics K -though we could relax these requirements <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our method starts with two well established components: self-supervised reprojection based training, and a multi-view cost volume. We then introduce three important innovations that enable cost volume matching to work with self-supervised training from monocular video: (1) adaptive cost volumes, (2) a method to prevent a failure mode we refer to as 'cost volume overfitting', and (3) augmentation for static cameras and single frame inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-supervised monocular depth estimation</head><p>Following <ref type="bibr" target="#b98">[99]</ref>, we train a self-supervision depth network using only video frames that are temporally close to I t . We use the current estimated depth D t and the pose network θ pose 's estimate of relative camera pose T t→t+n to synthesize the scene from the same viewpoint as I t , but only using pixels from neighboring source frames i.e. {I t+n , n ∈ {−1, 1} }. The synthesized counterpart to I t is</p><formula xml:id="formula_2">I t+n→t = I t+n ⟨proj(D t , T t→t+n , K)⟩,<label>(3)</label></formula><p>where ⟨⟩ is the sampling operator and proj returns the 2D coordinates of the depths in D t when reprojected into the camera of I t+n . Note that while our cost volume, described later, only uses preceding frames to enable online applications, at training-time our reprojection loss uses future frames too. Following <ref type="bibr" target="#b23">[24]</ref>, for each pixel we optimize the loss for the best matching source image, by selecting the per pixel minimum over the reconstruction loss pe, L p = min n pe(I t , I t+n→t ).</p><p>We set pe as a combination of SSIM and L 1 losses, and we minimize this loss over all the pixels in the training images over four output scales; see <ref type="bibr" target="#b23">[24]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Building a cost volume</head><p>To exploit multiple input frames, inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> we build a cost volume which measures the geometric compatibility at different depth values between the pixels from I t and nearby source frames from the input video. This requires knowledge of relative pose T , which we estimate with the pose network θ pose , trained using a reprojection loss. We define a set of ordered planes P, each perpendicular to the optical axis at I t and with depths linearly spaced between d min and d max . Each frame is encoded into a deep feature map F t and warped to the viewpoint of I t using each of the hypothesised alternative depths d ∈ P using the known camera intrinsics and estimated pose. This creates <ref type="figure">Figure 2</ref>. Our cost volume based depth estimator. Our depth network θ depth has three main components: a feature extractor, an encoder, and a depth decoder. Our pose network θpose estimates the relative pose between pairs of images, which is then used to build a cost volume in the reference frame of the target image It by warping features extracted from images at different time points. The encoder and depth decoder processes the cost volume to produce a depth image for It. a warped feature map F t+n→t,d . The final cost volume is constructed as the absolute difference between the warped features and the features from I t , at each d ∈ P. This is averaged over all source images, following <ref type="bibr" target="#b64">[65]</ref>. The cost volume effectively says 'for pixel (i, j), what is the likelihood of the correct depth being d, for each d in P?'. Following <ref type="bibr" target="#b18">[19]</ref>, the cost volume is concatenated with features F t and used as input to a convolutional decoder which regresses the depth D t . See <ref type="figure">Fig. 2</ref> for an overview.</p><p>Cost volumes have the benefit of allowing the network to leverage inputs from multiple viewing angles. However, they typically require d min and d max to be chosen as hyperparameters, and they assume that world is static. In the following sections we show how to relax these assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive cost volumes</head><p>Cost volume approaches have a problem of needing a known depth range i.e. d min and d max . These are typically selected as hyperparameters in advance of training based on prior knowledge of the dataset <ref type="bibr" target="#b12">[13]</ref> or from known camera poses <ref type="bibr" target="#b36">[37]</ref>. We are unable to do this, as self-supervised depth estimation trained on monocular sequences only estimates depth 'up to scale'. This means that while we assume that the final predicted depths, and corresponding poses from the pose network, will all end up in broad agreement with each other, they will be different from real-world depth by an unknown scaling factor.</p><p>To solve this problem we introduce a novel adaptive cost volume, by allowing d min and d max to be learned from the data, so they can adjust during training as the network finds its own scaling. This is done using the current predictions from the network of D t , whereby we compute the average min and max of each D t over a training batch. These are then used to update an exponential moving average estimates of d min and d max with momentum 0.99. d min and d max are saved along with the model weights and then kept fixed at test time. Our approach contrasts to <ref type="bibr" target="#b26">[27]</ref> who adapt d min , d max at test time in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Addressing cost volume overfitting</head><p>We observe that our baseline cost volume model trained with monocular supervision suffers from severe artefacts, including large 'holes' punched on moving objects. These are similar to artefacts observed in monocular I t → D t models (see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> for a description). However, in our cost volume network they are far more severe (see <ref type="figure">Fig. 3 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why does the monocular-trained cost volume fail?</head><p>In theory, our model should do well. It is trained with a similar reprojection loss used to train state-of-the-art singleframe depth estimators, but it also has access to an additional source of information via the cost volume. However, the information contained in the cost volume is only reliable in specific scenarios e.g. in static regions with textured surfaces. In regions where objects are moving, or where surfaces are untextured, the cost volume will be an unreliable source of depth information ( <ref type="figure">Fig. 3 (b)</ref>). For example, the moving car in <ref type="figure">Fig. 3</ref> results in a match in the cost volume at an incorrect depth and corresponds to a very low reprojection loss. During training, the network becomes over-reliant on the cost volume. Instead of ignoring the cost volume around moving objects, it trusts it too much. Artefacts in the cost volume from moving objects are then introduced in the final depth map, at both training and test time. Ultimately, the final predicted depths inherit the cost volume's mistakes. We introduce a method to correct this during training, by teaching the network not to trust the cost volume in these unreliable regions.</p><p>Using a separate network to regularize. We make the observation that single-image depth networks do not have a cost volume, so are unaffected by 'cost volume overfitting'. While moving objects can still be a problem for these meth-ods during training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b68">69]</ref>, in general they make far less severe mistakes on moving objects. We therefore use a monocular network at training time to help 'teach' our cost volume network the right answer -but only in regions we suspect the cost volume to be problematic. This separate network θ consistency produces a depth mapD t for each training image, and is discarded after training. θ consistency shares θ pose with our main network to help ensure scale-consistent predictions between θ depth and θ consistency . Potentially problematic pixels in our multi-frame output are identified by a binary mask M . In these masked regions we apply an L 1 loss on D t , encouraging the predictions to be similar toD t ,</p><formula xml:id="formula_4">L consistency = M D t −D t .<label>(5)</label></formula><p>Gradients toD t are blocked, ensuring knowledge only transfers from teacher to student and not vice versa.</p><p>Identifying unreliable pixels. Our binary mask M is 1 in regions considered to be unreliable, and 0 otherwise. To generate this mask we again make use ofD t . We reason that in regions where the cost volume is reliable, the depth represented byD t will be similar to the depths represented by the argmin of the cost volume. We therefore compare the depth represented by the argmin of the cost volume (i.e. D cv , not D t ) to the depthD t predicted by θ consistency . The mask M is set to 1 only in regions whereD t and D cv differ significantly, so</p><formula xml:id="formula_5">M = max D cv −D t D t ,D t − D cv D cv &gt; 1.<label>(6)</label></formula><p>The idea of using a separate 'disposable' network to help to regularize training is not new, e.g. <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b98">99]</ref>. Our novelty is in using a single-frame depth network to improve a multi-frame system. Our approach is also less costly and less constrained than using offline semantic segmentation <ref type="bibr" target="#b25">[26]</ref>, and makes fewer assumptions than RANSAC-based filtering <ref type="bibr" target="#b28">[29]</ref>. In our experiments we compare to two alternative masking schemes from <ref type="bibr" target="#b68">[69]</ref> and <ref type="bibr" target="#b23">[24]</ref>, and show that our approach is superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Static cameras and start-of-sequences</head><p>Using multiple frames at test time introduces two potential challenges for our method. The first issue is when I t−1 does not exist, i.e. when predicting depth for a single image or those at the start of a sequence. This case is trivially handled by monocular methods as they only require single frames as input. However, MVS approaches fail in these situations. To address this problem, during training with probability p, we replace the cost volume with a tensor of zeros. For these images, this encourages the network to learn to rely only on features directly from I t . Then at test time, when I t−1 does not exist, we simply replace the cost <ref type="figure">Figure 3</ref>. We enable self-supervised training to work with a cost volume. When we build a cost volume from a sequence (a), moving objects create incorrect depths in the cost volume (b). These errors are propagated through to our predicted depth maps (c). 'Traditional' single-image depth estimation does not exhibit this failure mode, but can still produce biased depth (d). We therefore use a single-image network to help our model recover from this failure mode, but only on pixels identified to be unreliable by our mask M (e). Our final prediction (f) is superior to both our baseline and the single-image network. volume with zeros. The second case arises when the camera does not move between I t−1 and I t , e.g. a car stopped at traffic lights. Again this is another failure case for MVS methods. To address this at training time, with probability q, we replace the I t−1 input to the cost volume with a coloraugmented version of I t , but still supervise with the 'real' I t−1 , I t+1 in Eqn. 4. This enables the network to predict plausible depths even when the cost volume is constructed from images with no camera baseline.</p><p>Our final loss is L = (1 − M )L p + L consistency + L smooth , where L smooth is the smoothness loss from <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation details</head><p>We use training-time color and flip augmentations on images being fed to the depth and pose networks, using the settings from <ref type="bibr" target="#b23">[24]</ref>. Unless otherwise stated, all our models are trained with an input and output resolution of 640×192, and we fix N = 1, so the cost volume is constructed with frames {I t , I t−1 }, at both training and test time. In all cases selfsupervision during training is from frames {I t−1 , I t , I t+1 }. We train with Adam <ref type="bibr" target="#b46">[47]</ref> for 20 epochs with a learning rate of 10 −4 , dropping by a factor of 10 for the final 5 epochs. After Q epochs, we fix d min and d max and the weights of θ pose and θ consistency . This allows θ depth to finetune with a non-moving target. We set Q = 15 for KITTI, and Q = 5 for Cityscapes to account for the larger number of images Input &amp; GT Ground truth depth Ground truth depth <ref type="bibr" target="#b23">[24]</ref> MR Error map Error map <ref type="bibr" target="#b27">[28]</ref> [66]</p><p>Ours MR Ours + TTR <ref type="figure">Figure 4</ref>. Qualitative results on KITTI. Error maps in columns 2 and 4 show the abs. rel. error compared to new ground truth <ref type="bibr" target="#b75">[76]</ref>, from good (blue) to bad (red). All error maps are colormapped equivalently. While depth maps look qualitatively similar between our multi-frame predictions (bottom rows) and the baselines, the error maps reveal the large 'hidden mistakes' made by methods which only have access to a single test-time image. This is particularly apparent in ambiguous regions, e.g. in the dark embankment on the right of the freeway. Additionally, note that <ref type="bibr" target="#b65">[66]</ref> also has access to multiple frames at test time, however their method does not explicitly utilize geometry. This results in an improvement over the single frame <ref type="bibr" target="#b23">[24]</ref>, but there is noticeably higher error than for our approach.</p><p>in the Cityscapes training set. The feature extractor in θ depth comprises the first five ResNet18 layers <ref type="bibr" target="#b32">[33]</ref>. These features are aggregated into a cost volume, the result of which is concatenated with our input image features, and followed by the remaining ResNet18 convolutional layers. We use the depth decoder from <ref type="bibr" target="#b23">[24]</ref>. Our pose network θ pose and skip connections for θ depth are the same as <ref type="bibr" target="#b23">[24]</ref>. θ consistency uses the standard architecture from <ref type="bibr" target="#b23">[24]</ref> with no modifications. Full architecture details are in the supplementary. Following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b30">31]</ref>, we use weights pretrained on ImageNet <ref type="bibr" target="#b69">[70]</ref>, but provide results trained from scratch in the supplementary material. For all our experiments we set p = q = 0.25 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Here we evaluate our ManyDepth model and <ref type="formula" target="#formula_0">(1)</ref> show that it gives SOTA results by comparing, in a standardized way, to both single-frame and multi-frame depth estimation and (2) validate our design decisions via ablations. Additional results are provided in the supplementary material.</p><p>We evaluate on two challenging depth estimation datasets, both of which exhibit moving objects. For both, we use the standard depth evaluation metrics from <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>(a) KITTI <ref type="bibr" target="#b21">[22]</ref>. We use the Eigen split from <ref type="bibr" target="#b13">[14]</ref>. This is commonly used for single frame depth estimation, but is more recently also used for multi-frame approaches e.g. <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b65">66]</ref>. 22 frames in the KITTI Eigen test set are at the start of a sequence, and do not have a previous frame. We still include these images in the evaluation. For these images, the network does not have access to any other frames and thus makes a prediction based on one frame only. In the supplementary material, we additionally include models evaluated on the improved KITTI ground truth <ref type="bibr" target="#b75">[76]</ref>.</p><p>(b) Cityscapes <ref type="bibr" target="#b11">[12]</ref>. Following <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b91">92]</ref>, we train on 69,731 images from the monocular sequences, which we preprocess into triples using the scripts from <ref type="bibr" target="#b98">[99]</ref>. We do not use stereo pairs or semantics. We evaluate on the 1,525 test images using the provided SGM <ref type="bibr" target="#b33">[34]</ref> disparity maps. As with KITTI, we clip predicted depths at 80m, and only evaluate on ground truth depths less than 80m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">KITTI results</head><p>In <ref type="table" target="#tab_2">Table 2</ref> we compare to multi-frame approaches, some of which, e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b61">62]</ref>, see more frames than ours or also use future frames e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b61">62]</ref>. We do not include results from <ref type="bibr" target="#b55">[56]</ref> as they do not provide their scores on the KITTI Eigen split (see <ref type="bibr" target="#b87">[88]</ref>). We additionally compare to the best-performing self-supervised monocular depth estimation approaches. To control for resolution, we separate low and high resolution models, and we also split methods which use expensive multi-pass test-time refinement into separate sections. We observe that our approach outperforms all previously published self-supervised methods that do not use semantic supervision on most metrics. We also  Qualitative results are presented in <ref type="figure">Fig. 4</ref>. In some cases the predicted depth maps looks qualitatively similar to the monocular only models, but the error maps show the high magnitude of mistakes which can be present.</p><p>Efficiency comparison. <ref type="figure">Fig. 5</ref> illustrates the runtime efficiency of our ManyDepth models (640 x 192: , and 1024 x 320: , ) compared to other methods, including test-time refinement approaches ( ). We report multiply-add computations (MACs) for each method and show that test-time refinement models which perform multiple forward-backward passes are too computationally demanding for use in realtime applications. The supplementary material contains a full results table and additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">KITTI ablation</head><p>In <ref type="table" target="#tab_5">Table 4</ref> we show the importance of the various components of our approach by turning them on and off in turn. ManyDepth w/o motion masking: We omit L consistency from our loss and set M to zeros everywhere. ManyDepth w/o motion masking, w/o augmentation:</p><p>As above, but also omitting our augmentations. -Multi-frame models with a single forward pass at test time. Methods which are more accurate than ours take over two orders of magnitude more time to compute (note the logarithmic scale on the x-axis.). Our MR multi-frame version ( ) has better accuracy than <ref type="bibr" target="#b78">[79]</ref>, who has a similar runtime.  <ref type="table">Table 3</ref>. Results on Cityscapes. Our method gives superior performance to all competing models. † is trained by us with the authors' code, with preprocessing from <ref type="bibr" target="#b98">[99]</ref>. Results from <ref type="bibr" target="#b66">[67]</ref> are their 'Half-Cycle Mono' model, their only variant not requiring test-time stereo pairs. We evaluate using the cropping scheme of <ref type="bibr" target="#b5">[6]</ref> following conversations with the authors; see supplementary material for specifics.</p><p>ManyDepth with motion masking but no teacher: We remove L consistency but still use M to mask L p . Stack of 2 frames as input: A baseline which directly maps (I t−1 , I t ) to D t . We modify <ref type="bibr" target="#b23">[24]</ref>'s network to accept two images as input, and train using their loss. ManyDepth with motion masking from <ref type="bibr" target="#b68">[69]</ref>: We use our full loss, but our mask M is the same as <ref type="bibr" target="#b68">[69]</ref>. We use their pretrained models to compute these masks offline for the entire training set. ManyDepth with motion masking from <ref type="bibr" target="#b23">[24]</ref>: Here we use our full loss, but set the mask M to an 'automask' from <ref type="bibr" target="#b23">[24]</ref>. Khot et al. <ref type="bibr" target="#b45">[46]</ref>: We trained this unsupervised MVS approach on KITTI, with the implementation from <ref type="bibr" target="#b29">[30]</ref>. ManyDepth (I t−2 , I t−1 , I t ) and ManyDepth (I t−1 , I t , I t+1 ):</p><p>Retrained variants of our model which build the cost volume from three frames instead of just two. This improves some metrics but not all.</p><p>Benefit of our augmentations. In <ref type="table">Table 5</ref> we evaluate three different scenarios, comparing our model to a baseline which was trained without our augmentations from Section 4.5. When evaluating in 'standard' mode (i.e. using the previous and current frames as input) on the entire KITTI test set, the difference between the two models is negligible. This is partially because the KITTI test images are predominately from a moving camera. However, when we evaluate in 'start-of-sequence' mode (i.e. the standard monocular setting using only (I t ) as input) and 'static camera' evaluation mode (i.e. simulating a static camera with inputs (I t , I t )), our augmentation scheme is significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Cityscapes results</head><p>In <ref type="table">Table 3</ref> we perform additional comparisons where we train and test on the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>. Again, we consistently outperform competing methods, even those that use semantic supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented a fully self-supervised online method that predicts superior depths from a single image, or from multiple images when they are available. We achieve the benefits  <ref type="table">Table 5</ref>. Our augmentations help in static camera and startof-sequence cases. We compare two variants of our model, one trained with our novel data augmentations ('Ours') and one without. We create two artificial scenarios to test each model's performance on start-of-sequence images (where we just input It) and static cameras (where both input frames are the exact same).</p><p>of both multi-frame and monocular methods, while being more robust on moving objects and static cameras compared to a naive integration of a cost volume. We presented stateof-the-art results on both the KITTI and Cityscapes datasets. While test-time refinement methods are close competitors in terms of depth accuracy, we have shown that our method is significantly more efficient during inference. We expect that our results could be further improved via recent complimentary advances in monocular depth estimation e.g. discretized output depths <ref type="bibr" target="#b24">[25]</ref> or feature based losses <ref type="bibr" target="#b71">[72]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Trained using only self-supervision, our model not only predicts depth from single frames (a) but can also utilize multiple frames, when they are available, using the same model (b). This results in superior depth predictions at test time. Error maps on the bottom row show large depth errors as red, small as blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our method to existing self-supervised approaches on the KITTI<ref type="bibr" target="#b21">[22]</ref> Eigen split. At the top we compare medium and low resolution results without and with test-time refinement (TTR). At bottom we compare high resolution results without and with TTR. The best results in each subsection are in bold; second best are underlined. Our method outperforms all previous methods in all subsections across most metrics, whether or not the baselines use multiple frames at test time. We indicate if a method uses semantic supervision (Semantics) and methods indicated by N take a long sequence of frames as input for each test image (e.g. the preceding frames or frames before and after in time).</figDesc><table><row><cell>implement the test-time refinement scheme of [62] on our</cell></row><row><cell>model, updating the weights of the depth and pose encoders</cell></row><row><cell>using sequential pairs of images from the test set, for 50</cell></row><row><cell>steps. Not surprisingly, this further improves our results,</cell></row><row><cell>and we outperform other test-time refinement methods.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Our single-pass network is significantly more efficient than test-time optimisation. We compare abs. rel. error (y-axis) against MACs (x-axis) on the KITTI Eigen test set.-Single frame models. These tend to have low MACs.-Multi-frame models which use test-time refinement.</figDesc><table><row><cell></cell><cell>0.16</cell><cell>[68]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>[5]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell>[9]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Abs. rel. error</cell><cell>0.12</cell><cell>[24] [66] Ours MR [79] [79]</cell><cell cols="2">[72] +TTR Ours HR +TTR</cell><cell>[5]</cell><cell>[28]</cell><cell>[59]</cell></row><row><cell></cell><cell>0.1</cell><cell cols="2">Ours MR</cell><cell>Ours HR</cell><cell></cell><cell>[9] +TTR</cell><cell>[72]</cell></row><row><cell></cell><cell>10 9 0.08</cell><cell cols="2">10 10</cell><cell cols="2">10 11</cell><cell>10 12</cell><cell>10 13</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Multiply-add computations</cell></row><row><cell cols="2">Figure 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method Test frames SemanticsWxH Abs Rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3</figDesc><table><row><cell>Struct2Depth 2 [6]</cell><cell>1</cell><cell>•</cell><cell>416 x 128</cell><cell>0.145</cell><cell>1.737</cell><cell>7.280</cell><cell>0.205</cell><cell>0.813</cell><cell>0.942</cell><cell>0.976</cell></row><row><cell>Pilzer et al. [67]</cell><cell>1</cell><cell></cell><cell>512 x 256</cell><cell>0.240</cell><cell>4.264</cell><cell>8.049</cell><cell>0.334</cell><cell>0.710</cell><cell>0.871</cell><cell>0.937</cell></row><row><cell>Monodepth2 † [24]</cell><cell>1</cell><cell></cell><cell>416 x 128</cell><cell>0.129</cell><cell>1.569</cell><cell>6.876</cell><cell>0.187</cell><cell>0.849</cell><cell>0.957</cell><cell>0.983</cell></row><row><cell>Videos in the Wild [26]</cell><cell>1</cell><cell>•</cell><cell>416 x 128</cell><cell>0.127</cell><cell>1.330</cell><cell>6.960</cell><cell>0.195</cell><cell>0.830</cell><cell>0.947</cell><cell>0.981</cell></row><row><cell>Li et al. [53]</cell><cell>1</cell><cell></cell><cell>416 x 128</cell><cell>0.119</cell><cell>1.290</cell><cell>6.980</cell><cell>0.190</cell><cell>0.846</cell><cell>0.952</cell><cell>0.982</cell></row><row><cell>Struct2Depth 2 [6]</cell><cell>3 (-1, 0, +1)</cell><cell></cell><cell>416 x 128</cell><cell>0.222</cell><cell>5.737</cell><cell>8.613</cell><cell>0.258</cell><cell>0.774</cell><cell>0.908</cell><cell>0.954</cell></row><row><cell>Struct2Depth 2 [6]</cell><cell>3 (-1, 0, +1)</cell><cell>•</cell><cell>416 x 128</cell><cell>0.151</cell><cell>2.492</cell><cell>7.024</cell><cell>0.202</cell><cell>0.826</cell><cell>0.937</cell><cell>0.972</cell></row><row><cell>ManyDepth</cell><cell>2 (-1, 0)</cell><cell></cell><cell>416 x 128</cell><cell>0.114</cell><cell>1.193</cell><cell>6.223</cell><cell>0.170</cell><cell>0.875</cell><cell>0.967</cell><cell>0.989</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Our contributions lead to better scores. Here we ablate our ManyDepth method on KITTI 2015<ref type="bibr" target="#b21">[22]</ref> using the Eigen split. Full numbers are in the supplementary material.</figDesc><table><row><cell>Ablation</cell><cell></cell><cell cols="3">Abs Rel Sq Rel RMSE</cell></row><row><cell>ManyDepth full</cell><cell></cell><cell>0.098</cell><cell>0.770</cell><cell>4.459</cell></row><row><cell cols="2">ManyDepth (w/o motion masking)</cell><cell>0.113</cell><cell>1.354</cell><cell>5.228</cell></row><row><cell cols="2">ManyDepth (w/o motion masking, w/o aug.)</cell><cell>0.284</cell><cell cols="2">11.240 8.516</cell></row><row><cell cols="2">ManyDepth (with motion masking, w/o teacher)</cell><cell>0.154</cell><cell>2.682</cell><cell>6.573</cell></row><row><cell cols="2">Stack of 2 frames as input (I t−1 , I t )</cell><cell>0.121</cell><cell>1.028</cell><cell>5.016</cell></row><row><cell cols="2">ManyDepth (with motion masking from [69])</cell><cell>0.099</cell><cell>0.783</cell><cell>4.447</cell></row><row><cell cols="2">ManyDepth (with motion masking from [24])</cell><cell>0.099</cell><cell>0.780</cell><cell>4.465</cell></row><row><cell cols="2">Khot et al. [46] reimplementation</cell><cell>0.200</cell><cell>4.694</cell><cell>7.232</cell></row><row><cell cols="2">ManyDepth with 3-frame input (I t−2 , I t−1 , I t )</cell><cell>0.098</cell><cell>0.780</cell><cell>4.430</cell></row><row><cell cols="2">ManyDepth with 3-frame input (I t−1 , I t , I t+1 )</cell><cell>0.097</cell><cell>0.768</cell><cell>4.431</cell></row><row><cell>Test-time input</cell><cell>Model</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell></row><row><cell>Standard: (It−1, It)</cell><cell>No augmentation ManyDepth</cell><cell>0.100 0.098</cell><cell>0.794 0.770</cell><cell>4.432 4.459</cell></row><row><cell></cell><cell>Monodepth2 [24]</cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell></row><row><cell>Start-of-sequence: (It)</cell><cell>No augmentation</cell><cell>0.148</cell><cell>1.076</cell><cell>5.161</cell></row><row><cell></cell><cell>ManyDepth</cell><cell>0.118</cell><cell>0.892</cell><cell>4.764</cell></row><row><cell>Static camera: (It, It)</cell><cell>No augmentation ManyDepth</cell><cell>0.158 0.117</cell><cell>1.132 0.886</cell><cell>5.228 4.754</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Thanks to Jamie Shotton; a conversation with him at a BMVA workshop motivated work in this area. Also to Daniyar Turmukhambetov and Sara Vicente for their valuable feedback, and to the authors of <ref type="bibr" target="#b5">[6]</ref> for CityScapes evaluation help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reversing the cycle: self-supervised deep stereo through enhanced monocular distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Undemon: Unsupervised deep network for depth and ego-motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>V Madhu Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagat</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CodeSLAM-learning a compact, optimisable representation for dense visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth and ego-motion learning with structure and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multiimage matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MVS 2 : Deep unsupervised multi-view stereo with multi-view symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single-view and multi-view depth fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejo</forename><surname>Fácil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Concha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CAM-Convs: Camera-aware multi-scale convolutions for singleview depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Civera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Forget about the LiDAR: Self-supervised depth estimators with MED probability volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Luis</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3D packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rares Ambrus, and Adrien Gaidon. Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">PyTorch implementation of MVS-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://github.com/xy-guo/MVSNet_pytorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hae-Gon Jeon, and In So Kweon. High-quality depth from uncalibrated small motion clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyowon</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view stereo by temporal nonparametric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">M 3 VSNet: Unsupervised multi-metric multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00363</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepMVS: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">DPSNet: End-to-end deep plane sweep stereo. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SurfaceNet: An end-to-end 3D neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Micro-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Microsoft Research Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Handling occlusions in dense multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning unsupervised multi-view stereopsis via robust photometric consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Termöhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning SFM from SFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DepthNet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suchendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CoMoDA: Continuous monocular depth adaptation using past experiences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DeepFusion: real-time dense 3D reconstruction for monocular SLAM using single-view depth and gradient predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Laidlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth learning in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural RGB→D sensing: Depth and uncertainty from a video camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Occlusion-aware depth estimation with adaptive normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Every pixel counts++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fusion of stereo and still monocular depth estimates in a selfsupervised learning context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Van Hecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Monocular depth estimation with selfsupervised instance adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mccraith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Live dense reconstruction with a single moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">DTAM: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the past: Recurrent depth estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial depth estimation using cycled generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Mihai Marian Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on CPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Depth estimation using monocular and stereo cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">On the importance of stereo for accurate depth estimation: An efficient semi-supervised deep neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cnn-slam: Real-time dense monocular slam with learned depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Distilled semantics for comprehensive scene understanding from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant CNNs. In 3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">SfM-Net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Self-supervised joint learning framework of depth estimation via implicit cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09876</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">MVDepthNet: Real-time multiview depth estimation neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In 3DV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Recurrent neural network for (un-)supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">UnOS: Unified unsupervised optical-flow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">DeepSFM: Structure from motion via deep bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">MonoRec: Semi-supervised dense reconstruction in dynamic environments from a single moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wimbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niclas</forename><surname>Lukas Von Stumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Spatial correspondence with generative adversarial network: Learning depth from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Video depth estimation by fusing flow-todepth proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Yasutaka Furukawa, and Ping Tan. Polarimetric dense monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feitong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">LEGO: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">GA-Net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Domain-invariant stereo matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Consistent depth maps recovery from a video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for stereo matching with self-improving ability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00930</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
