<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person re-identification</term>
					<term>part-based models</term>
					<term>misalignment</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Part-level representations are important for robust person re-identification (ReID), but in practice feature quality suffers due to the body part misalignment problem. In this paper, we present a robust, compact, and easy-to-use method called the Multi-task Part-aware Network (MPN), which is designed to extract semantically aligned part-level features from pedestrian images. MPN solves the body part misalignment problem via multi-task learning (MTL) in the training stage. More specifically, it builds one main task (MT) and one auxiliary task (AT) for each body part on the top of the same backbone model. The ATs are equipped with a coarse prior of the body part locations for training images. ATs then transfer the concept of the body parts to the MTs via optimizing the MT parameters to identify part-relevant channels from the backbone model. Concept transfer is accomplished by means of two novel alignment strategies: namely, parameter space alignment via hard parameter sharing and feature space alignment in a class-wise manner. With the aid of the learned high-quality parameters, MTs can independently extract semantically aligned part-level features from relevant channels in the testing stage. MPN has three key advantages: 1) it does not need to conduct body part detection in the inference stage; 2) its model is very compact and efficient for both training and testing; 3) in the training stage, it requires only coarse priors of body part locations, which are easy to obtain. Systematic experiments on four large-scale ReID databases demonstrate that MPN consistently outperforms state-of-the-art approaches by significant margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ERSON re-identification (ReID) is a critical component of modern surveillance systems. The process is aimed at spotting a person of interest, e.g. a missing child or a suspect, across disjoint camera views distributed at different physical locations. Due to the widespread deployment of visual surveillance networks, ReID has recently attracted increasing attention from both academia and industry. Despite this, however, ReID remains a challenging problem; this is largely caused by the dramatic variations in intrapersonal appearance and high inter-personal similarity <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Accordingly, to enhance the discriminative power of ReID models, a large proportion of the recent literature has explored the learning of part-level representations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which incorporate more fine-grained features and reduce the overfitting risk of deep models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>However, the extraction of high-quality part-level representations is difficult. This is because human body parts are often not semantically aligned across images, meaning that the same spatial position across two images may not correspond to the same body part. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, one reason is that pedestrian detection is still challenging <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The error in pedestrian detection causes both the position and scale of body parts to vary dramatically in images. Moreover, some body parts, such as arms and legs, are inherently flexible, meaning that their position and shape will change even in cases where the pedestrian detection algorithm works perfectly.</p><p>Since the positions of these body parts are variable, one intuitive solution is to detect body parts in the spatial dimension before part-level feature extraction occurs; most existing works adopt this approach <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>. However, body part detection is inherently challenging: interference such as severe image blur, background clutter, and occlusions may cause failures in part detection, thereby degrading the quality of the part-level representations. There have been recent attempts to bypass part detection at the inference stage using teacher-student style training strategies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The teacher model employs prior information regarding body part locations to guide a separate student model in extracting semantically aligned part-level features from the original image. However, there are two disadvantages to this approach. Firstly, the architecture of the model used for training is complex. Secondly, only the feature space is constrained in a sample-wise manner, with the result that the performance of the student model is sensitive to the robustness of the teacher model.</p><p>It is therefore reasonable to seek a robust, compact, and easy-to-use method capable of learning semantically aligned part-level representations. To this end, we propose a Multitask Part-aware Network (MPN) that only slightly increases the time and space complexities of a very basic part-based model <ref type="bibr" target="#b18">[19]</ref> for both training and testing. Unlike existing works, MPN extracts part-specific information from a deep backbone model by explicitly regularizing the model parameters to select part-relevant channels via the introduction of inductive bias with multi-task learning (MTL). Global max-pooling on selected channels results in translation-and scale-invariant body part features being obtained. Moreover, as the selected channels for each body part are fixed after training, MPN is naturally robust to various forms of interference such as image blur and background clutter.</p><p>One primary contribution of MPN is the way it explicitly learns part-relevant channels. The first challenge is that there is no universally recognized definition of the area for each body part. Moreover, a plain network cannot learn the concept of 'body parts' without any priors. In this paper, we define the body part regions in training images using a method that is coarse but robust and easy to use. In the training stage, MPN includes one main task (MT) and one auxiliary task (AT) for each body part, both of which are built on the same backbone model. Both tasks select and combine body part relevant channels to construct the respective part-level representations for person classification. Their main difference lies in the input feature maps: the inputs of MT are the original feature maps produced by the backbone model, while those for AT are the cropped feature maps according to the coarse part location priors. By sharing all parameters (except for their respective classifiers) of MT and AT for each respective part, MT is regularized to enable the choosing of part-relevant channels. Another noticeable advantage of this strategy is that the obtained model architecture for training is extremely compact.</p><p>Implementing the above simple strategy enables MPN to achieve state-of-the-art performance. However, due to the difference between the input feature maps, the sharing parameter alone cannot ensure alignment of the MT and AT feature spaces. Accordingly, to eliminate this discrepancy, we further introduce a novel constraint between MT and AT in the feature space, which is applied in a class-wise rather than a sample-wise manner <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Briefly, we compute the mean representation of each identity in one batch for MT and AT respectively, and then penalize their cosine distance. The motivation behind this is that the prior location of each body part employed by AT is coarse, which degrades the quality of the feature vectors obtained by AT; therefore, a feature space constraint at the class level will statistically be more robust than one at the sample level <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>In the inference stage, all ATs are abandoned and part-level features obtained by MTs are used for ReID. To demonstrate the efficacy of MPN, we conduct extensive experiments on four large-scale benchmark datasets: Market-1501 <ref type="bibr" target="#b24">[25]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b25">[26]</ref>, CUHK03 <ref type="bibr" target="#b26">[27]</ref>, and MSMT17 <ref type="bibr" target="#b27">[28]</ref>. The results show that our simple MPN model consistently and significantly outperforms existing approaches with the further advantages of being compact and easy to use.</p><p>The remainder of the paper is organized as follows. Related works on ReID and MTL are briefly reviewed in Section 2. The MPN model structure and training scheme are described in Section 3. ReID during inference using MPN is introduced in Section 4. Detailed experiments and their analysis are presented in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>A number of effective approaches have been proposed for ReID <ref type="bibr" target="#b28">[29]</ref>. In particular, part-based models have been shown to be effective and have become popular <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. We therefore review the literature on (i) part-based ReID models and (ii) MTL methods for ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Part-based ReID Models</head><p>While part-based models are powerful, they suffer from the problem of the semantic misalignment of body parts <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Existing approaches to this problem can be divided into three categories: (i) methods that extract multi-scale features (MSF) to address the body part misalignment problem; (ii) methods that detect body parts in the spatial dimension before part-level feature extraction is performed; and (iii) methods that guide the deep model to learn semantically aligned part-level features via teacher-student training schemes.</p><p>MSF-based methods extract part-level features from multi-scale image patches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>. As the patch size increases, the extracted features are less affected by body part misalignment but at the cost of reduced discriminative power. Multi-scale patch features are concatenated as the image representation. However, the dimension of the final representation is high, and the body part misalignment problem is only partly solved.</p><p>Body part detection-based methods first detect body parts in the spatial dimension before part-level feature extraction is performed. Most existing works fall into this category. According to the way in which body parts are detected, approaches in this category can be further classified into (i) outside tools-based methods, (ii) spatial attentionbased methods, and (iii) unsupervised methods. Outside tools-based methods rely on outside tools (e.g., pose estimation models <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and human parsing models <ref type="bibr" target="#b32">[33]</ref>), to provide body part locations during both training and testing. Notable downsides to this approach include the extra computational cost and the low reliability of the outside tools. Spatial attention-based methods can overcome the above problem by inferring the location of body parts directly from feature maps produced by ReID networks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The output of the attention modules can take the form of either rigid bounding boxes <ref type="bibr" target="#b22">[23]</ref> or soft spatial masks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b33">[34]</ref> that represent the location of body parts. Attention module parameters are optimized together with the entire ReID network using supervision signals for ReID only, meaning that the parameters of the attention modules lack a direct constraint. Unsupervised methods adopt hand-crafted approaches to locating body parts based on the feature maps of each image <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b35">[36]</ref>. For example, Yao et al. <ref type="bibr" target="#b17">[18]</ref> proposed using K-means clustering to cluster channels based on the locations of maximum response, with individual average pooling of the selected channels for each cluster indicating the location of a body part.</p><p>Despite these efforts, however, body part detectionbased methods still face major challenges, because body part detection can fail when the image contains interference such as severe image blur, background clutter, and significant occlusion.</p><p>The third category of methods bypass body part detection during inference <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In the training stage, these methods adopt complex model architectures with teacherstudent-style training strategies. As the teacher model is equipped with prior information regarding body part locations, it can extract semantically aligned part-level features. Moreover, through alignment with the teacher model in the feature space, a separate student model without any body part priors is guided to produce similar features. Existing works guide in a sample-wise manner <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>; therefore, the quality of guidance is vital. To provide precise guidance, Zhang et al. <ref type="bibr" target="#b9">[10]</ref> used a 3D alignment tool to achieve pixellevel semantic alignment of body parts. However, the performance of the outside tool used was restricted by training image quality (e.g., severe image blur), with the result that the obtained body part location priors may not be robust. Therefore, sample-wise guidance between the teacher and student models may be suboptimal for ReID.</p><p>Compared with existing works, our proposed approach not only bypasses body part detection during inference, but also has the advantages of compactness, robustness, and ease-of-use both during training and testing. In particular, we here solve the body part misalignment problem from a novel perspective: in short, we explicitly select part-relevant channels from the backbone model by introducing inductive bias with auxiliary tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MTL Methods for ReID</head><p>MTL is a commonly used strategy that simultaneously optimizes multiple relevant tasks. These relevant tasks introduce inductive bias, which improves the generalization ability of the main task. Therefore, MTL has been successfully applied in many computer vision tasks. Here we focus on MTL-based approaches for ReID. For a more comprehensive summary of MTL, we direct readers to <ref type="bibr" target="#b36">[37]</ref>.</p><p>Existing MTL approaches for ReID can be divided into four categories. First, many works combine loss functions for image classification and metric learning to improve the quality of learned pedestrian representations <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Second, other models conduct person ReID and attribute recognition jointly, since these tasks are closely related <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Third, some recent part-based approaches have generated feature maps for each body part from the output of the same backbone model, regarding ReID based on each body part as an independent task <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Fourth, body part detection and part-based ReID were integrated into a single model as two parallel tasks during training in <ref type="bibr" target="#b41">[42]</ref>. In the testing phase, these two tasks run sequentially for ReID.</p><p>In this paper, we employ MTL for a new purpose. Briefly, MTL regularizes the model parameters to select channels relevant to each respective body part, such that the subsequently extracted part-level features are semantically aligned. Moreover, compared with <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b41">[42]</ref>, our MTLbased approach is very compact due to its use of hard parameter sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK PART-AWARE NETWORK</head><p>We first introduce the motivation and problem formulation of MPN before presenting the MPN framework and describing each of its key components: namely, the coarse priors of part locations for training images, the part-relevant channel selection via MTL, and the class-wise feature space alignment (FSA) between the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Recent works have shown that different channels of a ReID network activate local responses at their corresponding body parts <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>. In other words, there are correspondences between the channels and body parts, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. This property has been utilized to detect body parts in the spatial dimension <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>In this paper, we explore this property from a more straightforward perspective. During training, we train a light-weight module, i.e., one 1 × 1 convolutional (Conv) layer, to select and combine relevant channels for each individual body part from the output feature maps of a deep backbone model. Global max-pooling (GMP) on each of the produced channels results in translation-and scale-invariant body part features. The parameters of the 1 × 1 Conv layers are fixed during the inference stage; as a result, part-level features can be robustly extracted without the need for body part detection for each image.</p><p>The problem is then reformulated for the identification of the relevant channels for each body part. Unfortunately, a plain deep model cannot automatically learn the concept of body parts without proper guidance. Accordingly, in this section, we propose MPN to solve this problem via MTL.</p><p>As illustrated in <ref type="figure">Fig. 3</ref>, MPN in the training stage includes two tasks for each of K body parts: one main task (MT) and one auxiliary task (AT). Both tasks are optimized for ReID purposes, i.e. person classification. The K ATs are equipped with the coarse prior of the body part locations in the training images; as a result, they can provide inductive bias assisting the MTs to select the relevant channels for each body part. In the inference stage, ATs are removed, and only the MTs are used to extract part-level representations. We next introduce each of the key components of MPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coarse Prior of Body Part Locations</head><p>The prior of body part locations for training images provides the network with the concept of body parts. Body parts can be represented as size-fixed strips <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>, sizevaried bounding boxes <ref type="bibr" target="#b17">[18]</ref>, or even a group of pixels of irregular shape <ref type="bibr" target="#b9">[10]</ref>. We here adopt the strip-based representation, which is coarse but robust. The prior is generated based on two existing tools: one for human parsing <ref type="bibr" target="#b43">[44]</ref> and another for human segmentation <ref type="bibr" target="#b44">[45]</ref>.</p><p>As explained in <ref type="figure">Fig. 4</ref>(a-b), the former tool in <ref type="bibr" target="#b43">[44]</ref> segments and distinguishes a set of pre-defined body parts, but ignores discriminative accessories (e.g., backpacks) and undefined body parts (e.g., necks). The latter tool in <ref type="bibr" target="#b44">[45]</ref> segments the human body with accessories as a whole, thereby losing part-specific information. As shown in <ref type="figure">Fig. 4</ref>(c-d), each of them may fail; however, the chance that they both fail for the same image (e.g., <ref type="figure">Fig. 4</ref>(e)) is small. Therefore, the two tools are complementary.</p><p>Given one training image, we propose the following pipeline to combine the outputs of both tools, as illustrated in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>. First, we examine whether both the head and at least one leg are present in the parsing map by counting their respective numbers of pixels. Second, if both are present, we obtain a more reliable mask of the human body via the union of both segmentation maps. Third, the mask is resized to the size of the feature maps produced by the backbone model (i.e., 24 × 8 in our implementation). We then binarize and dilate the resized mask via a 1 × 2 kernel, thus further reducing the impact of errors on human segmentation.</p><p>Similar to <ref type="bibr" target="#b44">[45]</ref>, the influence of background clutter is reduced through the use of the final mask M. In contrast to <ref type="bibr" target="#b44">[45]</ref>, we also obtain the upper and lower boundaries of the human body in the mask. These two boundaries define the region of interest (ROI) of the human body. The uniform division between the two boundaries in the vertical direction indicates the coarse location of K body parts. Furthermore, as shown in <ref type="figure">Fig. 4</ref>(d-f), either the head or both legs may be absent in a small number of low-quality images; under these circumstances, we cannot obtain the precise upper or lower boundary. In these cases, we divide the entire image evenly in the vertical direction to estimate the coarse locations of the K body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Channel Selection via Parameter Space Alignment</head><p>In this subsection, we explain how MPN solves the partrelevant channel selection problem for MTs via parameter space alignment (PSA) between each MT-AT pair during training. As illustrated in <ref type="figure">Fig. 3</ref>, the MT and AT for each body part are built on the same backbone model, i.e., ResNet-50 <ref type="bibr" target="#b45">[46]</ref>, and both extract part-relevant channels to construct part-level representations for ReID. Following <ref type="bibr" target="#b18">[19]</ref>, we remove the last spatial down-sampling operation of ResNet-50 to increase the size of the output feature maps. These feature maps are denoted as F for simplicity below.</p><p>All MT and AT model structures are similar. Taking one MT as an example, it incorporates one 1 × 1 Conv layer, one GMP layer, one optional channel attention (CA) module <ref type="bibr" target="#b46">[47]</ref>, another 1 × 1 Conv layer, and one fully connected (FC) layer for classification. Each Conv layer is followed by one batch normalization (BN) <ref type="bibr" target="#b47">[48]</ref> layer and one ReLU layer <ref type="bibr" target="#b48">[49]</ref> by default. The dimension of both Conv layers is set as 512. Moreover, the configuration of the CA module is illustrated in more detail in <ref type="figure">Fig. 6</ref>. The loss functions for the MTs and ATs will be introduced in Sec. 3.4.</p><p>The first Conv layer selects and combines part-relevant channels from F. The degree of relevance is determined by the entire training set; therefore, it may not be optimal for each individual image. GMP transforms the feature maps into one feature vector, the elements of which are robust to the translation of body parts. The feature vector is fed into the CA module, which overcomes the problem of the first Conv layer by recalibrating each channel according to its importance in each specific image. The downside of using CA is that it increases the degree of model complexity; therefore, we consider this module optional in our model. The second Conv layer projects the feature vector to a more discriminative space.</p><p>The main difference between the MT and AT of one body part has to do with their inputs. The input of MT is the original F, which means that the MT itself contains no cues for use in identifying part-relevant channels. In comparison, we process F to obtain the part-specific feature maps P k (1 ≤ k ≤ K), which are the input of the k-th AT. This procedure includes three steps, which are illustrated in  <ref type="bibr" target="#b43">[44]</ref>, and human segmentation result by <ref type="bibr" target="#b44">[45]</ref>, respectively. (a) The tool in <ref type="bibr" target="#b43">[44]</ref> may ignore discriminative accessories, e.g., a backpack. (b) It may also neglect undefined body parts, e.g., the neck.  As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, channels in F activate local responses at their corresponding body parts. This means that only relevant channels have high responses in P k following the division operation; therefore, it is much easier to optimize the first Conv layer of AT than it is to optimize MT. The parameters of this layer can create inductive bias, assisting MT in selecting part-relevant channels. We propose to utilize this inductive bias by simply sharing the parameters of the This process is applied to each channel of F, respectively.</p><p>first 1 × 1 Conv layer between each MT-AT pair. However, sharing parameters for channel selection alone cannot ensure that only the part-relevant channels will be selected for MT, because MT itself contains no cues for partrelevant channel selection. When MT and AT optimize the shared Conv layer together, irrelevant channels may also be selected. This means that there is a gap between the features extracted by MT and those extracted by AT. To resolve this problem, we apply stronger regularization by further sharing the parameters of the CA module and the second 1 × 1 Conv layer between MT and AT, respectively. By sharing parameters of each respective layer, we implicitly require that its input feature vectors from MT and AT will be similar for each image. This constraint, in turn, regularizes  <ref type="figure">Fig. 6</ref>: Structure of the adopted channel attention (CA) module <ref type="bibr" target="#b46">[47]</ref>. The items in each bracket denote the number of filters, kernel size, and stride, respectively. Each Conv layer is followed by a BN layer by default.</p><p>the first 1 × 1 Conv layer to select part-relevant channels.</p><p>In conclusion, MPN shares the parameters of the two Conv layers and the CA module between each MT-AT pair, respectively. We refer to this hard parameter sharing strategy as PSA, which forces the first 1 × 1 Conv layer to select part-relevant channels for ReID purposes. In other words, MT is regularized to extract semantically aligned part-level representations. Compared to existing works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, one important advantage of hard parameter sharing is that it makes the MPN architecture very compact in the training stage. During testing, all ATs are removed, meaning that MPN is free from body part detection after training is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Respective Loss Functions for MTs and ATs</head><p>We employ two popular loss functions to train MTs and ATs. First, we attach one cross-entropy loss function to the classification layer of each MT and AT, respectively:</p><formula xml:id="formula_0">L ID = − 1 N N l=1 K k=1 L ce W mt k f l k + L ce W at k z l k ,<label>(1)</label></formula><p>where N denotes batch size. f l k and z l k represent the feature vectors extracted from the k-th part of the l-th image by MTs and ATs, respectively. W mt k and W at k represent parameters of the classification layers for the k-th MT and AT, respectively. As illustrated in <ref type="figure">Fig. 3</ref>, f l k and z l k are the outputs of the second Conv layer of MTs and ATs, respectively. L ce stands for the cross-entropy loss function.</p><p>The K part-level features extracted by MTs are concatenated as the holistic representation h of one image:</p><formula xml:id="formula_1">h = [f 1 , f 2 , ..., f K ].</formula><p>(</p><p>We then apply the triplet loss <ref type="bibr" target="#b49">[50]</ref> to ensure that the distance between the representations of intra-class image pairs is smaller than that of the inter-class image pairs. To ensure that sufficient triplets are sampled, we randomly choose A images in each of S random subjects to create a mini-batch. We follow the BatchHard strategy used in <ref type="bibr" target="#b49">[50]</ref> to sample the triplets. The triplet loss can be formulated as</p><formula xml:id="formula_3">L T RI = 1 N T S i=1 A a=1 [ max p=1...A D (h a i , h p i ) − min n=1...A j=1...S j =i D h a i , h n j + α] + ,<label>(3)</label></formula><formula xml:id="formula_4">where {h a i , h p i , h n j } compose a triplet.</formula><p>The anchor and positive images are sampled from the i-th subject, while the negative image is sampled from the j-th subject. α denotes the margin of the triplet constraint and N T represents the number of triplets in a batch that violate the triplet constraint <ref type="bibr" target="#b49">[50]</ref>.</p><p>[ * ] + = max(0, * ) is the hinge loss. D (h a i , h p i ) and D h a i , h n j denote the cosine distance between two feature vectors. For example,</p><formula xml:id="formula_5">D (h a i , h p i ) = 1 − h a i T h p i h a i h p i .<label>(4)</label></formula><p>Note that the importance of ATs and MTs are asymmetric. ATs are removed during testing; therefore, we do not apply the triplet loss to the features extracted by ATs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature Space Alignment between MTs and ATs</head><p>The above MTL strategy enables MPN to achieve strong performance. However, due to the difference between the input feature maps of each MT-AT pair, a gap still exists between the features extracted by MTs and ATs. Accordingly, to bridge this discrepancy, we propose the following method to align their features in a class-wise manner.</p><p>First, the K part-level features extracted by ATs are concatenated to form another holistic representation g of one image:</p><formula xml:id="formula_6">g = [z 1 , z 2 , ..., z K ].<label>(5)</label></formula><p>Second, we calculate the mean representations for each subject in one batch:</p><formula xml:id="formula_7">h i = 1 A A a=1 h a i ,<label>(6)</label></formula><p>and,</p><formula xml:id="formula_8">g i = 1 A A a=1 g a i .<label>(7)</label></formula><p>Finally, we penalize the cosine distance between h i and g i :</p><formula xml:id="formula_9">L CF = 1 S S i=1 D h i , g i .<label>(8)</label></formula><p>Our class-wise feature alignment strategy can be contrasted with the sample-wise feature alignment approaches in recent works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. For example, consider the approach adopted in <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_10">L SF = 1 N S i=1 A a=1 D (h a i , g a i ) .<label>(9)</label></formula><p>Compared with L CF , L SF imposes a stronger constraint, as it requires that the distance between each pair of h a i and g a i should be minimized. This requirement is reasonable when the quality of g a i is very high; in practice, however, the quality of g a i is limited due to errors in the body part location priors. First, the outside tools adopted in this paper and in <ref type="bibr" target="#b9">[10]</ref> may fail for low-quality pedestrian images. Second, the prior that we employ in Sec. 3.2 is coarse. The uniform division operation on the human body presented in <ref type="figure" target="#fig_4">Fig. 5(b)</ref> may not account for dramatic movement of flexible body parts in one image.</p><p>Therefore, L CF is a more robust constraint. By penalizing the distance between the class-wise mean representations of MTs and ATs, L CF becomes less vulnerable to errors in body part location priors. In the experimental section, we will demonstrate that L CF outperforms both constraints proposed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERSON REID VIA MPN</head><p>In the training stage, the overall objective function of MPN can be formulated as follows:</p><formula xml:id="formula_11">L = L ID + L T RI + λL CF ,<label>(10)</label></formula><p>where λ is a weight term.</p><p>In the testing stage, all ATs are removed. We employ h in Eq. 2 as the representation of one image. We consistently adopt the cosine metric to measure the similarity ρ between two representations h 1 and h 2 :</p><formula xml:id="formula_12">ρ = h T 1 h 2 h 1 h 2 .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct comprehensive experiments on four publicly available large-scale benchmark datasets: Market-1501 <ref type="bibr" target="#b24">[25]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b25">[26]</ref>, CUHK03 <ref type="bibr" target="#b26">[27]</ref>, and MSMT17 <ref type="bibr" target="#b27">[28]</ref>. We follow the official evaluation protocols for each of these databases and further adopt both Rank-1 accuracy and mean Average Precision (mAP) as evaluation metrics for all benchmarks.</p><p>The Market-1501 database <ref type="bibr" target="#b24">[25]</ref> consists of 32,668 pedestrian images captured by six cameras of 1,501 identities. The Deformable Part Model (DPM) <ref type="bibr" target="#b50">[51]</ref> is employed to detect bounding boxes for these pedestrians. Market-1501 is divided into a training set and a testing set: the former includes 12,936 images of 751 identities, while the latter comprises images of the remaining 750 identities. Moreover, the testing set is further split into a gallery set and a query set, which contain 19,732 and 3,368 images, respectively.</p><p>The DukeMTMC-ReID database <ref type="bibr" target="#b25">[26]</ref> contains 36,441 pedestrian images of 1,404 identities. The images were captured by eight high-resolution cameras. A total of 16,522 images of 702 identities make up the training set, while images of the other 702 identities make up the testing set. The testing set is further split into a gallery set, containing 17,661 images, and a query set, containing the remaining 2,268 images.</p><p>The CUHK03 database <ref type="bibr" target="#b26">[27]</ref> consists of 14,097 pedestrian images of 1,467 identities. The images were captured by two disjoint cameras. The bounding boxes of pedestrians in CUHK03 are obtained by means of two methods, namely human annotation and DPM detection. We report results using each of these two types of bounding boxes. We adopt the training/testing splitting protocol proposed in <ref type="bibr" target="#b51">[52]</ref>. In this protocol, images of 767 identities are used for training, while images of the remaining 700 identities are utilized for testing.</p><p>The MSMT17 database <ref type="bibr" target="#b27">[28]</ref> contains 126,441 pedestrian images of 4,101 identities in total. This dataset was collected by a camera network comprising 12 outdoor cameras and three indoor ones. Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> is used for pedestrian detection. MSMT17 is split into a training set, containing 32,621 images of 1,041 identities, and a testing set, consisting of 93,820 images of 3,060 identities. Furthermore, the testing set is randomly divided into a gallery set and a query set, which consist of 82,161 and 11,659 images respectively. Compared with the above datasets, MSMT17 is more challenging, because its scale is larger and it includes more complex background and illumination changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Firstly, all images in the above four databases are resized to 384 × 128 pixels. Example images can be found in <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>. Data augmentation is utilized to reduce overfitting in the training stage. First, offline translation <ref type="bibr" target="#b52">[53]</ref> is adopted to enlarge each training set by a factor of five. Second, random erasing <ref type="bibr" target="#b53">[54]</ref> and horizontal flipping with a ratio of 0.5 are utilized for online augmentation. We set S as 6 and A as 8 to construct a mini-batch; thus, the batch size is 48. There are only a few hyper-parameters for MPN. We empirically set K as 6, according to the evaluation results in <ref type="figure" target="#fig_6">Fig 7.</ref> α and λ are consistently set to 0.2 and 1 respectively for the sake of simplicity. The PyTorch framework is used for implementation. The standard stochastic gradient descent (SGD) optimizer, with a weight decay of 5×10 −4 , is utilized for model optimization. The momentum <ref type="bibr" target="#b54">[55]</ref> value is set as 0.9. The parameters of MPN are initialized from those of the IDE model <ref type="bibr" target="#b55">[56]</ref> trained on each respective database; subsequently, MPN is trained in an end-to-end fashion for 70 epochs. The learning rate is initially set to 0.01, then multiplied by 0.1 for every 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>In the following, we systematically investigate the effectiveness of each key component of MPN: namely, MTL structure, along with the parameter and feature space alignment between each MT-AT pair, respectively. Experiments are conducted on three popular databases: Market-1501, DukeMTMC-ReID, and CUHK03. Results are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Effectiveness of Naïve Multi-task Learning</head><p>We first evaluate the performance of one naïve MTL approach. In this approach, both PSA and FSA are removed from MPN. To facilitate a clean comparison, the models in We compare its performance with two basic methods. The first method is similar to the popular Part-based Convolutional Baseline (PCB) approach <ref type="bibr" target="#b18">[19]</ref>. In this method, only ATs are reserved in MPN for both the training and the testing stages. Following <ref type="bibr" target="#b18">[19]</ref>, F is uniformly divided into K horizontal stripes, which are used as the input of ATs. Accordingly, this method is incapable of handling the body part misalignment problem. Moreover, to facilitate fair comparison with the naïve MTL approach, we also concatenate the K part-level features produced by ATs and add triplet loss in the training stage. In the testing stage, the concatenated part-level features are used as the representation of one image. This method is employed as the baseline in this paper. In the second method, only MTs are reserved in both training and testing stages; thus, there is no guidance for MTs to learn part-specific representations. The other details remain the same as MPN. This method is denoted as MT Only in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We also show the upper bound of the baseline's performance, which is referred to as Baseline (UB) in <ref type="table" target="#tab_1">Table 1</ref>. In Baseline (UB), we first correct pedestrian detection errors for both training and testing images, and then test the performance of the baseline. For the first two databases, the detection errors are corrected according to the scheme in <ref type="figure" target="#fig_4">Fig 5.</ref> For CUHK03, we can directly report the baseline's performance on the CUHK03-labeled dataset, where pedestrian detection was manually performed.</p><p>From the comparison results in <ref type="table" target="#tab_1">Table 1</ref>, it can be seen that MT Only outperforms the baseline approach. This may be because MT Only is not affected by the body part misalignment problem in the spatial dimension. In comparison, the uniform division operation in baseline is sensitive to the subtle change of body part locations. Moreover, the naïve MTL approach consistently outperforms both basic methods; this is because ATs can regularize the backbone model in order to learn more diverse local features in F <ref type="bibr" target="#b17">[18]</ref>, which both relieves the overfitting problem and enables MTs to extract stronger representations. The above results verify the effectiveness of the naïve MTL structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effectiveness of PSA</head><p>In this experiment, FSA is removed from MPN, and the effectiveness of hard parameter sharing for PSA is evaluated.</p><p>The CA modules are removed to facilitate clean comparison (in a similar way to the above experiment). In <ref type="table" target="#tab_1">Table 1</ref>, C1-S and C2-S denote whether the parameters of the first and the second 1 × 1 Conv layer are shared between each MT-AT pair, respectively.</p><p>Experimental results in <ref type="table" target="#tab_1">Table 1</ref> demonstrate that both C1-S and C2-S can further improve the performance of naïve MTL by a considerable margin. For example, C1-S outperforms the naïve MTL in terms of Rank-1 accuracy by 0.9%, 1.7%, 5.6%, and 4.1% on each database in <ref type="table" target="#tab_1">Table 1</ref>, respectively. Moreover, performance promotion via C1-S is more significant than that achieved by C2-S. The above results verify the importance of sharing the first 1 × 1 Conv layer between each MT-AT pair for part-relevant channel selection.</p><p>Finally, by sharing both 1 × 1 Conv layers, stronger performance is consistently achieved on all databases; this indicates that C1-S and C2-S are complementary. Compared with the baseline approach, MTL with PSA achieves significantly better performance. In particular, the mAP is promoted by 4.3%, 3.5%, 10.4%, and 8.2% respectively on each database in <ref type="table" target="#tab_1">Table 1</ref>. The above experimental results justify the effectiveness of PSA for part-aware ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Effectiveness of FSA</head><p>In this experiment, PSA is removed from MPN so that we can investigate the effectiveness of the proposed class-wise feature alignment strategy (abbreviated as CF in <ref type="table" target="#tab_1">Table 1</ref>). All CA modules are removed from MPN to facilitate clean comparison. As shown in <ref type="table" target="#tab_1">Table 1</ref>, MTL with FSA consistently outperforms the baseline approach by a large margin: in brief, Rank-1 accuracy is improved by 1.3%, 2.3%, 10.1%, and 7.0%, while mAP is also promoted by 3.9%, 3.3%, 10.2%, and 7.8% on each benchmark, respectively. These experimental results verify the effectiveness of the proposed FSA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Combination of PSA and FSA</head><p>The next step is to combine the parameter and feature space alignments together. Again, to ensure fair comparison with the above results, we test the performance of MPN without the CA modules (denoted as MPN o in <ref type="table" target="#tab_1">Table 1</ref>). As shown in <ref type="table" target="#tab_1">Table 1</ref>, MPN o consistently outperforms all models in the above experiments; this indicates that parameter and feature space alignments work in a complementary fashion to help MTs learn semantically aligned part-level features.</p><p>The performance achieved by MPN o is significantly higher than that obtained using the baseline approach. For example, MPN o outperforms the baseline approach by 4.8%, 4.2%, 11.7%, and 9.0%, respectively, in terms of mAP on each dataset in <ref type="table" target="#tab_1">Table 1</ref>. In particular, pedestrian detection was manually performed on the CUHK03-labeled database, which means that there are rarely pedestrian detection errors. Comparisons on this database indicate that MPN effectively handles the body part misalignment problem caused by other factors, e.g. pose variation. Besides, it is worth noting that MPN o is both powerful and compact. Compared with the baseline model, moreover, MPN o has more parameters only on the extra classification layers of ATs in the training stage. In the testing stage, the number of parameters for MPN o and the baseline is exactly the same.</p><p>Finally, we equip MPN o with CA modules; this is denoted as MPN in <ref type="table" target="#tab_1">Table 1</ref>. In our implementation, we share the parameters of the CA module for each MT-AT pair, respectively. Experimental results indicate that implementation of the CA modules results in the consistently improved performance of MPN on all four databases. This is because the role of the CA modules is complementary to that of the first 1 × 1 Conv layers in MTs, as explained in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Visualization of Attention Maps for MTs</head><p>We further support the above experimental results by visualizing the attention maps for each of the K classifiers' prediction using Grad-CAM <ref type="bibr" target="#b56">[57]</ref>. Three representative models in <ref type="table" target="#tab_1">Table 1</ref> are compared: the baseline, naïve MTL, and MPN. The attention maps on one pedestrian image with a misalignment problem are illustrated in <ref type="figure" target="#fig_7">Fig. 8</ref>.</p><p>The following observations can be made. First, attention maps for the baseline model are not reasonable for the image in <ref type="figure" target="#fig_7">Fig. 8</ref>. For example, attention for the first classifier focuses on the background area. This is because the uniform division operation on the feature maps brings about the semantic misalignment problem for the K classifiers. Second, in the absence of any guidance, the K attention maps for the naïve MTL approach are very similar, which means it lacks diversity for the features extracted by the K MTs in the naïve MTL model. Third, with the guidance of the proposed dual alignment strategies, MPN can learn reasonable spatial localization of body parts; therefore, part-level features extracted by MPN are well aligned in semantics.</p><p>Furthermore, in <ref type="figure" target="#fig_8">Fig. 9</ref>, we visualize the attention maps for each MT's classifier of MPN on more images in the Market-1501 database. It can thereby be seen that the attention maps are both reasonable and semantically consistent when faced with various types of challenges, e.g. errors in pedestrian detection ( <ref type="figure" target="#fig_8">Fig. 9a and Fig. 9b</ref>), dramatic pose variations ( <ref type="figure" target="#fig_8">Fig. 9c and Fig. 9e</ref>), background clutter and occlusion <ref type="figure" target="#fig_8">(Fig. 9d</ref>). In particular, MPN can extract features robustly from the upper arms and legs, regardless of pose variations. However, the lower arms are ignored in <ref type="figure" target="#fig_8">Fig. 9</ref>. This is because pedestrians in Market-1501 wear short sleeves. There are no clothes on the lower arms and therefore the lower arms lack discriminative power. In the supplementary file, we show MPN can accurately attend to the lower arms if pedestrians in one database also wear long sleeves. The ability of MPN that attends to flexible body parts can be explained as follows. Let us take the upper arms as an example, they are in a similar pose in most images. According to our definition of body parts in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, most upper arms lie in the second body part region, which means we obtain good priors of upper-arm locations for most images. Therefore, the first Conv layer in the second MT-AT pair will select channels corresponding to upper arms for part-level feature extraction. The same explanation applies to the lower arms and legs. The above analysis proves that MPN can extract semantically aligned part-level features in a robust manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with Variants of MPN</head><p>We now compare the performance of MPN with some of its possible variants. In the interests of efficient evaluation, experiments are conducted on the Market-1501 and DukeMTMC-ReID databases only. Moreover, models in this subsection are not equipped with the CA modules to facilitate clean comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparisons with Variants of the Prior Information</head><p>We compare the performance of the proposed priors of body part locations in Sec. 3.2 with two possible variants, namely Uniform Division and ROI Resize. The other implementation details of MPN are kept the same for the different priors. Uniform Division means that F of each training image is uniformly divided into K horizontal stripes, which form the input of ATs. For ROI Resize, the element-wise multiplication step in <ref type="figure" target="#fig_4">Fig. 5(b)</ref> is skipped, while the other operations remain the same as those in <ref type="figure" target="#fig_4">Fig. 5</ref>. We also compare with the MT Only approach in <ref type="table" target="#tab_1">Table 1</ref>, which adopts no prior information of body part locations.</p><p>Results of the comparison are presented in <ref type="table" target="#tab_2">Table 2</ref>. From the table, it can be seen that with any prior in <ref type="table" target="#tab_2">Table 2</ref>, MPN significantly outperforms the models that adopt no prior information. Interestingly, the coarse prior of Uniform Division also brings about a noticeable improvement. This occurs for two main reasons. First, the pedestrian detection error for most training images is slight or moderate; as a result, Uniform Division produces good priors for the well-aligned training images. Second, the two alignment strategies in MPN are robust to errors in the priors. The above analysis indicates that MPN can work well with coarse priors, which are easy to obtain in practice.</p><p>We can also observe that the more accurate the prior, the better the performance. For example, ROI Resize rescales the human body to a canonical size and position. As it thereby corrects the majority of misalignment errors, it achieves better performance than Uniform Division. Moreover, the element-wise multiplication operation in <ref type="figure" target="#fig_4">Fig. 5</ref> suppresses the background clutter around body parts, meaning that it is also helpful in promoting the performance of MPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Comparisons with Variant for PSA</head><p>We constrain the parameter space of MTs via hard parameter sharing with ATs. This strategy results in a compact and efficient model in the training stage. One natural alterna-  tive is soft parameter sharing <ref type="bibr" target="#b36">[37]</ref>, which constrains the parameters between each MT-AT pair to be similar rather than identical. To facilitate clean comparison, we compare their performance on one of the two 1 × 1 Conv layers each time and do not apply any constraints to the other layer. Constraints in the feature space are also removed. For soft parameter sharing, we utilize the L2 loss to penalize the distance between the parameters of each MT-AT pair, respectively. Four representative values (i.e., 0.01, 0.1, 1, and 10) are used as the weights for the L2 loss, respectively. By contrast, hard parameter sharing does not include hyperparameters.</p><p>Results of the comparison are illustrated in <ref type="figure" target="#fig_0">Fig. 10</ref>. It is shown that hard parameter sharing consistently outperforms soft parameter sharing on each of the two 1 × 1 Conv layers. Taking the experiments on the first 1 × 1 Conv layer as an example, hard parameter sharing outperforms the best performance of soft parameter sharing by 0.4%/0.3% on Market-1501 and 0.3%/0.4% on DukeMTMC-ReID in terms of Rank-1 accuracy and mAP, respectively. The above experiments demonstrate the effectiveness of hard parameter sharing for PSA in MPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Comparisons with Variants for FSA</head><p>The next step is to compare the proposed class-wise FSA method with some possible variants. Three variants are considered: batch-wise constraint, sample-wise constraint in Eq. 9, and sample-wise constraint in <ref type="bibr" target="#b9">[10]</ref>. Each type of constraint is applied to the concatenated features of K body parts, respectively. The batch-wise constraint penalizes the cosine distance of the mean representations of the complete </p><formula xml:id="formula_13">_j_ -------Ran k-1 ------mAP 96 上 下 一· 一· 一 · 一· 一· 一 · 一· 一· 一 ． 一 · 一· 一· 一 · 一 i一· 一· 一 · 一· 一· 一 · 一 · 一· 一· 一 · 一· 一· 一· 一 · 一· 一· 一 · 一 I-一一一一一一一一一一一一一一一一一一 仁 一一一一一一一一一一一一一一一一一一 十 一一一一一一一一一一一一一一一一一一 _ 今 Q ..- I 总 94尸三一l_ -· --------------_j__ ---------------- ! 9 2 ------------------------------------------------------</formula><p>岳 90 t-----------------『-· -·-·-·-·-·-·-·-·-·-·-·-·-·-·-·-r-·-·-·-·-·-·-·-·--·-·-·-·-·-·-·- ! ------mAP 一· 一 · 一· 一· 一 · 一· 一· 一· 一 · 一· 一· 一 · 一· 一· 一· 一 · 一 1一·一 · 一· 一· 一· 一 · 一· 一· 一 · 一 · 一· 一· 一 · 一· 一· 一· 一 · 一 batch between MTs and ATs, ignoring the label information; this is similar to the popular Maximum Mean Discrepancies (MMD) approach <ref type="bibr" target="#b57">[58]</ref> to domain adaptation. The second type of sample-wise constraint is realized according to the descriptions in <ref type="bibr" target="#b9">[10]</ref>: in brief, we sum the features h and g in an element-wise manner for each image, then apply the triplet loss to the summed features rather than to h only. Results of the comparison are tabulated in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>. We can make the following observations. First, all four types of constraints can promote the performance of the naïve MTL model that adopts no constraint between MTs and ATs; this indicates that alignment in the feature space can robustly promote the quality of the representations of MTs. Second, the batch-wise constraint is inferior to both the sample-wise constraints and the proposed class-wise constraint. This is because the batch-wise constraint is not discriminative, as it neglects the labels of the samples. Third, the proposed class-wise constraint achieves the best performance. In particular, it outperforms both sample-wise constraints. We can thus speculate that the quality of the representations of ATs is limited due to the errors in the prior of the body part locations. Therefore, sample-wise constraints are rigid. By contrast, the class-wise constraint is more robust to these errors via the averaging operation on samples for each class, with the result that it achieves the best performance in both <ref type="table" target="#tab_3">Table 3 and Table 4</ref>.</p><formula xml:id="formula_14">： ： ： ： I-一一一一一一一一一一一一一一一一一一 匕------------------+ 一一一一一一一一一一一一一一一一一一 · =-· 一 ， 皇�--一 · 一 '..一'..一'..一'.. 一'. .一·一'. 一'._. 一 ． ，一 . '一.' 一.'.....�. 一· 一 · 一· 一· 一 · 一 · 一· 一· 一 · 一· 一· 一 ._----:=:= . ! ： ： ： ： ..--...</formula><formula xml:id="formula_15">------------------r------------------上__________________ 一、 90 --一· 一 · 一· 一· 一 · 一· 一· 一· 一 · 一· 一· 一 · 一 -� . 一· 一 · 一· 一· 一· 一 . . 一 · 一· 一 一 ·一·一 · � i 。 、....,,,, �8 8 -----------------+-----------------�----------------- 仁 ro E 86 ---------------------------------· -r· ---------------------------------1 ----------------------------------- L.. 0 i w 8 4 -----------------+-----------------r----------------- c.. 82� i 一 一一一·一·一一·一一一一一一· 一一·一一一七-一· 一·一·一· 一一一一一·一·一一·一·一一一1一一一·一·一·一一一一一一·一·一· 一·一一一一 I-一一一一一一一一一一一一一一一一一一 i,. __________________上__________________ 8 0 -,,:-=-=--------------·-·-·-· · 一·一· 一广·一· 一· 一·一·一· 一·一·一·一· 一·一· --- · 一·一·一· 一1一·一· 一·一·一·一· 一·一· ｝ · 一·一·一· 一·一........</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons with State-of-the-Art Methods</head><p>We compare the performance of MPN with state-ofthe-art methods on four large-scale benchmark datasets: Market-1501 <ref type="bibr" target="#b24">[25]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b25">[26]</ref>, CUHK03 <ref type="bibr" target="#b26">[27]</ref>, and MSMT17 <ref type="bibr" target="#b27">[28]</ref>. According to the properties of the features, methods in this subsection are divided into three groups: holistic feature-based methods, single-scale part featurebased methods, and multi-scale feature-based methods. In the following, these are abbreviated as HF-, SPF-, and MSFbased methods, respectively. The proposed MPN model belongs to the category of SPF-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Performance Comparisons on Market-1501</head><p>Comparison results are tabulated in <ref type="table" target="#tab_5">Table 5</ref>. The following observations can be made. First, MPN outperforms all stateof-the-art approaches in terms of both Rank-1 accuracy and mAP. In particular, with the same backbone model (i.e. ResNet-50), MPN outperforms the DSA-Local(Single) <ref type="bibr" target="#b9">[10]</ref> approach by 2.3% (96.3%-94.0%) in terms of Rank-1 accuracy and 6.2% (89.4%-83.2%) in terms of mAP under the singlequery mode. Moreover, there are another two important advantages of MPN: 1) its model in the training stage is much more compact than that of DSA; 2) MPN requires only coarse priors, which are easy to obtain, while DSA depends on fine-grained 3D priors. This comparison justifies the effectiveness of the parameter and feature space alignment strategies utilized in MPN.</p><p>Second, with single-scale part features, MPN outperforms all existing MSF-based approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Multi-scale features are usually adopted to mitigate the problem of body part misalignment <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>. This comparison indicates that features extracted via MPN have been semantically well-aligned and are therefore powerful.</p><p>Third, some recent HF-based methods also achieve competitive performance by enhancing the representation power of the backbone models <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. For example, IANet <ref type="bibr" target="#b60">[61]</ref> and RGA-SC <ref type="bibr" target="#b58">[59]</ref> insert attention modules into the backbone model that highlight body-relevant information. The contributions of IANet <ref type="bibr" target="#b60">[61]</ref> and RGA-SC <ref type="bibr" target="#b58">[59]</ref> are complementary to that in this paper. Therefore, we also equip the backbone of MPN with the spatial attention module in <ref type="bibr" target="#b58">[59]</ref>. Hyper-parameters of the attention module are kept the same as in the original paper. The combined model is denoted as MPN * in <ref type="table" target="#tab_5">Table 5</ref>. It can be seen that the performance of MPN is further promoted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Performance Comparisons on DukeMTMC-ReID</head><p>Comparison results on the DukeMTMC-ReID database are summarized in <ref type="table" target="#tab_6">Table 6</ref>. From the table, it can be seen that MPN outperforms all other SPF-based methods by significant margins. For example, MPN beats the PCB+RPP method <ref type="bibr" target="#b71">[72]</ref> that adopt the same backbone model (i.e. ResNet-50) by 7.0% (91.5%-84.5%) in terms of Rank-1 accuracy and 10.5% (82.0%-71.5%) in terms of mAP. It also outperforms one of the most recent HF-based methods, i.e. BDB+Cut <ref type="bibr" target="#b61">[62]</ref>, by 2.5% and 6.0% respectively in terms of Rank-1 accuracy and mAP. Moreover, even when compared with one complex MSF-based method <ref type="bibr" target="#b29">[30]</ref>, MPN still achieves a large performance improvement margin as high as 2.5% and 3.0% in terms of Rank-1 accuracy and mAP, respectively. The above comparison results are consistent with those obtained on the Market-1501 database. These experimental results justify the proposed methods' effectiveness at solving the body part misalignment problem for ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Performance Comparisons on CUHK03</head><p>We next compare the performance of MPN with that of the state-of-the-art approaches on the CUHK03 database. Results of this comparison are presented in <ref type="table" target="#tab_7">Table 7</ref>. Both manually labelled and auto-detected bounding boxes are employed for evaluation.</p><p>Results show that MPN still outperforms all other methods in <ref type="table" target="#tab_7">Table 7</ref> by large margins. In particular, it outperforms the PCB+RPP approach <ref type="bibr" target="#b71">[72]</ref>, which is based on the same backbone model, by 19.7% in terms of Rank-1 accuracy and 21.6% in terms of mAP. Note that another advantage of MPN relative to PCB+RPP is that MPN can be trained with the standard end-to-end strategy in a single stage; by contrast, PCB+RPP depends on a four-stage training scheme, as its PCB and RPP modules have to be optimized sequentially <ref type="bibr" target="#b71">[72]</ref>. MPN also outperforms another two most recent SPF-based approaches <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref> that adopt more powerful backbone models. Furthermore, when compared with one of the most recent MSF-based methods (i.e. Pyramid <ref type="bibr" target="#b29">[30]</ref>), MPN still exhibits a clear advantage. In brief, its Rank-1 accuracy is higher than that of Pyramid by 4.5% and 6.1% on CUHK03-Detected and CUHK03-Labeled data, respectively. The above comparisons justify the effectiveness of MPN.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Performance Comparisons on MSMT17</head><p>Finally, we evaluate the performance of MPN on the MSMT17 database, which features complex background and illumination changes. As MSMT17 was released only relatively recently, only a few works have conducted experiments on this database. We compare the performance of MPN with these methods in <ref type="table" target="#tab_8">Table 8</ref>. In this table, we merge the SPF-and MSF-based methods into one category, which is named 'Part-based methods'. Experimental results demonstrate that MPN outperforms all other methods by significant margins, which is consistent with the experimental results on the first three databases. For example, MPN outperforms one of the most recent methods, i.e. OSNet <ref type="bibr" target="#b59">[60]</ref>, by 4.8% and 9.8% in terms of Rank-1 accuracy and mAP, respectively. The above compar- isons justify the effectiveness of MPN for pedestrian images in more complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Comparisons of Model Complexity</head><p>In this experiment, we demonstrate that MPN not only achieves superior performance in terms of ReID accuracy, but also offers advantages in terms of both its time and space complexities. Four powerful part-based approaches are compared: PCB <ref type="bibr" target="#b18">[19]</ref>, MGN <ref type="bibr" target="#b6">[7]</ref>, Pyramid <ref type="bibr" target="#b29">[30]</ref>, and DSA-reID <ref type="bibr" target="#b9">[10]</ref>. All four of these models adopt the ResNet-50 backbone model, meaning that they are directly comparable.</p><p>To further facilitate fair comparison, input images for all the five models are resized to 384 × 128 pixels. Batch sizes of all methods are also unified. Moreover, the number of parameters for classification layers in the training stage depends on the identity number of each database; therefore, their parameters are not taken into account for all models.</p><p>Since the CA modules are optional for MPN, we here test the model complexity of MPN o . It is worth noting that MPN o also consistently outperforms all other models in terms of ReID accuracy, as shown in <ref type="table" target="#tab_1">Table 1</ref>. Comparisons are conducted on a Titan V GPU, and results are summarized in <ref type="table" target="#tab_9">Table 9</ref>. The time cost in <ref type="table" target="#tab_9">Table 9</ref> refers to the average time required to process one image.</p><p>We can thus make the following observations. First, the time cost of MPN o is only slightly higher than that of a very basic part-based model, i.e. PCB <ref type="bibr" target="#b18">[19]</ref>. Moreover, the time cost of MPN o is lower than that of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref> in both the training and testing stages. Second, except for the classification layers, the number of parameters for MPN o is the same at both the training and testing stages; by contrast, the model size of DSA-reID <ref type="bibr" target="#b9">[10]</ref> in the training stage is significantly larger than that in the testing stage. Third, compared with <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref> (the models of which are also compact in the training stage), MPN o solves the body part misalignment problem more effectively. In fact, MPN o has more parameters than <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref> as it adopts more 1×1 Conv layers, which are computationally very efficient in practice. Accordingly, the above comparisons demonstrate that the proposed MPN model is both compact and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a robust, compact, and easy-touse model, named Multi-task Part-aware Network (MPN), to extract semantically aligned part-level representations. In the training stage, MPN includes one main task (MT) and one auxiliary task (AT) for each body part. We equip ATs with a coarse prior of body part locations for training images, and further propose a dual alignment mechanism, i.e. parameter and feature space alignments, to guide the MTs in learning high-quality parameters for part-level feature extraction. In the testing stage, the ATs are removed, and only MTs are saved for feature extraction; therefore, MPN is freed from body part detection during inference. Due to the innovations of our design, the time and space complexities of MPN are only slightly increased relative to a very basic part-based model <ref type="bibr" target="#b18">[19]</ref> at both the training and testing stages. At the training stage, MPN is also robust to coarse priors, which are very easy to obtain. Moreover, comparisons on four large-scale ReID databases demonstrate that MPN significantly outperforms existing approaches at a relatively small computational cost. Therefore, MPN can be surmised to be both powerful and easily applicable to practical ReID applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The spatial misalignment of body parts across images is common in pedestrian images. (a) Misalignment caused by errors in pedestrian detection. Both the position and scale of the body parts change, as indicated by the blue rectangles. (b) Misalignment due to the movement of flexible body parts, such as arms and legs. Both the position and shape of these body parts vary, as indicated by the green rectangles. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of per-channel responses (heatmaps) on the human body. The first column presents the original images. Each of the other five columns represents responses on one representative channel, respectively. The channels are selected from the last convolutional layer of the ResNet-50 model. Red denotes stronger activation. The figure illustrates that there are correspondences between each body part and different channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Model architecture of MPN in the training stage. Based on the ResNet-50 backbone model, MPN builds two tasks for each of the K body parts: one main task (MT) and one auxiliary task (AT). For simplicity, only one MT-AT pair is shown in this figure. The K ATs are equipped with the coarse prior of body part locations for the training images, which provides inductive bias assisting the MTs to select and combine the relevant channels for each body part. Inductive bias is transferred via two key operations: parameter and feature space alignments between each MT-AT pair. The selected part-relevant channels are processed to obtain part-level representations. In the inference stage, all ATs are removed to leave only the MTs to extract image representations. The three images in each group show the pedestrian image, human parsing result by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c-d) Each tool may fail if the image quality is low. (e) A situation in which both tools have failed to work. (f) Segmentation results when there is a severe part missing problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 (</head><label>5</label><figDesc>b). First, each channel of F is multiplied by M in an element-wise manner, reducing the impact of background clutter. Second, the portion of feature maps within the upper and lower boundaries are resized to the original size of F via bilinear interpolation; this step corrects errors in pedestrian detection. Finally, the uniform division of the resized feature maps in the vertical direction produces K part-specific feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>(a) The pipeline to obtain the coarse prior of body part locations. The uniform division between the upper and lower boundaries of the obtained mask indicates the coarse location of each body part. (b) The pipeline to obtain partspecific feature maps P k (1 ≤ k ≤ K) as the input of ATs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Evaluation on the value of hyper-parameter K for the performance of MPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of attention maps for each of the K classifiers' prediction using Grad-CAM. Three representative models inTable 1are compared: (a) baseline model; (b) naïve MTL; (c) MPN. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization of attention maps for each MT's classifier of MPN using Grad-CAM. The attention maps of MPN are semantically consistent across images in the face of various challenges, e.g. errors in pedestrian detection (a, b), dramatic pose variations (c, e), background clutter and occlusion (d). (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 : 1</head><label>101</label><figDesc>Performance comparison between the hard and soft parameter sharing strategies. The horizontal axis stands for the weight of the L2 loss. The red and blue dashed lines represent the Rank-1 accuracy and mAP via hard parameter sharing, respectively. The solid lines denote the performance of soft parameter sharing. (a) Experiments on the first 1 × 1 Conv layers of MTs and ATs. (b) Experiments on the second 1 × Conv layers of MTs and ATs. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Ablation Study on Each Component of MPN</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="3">Components</cell><cell></cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-ReID</cell><cell cols="2">CUHK03-detected</cell><cell cols="2">CUHK03-labeled</cell></row><row><cell>Metric</cell><cell cols="2">MTL C1-S</cell><cell>C2-S</cell><cell>CF</cell><cell>CA</cell><cell cols="2">Rank-1 mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.2</cell><cell>84.4</cell><cell>88.2</cell><cell>77.4</cell><cell>70.9</cell><cell>66.7</cell><cell>75.6</cell><cell>71.3</cell></row><row><cell>Baseline (UB)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.9</cell><cell>85.8</cell><cell>88.9</cell><cell>78.9</cell><cell>75.6</cell><cell>71.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MT Only</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.4</cell><cell>85.6</cell><cell>88.6</cell><cell>78.0</cell><cell>74.1</cell><cell>69.3</cell><cell>78.7</cell><cell>73.9</cell></row><row><cell>Naïve MTL</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.6</cell><cell>87.2</cell><cell>88.7</cell><cell>78.8</cell><cell>75.5</cell><cell>70.7</cell><cell>78.9</cell><cell>75.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.5</cell><cell>88.5</cell><cell>90.4</cell><cell>80.7</cell><cell>81.1</cell><cell>76.9</cell><cell>83.0</cell><cell>79.1</cell></row><row><cell>PSA Alone</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>95.1</cell><cell>88.2</cell><cell>90.1</cell><cell>80.1</cell><cell>80.6</cell><cell>76.2</cell><cell>82.5</cell><cell>78.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>95.8</cell><cell>88.7</cell><cell>90.8</cell><cell>80.9</cell><cell>81.6</cell><cell>77.1</cell><cell>83.4</cell><cell>79.5</cell></row><row><cell>FSA Alone</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>95.5</cell><cell>88.3</cell><cell>90.5</cell><cell>80.7</cell><cell>81.0</cell><cell>76.9</cell><cell>82.6</cell><cell>79.1</cell></row><row><cell>MPN o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>96.1</cell><cell>89.2</cell><cell>91.2</cell><cell>81.6</cell><cell>82.6</cell><cell>78.4</cell><cell>84.1</cell><cell>80.3</cell></row><row><cell>MPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96.3</cell><cell>89.4</cell><cell>91.5</cell><cell>82.0</cell><cell>83.4</cell><cell>79.1</cell><cell>85.0</cell><cell>81.1</cell></row><row><cell cols="7">this experiment are not equipped with the CA module for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">either MTs or ATs. The other details of the model architec-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ture and training strategy remain the same as those in MPN.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Performance Comparison of Different Types of Prior for Body Part Locations in the Training Stage</figDesc><table><row><cell>Dataset</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell>Metric</cell><cell cols="2">Rank-1 mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>94.2</cell><cell>84.4</cell><cell>88.2</cell><cell>77.4</cell></row><row><cell>MT Only</cell><cell>94.4</cell><cell>85.6</cell><cell>88.6</cell><cell>78.0</cell></row><row><cell>Uniform Division</cell><cell>95.6</cell><cell>88.8</cell><cell>90.8</cell><cell>81.2</cell></row><row><cell>ROI Resize</cell><cell>95.8</cell><cell>89.0</cell><cell>91.0</cell><cell>81.3</cell></row><row><cell>ours</cell><cell>96.1</cell><cell>89.2</cell><cell>91.2</cell><cell>81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Performance Comparison with Variants for FSA</cell></row><row><cell>(without PSA)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell>Metric</cell><cell cols="2">Rank-1 mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>94.2</cell><cell>84.4</cell><cell>88.2</cell><cell>77.4</cell></row><row><cell>Naïve MTL</cell><cell>94.6</cell><cell>87.2</cell><cell>88.7</cell><cell>78.8</cell></row><row><cell>Batch-wise</cell><cell>94.7</cell><cell>87.8</cell><cell>89.7</cell><cell>80.2</cell></row><row><cell>Sample-wise [10]</cell><cell>95.1</cell><cell>87.8</cell><cell>90.1</cell><cell>80.6</cell></row><row><cell>Sample-wise (Eq. 9)</cell><cell>95.0</cell><cell>87.9</cell><cell>90.1</cell><cell>80.5</cell></row><row><cell>Class-wise</cell><cell>95.5</cell><cell>88.3</cell><cell>90.5</cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Performance Comparison with Variants for FSA</cell></row><row><cell>(with PSA)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Market-1501</cell><cell cols="2">DukeMTMC-ReID</cell></row><row><cell>Metric</cell><cell cols="2">Rank-1 mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Batch-wise</cell><cell>95.4</cell><cell>88.1</cell><cell>90.1</cell><cell>80.3</cell></row><row><cell>Sample-wise [10]</cell><cell>95.7</cell><cell>88.6</cell><cell>90.6</cell><cell>81.2</cell></row><row><cell>Sample-wise (Eq. 9)</cell><cell>95.6</cell><cell>88.7</cell><cell>90.5</cell><cell>81.2</cell></row><row><cell>Class-wise</cell><cell>96.1</cell><cell>89.2</cell><cell>91.2</cell><cell>81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Performance Comparisons on Market-1501</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">Single Query Rank-1 mAP</cell><cell cols="2">Multiple Query Rank-1 mAP</cell></row><row><cell></cell><cell>HGD [63]</cell><cell>87.0</cell><cell>70.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PSE [64]</cell><cell>87.7</cell><cell>69.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SNL [65]</cell><cell>88.3</cell><cell>73.4</cell><cell>92.1</cell><cell>80.3</cell></row><row><cell></cell><cell>DaRe [66]</cell><cell>89.0</cell><cell>76.0</cell><cell>-</cell><cell>-</cell></row><row><cell>HF-based</cell><cell>MLFN [67] Mancus [35] SFT [68] DNN+CRF [69] PGR [70]</cell><cell>90.0 93.1 93.4 93.5 93.9</cell><cell>74.3 82.3 82.7 81.6 77.2</cell><cell>92.3 95.4 ---</cell><cell>82.4 87.5 ---</cell></row><row><cell></cell><cell>IANet [61]</cell><cell>94.4</cell><cell>83.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>OSNet [60]</cell><cell>94.8</cell><cell>84.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DCDS [71]</cell><cell>94.8</cell><cell>85.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BDB+Cut [62]</cell><cell>95.3</cell><cell>86.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PAR [16]</cell><cell>81.0</cell><cell>63.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>AACN [11]</cell><cell>85.9</cell><cell>66.9</cell><cell>89.8</cell><cell>75.1</cell></row><row><cell></cell><cell>Part-Aligned [9]</cell><cell>91.7</cell><cell>79.6</cell><cell>94.0</cell><cell>85.2</cell></row><row><cell>SPF-based</cell><cell>PCB [19] PCB+RPP [72] DSA-Local(Single) [10] FANN [73] Auto-ReID [74]</cell><cell>92.3 93.8 94.0 94.4 94.5</cell><cell>77.4 81.6 83.2 82.5 85.1</cell><cell>-----</cell><cell>-----</cell></row><row><cell></cell><cell>MHN-6 (PCB) [75]</cell><cell>95.1</cell><cell>85.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MPN MPN  *</cell><cell>96.3 96.4</cell><cell>89.4 90.1</cell><cell>97.0 97.3</cell><cell>92.7 93.1</cell></row><row><cell></cell><cell>PL-NET [18]</cell><cell>88.2</cell><cell>69.3</cell><cell></cell><cell></cell></row><row><cell>MSF-based</cell><cell>HA-CNN [23] HPM [17] MuDeep [76] FPR [77] MGN [7]</cell><cell>91.2 94.2 95.3 95.4 95.7</cell><cell>75.7 82.7 84.7 86.6 86.9</cell><cell>93.8 ---96.9</cell><cell>82.8 ---90.7</cell></row><row><cell></cell><cell>DSA-reID [10]</cell><cell>95.7</cell><cell>87.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Pyramid [30]</cell><cell>95.7</cell><cell>88.2</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Performance Comparisons on DukeMTMC-ReID</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">Rank-1 mAP</cell></row><row><cell></cell><cell>BraidNet [78]</cell><cell>76.4</cell><cell>59.5</cell></row><row><cell></cell><cell>SVDNet [79]</cell><cell>76.7</cell><cell>56.8</cell></row><row><cell></cell><cell>PSE [64]</cell><cell>79.8</cell><cell>62.0</cell></row><row><cell>HF-based</cell><cell>GSRW [80] DuATM [81] PGR [70] DNN+CRF [69] Mancus [35]</cell><cell>80.7 81.8 83.6 84.9 84.9</cell><cell>66.4 64.6 66.0 69.5 71.8</cell></row><row><cell></cell><cell>SFT [68]</cell><cell>86.9</cell><cell>73.2</cell></row><row><cell></cell><cell>IANet [61]</cell><cell>87.1</cell><cell>73.4</cell></row><row><cell></cell><cell>OSNet [60]</cell><cell>88.6</cell><cell>73.5</cell></row><row><cell></cell><cell>BDB+Cut [62]</cell><cell>89.0</cell><cell>76.0</cell></row><row><cell></cell><cell>AACN [11]</cell><cell>76.8</cell><cell>59.3</cell></row><row><cell>SPF-based</cell><cell>PCB [19] Part-aligned [9] PCB+RPP [72] FANN [73] MHN-6 (PCB) [75]</cell><cell>81.8 84.4 84.5 85.2 89.1</cell><cell>66.1 69.3 71.5 70.2 77.2</cell></row><row><cell></cell><cell>MPN</cell><cell>91.5</cell><cell>82.0</cell></row><row><cell></cell><cell>HA-CNN [23]</cell><cell>80.5</cell><cell>63.8</cell></row><row><cell>MSF-based</cell><cell>DSA-reID [10] HPM [17] MuDeep [76] FPR [77] MGN [7]</cell><cell>86.2 86.6 88.2 88.6 88.7</cell><cell>74.3 74.3 75.6 78.4 78.4</cell></row><row><cell></cell><cell>Pyramid [30]</cell><cell>89.0</cell><cell>79.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 :</head><label>7</label><figDesc>Performance Comparisons on CUHK03</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">Detected Rank-1 mAP</cell><cell cols="2">Labeled Rank-1 mAP</cell></row><row><cell></cell><cell>PAN [82]</cell><cell>36.3</cell><cell>34.0</cell><cell>36.9</cell><cell>35.0</cell></row><row><cell>HF-based</cell><cell>SVDNet [79] MGCAM [45] Rolling-back [83] SFT [68] Mancus [35]</cell><cell>41.5 46.7 55.6 65.5</cell><cell>37.3 46.9 50.5 60.5</cell><cell>-50.1 59.8 68.2 69.0</cell><cell>-50.2 55.7 62.4 63.9</cell></row><row><cell></cell><cell>OSNet [60]</cell><cell>72.3</cell><cell>67.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BDB+Cut [62]</cell><cell>76.4</cell><cell>73.5</cell><cell>79.4</cell><cell>76.7</cell></row><row><cell>SPF-based</cell><cell>PCB [19] PCB+RPP [72] HPDN [84] MHN-6 (PCB) [75] Auto-ReID [74]</cell><cell>61.3 63.7 -71.7 73.3</cell><cell>54.2 57.5 -65.4 69.3</cell><cell>--64.3 77.2 77.9</cell><cell>--58.2 72.4 73.0</cell></row><row><cell></cell><cell>MPN</cell><cell>83.4</cell><cell>79.1</cell><cell>85.0</cell><cell>81.1</cell></row><row><cell>MSF-based</cell><cell>HA-CNN [23] HPM [17] MGN [7] MuDeep [76] DSA-reID [10]</cell><cell>41.7 63.9 66.8 71.9 78.2</cell><cell>38.6 57.5 66.0 67.2 73.1</cell><cell>44.4 -68.0 75.6 78.9</cell><cell>41.0 -67.4 70.5 75.2</cell></row><row><cell></cell><cell>Pyramid [30]</cell><cell>78.9</cell><cell>74.8</cell><cell>78.9</cell><cell>76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 :</head><label>8</label><figDesc>Performance Comparisons on MSMT17</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">Rank-1 mAP</cell></row><row><cell>HF-based</cell><cell>Verif-Identif [85], [86] PGR [70] SFT [68] IANet [61] DG-Net [86]</cell><cell>60.5 66.0 73.6 75.5 77.2</cell><cell>31.6 37.9 47.6 46.8 52.3</cell></row><row><cell></cell><cell>OSNet [60]</cell><cell>78.7</cell><cell>52.9</cell></row><row><cell>Part-based</cell><cell>PDC [28], [31] GLAD [28], [87] PCB+RPP [72] Our Baseline Auto-ReID [74]</cell><cell>58.0 61.4 69.8 72.4 78.2</cell><cell>29.7 34.0 43.6 47.5 52.5</cell></row><row><cell></cell><cell>MPN</cell><cell>83.5</cell><cell>62.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9 :</head><label>9</label><figDesc>Comparisons of Model Complexity</figDesc><table><row><cell>Methods</cell><cell cols="2">Training # params time cost</cell><cell cols="2">Testing # params time cost</cell></row><row><cell>PCB [19]</cell><cell>26.8M</cell><cell>8.3ms</cell><cell>26.8M</cell><cell>4.9ms</cell></row><row><cell>MGN [7]</cell><cell>68.8M</cell><cell>16.2ms</cell><cell>68.8M</cell><cell>11.9ms</cell></row><row><cell>Pyramid [30]</cell><cell>29.1M</cell><cell>14.5ms</cell><cell>29.1M</cell><cell>6.4ms</cell></row><row><cell>DSA-reID [10]</cell><cell>187.8M</cell><cell>34.7ms</cell><cell>38.5M</cell><cell>5.4ms</cell></row><row><cell>MPN o</cell><cell>31.5M</cell><cell>10.9ms</cell><cell>31.5M</cell><cell>5.1ms</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic evaluation and benchmark for person reidentification: Features, metrics, and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rates-Borras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person reidentification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="408" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by cross-view multi-level dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2963" to="2977" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person reidentification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Part-based deep hashing for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4806" to="4817" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person re-identification based on heterogeneous part-based deep network in camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Topics Comput. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pedhunter: Occlusion robust pedestrian detector in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Factorized distillation: Training holistic person re-identification model by distilling an ensemble of partial reid models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08073</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing humanlevel performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multitask learning with low rank attribute embedding for multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1167" to="1181" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Person reidentification using deep convnets with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cdpm: convolutional deformable part models for semantically aligned person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3416" to="3428" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom</title>
		<meeting>IEEE Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2318" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="803" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Relation-aware global attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02998</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptors with application to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Support neighbor loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8042" to="8051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pose-guided representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep constrained dominant sets for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning part-based convolutional features for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2938523</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Discriminative feature learning with foreground attention for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4671" to="4684" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Leaderbased multi-scale attention deep architecture for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Person re-identification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5363" to="5372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Man, Cybern., Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE Trans. Syst.</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Backbone can not be trained at once: Rolling back to pre-trained network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Person re-identification based on heterogeneous part-based deep network in camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Topics Comput. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput., Commun., Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Glad: Globallocal-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2017</title>
		<meeting>ACM Int. Conf. Multimedia, 2017</meeting>
		<imprint>
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
