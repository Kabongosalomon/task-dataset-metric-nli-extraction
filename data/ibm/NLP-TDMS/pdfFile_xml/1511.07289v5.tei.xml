<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
							<email>unterthiner@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<email>hochreit@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University</orgName>
								<address>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2016 FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PRe-LUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Currently the most popular activation function for neural networks is the rectified linear unit (ReLU), which was first proposed for restricted Boltzmann machines <ref type="bibr" target="#b22">(Nair &amp; Hinton, 2010)</ref> and then successfully used for neural networks <ref type="bibr" target="#b2">(Glorot et al., 2011)</ref>. The ReLU activation function is the identity for positive arguments and zero otherwise. Besides producing sparse codes, the main advantage of ReLUs is that they alleviate the vanishing gradient problem <ref type="bibr" target="#b7">(Hochreiter, 1998;</ref><ref type="bibr" target="#b9">Hochreiter et al., 2001</ref>) since the derivative of 1 for positive values is not contractive <ref type="bibr" target="#b2">(Glorot et al., 2011)</ref>. However ReLUs are non-negative and, therefore, have a mean activation larger than zero.</p><p>Units that have a non-zero mean activation act as bias for the next layer. If such units do not cancel each other out, learning causes a bias shift for units in next layer. The more the units are correlated, the higher their bias shift. We will see that Fisher optimal learning, i.e., the natural gradient , would correct for the bias shift by adjusting the weight updates. Thus, less bias shift brings the standard gradient closer to the natural gradient and speeds up learning. We aim at activation functions that push activation means closer to zero to decrease the bias shift effect.</p><p>Centering the activations at zero has been proposed in order to keep the off-diagonal entries of the Fisher information matrix small <ref type="bibr" target="#b25">(Raiko et al., 2012)</ref>. For neural network it is known that centering Published as a conference paper at ICLR 2016 the activations speeds up learning <ref type="bibr" target="#b14">(LeCun et al., 1991;</ref><ref type="bibr" target="#b26">Schraudolph, 1998)</ref>. "Batch normalization" also centers activations with the goal to counter the internal covariate shift <ref type="bibr" target="#b10">(Ioffe &amp; Szegedy, 2015)</ref>. Also the Projected Natural Gradient Descent algorithm (PRONG) centers the activations by implicitly whitening them <ref type="bibr" target="#b1">(Desjardins et al., 2015)</ref>.</p><p>An alternative to centering is to push the mean activation toward zero by an appropriate activation function. Therefore tanh has been preferred over logistic functions <ref type="bibr" target="#b14">(LeCun et al., 1991;</ref>. Recently "Leaky ReLUs" (LReLUs) that replace the negative part of the ReLU with a linear function have been shown to be superior to ReLUs <ref type="bibr" target="#b19">(Maas et al., 2013)</ref>. Parametric Rectified Linear Units (PReLUs) generalize LReLUs by learning the slope of the negative part which yielded improved learning behavior on large image benchmark data sets <ref type="bibr" target="#b6">(He et al., 2015)</ref>. Another variant are Randomized Leaky Rectified Linear Units (RReLUs) which randomly sample the slope of the negative part which raised the performance on image benchmark datasets and convolutional networks <ref type="bibr" target="#b32">(Xu et al., 2015)</ref>.</p><p>In contrast to ReLUs, activation functions like LReLUs, PReLUs, and RReLUs do not ensure a noise-robust deactivation state. We propose an activation function that has negative values to allow for mean activations close to zero, but which saturates to a negative value with smaller arguments. The saturation decreases the variation of the units if deactivated, so the precise deactivation argument is less relevant. Such an activation function can code the degree of presence of particular phenomena in the input, but does not quantitatively model the degree of their absence. Therefore, such an activation function is more robust to noise. Consequently, dependencies between coding units are much easier to model and much easier to interpret since only activated code units carry much information. Furthermore, distinct concepts are much less likely to interfere with such activation functions since the deactivation state is non-informative, i.e. variance decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BIAS SHIFT CORRECTION SPEEDS UP LEARNING</head><p>To derive and analyze the bias shift effect mentioned in the introduction, we utilize the natural gradient. The natural gradient corrects the gradient direction with the inverse Fisher information matrix and, thereby, enables Fisher optimal learning, which ensures the steepest descent in the Riemannian parameter manifold and Fisher efficiency for online learning . The recently introduced Hessian-Free Optimization technique <ref type="bibr" target="#b20">(Martens, 2010)</ref> and the Krylov Subspace Descent methods <ref type="bibr" target="#b31">(Vinyals &amp; Povey, 2012)</ref> use an extended Gauss-Newton approximation of the Hessian, therefore they can be interpreted as versions of natural gradient descent <ref type="bibr" target="#b24">(Pascanu &amp; Bengio, 2014)</ref>.</p><p>Since for neural networks the Fisher information matrix is typically too expensive to compute, different approximations of the natural gradient have been proposed. Topmoumoute Online natural Gradient Algorithm (TONGA) <ref type="bibr" target="#b17">(LeRoux et al., 2008)</ref> uses a low-rank approximation of natural gradient descent. FActorized Natural Gradient (FANG) <ref type="bibr" target="#b5">(Grosse &amp; Salakhudinov, 2015)</ref> estimates the natural gradient via an approximation of the Fisher information matrix by a Gaussian graphical model. The Fisher information matrix can be approximated by a block-diagonal matrix, where unit or quasi-diagonal natural gradients are used <ref type="bibr" target="#b23">(Olivier, 2013)</ref>. Unit natural gradients or "Unitwise Fisher's scoring" <ref type="bibr" target="#b13">(Kurita, 1993)</ref> are based on natural gradients for perceptrons <ref type="bibr" target="#b33">Yang &amp; Amari, 1998)</ref>. We will base our analysis on the unit natural gradient.</p><p>We assume a parameterized probabilistic model p(x; w) with parameter vector w and data x. The training data are X = (x 1 , . . . , x N ) ∈ R (d+1)×N with x n = (z T n , y n ) T ∈ R d+1 , where z n is the input for example n and y n is its label. L(p(.; w), x) is the loss of example x = (z T , y) T using model p(.; w). The average loss on the training data X is the empirical risk R emp (p(.; w), X). Gradient descent updates the weight vector w by w new = w old − η∇ w R emp where η is the learning rate. The natural gradient is the inverse Fisher information matrixF −1 multiplied by the gradient of the empirical risk: ∇ nat w R emp =F −1 ∇ w R emp . For a multi-layer perceptron a is the unit activation vector and a 0 = 1 is the bias unit activation. We consider the ingoing weights to unit i, therefore we drop the index i: w j = w ij for the weight from unit j to unit i, a = a i for the activation, and w 0 for the bias weight of unit i. The activation function f maps the net input net = j w j a j of unit i to its activation a = f (net). For computing the Fisher information matrix, the derivative of the log-output probability ∂ ∂wj ln p(z; w) is required. Therefore we define the δ at unit i as δ = ∂ ∂net ln p(z; w), which can be computed via backpropagation, but using the log-output probability instead of the conventional loss function. The derivative is ∂ ∂wj ln p(z; w) = δa j . We restrict the Fisher information matrix to weights leading to unit i which is the unit Fisher information matrix F . F captures only the interactions of weights to unit i. Consequently, the unit natural gradient only corrects the interactions of weights to unit i, i.e. considers the Riemannian parameter manifold only in a subspace. The unit Fisher information matrix is</p><formula xml:id="formula_0">[F (w)] kj = E p(z;w) ∂ ln p(z; w) ∂w k ∂ ln p(z; w) ∂w j = E p(z;w) (δ 2 a k a j ) .<label>(1)</label></formula><p>Weighting the activations by δ 2 is equivalent to adjusting the probability of drawing inputs z. Inputs z with large δ 2 are drawn with higher probability. Since 0 ≤ δ 2 = δ 2 (z), we can define a distribution q(z):</p><formula xml:id="formula_1">q(z) = δ 2 (z) p(z) δ 2 (z) p(z) dz −1 = δ 2 (z) p(z) E −1 p(z) (δ 2 ) .<label>(2)</label></formula><p>Using q(z), the entries of F can be expressed as second moments:</p><formula xml:id="formula_2">[F (w)] kj = E p(z) (δ 2 a k a j ) = δ 2 a k a j p(z) dz = E p(z) (δ 2 ) E q(z) (a k a j ) .<label>(3)</label></formula><p>If the bias unit is a 0 = 1 with weight w 0 then the weight vector can be divided into a bias part w 0 and the rest w: (w T , w 0 ) T . For the row b = [F (w)] 0 that corresponds to the bias weight, we have:</p><formula xml:id="formula_3">b = E p(z) (δ 2 a) = E p(z) (δ 2 ) E q(z) (a) = Cov p(z) (δ 2 , a) + E p(z) (a) E p(z) (δ 2 ) . (4)</formula><p>The next Theorem 1 gives the correction of the standard gradient by the unit natural gradient where the bias weight is treated separately (see also <ref type="bibr" target="#b33">Yang &amp; Amari (1998)</ref>). Theorem 1. The unit natural gradient corrects the weight update (∆w T , ∆w 0 ) T to a unit i by following affine transformation of the gradient ∇ (w T ,w0) T R emp = (g T , g 0 ) T :</p><formula xml:id="formula_4">∆w ∆w 0 = A −1 (g − ∆w 0 b) s g 0 − b T A −1 g ,<label>(5)</label></formula><p>where A = [F (w)] ¬0,¬0 = E p(z) (δ 2 )E q(z) (aa T ) is the unit Fisher information matrix without row 0 and column 0 corresponding to the bias weight. The vector b = [F (w)] 0 is the zeroth column of F corresponding to the bias weight, and the positive scalar s is</p><formula xml:id="formula_5">s = E −1 p(z) (δ 2 ) 1 + E T q(z) (a) Var −1 q(z) (a) E q(z) (a) ,<label>(6)</label></formula><p>where a is the vector of activations of units with weights to unit i and q(z) = δ 2 (z)p(z)E −1 p(z) (δ 2 ).</p><p>Proof. Multiplying the inverse Fisher matrix F −1 with the separated gradient</p><formula xml:id="formula_6">∇ (w T ,w0) T R emp ((w T , w 0 ) T , X) = (g T , g 0 ) T gives the weight update (∆w T , ∆w 0 ) T : ∆w ∆w 0 = A b b T c −1 g g 0 = A −1 g + u s −1 u T g + g o u u T g + s g 0 . (7) where b = [F (w)] 0 , c = [F (w)] 00 , u = − s A −1 b , s = c − b T A −1 b −1 . (8)</formula><p>The previous formula is derived in Lemma 1 in the appendix. Using ∆w 0 in the update gives</p><formula xml:id="formula_7">∆w ∆w 0 = A −1 g + s −1 u ∆w 0 u T g + s g 0 , ∆w ∆w 0 = A −1 (g − ∆w 0 b) s g 0 − b T A −1 g .<label>(9)</label></formula><p>The right hand side is obtained by inserting u = −sA −1 b in the left hand side update. Since</p><formula xml:id="formula_8">c = F 00 = E p(z) (δ 2 ), b = E p(z) (δ 2 )E q(z) (a), and A = E p(z) (δ 2 )E q(z) (aa T ), we obtain s = c − b T A −1 b −1 = E −1 p(z) (δ 2 ) 1 − E T q(z) (a) E −1 q(z) (aa T ) E q(z) (a) −1 .<label>(10)</label></formula><p>Applying Lemma 2 in the appendix gives the formula for s.</p><p>The bias shift (mean shift) of unit i is the change of unit i's mean value due to the weight update. Bias shifts of unit i lead to oscillations and impede learning. See Section 4.4 in <ref type="bibr" target="#b15">LeCun et al. (1998)</ref> for demonstrating this effect at the inputs and in <ref type="bibr" target="#b14">LeCun et al. (1991)</ref> for explaining this effect using the input covariance matrix. Such bias shifts are mitigated or even prevented by the unit natural gradient.</p><p>The bias shift correction of the unit natural gradient is the effect on the bias shift due to b which captures the interaction between the bias unit and the incoming units. Without bias shift correction, i.e., b = 0 and s = c −1 , the weight updates are ∆w = A −1 g and ∆w 0 = c −1 g 0 . As only the activations depend on the input, the bias shift can be computed by multiplying the weight update by the mean of the activation vector a. Thus we obtain the bias shift</p><formula xml:id="formula_9">(E p(z) (a) T , 1)(∆w T , ∆w 0 ) T = E T p(z) (a)A −1 g + c −1 g 0 .</formula><p>The bias shift strongly depends on the correlation of the incoming units which is captured by A −1 .</p><p>Next, Theorem 2 states that the bias shift correction by the unit natural gradient can be considered to correct the incoming mean E p(z) (a) proportional to E q(z) (a) toward zero. Theorem 2. The bias shift correction by the unit natural gradient is equivalent to an additive correction of the incoming mean by −k E q(z) (a) and a multiplicative correction of the bias unit by k, where</p><formula xml:id="formula_10">k = 1 + E q(z) (a) − E p(z) (a) T Var −1 q(z) (a) E q(z) (a) .<label>(11)</label></formula><p>Proof. Using ∆w 0 = −sb T A −1 g + sg 0 , the bias shift is:</p><formula xml:id="formula_11">E p(z) (a) 1 T ∆w ∆w 0 = E p(z) (a) 1 T A −1 g − A −1 b ∆w 0 ∆w 0 (12) = E T p(z) (a) A −1 g + 1 − E T p(z) (a) A −1 b ∆w 0 = E T p(z) (a) − 1 − E T p(z) (a) A −1 b s b T A −1 g + s 1 − E T p(z) (a) A −1 b g 0 .</formula><p>The mean correction term, indicated by an underbrace in previous formula, is</p><formula xml:id="formula_12">s 1 − E T p(z) (a) A −1 b b = E −1 p(z) (δ 2 ) 1 − E T q(z) (a) E −1 q(z) (aa T ) E q(z) (a) −1 (13) 1 − E T p(z) (a) E −1 q(z) (aa T ) E q(z) (a) E p(z) (δ 2 ) E q(z) (a) = 1 − E T q(z) (a) E −1 q(z) (aa T ) E q(z) (a) −1 1 − E T p(z) (a) E −1 q(z) (aa T ) E q(z) (a) k E q(z) (a).</formula><p>The expression Eq. (11) for k follows from Lemma 2 in the appendix. The bias unit correction term is</p><formula xml:id="formula_13">s 1 − E T p(z) (a)A −1 b g 0 = kc −1 g 0 .</formula><p>In Theorem 2 we can reformulate</p><formula xml:id="formula_14">k = 1 + E −1 p(z) (δ 2 )Cov T p(z) (δ 2 , a)Var −1 q(z) (a)E q(z) (a)</formula><p>. Therefore k increases with the length of E q(z) (a) for given variances and covariances. Consequently the bias shift correction through the unit natural gradient is governed by the length of E q(z) (a). The bias shift correction is zero for E q(z) (a) = 0 since k = 1 does not correct the bias unit multiplicatively. Using Eq. (4), E q(z) (a) is split into an offset and an information containing term:</p><formula xml:id="formula_15">E q(z) (a) = E p(z) (a) + E −1 p(z) (δ 2 ) Cov p(z) (δ 2 , a) .<label>(14)</label></formula><p>In general, smaller positive E p(z) (a) lead to smaller positive E q(z) (a), therefore to smaller corrections. The reason is that in general the largest absolute components of Cov p(z) (δ 2 , a) are positive, since activated inputs will activate the unit i which in turn will have large impact on the output.</p><p>To summarize, the unit natural gradient corrects the bias shift of unit i via the interactions of incoming units with the bias unit to ensure efficient learning. This correction is equivalent to shifting the mean activations of the incoming units toward zero and scaling up the bias unit. To reduce the undesired bias shift effect without the natural gradient, either the (i) activation of incoming units can be centered at zero or (ii) activation functions with negative values can be used. We introduce a new activation function with negative values while keeping the identity for positive arguments where it is not contradicting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPONENTIAL LINEAR UNITS (ELUS)</head><p>The exponential linear unit (ELU) with 0 &lt; α is</p><formula xml:id="formula_16">f (x) = x if x &gt; 0 α (exp(x) − 1) if x ≤ 0 , f (x) = 1 if x &gt; 0 f (x) + α if x ≤ 0 .<label>(15)</label></formula><p>The ELU hyperparameter α controls the value to which an ELU saturates for negative net inputs (see <ref type="figure" target="#fig_0">Fig. 1</ref>). ELUs diminish the vanishing gradient effect as rectified linear units (ReLUs) and leaky ReLUs (LReLUs) do. The vanishing gradient problem is alleviated because the positive part of these functions is the identity, therefore their derivative is one and not contractive. In contrast, tanh and sigmoid activation functions are contractive almost everywhere. In contrast to ReLUs, ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient (see Theorem 2 and text thereafter). ELUs saturate to a negative value when the argument gets smaller. Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer. Therefore the representation is both noise-robust and low-complex <ref type="bibr" target="#b8">(Hochreiter &amp; Schmidhuber, 1999)</ref>. ELUs code the degree of presence of input concepts, while they neither quantify the degree of their absence nor distinguish the causes of their absence. This property of non-informative deactivation states is also present at ReLUs and allowed to detect biclusters corresponding to biological modules in gene expression datasets <ref type="bibr" target="#b0">(Clevert et al., 2015)</ref> and to identify toxicophores in toxicity prediction <ref type="bibr" target="#b21">Mayr et al., 2015)</ref>. The enabling features for these interpretations is that activation can be clearly distinguished from deactivation and that only active units carry relevant information and can crosstalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS USING ELUS</head><p>In this section, we assess the performance of exponential linear units <ref type="formula">(</ref> network had eight hidden layers of 128 units each, and was trained for 300 epochs by stochastic gradient descent with learning rate 0.01 and mini-batches of size 64. The weights have been initialized according to <ref type="bibr" target="#b6">(He et al., 2015)</ref>. After each epoch we calculated the units' average activations on a fixed subset of the training data. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the median over all units along learning. ELUs stay have smaller median throughout the training process. The training error of ELU networks decreases much more rapidly than for the other networks.</p><p>Section C in the appendix compares the variance of median activation in ReLU and ELU networks. The median varies much more in ReLU networks. This indicates that ReLU networks continuously try to correct the bias shift introduced by previous weight updates while this effect is much less prominent in ELU networks.  To evaluate ELU networks at unsupervised settings, we followed <ref type="bibr" target="#b20">Martens (2010)</ref> and <ref type="bibr" target="#b1">Desjardins et al. (2015)</ref> and trained a deep autoencoder on the MNIST dataset. The encoder part consisted of four fully connected hidden layers with sizes 1000, 500, 250 and 30, respectively. The decoder part was symmetrical to the encoder. For learning we applied stochastic gradient descent with minibatches of 64 samples for 500 epochs using the fixed learning rates (10 −2 , 10 −3 , 10 −4 , 10 −5 ). <ref type="figure" target="#fig_2">Fig. 3</ref> shows, that ELUs outperform the competing activation functions in terms of training / test set reconstruction error for all learning rates. As already noted by <ref type="bibr" target="#b1">Desjardins et al. (2015)</ref>, higher learning rates seem to perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON OF ACTIVATION FUNCTIONS</head><p>In this subsection we show that ELUs indeed possess a superior learning behavior compared to other activation functions as postulated in Section 3. Furthermore we show that ELU networks perform better than ReLU networks with batch normalization. We use as benchmark dataset CIFAR-100 and use a relatively simple convolutional neural network (CNN) architecture to keep the computational complexity reasonable for comparisons.  <ref type="figure">Figure 5</ref>: Pairwise comparisons of ELUs with ReLUs, SReLUs, and LReLUs with and without batch normalization (BN) on CIFAR-100. Panels are described as in <ref type="figure">Fig. 4</ref>. ELU networks outperform ReLU networks with batch normalization.</p><p>normalization and ZCA whitening. Additionally, the images were padded with four zero pixels at all borders. The model was trained on 32 × 32 random crops with random horizontal flipping.</p><p>Besides that, we no further augmented the dataset during training. Each network was run 10 times with different weight initialization. Across networks with different activation functions the same run number had the same initial weights.</p><p>Mean test error results of networks with different activation functions are compared in <ref type="figure">Fig. 4</ref>  <ref type="figure">Fig. 5</ref>). ELU networks significantly outperform ReLU networks with batch normalization (Wilcoxon signed-rank test with p-value&lt;0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CLASSIFICATION PERFORMANCE ON CIFAR-100 AND CIFAR-10</head><p>The following experiments should highlight the generalization capabilities of ELU networks. The CNN architecture is more sophisticated than in the previous subsection and consists of 18 convolutional layers arranged in stacks of (  <ref type="figure" target="#fig_0">100 × 1]</ref>). Initial drop-out rate, Max-pooling after each stack, L2-weight decay, momentum term, data preprocessing, padding, and cropping were as in previous section. The initial learning rate was set to 0.01 and decreased by a factor of 10 after 35k iterations. The minibatch size was 100. For the final 50k iterations fine-tuning we increased the drop-out rate for all layers in a stack to (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0), thereafter increased the drop-out rate by a factor of 1.5 for 40k additional iterations.  <ref type="bibr" target="#b16">(Lee et al., 2015)</ref>, NiN <ref type="bibr" target="#b18">(Lin et al., 2013)</ref>, Maxout <ref type="bibr" target="#b3">(Goodfellow et al., 2013)</ref>, All-CNN <ref type="bibr" target="#b28">(Springenberg et al., 2014)</ref>, Highway Network <ref type="bibr" target="#b29">(Srivastava et al., 2015)</ref> and Fractional Max-Pooling <ref type="bibr" target="#b4">(Graham, 2014)</ref>. The test error in percent misclassification are given in Tab. 1. ELU-networks are the second best on CIFAR-10 with a test error of 6.55% but still they are among the top 10 best results reported for CIFAR-10. ELU networks performed best on CIFAR-100 with a test error of 24.28%. This is the best published result on CIFAR-100, without even resorting to multi-view evaluation or model averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IMAGENET CHALLENGE DATASET</head><p>Finally, we evaluated ELU-networks on the 1000-class ImageNet dataset. It contains about 1.3M training color images as well as additional 50k images and 100k images for validation and testing, respectively. For this task, we designed a 15 layer CNN, which was arranged in stacks of (1 × 96 × 6, 3 × 512 × 3, 5 × 768 × 3, 3 × 1024 × 3, 2 × 4096 × F C, 1 × 1000 × F C) layers × units × receptive fields or fully-connected (FC). 2×2 max-pooling with a stride of 2 was applied after each stack and spatial pyramid pooling (SPP) with 3 levels before the first FC layer <ref type="bibr" target="#b6">(He et al., 2015)</ref>. For network regularization we set the L2-weight decay term to 0.0005 and used 50% drop-out in the two penultimate FC layers. Images were re-sized to 256×256 pixels and per-pixel mean subtracted. Trained was on 224 × 224 random crops with random horizontal flipping. Besides that, we did not augment the dataset during training. Currently ELU nets are 5% slower on ImageNet than ReLU nets. The difference is small because activation functions generally have only minor influence on the overall training time <ref type="bibr" target="#b11">(Jia, 2014)</ref>. In terms of wall clock time, ELUs require 12.15h vs. ReLUs with 11.48h for 10k iterations. We expect that ELU implementations can be improved, e.g. by faster exponential functions <ref type="bibr" target="#b27">(Schraudolph, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced the exponential linear units (ELUs) for faster and more precise learning in deep neural networks. ELUs have negative values, which allows the network to push the mean activations closer to zero. Therefore ELUs decrease the gap between the normal gradient and the unit natural gradient and, thereby speed up learning. We believe that this property is also the reason for the success of activation functions like LReLUs and PReLUs and of batch normalization. In contrast to LReLUs and PReLUs, ELUs have a clear saturation plateau in its negative regime, allowing them to learn a more robust and stable representation. Experimental results show that ELUs significantly outperform other activation functions on different vision datasets. Further ELU networks perform significantly better than ReLU networks trained with batch normalization. ELU networks achieved one of the top 10 best reported results on CIFAR-10 and set a new state of the art in CIFAR-100 without the need for multi-view test evaluation or model averaging. Furthermore, ELU networks produced competitive results on the ImageNet in much fewer epochs than a corresponding ReLU network. Given their outstanding performance, we expect ELU networks to become a real time saver in convolutional networks, which are notably time-intensive to train from scratch otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A INVERSE OF BLOCK MATRICES</head><p>Lemma 1. The positive definite matrix M is in block format with matrix A, vector b, and scalar c. The inverse of M is</p><formula xml:id="formula_17">M −1 = A b b T c −1 = K u u T s ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_18">K = A −1 + u s −1 u T (17) u = − s A −1 b (18) s = c − b T A −1 b −1 .<label>(19)</label></formula><p>Proof. For block matrices the inverse is</p><formula xml:id="formula_19">A B B T C −1 = K U U T S ,<label>(20)</label></formula><p>where the matrices on the right hand side are:</p><formula xml:id="formula_20">K = A −1 + A −1 B C − B T A −1 B −1 B T A −1 (21) U = − A −1 B C − B T A −1 B −1 (22) U T = − C − B T A −1 B −1 B T A −1 (23) S = C − B T A −1 B −1 .<label>(24)</label></formula><p>Further if follows that</p><formula xml:id="formula_21">K = A −1 + U S −1 U T .<label>(25)</label></formula><p>We now use this formula for B = b being a vector and C = c a scalar. We obtain</p><formula xml:id="formula_22">A b b T c −1 = K u u T s ,<label>(26)</label></formula><p>where the right hand side matrices, vectors, and the scalar s are:</p><formula xml:id="formula_23">K = A −1 + A −1 b c − b T A −1 b −1 b T A −1 (27) u = − A −1 b c − b T A −1 b −1 (28) u T = − c − b T A −1 b −1 b T A −1<label>(29)</label></formula><formula xml:id="formula_24">s = c − b T A −1 b −1 .<label>(30)</label></formula><p>Again it follows that</p><formula xml:id="formula_25">K = A −1 + u s −1 u T .<label>(31)</label></formula><p>A reformulation using u gives </p><formula xml:id="formula_26">K = A −1 + u s −1 u T (32) u = − s A −1 b (33) u T = − s b T A −1 (34) s = c − b T A −1 b −1 .<label>(35)</label></formula><p>Furthermore holds</p><formula xml:id="formula_28">1 − E T (a) E −1 (a a T ) E(a) −1 1 − E T p (a) E −1 (a a T ) E(a)<label>(38)</label></formula><p>= 1 + (E(a) − Ep(a)) T Var −1 (a) E(a) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The Sherman-Morrison Theorem states</head><formula xml:id="formula_29">A + b c T −1 = A −1 − A −1 b c T A −1 1 + c T A −1 b .<label>(39)</label></formula><p>Therefore we have</p><formula xml:id="formula_30">c T A + b b T −1 b = c T A −1 b − c T A −1 b b T A −1 b 1 + b T A −1 b (40) = c T A −1 b 1 + b T A −1 b − c T A −1 b b T A −1 b 1 + b T A −1 b = c T A −1 b 1 + b T A −1 b .</formula><p>Using the identity </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C VARIANCE OF MEAN ACTIVATIONS IN ELU AND RELU NETWORKS</head><p>To compare the variance of median activation in ReLU and ELU networks, we trained a neural network with 5 hidden layers of 256 hidden units for 200 epochs using a learning rate of 0.01, once using ReLU and once using ELU activation functions on the MNIST dataset. After each epoch, we calculated the median activation of each hidden unit on the whole training set. We then calculated the variance of these changes, which is depicted in <ref type="figure" target="#fig_7">Figure 7</ref> . The median varies much more in ReLU networks. This indicates that ReLU networks continuously try to correct the bias shift introduced by previous weight updates while this effect is much less prominent in ELU networks.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The rectified linear unit (ReLU), the leaky ReLU (LReLU, α = 0.1), the shifted ReLUs (SReLUs), and the exponential linear unit (ELU, α = 1.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>ELUs) if used for unsupervised and supervised learning of deep autoencoders and deep convolutional networks. ELUs with α = 1.0 are compared to (i) Rectified Linear Units (ReLUs) with activation f (x) = max(0, x), (ii) Leaky ReLUs (LReLUs) with activation f (x) = max(αx, x) (0 &lt; α &lt; 1), and (iii) Shifted ReLUs (SReLUs) with activation f (x) = max(−1, x). Comparisons are done with and without batch normalization. The following benchmark datasets are used: (i) MNIST (gray images in 10 classes, 60k train and 10k test), (ii) CIFAR-10 (color images in 10 classes, 50k train and 10k test), (iii) CIFAR-100 (color images in 100 classes, 50k train and 10k test), and (iv) ImageNet (color images in 1,000 classes, 1.3M train and 100k tests).4.1 MNIST4.1.1 LEARNING BEHAVIORWe first want to verify that ELUs keep the mean activations closer to zero than other units.Fully connected deep neural networks with ELUs (α = 1.0), ReLUs, and LReLUs (α = 0.1) were trained on the MNIST digit classification dataset while each hidden unit's activation was tracked. Each ELU networks evaluated at MNIST. Lines are the average over five runs with different random initializations, error bars show standard deviation. Panel (a): median of the average unit activation for different activation functions. Panel (b): Training set (straight line) and validation set (dotted line) cross entropy loss. All lines stay flat after epoch 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Autoencoder training on MNIST: Reconstruction error for the test and training data set over epochs, using different activation functions and learning rates. The results are medians over several runs with different random initializations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>shows the learning behavior of ELU vs. ReLU networks. Panel (b) shows that ELUs start reducing the error earlier. The ELU-network already reaches the 20% top-5 error after 160k iterations, while the ReLU network needs 200k iterations to reach the same error rate. The single-model performance was evaluated on the single center crop with no further augmentation and yielded a top-5 validation error below 10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>ELU networks applied to ImageNet. The x-axis gives the number of iterations and the y-axis the (a) training loss, (b) top-5 error, and (c) the top-1 error of 5,000 random validation samples, evaluated on the center crop. Both activation functions ELU (blue) and ReLU (purple) lead for convergence, but ELUs start reducing the error earlier and reach the 20% top-5 error after 160k iterations, while ReLUs need 200k iterations to reach the same error rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B</head><label></label><figDesc>QUADRATIC FORM OF MEAN AND INVERSE SECOND MOMENT Lemma 2. For a random variable a holds E T (a) E −1 (a a T ) E(a) ≤ 1 (36) and 1 − E T (a) E −1 (a a T ) E(a) −1 = 1 + E T (a) Var −1 (a) E(a) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>E</head><label></label><figDesc>(a a T ) = Var(a) + E(a) E T (a)(41)for the second moment and Eq. (40), we getE T (a) E −1 (a a T ) E(a) = E T (a) Var(a) + E(a) E T (a) a) Var −1 (a) E(a) 1 + E T (a) Var −1 (a) E(a) ≤ 1 .The last inequality follows from the fact that Var(a) is positive definite. From last equation, we obtain further1 − E T (a)E −1 (a a T ) E(a) −1 = 1 + E T (a)Var −1 (a) E(a) .(43)For the mixed quadratic form we get from Eq. (40)E T p(a) E −1 (a a T ) E(a) = E T p (a) Var(a) + E(a) E T (a) (a) Var −1 (a) E(a) 1 + E T (a) Var −1 (a) E(a) . 1 − E T p (a) E −1 (a a T ) E(a) = 1 − E T p (a) Var −1 (a) E(a) 1 + E T (a) Var −1 (a) E(a) (45) = 1 + E T (a) Var −1 (a) E(a) − E T p (a) Var −1 (a) E(a) 1 + E T (a) Var −1 (a) E(a) = 1 + (E(a) − Ep(a)) T Var −1 (a) E(a) 1 + E T (a) Var −1 (a) E(a) . Therefore we get 1 − E T (a) E −1 (a a T ) E(a) (E(a) − Ep(a)) T Var −1 (a) E(a) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Distribution of variances of the median hidden unit activation after each epoch of MNIST training. Each row represents the units in a different layer of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparison of ReLUs, LReLUs, and SReLUs on CIFAR-100. Panels (a-c) show the training loss, panels (d-f) the test classification error. The ribbon band show the mean and standard deviation for 10 runs along the curve. ELU networks achieved lowest test error and training loss.</figDesc><table><row><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu_bn relu elu_bn elu</cell><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">srelu_bn srelu elu_bn elu</cell><cell cols="2">100</cell><cell></cell><cell>leaky_bn leaky elu_bn elu</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell>Test Error [%]</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Error [%]</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Error [%]</cell><cell>60</cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell cols="2">50 Updates (1e3) 100</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="3">50 Updates (1e3) 100</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>50 Updates (1e3) 100</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) ELU -ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) ELU -SReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) ELU -LReLU</cell></row><row><cell cols="2">32</cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu_bn relu elu_bn elu</cell><cell cols="2">32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">srelu_bn srelu elu_bn elu</cell><cell cols="2">31</cell><cell></cell><cell>leaky_bn leaky elu_bn elu</cell></row><row><cell cols="2">4 30 31 Test Error [%]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu leaky srelu elu</cell><cell cols="2">4 30 31 Test Error [%]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu leaky srelu elu</cell><cell cols="2">30 Test Error [%]</cell><cell></cell><cell>relu leaky srelu elu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.6</cell><cell></cell></row><row><cell cols="2">2 3 Train Loss 29</cell><cell>90</cell><cell>110</cell><cell>130 Updates (1e3)</cell><cell>150</cell><cell>170</cell><cell cols="2">2 3 Train Loss 29</cell><cell>90</cell><cell>110</cell><cell></cell><cell>130 Updates (1e3)</cell><cell>150</cell><cell>170</cell><cell cols="2">0.5 29 Train Loss</cell><cell>90</cell><cell>110</cell><cell>130 Updates (1e3)</cell><cell>150</cell><cell>170</cell></row><row><cell></cell><cell></cell><cell cols="4">(d) ELU -ReLU (end)</cell><cell></cell><cell></cell><cell></cell><cell cols="5">(e) ELU -SReLU (end)</cell><cell></cell><cell cols="2">0.4</cell><cell cols="2">(f) ELU -LReLU (end)</cell></row><row><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell cols="2">50 Updates (1e3) 100</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell cols="2">50 Updates (1e3)</cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>110</cell><cell>130 Updates (1e3)</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Training loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Training loss (start)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) Training loss (end)</cell></row><row><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu leaky srelu elu</cell><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">relu leaky srelu elu</cell><cell cols="2">32</cell><cell></cell><cell>relu leaky srelu elu</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Error [%]</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Error [%]</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">30 31 Test Error [%]</cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">29</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell cols="2">50 Updates (1e3) 100</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">25</cell><cell>50 Updates (1e3)</cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>110</cell><cell>130 Updates (1e3)</cell><cell>150</cell><cell>170</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(d) Test error</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(e) Test error (start)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(f) Test error (end)</cell></row><row><cell cols="19">Figure 4: The CNN for these CIFAR-100 experiments consists of 11 convolutional layers arranged in stacks</cell></row><row><cell cols="19">of ([1 × 192 × 5], [1 × 192 × 1, 1 × 240 × 3], [1 × 240 × 1, 1 × 260 × 2], [1 × 260 × 1, 1 × 280 ×</cell></row><row><cell cols="19">2], [1 × 280 × 1, 1 × 300 × 2], [1 × 300 × 1], [1 × 100 × 1]) layers × units × receptive fields.</cell></row><row><cell cols="19">2×2 max-pooling with a stride of 2 was applied after each stack. For network regularization we</cell></row><row><cell cols="19">used the following drop-out rate for the last layer of each stack (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.0). The</cell></row></table><note>L2-weight decay regularization term was set to 0.0005. The following learning rate schedule was applied (0 − 35k[0.01], 35k − 85k[0.005], 85k − 135k[0.0005], 135k − 165k[0.00005]) (iterations [learning rate]). For fair comparisons, we used this learning rate schedule for all networks. During previous experiments, this schedule was optimized for ReLU networks, however as ELUs converge faster they would benefit from an adjusted schedule. The momentum term learning rate was fixed to 0.9. The dataset was preprocessed as described in Goodfellow et al. (2013) with global contrast</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ELUs achieve both lower training loss and lower test error than ReLUs, LReLUs, and SReLUs. Both the ELU training and test performance is significantly better than for other activation functions (Wilcoxon signed-rank test with p-value&lt;0.001). Batch normalization improved ReLU and LReLU networks, but did not improve ELU and SReLU networks (see</figDesc><table><row><cell>, which</cell></row><row><cell>also shows the standard deviation. ELUs yield on average a test error of 28.75(±0.24)%, while SRe-</cell></row><row><cell>LUs, ReLUs and LReLUs yield 29.35(±0.29)%, 31.56(±0.37)% and 30.59(±0.29)%, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of ELU networks and other CNNs on CIFAR-10 and CIFAR-100. Reported is the test error in percent misclassification for ELU networks and recent convolutional architectures like AlexNet, DSN, NiN, Maxout, All-CNN, Highway Network, and Fractional Max-Pooling. Best results are in bold. ELU networks are second best for CIFAR-10 and best for CIFAR-100.</figDesc><table><row><cell>Network</cell><cell>CIFAR-10 (test error %)</cell><cell cols="2">CIFAR-100 (test error %) augmented</cell></row><row><cell>AlexNet DSN NiN Maxout All-CNN Highway Network Fract. Max-Pooling</cell><cell>18.04 7.97 8.81 9.38 7.25 7.60 4.50</cell><cell>45.80 34.57 35.68 38.57 33.71 32.24 27.62</cell><cell>√ √ √ √ √ √</cell></row><row><cell>ELU-Network</cell><cell>6.55</cell><cell>24.28</cell><cell></cell></row><row><cell cols="4">ELU networks are compared to following recent successful CNN architectures: AlexNet</cell></row><row><cell cols="2">(Krizhevsky et al., 2012), DSN</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank the NVIDIA Corporation for supporting this research with several Titan X GPUs and Roland Vollgraf and Martin Heusel for helpful discussions and comments on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="276" />
		</imprint>
	</monogr>
	<note>Rectified factor networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1507.00210</idno>
		<ptr target="http://arxiv.org/abs/1507.00210" />
	</analytic>
	<monogr>
		<title level="j">Natural neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR W&amp;CP: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Gordon, G., Dunson, D., and Dudk, M.</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Maxout networks. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fractional max-pooling. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6071" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6071</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/grosse15.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature extraction through LOCOCODE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="679" to="714" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Field Guide to Dynamical Recurrent Neural Networks</title>
		<editor>Kremer and Kolen</editor>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning Semantic Image Representations at a Large Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.html" />
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative weighted least squares algorithms for neural networks classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Algorithmic Learning Theory (ALT92</title>
		<meeting>the Third Workshop on Algorithmic Learning Theory (ALT92</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">743</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenvalues of covariance matrices: Application to neural-network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2396" to="2399" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficient</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Orr, G. B. and Müller, K.-R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saining</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Topmoumoute online natural gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS)</title>
		<editor>Platt, J. C., Koller, D., Singer, Y., and Roweis, S. T.</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML10)</title>
		<editor>Fürnkranz, J. and Joachims, T.</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toxicity prediction using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeptox</surname></persName>
		</author>
		<idno type="DOI">http:/journal.frontiersin.org/article/10.3389/fenvs.2015.00080</idno>
		<ptr target="http://journal.frontiersin.org/article/10.3389/fenvs.2015.00080" />
	</analytic>
	<monogr>
		<title level="j">Front. Environ. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">80</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML10)</title>
		<editor>Fürnkranz, J. and Joachims, T.</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Riemannian metrics for neural networks i: feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Olivier</surname></persName>
		</author>
		<idno>abs/1303.0818</idno>
		<ptr target="http://arxiv.org/abs/1303.0818" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<ptr target="http://arxiv.org/abs/1301.3584" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning made easier by linear transformations in perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS12)</title>
		<editor>Lawrence, N. D. and Girolami, M. A.</editor>
		<meeting>the 15th International Conference on Artificial Intelligence and Statistics (AISTATS12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Centering neural network gradient factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Orr, G. B. and Müller, K.-R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="207" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compact Approximation of the Exponential Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="853" to="862" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6806" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1507.06228</idno>
		<ptr target="http://arxiv.org/abs/1507.06228" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Toxicity prediction using deep learning. CoRR, abs/1503.01445</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.01445" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Krylov subspace descent for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.4259</idno>
		<ptr target="http://arxiv.org/pdf/1111.4259v1" />
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
		<ptr target="http://arxiv.org/abs/1505.00853" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Complexity issues in natural gradient descent method for training multilayer perceptrons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
