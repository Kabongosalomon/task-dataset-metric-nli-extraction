<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Re-rank Coarse Classification with Local Region Enhanced Features for Fine-Grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ByteDance AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ByteDance AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ByteDance AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
							<email>wangchanghu@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ByteDance AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Re-rank Coarse Classification with Local Region Enhanced Features for Fine-Grained Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained image recognition is very challenging due to the difficulty of capturing both semantic global features and discriminative local features. Meanwhile, these two features are not easy to be integrated, which are even conflicting when used simultaneously. In this paper, a retrievalbased coarse-to-fine framework is proposed, where we rerank the TopN classification results by using the local region enhanced embedding features to improve the Top1 accuracy (based on the observation that the correct category usually resides in TopN results). To obtain the discriminative regions for distinguishing the fine-grained images, we introduce a weakly-supervised method to train a box generating branch with only image-level labels. In addition, to learn more effective semantic global features, we design a multi-level loss over an automatically constructed hierarchical category structure. Experimental results show that our method achieves state-of-the-art performance on three benchmarks: CUB-200-2011, Stanford Cars and FGVC Aircraft. Also, visualizations and analysis are provided for better understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual classification (FGVC) aims at recognizing subordinate categories that belong to the same superior class, such as distinguishing different kinds of wild birds <ref type="bibr" target="#b32">[33]</ref>, cars <ref type="bibr" target="#b18">[19]</ref> or airs <ref type="bibr" target="#b25">[26]</ref>. Unlike general classification tasks, fine-grained sub-categories usually have similar appearances, which can only be classified by some subtle details. For instance, the difference between Yellow Throated Vireo and White Eyed Vireo is the color of certain parts as shown in <ref type="figure" target="#fig_0">Fig 1 (a)</ref>.</p><p>To address this task, lots of methods were proposed that can be divided into global-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> and part/attention-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>. Most of them aim at learning more effective global/part features separately and adopt a softmax to predict final classification results. Unlike these methods, in this work, we propose a novel retrieval-based model that can simultaneously learn effective global features and discriminative local features inspired by DELG <ref type="bibr" target="#b3">[4]</ref>. We refer to the new model as Coarse Classification and Fine Re-ranking (CCFR), where local region enhanced features are generated to re-rank the classification results predicted by the conventional global pooling features. Specifically, our model includes two main branches. One targets to select topn classification results (named as Coarse Classification) predicted by global features, the other aims at learning discriminative local-region features that are fused with global features to build a search-ing database for re-ranking the Coarse Classification results. By this way, our CCFR can make use of the local region enhanced features to correct images which cannot be distinguished accurately only relying on global features. For instance, merely depending on global features failed when the object scale is too small in a full image or the subtle difference lies in some local regions as shown in <ref type="figure" target="#fig_0">Fig 1.</ref> Therefore, the key to our model is how to accurately localize the discriminative part regions. Recently, a number of methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> that utilized the object/part annotations (e.g. bird parts annotation in bird fine-grained classification) were proposed to accurately localize part regions. However, obtaining object/part annotations is very expensive. In this work, we propose a weakly-supervised method to effectively localize discriminative part regions with only image-level labels. Specifically, we leverage a Feature Pyramid Network (FPN) <ref type="bibr" target="#b23">[24]</ref> to generate a series of multi-scale regions, where triplet loss <ref type="bibr" target="#b27">[28]</ref> is used to make local regions more discriminative other than informative with ranking loss <ref type="bibr" target="#b37">[38]</ref>. It is reasonable that discriminative local regions (e.g., bird's head, bird's wings) are more important for the FGVC task than the whole object regions containing the highest informativeness. Visualizations are provided in section 4.5. In addition, to learn more effective global semantic features, we utilize an unsupervised method to automatically construct a hierarchical category structure and then design a Multi-level loss to train our model. Overall, the main contributions of our work are listed as follows:</p><p>• We build a Coarse Classification and Fine Reranking(CCFR) architecture that simultaneously uses both global and part features by re-ranking.</p><p>• We propose a weakly-supervised method to effectively select discriminative local regions without object/part annotations.</p><p>• We utilize an unsupervised method to automatically construct a hierarchical category structure to learn more effective global semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Fine-grained image classification aims at recognizing the objects of the sub-categories from visually similar category and has been studied for many years. Existing methods can be roughly categorized into two types: global-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> and part/attention-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>. The former focuses on how to extract more effective pre-training parameters and design more useful pooling techniques. The latter targets at training a detection network for localizing part regions which are used to perform classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Global-based methods for FGVC</head><p>Cui et al. <ref type="bibr" target="#b6">[7]</ref> utilizes large-scale datasets (e.g., ImageNet <ref type="bibr" target="#b7">[8]</ref> and Inaturalist <ref type="bibr" target="#b31">[32]</ref>) to learn more effective pre-training parameters for domain-specific FGVC tasks. To resolve the difference between these pre-training datasets and the target fine-grained datasets, they propose a measure to estimate domain similarity via Earth Mover's Distance. And then they pre-selects certain classes from the large-scale datasets which match best to the current fine-grained datasets. Similarly, Krause et al. <ref type="bibr" target="#b17">[18]</ref> collect images from the Internet to enrich the FGVC datasets proving the effectiveness of large-scale datasets for FGVC performance. Other methods aim at developing advanced pooling strategies. For instance, Lin et al. <ref type="bibr" target="#b22">[23]</ref> propose a bilinear structure to compute the pairwise feature interactions by two independent CNN. Similar works are <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45]</ref>. Besides, researchers also combine Fisher vectors to enhance the global representations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>. All of these works target at enriching the global features representation to obtain better classification results. In our model, to learn more effective global semantic features, we propose a Multi-level loss which depends on an automatically constructed hierarchical category structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Part-based methods for FGVC</head><p>One can design a part-based classification method by employing the part annotations if they exist. However, such annotations are expensive and usually unavailable, and the current researchers mainly focus on localizing part-region with only image-level labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35]</ref>. He et al. <ref type="bibr" target="#b12">[13]</ref> propose a sophisticated reinforcement learning method to estimate how many and which image regions are helpful to distinguish the categories. Ge et al. <ref type="bibr" target="#b11">[12]</ref> use Mask R-CNN and CRF-based segmentation alternately to extract rough object instances. However, such a multi-stage training process is hard to implement which significantly hamper the availability in practical use. Yang et al. <ref type="bibr" target="#b37">[38]</ref> adopt a ranking loss to train an FPN network to obtain the most informative regions. However, such a loss function encourages the model to select boxes containing entire objects other than discriminative local regions. To overcome such a problem, we utilize the triplet loss to select more discriminative regions.</p><p>Cao et al. <ref type="bibr" target="#b3">[4]</ref> design a DELG model that unifies global and local features into a single deep model to obtain accurate retrieval results on Image Retrieval Tasks. Inspired by this work, in our method we design a re-ranking policy to improve the performance by correcting the uncertain topn classification results. <ref type="figure">Figure 2</ref>. The overview of our CCFR architecture. It mainly includes two branches: 1) On the top branch, we utilize the Triplet loss and the Scale-separated NMS to discover more discriminative local regions, and integrate these region features with the whole image features to obtain the final embedding features, which are used to build a searching database. 2) On the bottom branch, we first obtain the Coarse Classifications (the topn Softmax probabilities), and then re-rank them by the statistic of the retrievals from the searching database. The Feature Extractor is pretrained by a proposed Multi-level loss and is updated by the combination of a Triplet loss and a Softmax loss during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe our Coarse Classification and Fine Re-ranking (CCFR) architecture in detail. The overview is shown in <ref type="figure">Fig 2,</ref> which includes two main branches: 1) The top branch first extracts discriminative local-region features used to enhance global features, and then builds a database for re-ranking. 2) The bottom branch first acquires the topn classification results with global features, and then re-ranks them with the constructed searching database to obtain the final results. As shown in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref>, satisfactory results were obtained based on global features, however, these methods usually fail in some cases depending on subtle parts (e.g., <ref type="figure" target="#fig_0">Fig 1)</ref>. To solve such problems, a series of methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed to learn local features. However most of them only adopt the convention softmax classification. Unlike these methods, our CCFR is a retrieval-based method which can effectively correct the misclassified images with the combination of the topn classification and searching results. Besides, comparing with the current state-of-the-art StackcLSTM <ref type="bibr" target="#b11">[12]</ref> which is a very messy multi-stage training model, the model in our method is easier to implement and train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-level loss for learning effective global features</head><p>As we have known, feature expression is fundamental for visual classification tasks. How to learn effective features is the key to these tasks. The human visual cognition system processes from rough to fine-grained. (e.g., we first realize that it belongs to a bird at a glance, and then we confirm that it belongs to Vireo or other bird types through careful observation.) To imitate the above cognition process, we organize the categories into Multi-level loss according to the visual appearance. Specifically, we first automatically construct a hierarchical category system through visual feature clustering. As shown in the <ref type="figure" target="#fig_1">Fig 3,</ref> the four birds on the top (bottom) are more similar to each other which can be naturally grouped into one super class. We then propose a Multi-level loss to utilize the constructed category system. We adopt two standard Softmax loss on the children (origin) category and super (cluster) category and then design a constraint loss between these two categories:</p><formula xml:id="formula_0">1 = − C i=1 y i log p i (1) 2 = − C f j=1 y j log p j (2) h = max(0, p children − p parent )<label>(3)</label></formula><p>where p i is the probability of class i in children categories, p j is the probability of class j in super categories, p children denotes the average of all children category probabilities belonging to the same super category, and p parent is the corresponding super category probability, C is the number of children classes, C f is the number of super classes, and 1 and 2 are the standard Softmax loss. h is the constraint loss between super category probability and children's. The final Multi-level loss is defined as follows:</p><formula xml:id="formula_1">multi = 1 + λ * 2 + h (4)</formula><p>where λ is the hyper-parameter. In our setting, λ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Localizing discriminative local regions with weakly supervising</head><p>To select discriminative local regions, we leverage a topdown pathway with lateral connections detection network inspired by the Feature Pyramid Networks (FPN) <ref type="bibr" target="#b23">[24]</ref> as shown in <ref type="figure">Fig 2.</ref> We can obtain a series of feature maps with different spatial resolutions. By using these multi-scale feature maps, we can generate local regions among different scales and ratios. Specifically, for an input image of size 448, we set the bounding-box scales to {96, 192} and aspect ratio to 1:1. Given that different parts of the object own different scales (e.g., the bird's head area is smaller than its wings), performing NMS over boxes of all scales may result in larger boxes suppressing the smaller ones. Thus, we perform NMS on each scale separately (named as Scaleseparated NMS). Then we resize the selected boxes to the pre-defined size (e.g., 224×224) and feed them into the feature extractor to obtain local features. To prevent overfitting, we share parameters of feature extractor used in local regions with the one used in global features. To better capture the spatial relationships among the different regions, for each scale, we concatenate the local features along the channel direction and fuse them with 1 × 1 convolution filters as shown in <ref type="figure">Fig 2 (</ref>we name it as Fusion Network).</p><p>The key of this process is how to select discriminative local regions. Given that the part/object annotations for FGVC are expensive, it is desirable to train the model with only image-level labels (weakly supervised). As mentioned above, the NTS utilized a ranking loss to select informative regions, other than discriminative regions, leading to a preference for larger boxes (as shown in the top row of <ref type="figure" target="#fig_2">Fig 4)</ref>. Triplet loss <ref type="bibr" target="#b27">[28]</ref> has shown beneficial for learning discriminative local features by maximizing distance of different classes and minimizing the distance of the same class. Thus, we adopt the triplet loss to learn discriminative local features, which can be defined as follows:</p><formula xml:id="formula_2">(x A , x p , x N ) = f (sim(x A , x P ) − sim(x A , x N ) − a)<label>(5)</label></formula><p>where sim(·) denotes the cosine similarity, x A is an anchor input, x P denotes a positive input of the same class as x A , x N is a negative input of a different class from x A , a is a margin between positive and negative pairs, f (·) is defined as follows:</p><formula xml:id="formula_3">f (y) = max(0, −y)<label>(6)</label></formula><p>where y is the value of sim(</p><formula xml:id="formula_4">x A , x P ) − sim(x A , x N ) − a.</formula><p>Each of (x A ), (x P ) and (x N ) is the embedding features by concatenating the fused local features and global features. Finally, the total loss is defined as:</p><formula xml:id="formula_5">= − C i=1 y i log p i + µ * (x i A , x i P , x i N )<label>(7)</label></formula><p>where µ is the hyper-parameter, in our setting µ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Re-rank the coarse classification by retrieval</head><p>Up to now, we have obtained the topn probability scores (coarse classification results) and discriminative local enhanced features (anchor x A features). Then we construct the searching database using these features extracted from the training samples. We treat the image that needs to be reorganized as a query (q), and by computing the cosine similarity between the query and the searching database we obtain the topm similarity scores with their labels. We finally re-rank the classification results based on these similarity scores and labels.</p><p>The detailed re-ranking formulation is listed as follows. Let X={x c k } be the selected training samples as the searching database, where x c k is the local enhanced feature vector influenced by our model (k = 1, 2, ..., N is the sample index, c = 1, 2, ..., C is the class label index). Again, let sim(q, X)={sim(q, x c k )} be the similarity of the testing sample q between all the training samples in the database. rank(q, X, topm) is the most similar topm training samples of testing q measured by sim(q, X)</p><p>Then the finally re-ranked score of the testing sample q w.r.t class c is  where α and β are the weights balancing the two terms, S f (q, c) is the Softmax probability of q w.r.t class c, T sf is a threshold for softmax score, and Sc(q, c) is the normlized similarity score between query and the class c which is calculated as</p><formula xml:id="formula_6">S c q = S f (q, c), S f (q, c) &gt;= T sf α * S f (q, c) + β * Sc(q, c), S f (q, c) &lt; T sf<label>(8)</label></formula><formula xml:id="formula_7">Sc(q, c) = Nc k=1 sim(q, x c k )/ topn c=1 Nc k=1 sim(q, x c k ) s.t.sim(q, x c k ) &gt; T sc , x c k ∈ rank(q, X, topm)<label>(9)</label></formula><p>where T sc is a threshold score for searching similarity. In this way, we could probably distinguish and correct the extremely similar sub-categories which usually reside in the topn classification results with non-dominant softmax scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments &amp; Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct comprehensive experiments to evaluate our proposed CCFR algorithm on Caltech-UCSD Birds (CUB-200-2011 <ref type="bibr" target="#b32">[33]</ref>), FGVC Aircraft <ref type="bibr" target="#b25">[26]</ref> and Stanford Cars <ref type="bibr" target="#b18">[19]</ref> which are popular benchmarks for fine-grained category classification. The details of these three datasets are shown in <ref type="table">Table 1</ref>. Note that we only use the image-level labels in our all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In all our implementations, ResNet-50 is chosen as the feature feature extractor. We preprocess each image to size 448×448, and select 2 local regions for each scale (there are two scales), and set the NMS threshold to 0.25. We adopt Momentum SGD with an initial learning rate 0.001 and multiply it by 0.1 for each 30 epochs, and set the weight decay to 1e-4, and set the batch-size to 16. We first utilize the ranking loss <ref type="bibr" target="#b37">[38]</ref> to train the FPN for satisfactory detection performance, and then use the Softmax loss and Triplet loss to train our model. For re-ranking in Equation8, we set the topn as 5, and set α to 0 and β to 1.0 which means only using the searching similarity when the Softmax probability is low. The threshold T sf is set differently according to the roughly average Top1 Softmax probabilities on different datasets (hear we set 0. ResNet-50 84.5 --Spatial-RNN <ref type="bibr" target="#b35">[36]</ref> M-Net/D-Net -88.4 -BCN <ref type="bibr" target="#b9">[10]</ref> ResNet-50 87.7 90.3 94.3 ACNet <ref type="bibr" target="#b13">[14]</ref> ResNet-50 88.1 92.4 94.6 DCL <ref type="bibr" target="#b5">[6]</ref> ResNet-50 87.8 93.0 94.5 DF-GMM <ref type="bibr" target="#b34">[35]</ref> ResNet-50 88.8 93.8 94.8 a-pooling <ref type="bibr" target="#b29">[30]</ref> ResNet-50 86.5 -91.6 MA-CNN <ref type="bibr" target="#b43">[44]</ref> VGG-19 86.5 89.9 -NTS <ref type="bibr" target="#b37">[38]</ref> ResNet-50 87.5 91.4 93.9 API-net <ref type="bibr" target="#b46">[47]</ref> ResNet-50 87.7 93.0 94.8 GCL <ref type="bibr" target="#b33">[34]</ref> ResNet-50 88.3 93.2 94.0 MGE <ref type="bibr" target="#b38">[39]</ref> ResNet-50 88.5 -93.9 CS-Parts <ref type="bibr" target="#b15">[16]</ref> ResNet-50 89.5 -92.5 Inceptin-v3 <ref type="bibr" target="#b6">[7]</ref> Inception-v3 89.6 90.7 93.5 PMG <ref type="bibr" target="#b8">[9]</ref> ResNet  <ref type="figure">Figure 5</ref>. Rectifying a misclassified image by re-ranking. The ground-truth of the testing image is class 7, and it is misclassified into class 8 by a low classification probability. The topm images with the highest similarity scores are retrieved from the database, and an new re-ranked score is calculated for each of the topn classes according to Equation8. Finally, the class 7 with the biggest re-ranked score is taken as the recognized class. and Airs). More analysis about these re-ranking parameters is given in the ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Analysis</head><p>We compare our CCFR with a number of recent works on three widely used benchmarks in <ref type="table">Table 2</ref>. From the results, we find that our method outperforms all previous methods. To conduct fair comparisons with existing mod-els, we use ResNet-50 as the feature extractor which is commonly used. Given that object/part annotations are laborintensive or infeasible in practice, we train our CCFR with only image-level labels.</p><p>The third column of <ref type="table">Table 2</ref> shows the comparison results on CUB-200-2011. It shows that our CCFR achieves the highest Top1 accuracy and increases the accuracy by 0.7%. Compared to NTS <ref type="bibr" target="#b37">[38]</ref> which tries to obtain informative local regions with a ranking loss, we utilize the triplet loss to select more discriminative local regions, leading to better attention on discriminative areas such as bird's head, wings and feet as shown in <ref type="figure" target="#fig_2">Fig 4.</ref> Comparing with StackLSTM <ref type="bibr" target="#b11">[12]</ref> which is the best result in CUB-200-2011 up to now, we achieve a 0.7% improvement. Besides, it is worth noting that StackLSTM is a very messy multi-stage training model that hampers the availability in practical use, while our proposed CCFR is easier to implement.</p><p>The fourth and fifth columns show the comparison results on Stanford Cars and FGVC Aircraft respectively. Our CCFR achieves new state-of-the-art results with 94.1% Top1 accuracy on FGVC Aircraft and 95.49% Top1 accuracy on Stanford Cars. In the second last row of <ref type="table">Table 2</ref>, we show that our CCFR w\o re-ranking also achieves satisfactory results which benefit from the elaborately designed architecture and effective training loss (i.e. the triplet loss for local regions, and the Multi-level loss for pretraining).</p><p>Also, from the last two rows, it is clear that re-ranking can bring further improvement for the Top1 accuracy (especially when the Top1 softmax probability is relatively low, which usually means the model hesitates among the topn classes). By re-ranking the topn classes with the statistic of the topm (topm=50) most similar retrievals from the database (usually constructed with the training samples), we probably rectify the uncertain Top1 class and rank the more confident class to Top1, and thus increase the Top1 accuracy (see <ref type="bibr">Equation 8</ref>). <ref type="figure">Fig 5 shows</ref> the misclassified image is rectified by re-ranking. Specifically, the ground-truth of the testing image is class 7 which is misclassified into class 8 by a low Softmax probability. Then we select the topm images with the highest similarity scores from the database, and get an new score for each of the topn classes according to our formulation, and finally take the class (class 7) with the biggest new score as the recognized class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>To investigate the influence of different components of our CCFR architecture, we conduct ablation studies and report the results.</p><p>Influence of Multi-level Loss. We investigate the influence of our Multi-level loss through experiments with standard Softmax loss. As shown in the second and third rows of <ref type="table" target="#tab_2">Table 3</ref>, by applying our designed Multi-level loss on ResNet-50, the Top1 accuracy is improved by 0.7%, which demonstrates its effectiveness. In this work, we utilize the Multi-level loss to pretrain the ResNet-50 as initialization of the backbone of our CCFR for better performance.</p><p>Influence of Scale-separated NMS. As shown in the fourth and fifth rows of <ref type="table" target="#tab_2">Table 3</ref>, by applying the Scaleseparated NMS, the performance improves from 90.3% to 90.4%, where the fourth row conducts the conventional NMS on all boxes collected from different scales, and the fifth row adopts the scale-separated NMS to reduce the mu- tual influence among different scale boxes. The results verify the effectiveness of the Scale-separated NMS.</p><p>Influence of Fusion Network. From the comparison between the fifth and the sixth rows in <ref type="table" target="#tab_2">Table 3</ref>, we can find that the Top1 accuracy is improved from 90.4% to 90.7%, where the sixth row adopts Fusion Network (describe in section 3.2) to fuse the local region features obtaining from scaleseparated NMS, while the fifth row directly concatenates these features after the global average pooling.</p><p>Influence of Re-ranking. Since our CCFR is a retrievalbased model, we also compare the results obtained only by retrieval in <ref type="table">Table 4</ref>. Specifically, we first compute the cosine similarity between the testing image and all the training images in database, and then use the Top1 image label as the final result. We can find that classification-based results are much better than retrieval-based results on CUB- <ref type="figure">Figure 7</ref>. Illustration of the Top4 selected boxes by our model on three benchmark datasets. The bird's head and wings are deemed as the most crucial areas for birds, and for the cars are the light and head/tail, and for the airs are the turbine and tail. <ref type="table">Table 4</ref> verifies the idea that the combination of the retrieval and the classification can bring performance improvement. To further investigate the influence of the main re-ranking parameters, i.e. T sf , T sc and topn as described in Equation8, more ablation experiments are conducted on the FGVC Aircraft dataset. Specifically, we investigate the change of the CCFR accuracy by ranging topn from 2 to 6, T sf from 0.4 to 0.95, and T sc from 0.5 to 0.95 (Note that topm is completely determined by T sc , α and β are simply set to 0 and 1.0). All the results as shown in <ref type="figure" target="#fig_5">Fig 6. From Fig 6 (a)</ref>, we can find that for different T sc , the curves have the same trend and CCFR accuracy achieves the highest value when T sc is 0.7 and T sf is 0.75 (here topn is fixed to 2, and a consistent phenomenon can be observed for other topn values). In <ref type="figure" target="#fig_5">Fig 6 (b)</ref>, we compare the effect of different T sc and topn values while fixing T sf to 0.75 (the optimal value in (a)), and we can find the accuracy achieves best when T sc is 0.7 and topn is 2 (it seems that most misclassified cases occur between the fist and second place ranked by the Softmax probabilities). With topn fixed to 2, <ref type="figure" target="#fig_5">Fig 6 (c) (d)</ref> visualize the surface of the CCFR accuracy over T sc and T sf respectively from different views, where warm color (yellow) represents high accuracy and cold color (blue) represents low accuracy. It can be clearly see that, T sf has the greatest influence on the re-ranking effect (which is reasonable), and the accuracy is insensitive to T sc (which means a wide range of T sc could achieve similar accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200-2011 and Stanford Cars (competitive on the FGVC Aircraft). And the last row in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>We draw the selected boxes in <ref type="figure">Fig 7 to</ref> better analyze the regions where our model focuses on. Two boxes with the highest confidence score are shown for each scale. It can be seen that, except for the region containing the whole object, our model deems head and wing as the most crucial areas for birds (the top two rows), and the light and head/tail for cars, and the turbine and tail for airs. All of these are consistent with human intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a Coarse Classification and Fine Re-ranking framework for the FGVC problem and get state-of-the-art results on three common datasets, where local region enhanced features are generated to re-rank the topn classification results predicted by the semantic global features. We design an effective structure to integrate the local and global features, and use the triplet loss to discover more discriminative regions for distinguishing hard samples (supervised with only image-level labels). And to learn more effective semantic global features, we design a Multilevel loss acted on an auto-clustered hierarchical category structure for the backbone network pretraining. Some ablation experiments are provided for further analysis, which makes the work more convinced and solid.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some hard instances from the CUB-200-2011[33]. Subfigure (a) and (b) show very similar categories, where the subtle differences lie in the color of certain parts. Sub-figure (c) shows the different poses of the same category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The generated hierarchical categories by clustering. Birds with similar appearances are grouped into the same super class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The Top4 selected boxes by NTS and our CCFR. Compared to NTS (the top row), our model tends to focus on smaller boxes containing discriminative parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Caltech-UCSD Birds. CUB-200-2011 is the most widely used dataset with 200 wild bird species. It contains 11,788 images spanning 200 sub-categories. The ratio of train data and test data is roughly 1:1 and each species has only 30 images for training. FGVC Aircraft. FGVC Aircraft dataset consists of 10,000 images with 100 categories. The ratio of train data and test data is roughly 2:1. The dataset is organized in a four-level hierarchy, from finer to coarser: Model, Variant, Family, Manufacturer. Most images are airplanes. Stanford Cars. Stanford Cars dataset consists of 16,185 images over 196 categories. The data is divided into 8,144 training images and 8,041 testing images, and each category has been divided roughly into a 50-50 split. Categories are typically at the level of Make, Model, Year (e.g. 2012 BMW M3 coupe).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>5 on CUB-200-2011, and 0.7 on Cars Method Base Model CUB Acc.(%) Airs Acc.(%) Cars Acc.(%) ResNet-50[21]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Ablation experiments for the main re-ranking parameters on FGVC Aircraft. Sub-figure (a) shows the influence of different T sf and Tsc values (topn is fixed to 2). Sub-figure (b) shows the impact of different topn and Tsc values while fixing T sf to 0.75. Sub-figure (c) and (d) visualize the surface over Tsc and T sf respectively from different views, where warm color (yellow) means high accuracy and cold color (blue) represents low accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments for investigating the influence of different components about our method on CUB. The columns represent the different components (Multi-level loss, Scale-separated NMS, Fusion Network and Re-ranking). The rows represent the different models, where "ResNet-50 + local region" means that local regions are utilized in addition to the global features, and CCFR is our proposed method.</figDesc><table><row><cell>Model</cell><cell>Multi-level loss</cell><cell>Scale-separated NMS</cell><cell>Fusion Network</cell><cell>Re-ranking CUB Acc.(%)</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell>84.5</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell>85.2</cell></row><row><cell cols="2">ResNet-50 + local region</cell><cell></cell><cell></cell><cell>90.3</cell></row><row><cell cols="2">ResNet-50 + local region</cell><cell></cell><cell></cell><cell>90.4</cell></row><row><cell cols="2">ResNet-50 + local region</cell><cell></cell><cell></cell><cell>90.7</cell></row><row><cell>CCFR</cell><cell></cell><cell></cell><cell></cell><cell>91.1</cell></row><row><cell>Method</cell><cell>CUB Airs Cars</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retrieval</cell><cell>88.8 93.4 91.54</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Classification 90.7 93.1 95.37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCFR</cell><cell>91.1 94.1 95.49</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Table 4. The comparison of the accuracy among Retrieval, Classi-</cell><cell></cell><cell></cell></row><row><cell cols="2">fication and CCFR on different datasets.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archith John</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets. CoRR, abs/1406.2952</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unifying deep local and global features for efficient image search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise confusion for finegrained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1713" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1235" to="1255" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10468" to="10477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Twophase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3554" to="3563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification-specific parts for improving fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Endto-end learning of a fisher vector encoding for part features in fine-grained recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attribute mix: Semantic data augmentation for fine grained recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep LAC: deep localization, alignment and classification for finegrained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized orderless pooling performs implicit salient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4970" to="4979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The whole is more than its parts? from explicit to implicit pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="749" to="763" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessie</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-propagation based correlation learning for weakly supervised fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12289" to="12296" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via guassian mixture model oriented discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep attention-based spatially recursive networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical part matching for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="438" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fine-grained pose prediction, normalization, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1511.07063</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4279" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
