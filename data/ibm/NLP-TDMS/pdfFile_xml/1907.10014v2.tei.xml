<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally Consistent Horizon Lines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kluger</surname></persName>
							<email>kluger@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informationsverarbeitung</orgName>
								<orgName type="institution">Leibniz Universität Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
							<email>ackermann@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informationsverarbeitung</orgName>
								<orgName type="institution">Leibniz Universität Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
							<email>michael.yang@utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Scene Understanding Group</orgName>
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<email>rosenhahn@tnt.uni-hannover.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Informationsverarbeitung</orgName>
								<orgName type="institution">Leibniz Universität Hannover</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally Consistent Horizon Lines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The horizon line is an important geometric feature for many image processing and scene understanding tasks in computer vision. For instance, in navigation of autonomous vehicles or driver assistance, it can be used to improve 3D reconstruction as well as for semantic interpretation of dynamic environments. While both algorithms and datasets exist for single images, the problem of horizon line estimation from video sequences has not gained attention. In this paper, we show how convolutional neural networks are able to utilise the temporal consistency imposed by video sequences in order to increase the accuracy and reduce the variance of horizon line estimates. A novel CNN architecture with an improved residual convolutional LSTM is presented for temporally consistent horizon line estimation. We propose an adaptive loss function that ensures stable training as well as accurate results. Furthermore, we introduce an extension of the KITTI dataset which contains precise horizon line labels for 43699 images across 72 video sequences. A comprehensive evaluation shows that the proposed approach consistently achieves superior performance compared with existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Horizon lines are important low-level geometric image features that provide essential information about the relation between a 3D scene and the camera observing it. They can be used to infer the camera pose in form of a ground plane normal or a gravity vector, respectively. In autonomous driving, ground planes are often used to infer semantic properties of the dynamic environment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. Other applications include estimation of vanishing points <ref type="bibr" target="#b46">[47]</ref>, which provide information about the 3D structure of a scene, image metrology <ref type="bibr" target="#b6">[7]</ref>, perspective correction <ref type="bibr" target="#b25">[26]</ref> and camera pose estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. <ref type="figure">Figure 1</ref>: Example sequence with our temporally consistent estimation in green (long dashes) and the best single frame algorithm in yellow (short dashes). Ground truth in white/black. Top three rows: sample frames with horizon lines from the sequence. Bottom row: Horizon offset trajectory over time, best viewed in colour. The temporally consistent estimation is more accurate on average and contains fewer outliers. For many applications, utilising temporal consistency has been demonstrated to improve performance. Examples include depth estimation <ref type="bibr" target="#b38">[39]</ref>, motion segmentation <ref type="bibr" target="#b3">[4]</ref>, action recognition <ref type="bibr" target="#b21">[22]</ref>, super resolution <ref type="bibr" target="#b16">[17]</ref> and superpixel segmentation <ref type="bibr" target="#b32">[33]</ref>. Single image approaches for horizon line estimation may do gross mistakes when the image provides few or misleading clues. As illustrated by <ref type="figure">Fig. 1</ref>, an approach based on multiple images is less susceptible to these problems if it is able to transfer information from previous images of a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>In this work, we present a novel approach for temporally consistent horizon line estimation based on a convolutional neural network combined with an improved convolutional long short-term memory (LSTM). A comprehensive evaluation demonstrates the ability of this approach to generate more accurate horizon line estimates with less variance. Since a naïve loss function does not track the geometric error of horizon lines very well, and a loss based on the geometric error exhibits singularities that may cause instability, we propose an adaptive loss function that combines both losses with a cosine annealing schedule. This loss function yields significantly more accurate horizon estimates, yet ensures that the neural network training remains stable. In an ablation study, we investigate the influence of several hyperparameters and architecture choices on the performance of the neural network models. Furthermore, the KITTI Horizon dataset is presented, an extension of the well established KITTI benchmark <ref type="bibr" target="#b13">[14]</ref>. It contains accurate horizon line annotations for all video sequences of the KITTI dataset <ref type="bibr" target="#b12">[13]</ref>. In summary, our main contributions are:</p><p>1. We present a novel CNN architecture for temporally consistent horizon line estimation based on an improved residual convolutional LSTM.</p><p>2. We propose an adaptive loss function that yields accurate horizon line estimates and ensures stable training. <ref type="bibr" target="#b2">3</ref>. A large-scale video dataset for temporally consistent horizon line estimation, the KITTI Horizon dataset. To the best of our knowledge, this is the first video dataset with accurate horizon line ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Types of Horizon Lines</head><p>It is possible to distinguish three types of horizon lines: the visible horizon, the true horizon and the geometrical horizon. The visible horizon is the apparent line which separates earth and sky. Its appearance is often shaped by the surroundings of an observer in the presence of entities like mountains, buildings or trees. If the view of an observer is unobstructed -at sea, for example -the visible horizon becomes identical to the true horizon. Assuming a spherical earth surface, the true horizon is the projection of a circle containing all points on the earth which are tangent to light rays passing through the point of view of an observer.</p><p>The geometrical horizon h is defined as the vanishing line, i.e. the projection of the line at infinity, for any plane orthogonal to the local gravity vector g: with R being the orientation and K being the intrinsic calibration of the camera. Without loss of generality, we assume that g ∝ 0, 1, 0 T is parallel to the the zenith direction. As illustrated by <ref type="figure" target="#fig_0">Fig. 2</ref>, the geometrical horizon is generally not identical to the vanishing line of the plane an observer is standing on, as its normal vector may not be parallel to g, e.g. when located on an incline. Being a theoretical construction, the geometrical horizon is imperceptible to an observer. However, given the intrinsic calibration K, knowledge of the geometrical horizon is sufficient to estimate camera tilt and roll w.r.t. a global coordinate system. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the conceptual differences between the three horizons. Since the remainder of this paper considers the geometrical horizon, it will be simply referred to as the horizon from hereon.</p><formula xml:id="formula_0">h ∝ K −T Rg ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Related Work</head><p>In the past, numerous approaches for horizon line estimation have been proposed, and they can be differentiated into a number of categories. Most methods rely on vanishing points (VPs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref> which they detect by grouping oriented elements like line segments or edges into clusters which have the same orientation in 3D space. If at least two vanishing points are known, the horizon line can be derived. Some of these  methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> rely on the Manhattan-world assumption <ref type="bibr" target="#b5">[6]</ref>, i.e. they restrict their solution space to three VPs of orthogonal directions and are hence applicable to only a limited number of scenes. Others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> use the more permissible Atlanta-world assumption <ref type="bibr" target="#b35">[36]</ref>, which expects all horizontal VPs to be of orthogonal direction to a zenith VP. This assumption is still restrictive, as it does not cover scenes which contain planes that are oblique to a defined zenith. Several of the aforementioned methods consider two benchmark datasets in their evaluation: the York Urban Dataset <ref type="bibr" target="#b8">[9]</ref> (YUD) and the Eurasian Cities Dataset <ref type="bibr" target="#b2">[3]</ref>. Both are relatively small and of limited diversity w.r.t. the types of scenes they depict. In 2016, Workman et al. <ref type="bibr" target="#b42">[43]</ref> presented the Horizon Lines in the Wild (HLW) dataset, which contains horizon line ground truth for 100553 images taken at various locations. Availability of such a large-scale dataset has lead to an emergence of deep-learning based algorithms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref> more recently. Workman et al. <ref type="bibr" target="#b42">[43]</ref> present a convolutional neural network (CNN) which directly estimates the horizon line from a single image, formulated as either a regression or a classification task. Lee et al. <ref type="bibr" target="#b26">[27]</ref> use a different approach: they randomly sample lines within the image borders and feed them, along with the image, into a CNN which incorporates their proposed line pooling layer. This CNN then provides a classification whether the sampled line is the horizon of the image and, in addition, computes refined line coordinates. The method of Zhai et al. <ref type="bibr" target="#b46">[47]</ref> is a hybrid approach. It uses a CNN, similar to <ref type="bibr" target="#b42">[43]</ref>, to predict a horizon line, but then jointly optimises its location together with VPs which are estimated based on line segments that have been detected in a preprocessing step. All these works have in common that they target the problem of single image horizon line estimation. To the best of our knowledge, general datasets and algorithms targeted specifically at horizon line estimation from video sequences do not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">KITTI Horizon Dataset</head><p>We introduce the KITTI Horizon Dataset, a new addition to the KITTI raw dataset <ref type="bibr" target="#b12">[13]</ref> with accurate horizon line annotations for all video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Limitations of Existing Datasets</head><p>Three datasets have been commonly used for horizon line estimation in recent years: the York Urban Dataset <ref type="bibr" target="#b8">[9]</ref> (YUD), the Eurasian Cities Dataset <ref type="bibr" target="#b2">[3]</ref> (ECD) and Horizon Lines in the Wild <ref type="bibr" target="#b42">[43]</ref> (HLW). YUD is a relatively small dataset of 102 images depicting in-and outdoor scenes within a confined area, taken with the same camera under similar conditions. While ECD is somewhat more diverse than YUD, it is still very small with just 103 images. HLW, on the other hand, is significantly larger and contains 100553 images, making it much better suited for data-intensive deep learning approaches. Unlike YUD and ECD, HLW was not labelled manually, but in an automatic process using structure from motion. It appears, however, that this process has limited precision, as some images in HLW have clearly inaccurate horizon line labels. Beyond that, all three datasets have in common that they do not contain video sequences, which means that they can only be used for single image horizon line estimation and are ill-suited for research on temporally consistent horizon line estimation. To our knowledge, the Singapore Maritime Dataset <ref type="bibr" target="#b31">[32]</ref> (SMD) is the only video dataset with annotated horizon lines. Although it is relatively large, containing 21981 annotated frames, its diversity is very limited since it exclusively shows maritime scenes of similar appearance. More importantly, however, the horizon labels in SMD describe the true horizon as opposed to the geometrical horizon. Consequently, a new dataset is needed for temporally consistent geometrical horizon line estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">KITTI</head><p>KITTI <ref type="bibr" target="#b12">[13]</ref> is a computer vision dataset which was captured using a sensor array mounted on top of a vehicle. Sensors used for the recordings include four front-facing video cameras and a high accuracy inertial measurement unit (IMU), among others. Several benchmarks for various applications, such as object detection, depth estimation or semantic segmentation, have been published <ref type="bibr" target="#b13">[14]</ref>. For horizon line estimation such a benchmark does not exist. We can, however, compute accurate horizon line ground truth using the IMU data provided by KITTI, at no additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Horizon Line Ground Truth</head><p>KITTI provides an accurate absolute pose R IMU of the IMU in 3D space for every image. Together with the relative pose between IMU and camera N ∈ {1, 2, 3, 4}, R IMU→N , we can compute the normalised gravity vector g N ∝ R IMU→N R IMU 0, 1, 0 T in the coordinate system of the camera. As explained in Sec. 1.2, the projection of a gravity vector g into the camera using Eq. 1 yields the horizon line in homogeneous coordinates:</p><formula xml:id="formula_1">h N ∝ K −T N R IMU→N R IMU 0, 1, 0 T .<label>(2)</label></formula><p>As this process requires no manual labelling or other human intervention, we can compute the ground truth horizon for all images fully automatically. <ref type="figure" target="#fig_3">Fig. 4</ref> shows a few examples.</p><p>In the left-hand image, the ground plane appears nearly perpendicular to the gravity vector, hence the horizon line is virtually identical to the vanishing line of that ground plane.</p><p>In the other two images, however, they are clearly distinct due to the fact that the ground plane is sloping downwards (middle image) or upwards (right-hand image). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Train, validation and test split</head><p>The complete published KITTI dataset consists of 47962 frames across 157 sequences. Several sequences show the same stationary scene, and only differ w.r.t. the people walking across the image. As these are of negligible value for our task, we discarded all but one, so that 72 sequences with 43699 frames remain. As no official split exists for the raw dataset, we divided the video sequences into roughly 70% training, and 15% validation and test data each. Care was taken to ensure that sequences showing very similar scenes, e.g. the same intersection, do not end up in different parts of the split. As there is a strong imbalance in sequence length, e.g. some sequences contain less than 100 frames and others have several thousand, we divided one of the longer videos equally and put it into the test and validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Single Image Estimation</head><p>We obtained the source code of recent single image algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. In addition, we compare our own single image algorithm along with these methods. Thereby, we obtain a detailed and unbiased comparison that clearly highlights the features of our temporally consistent approach. Our single image algorithm is based on a CNN, similar to the regression approach presented in <ref type="bibr" target="#b42">[43]</ref>. We parametrise the horizon line h by offset ω and slope θ. With W being the image width, its representation in homogeneous coordinates is defined as:</p><formula xml:id="formula_2">h(ω, θ) = sin θ, cos θ, − W 2 sin θ − ω cos θ T . (3)</formula><p>We replace the GoogleNet <ref type="bibr" target="#b37">[38]</ref> of <ref type="bibr" target="#b42">[43]</ref> with the more recent and efficient ResNet <ref type="bibr" target="#b17">[18]</ref>, and use the most shallow 18-layer variant (ResNet18). The classification layer of the ResNet is replaced by two fully connected layers with single real valued outputs for ω and θ. Apart from downscaling the image, we do not perform any pre-or post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Temporally Consistent Estimation</head><p>Possibly the simplest way to utilise the temporal consistency of video sequences is applying a single-frame algorithm first, and then averaging the results. For online applications, a reasonable choice of filter would be the exponential moving average, or exponential smoothing filter <ref type="bibr" target="#b4">[5]</ref>.</p><p>Given a sequence x t , the output of the filter is defined as:</p><formula xml:id="formula_3">s t = αx t + (1 − α)s t−1<label>(4)</label></formula><p>While easy to implement, it always just achieves a compromise between suppressing noise and outliers, and preserving actual trajectory changes. Bai et al. <ref type="bibr" target="#b1">[2]</ref> propose temporal convolutional networks (TCN), an extension of regular CNNs by causal convolutions <ref type="bibr" target="#b29">[30]</ref> along an additional temporal dimension. Across time, the TCN has a fixed field of view which limits the sequence length along which it is able to infer correlations. We therefore chose to investigate an approach based on long-short term memory (LSTM) <ref type="bibr" target="#b18">[19]</ref>. We devised a novel approach combining the ResNet <ref type="bibr" target="#b17">[18]</ref> architecture with an improved convolutional LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convolutional LSTM</head><p>LSTM cells are a particular type of recurrent neural network (RNN) that have been proven effective in modelling both long-and short-term dependencies of sequential data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. The convolutional LSTM (Con-vLSTM) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref> is a variant that operates on 3D tensors instead of vectors and replaces all matrix multiplications with kernel convolutions. Given a sequence of inputs X 1 , . . . , X t , the cell state C t and hidden state H t of a Con-vLSTM can be computed as follows, where ' * ' is the convolution operator and ' ' denotes the Hadamard product:</p><formula xml:id="formula_4">i t = σ(W xi * X t + W hi * H t−1 + b i ) (5) f t = σ(W xf * X t + W hf * H t−1 + b f ) (6) o t = σ(W xo * X t + W ho * H t−1 + b o ) (7) C t = f t C t−1 + i t tanh(W xc * X t + W hc * H t−1 + b c ) (8) H t = o t tanh(C t )<label>(9)</label></formula><p>The hidden state is usually treated as the output of the cell, i.e. Y t = H t . Variants with additional connections <ref type="bibr" target="#b14">[15]</ref>, or other activation functions <ref type="bibr" target="#b38">[39]</ref> exist as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Residual Convolutional LSTM</head><p>We propose an improved convolutional LSTM structure that incorporates both residual and dense connections. As previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref>  makes them easier and faster to train. He et al. <ref type="bibr" target="#b17">[18]</ref> integrated residual connections into a CNN. If we consider a shallow stack l of convolutional layers performing an operation F l (x) on an input x l−1 , the output x l of such a stack is:</p><formula xml:id="formula_5">x l = g(F l (x l−1 ) + x l−1 ) , with g(·)</formula><p>being a nonlinear activation function, e.g. ReLU. In <ref type="bibr" target="#b43">[44]</ref>, this idea was applied to a network of stacked LSTM cells. Each LSTM cell computes a hidden state h t and a cell state c t based on an input x t and the states at the previous time step:</p><formula xml:id="formula_6">h t , c t = LSTM(h t−1 , c t−1 , x t ) .</formula><p>A residual connection is then applied to generate the final output of the layer:</p><formula xml:id="formula_7">y t = h t + x t .<label>(10)</label></formula><p>In this case, the non-linearity g(·) is part of the LSTM, i.e. it is applied before the residual connection. The notion of improving information flow through a neural network via connections that skip a number of layers was implemented in yet another manner by Huang et al. <ref type="bibr" target="#b20">[21]</ref>. In their DenseNet CNN architecture, feature-maps of M preceding layers x l−M , . . . x l−1 are concatenated channel-wise and fed into the current layer F l (x):</p><formula xml:id="formula_8">x l = g(F l ([x l−M , . . . , x l−1 ])</formula><p>. In order to arrive at our improved ConvLSTM, we combine the aforementioned principles and incorporate them as follows. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates our proposed structure on the left side, while the right side shows the standard ConvLSTM with a naïve residual connection as per Eq. 10 for comparison. In keeping with the original ResNet definition, we define a residual connection between input and output:</p><formula xml:id="formula_9">Y t = tanh(Ŷ t + X t ) .<label>(11)</label></formula><p>As Eq. 9 shows, the hidden state H t amounts to a masked cell state C t . We argue that this inhibits the flow of information from both X t and H t−1 to the output Y t . Normally, information must pass through Eqs. 5-9 and thus through C t before it eventually reaches Y t . We therefore introduce an additional convolutional layer into the ConvLSTM, which directly takes the concatenation of X t , H t−1 and an intermediate hidden stateĤ t as an input, similar to the way convolution layers in DenseNet operate, in order to produce an intermediate outputŶ t :</p><formula xml:id="formula_10">Y t = W xy * X t + W hy * H t−1 + Wĥ y * Ĥ t .<label>(12)</label></formula><p>Finally, in order to avoid application of the tanh activation twice onto the information from C t , we switch the order of operation in Eq. 9, i.e.:</p><formula xml:id="formula_11">H t = o t C t ,<label>(13)</label></formula><formula xml:id="formula_12">H t = tanh(Ĥ t ) .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Horizon Line Estimation Network</head><p>We expand our single image CNN described in Sec. 3 with our modified ConvLSTM presented in Sec. 4.2 in order to create a temporally consistent architecture. As <ref type="figure" target="#fig_5">Fig. 6</ref> shows, two ConvLSTM layers are inserted between the last convolutional layer and the global average pooling layer of our ResNet18-based CNN. Intuitively, applying the ConvL-STM at this stage makes most sense, as we would expect it to find temporal correlations between higher-level features which are most pertinent to the task of horizon estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss Function</head><p>Our CNN has two real valued outputs: offset ω and slope θ of the predicted horizon line. We compute two loss terms; the first one is the Huber loss of ω and θ computed w.r.t. the ground truth; the second one is the maximum horizon error within the image. Combining these two losses allows us to benefit from a gain in accuracy elicited by minimising the maximum horizon error, while avoiding the instability it can cause. The Huber loss <ref type="bibr" target="#b22">[23]</ref> is defined as: We define the first loss term as the Huber loss of ω and θ computed w.r.t. the ground truthω andθ:</p><formula xml:id="formula_13">L H (x,x) = 1 2 (x −x) 2 for |x −x| ≤ 1 , |x −x| − 1 2 otherwise.</formula><formula xml:id="formula_14">L ω,θ = L H (ω,ω) + L H (θ,θ) .<label>(15)</label></formula><p>As this loss term does not exactly track the maximum horizon error, which is the quantity we actually seek to minimise, we have defined a second loss term. The maximum horizon error is defined as the maximum distance between the estimated horizon h(ω, θ), as defined by Eq. 3, and the ground truth horizon h(ω,θ) between the left-and rightmost borders of the image, normalised to image height H.</p><p>The y-coordinate of the intersection of h with a vertical line at x is determined by:</p><formula xml:id="formula_15">y(ω, θ, x) = x − W 2 tan θ − ω .<label>(16)</label></formula><p>Let d y,0 and d y,W be the left-and right-most distances between the two horizons, d y,x = y(ω, θ, x) − y(ω,θ, x) . The maximum horizon error L e can then be defined as:</p><formula xml:id="formula_16">L e = 1 H d y,0 for d y,0 ≥ d y,W , 1 H d y,W otherwise.<label>(17)</label></formula><p>While L e directly reflects the quantity we aim to minimise, it contains singularities for θ = ( π /2 + nπ), n ∈ N, due to the tan θ term in Eq. 16. This causes L e to become excessively large if θ is poorly estimated, which may be the case especially at the beginning of neural network training. We therefore use only L ω,θ at first, when estimates are still very inaccurate and noisy, and gradually switch over to L e on a cosine schedule similar to <ref type="bibr" target="#b28">[29]</ref>. With t being the current epoch and T being the maximum number of epochs, the schedule is defined by: λ(t) = 1 2 + 1 2 cos π · t T . Using this, the final loss L is defined as:</p><formula xml:id="formula_17">L(t) = λ(t) · L ω,θ + (1 − λ(t)) · L e .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We empirically demonstrate the effectiveness of our temporally consistent horizon line estimation pipeline on the KITTI Horizon validation and test sets and compare it with state-of-the-art single-image algorithms and other temporally consistent baselines. Additional ablation studies show the importance of individual parts of this pipeline for achieving these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We implemented the proposed neural network architectures using PyTorch <ref type="bibr" target="#b30">[31]</ref>. On KITTI Horizon, all networks were trained for 160 epochs with stochastic gradient descent using a cosine annealing learning rate schedule <ref type="bibr" target="#b28">[29]</ref> starting at 10 −1 and ending at 10 −3 . Training was repeated four times with different random seeds, and the model with the highest validation AUC chosen. We downscale each image by a factor of two and apply cutout <ref type="bibr" target="#b9">[10]</ref>, colour jitter, random rotations and random shifts for data augmentation. We initialise the weights of the first nine convolutional layers of the networks from a ResNet18 pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> while other layers are initialised randomly. Training batches always contain B sequences of S consecutive frames from the KITTI Horizon training set, and batch size B and sequence length S were set to fulfil S · B = 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Metrics</head><p>As in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>, we compute the maximum horizon error defined in Eq. 17 for every image in the dataset. A cumulative error histogram for errors up to 0.25 is generated and its area under the curve (AUC) determined for a set of images. This horizon error AUC value gauges the overall accuracy of the estimated horizon lines. We also report the mean squared error (MSE), which is more sensitive to outliers than the AUC. In addition, we compute the estimated camera pose vector p ∝ Rg ∝ K T h via inversion of Eq. 1. We determine the angular error ξ between p and the ground truth posep for every image and report the AUC of the cumulative error histogram for ξ ≤ 5 • : cos ξ = p Tp p 2 p 2 . For applications that rely on horizon lines estimated from a video stream, it is desirable for the estimations be accurate as well as stable. We propose another metric to measure undesirable fluctuations that do not reflect actual changes of the horizon over time: the average AUC (horizon) total variation A T V . For a sequence n of length T n of estimated horizons h n,t and corresponding ground truthĥ n,t , with t ∈ [1, T n ] and n ∈ [1, N ], we compute the derivative ∂L n,t e /∂t of the horizon error according to Eq. 17 using second order approximation. With M = N n=1 T n being the total number of images, the mean of its absolute calculated over all sequences yields the average total variation:</p><formula xml:id="formula_18">MSE ×10 −3 A T V × 10 −</formula><formula xml:id="formula_19">A T V = 1 M N n=1 Tn t=1 ∂L n,t e ∂t .<label>(19)</label></formula><p>This metric is invariant to constant deviations from the ground truth but sensitive to higher frequency fluctuations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">KITTI Horizon Results</head><p>We report all metrics on the KITTI Horizon validation and test set for for following single frame algorithms: the VP based methods of Lezama et al. <ref type="bibr" target="#b27">[28]</ref>, Kluger et al. <ref type="bibr" target="#b23">[24]</ref> and Simon et al. <ref type="bibr" target="#b36">[37]</ref>, the hybrid approach of Zhai et al. <ref type="bibr" target="#b46">[47]</ref> and the CNN based approach of Workman et al. <ref type="bibr" target="#b42">[43]</ref>. We also include results for our single frame CNN baseline (cf. Sec. 3), trained on either HLW or KITTI, for an average baseline which simply always predicts the mean of the training set, a TCN <ref type="bibr" target="#b1">[2]</ref> based temporally consistent approach with causal convolutions in the last three layers, and of course for our temporally consistent pipeline presented in Sec. 4. The results are listed in Tab. 1 and <ref type="figure" target="#fig_7">Fig. 7</ref>. As these numbers show, methods based on line segments and vanishing points <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref> are unable to deliver consistent and accurate horizon estimates on KITTI. The best performing method among them is Zhai et al. <ref type="bibr" target="#b46">[47]</ref> with 60.97%/50.98% AUC (validation/test), which still lags behind the simplest average baseline (69.40%/64.18%). In    addition, the very large mean squared error (MSE) and average total variation (A T V ) values -up to several thousand -indicate that these methods may fail catastrophically in some outlier cases. In comparison, all CNN based methods -including Workman et al. <ref type="bibr" target="#b42">[43]</ref> and our own single frame CNN -are significantly more accurate with at least 70.32%/63.64% AUC. More importantly, the comparatively smaller MSE and A T V show that these methods are much less prone to extreme outliers. If we compare the CNN of <ref type="bibr" target="#b42">[43]</ref> with our own single-frame CNN trained on HLW, we observe that <ref type="bibr" target="#b42">[43]</ref> performs better overallall metrics but validation AUC are better to a relevant degree. This is unsurprising, as <ref type="bibr" target="#b42">[43]</ref> augmented their training with an additional 500000 images sampled from Google Street View, while we just used HLW. Naturally, if trained on the KITTI Horizon dataset, the accuracy of our single frame CNN increases significantly: from 71.10%/63.64% to 77.42%/74.08%, which is a 21.8%/28.7% relative increase. Best results on all metrics are obtained with our temporally consistent approach (Sec. 4), with relative improvements upon the single frame CNN between 1.8% (test AUC) and 12.1% (test A T V ). While the smoothness A T V of the single frame CNN improves measurably without diminishing overall accuracy if we additionally apply an exponential smoothing filter (Eq. 4, α = 0.5), similar gains can be achieved when applied to the temporally consistent CNN as well, so it still retains its advantage. We also trained a TCN <ref type="bibr" target="#b1">[2]</ref> based on our single frame CNN with causal temporal convolutions of widths 3, 3, and 5 in the last three layers and a receptive field of nine frames. Surprisingly, it performs worse than the single frame CNN on all metrics but A T V . We suspect that the TCN is more susceptible to overfitting, as it achieved a lower training loss, but higher validation loss compared to our other CNNs. Compared to our ConvLSTM based network, it is on par w.r.t. A T V on the test set, but measurably worse otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Loss function</head><p>In order to investigate whether our new loss defined in Sec. 4.4 had the desired effect on estimation accuracy, we also trained our main CNN model described in Sec. 4.3 using just the Huber loss defined in Eq. 15 and also used by <ref type="bibr" target="#b42">[43]</ref>. As Tab. 2 shows, we report an AUC of 71.96% and an MSE of 7.851 · 10 −3 on the test set. Using our newly defined loss, however, we achieve an AUC of 74.55% and an MSE of 6.731 · 10 −3 , which marks a considerable relative improvement of 9.2% and 14.3% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Temporal information</head><p>As Tab. 1 shows, our temporally consistent approach based on ConvLSTMs is able to achieve more accurate horizon estimates with significantly less variance. In order to ascertain that this is due to the ConvLSTM utilising temporal correlations, and not simply due other architecture changes that arose as a result, we retrained our main CNN model with all temporal connections disabled, i.e. we reset the LSTM states at every time step. On the test set, this yields an AUC of 74.36% and A T V of 5.699 · 10 −3 . When we enable the temporal connections of the LSTM, overall accuracy increases moderately -74.55% AUC -and A T V decreases noticeably to 4.984 · 10 −3 , which is a relative improvement of 12.6%. We conclude that the ConvLSTM is indeed able to retain temporal consistency in a meaningful way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">ConvLSTM Architecture</head><p>We compare our ConvLSTM architecture described in Sec 4.2 against a ConvLSTM using a naïve residual path implementation and ConvLSTM without the residual path. As Tab. 2 shows, the naïve residual path already increases accuracy dramatically, from 64.29% to 74.01% AUC, and is evidently crucial for deep LSTM networks. On par w.r.t. A T V , our proposed ConvLSTM improves AUC and MSE upon the naïve implementation, yielding a relative improvement of 2.1% and 4.0% respectively. While both approaches are able to generate smooth trajectories, our improved ConvL-STM is measurably more accurate on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The horizon line is an important geometric feature which can be used in many computer vision tasks, such as camera pose and ground plane estimation. Due to their importance, horizon lines have received considerable attention in recent years. Nonetheless, neither has any other work has focused on temporal consistency, nor are there appropriate datasets available. In this work, an extension of the well-known KITTI database is presented that adds horizon line annotations to 72 sequences. We furthermore propose a neural network for temporally consistent horizon line estimation in video sequences. It utilises an improved convolutional LSTM and an adaptive loss function that yields more accurate horizon line estimates and ensures stable training. The experimental evaluation demonstrates that the proposed architecture achieves superior performance for a diverse set of metrics which measure, for instance, accuracy and smoothness of trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>We implemented all proposed neural network architectures using PyTorch <ref type="bibr" target="#b30">[31]</ref> version 0.4.1. We used stochastic gradient descent with momentum (0.9) and L 2 regularisation (10 −4 ), and a cosine annealing learning rate schedule <ref type="bibr" target="#b28">[29]</ref> starting at 10 −1 and ending at 10 −3 . On KITTI Horizon, all networks were trained for 160 epochs, and for 256 epochs on HLW. Where applicable (see Sec. C), training was repeated four times with different random seeds, and the model with the highest validation AUC chosen. We downscale each image by a factor of two and apply the following augmentation techniques:</p><formula xml:id="formula_20">• Random rotations β ∼ U(−2 • , 2 • ) w.r.t. the image centre.</formula><p>• Random shifts s x ∼ U(−10 px, 10 px) and s y ∼ U(−10 px, 10 px).</p><p>• Horizontal flips with probability p = 0.5.</p><p>• Colour jitter with brightness factor γ b ∼ U(0.75, 1.25), contrast factor γ c ∼ U(0.75, 1.25), saturation factor γ s ∼ U(0.75, 1.25) and hue factor γ h ∼ U(−0.25, 0.25).</p><p>• Greyscale transformation with probability p = 0.1.</p><p>• Cutout <ref type="bibr" target="#b9">[10]</ref> with width w ∼ U(0, 512) and height h ∼ U(0, 512).</p><p>We initialise the weights of the first nine convolutional layers of the networks from a ResNet18 pretrained on Ima-geNet <ref type="bibr" target="#b7">[8]</ref> while other layers are initialised randomly. Training batches always contain B sequences of S consecutive frames from the KITTI Horizon training set, and batch size B and sequence length S were set to fulfil S · B = 128. We set S = 1 for single frame approaches and S = 32 for temporally consistent approaches. Sampled sequences were always non-overlapping. At test time, we process the whole sequence as it appears in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporal Convolutional Networks</head><p>As a possible alternative to our ConvLSTM based CNN, we briefly discussed Temporal Convolutional Networks (TCN) <ref type="bibr" target="#b1">[2]</ref> in our paper. The authors of <ref type="bibr" target="#b1">[2]</ref> propose it as a purely feed-forward alternative to recurrent neural network structures -such as LSTM and ConvLSTM -for sequence modelling, and present promising results. We therefore implemented a TCN for the horizon line estimation task and compared it to our proposed ConvLSTM based architecture. The concept of TCNs is based on causal convolutions along the temporal dimension of data. For a sequence of vectors x t ∈ R C , the 1D causal convolution across time with a kernel h ∈ R M ×D×C can be defined as:</p><formula xml:id="formula_21">y t = M m=1 h m x t−m+1 , y t ∈ R D ,<label>(20)</label></formula><p>where M denotes the number of elements of the sequence included in the convolution. Unlike a regular convolution, the result y t of the causal convolution only depends on values of x τ for τ ≤ t, i.e. no information from the future is considered. This can easily be generalised for sequences of images or feature maps X t ∈ R W ×H×C and a corresponding kernel H ∈ R M ×A×B×D×C :</p><formula xml:id="formula_22">Y t = M m=1 H m * X t−m+1 ,<label>(21)</label></formula><p>where ' * ' denotes the 2D convolution operator commonly used in CNNs, W and H are image width and height, and A × B is the kernel size. Using regular convolutional layers readily available in deep learning frameworks, causal convolutional layers can be realised by simply shifting the output along the temporal axis by M /2 steps. If L such layers with temporal convolution lengths M l are stacked to form a deeper network, the temporal field of view of this network becomes:   <ref type="figure" target="#fig_8">Fig. 8</ref>, we compare the training and validation losses of the 3-3-5-7 TCN with our proposed ConvLSTM network. The TCN achieves a noticeably lower training loss, but converges to a significantly higher validation loss. This indicates a lower ability of the TCN to generalise and may explain the poor validation and test performance.</p><formula xml:id="formula_23">S fov = 1 − L + L l=1 M l .<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Quantitative</head><p>In order to gauge the uncertainty of our results arising from the randomness involved in neural network training, we repeated training for most of our experiments four times with varying random seeds. These experiments include: (a) our proposed ConvLSTM based CNN (Ours), (b) the ablation study using the naïve residual ConvLSTM, (c) the ablation study using the ConvLSTM with disabled temporal connections (non-temporal) and (d) the single frame CNN. In addition to the results for the training runs which perform best on the validation set, we also report mean and standard deviation over all four runs in Tab. 4. As these results show, the single-frame and non-temporal variants perform worse than the temporally consistent approaches, even when averaged over multiple runs. Comparing our proposed Con-vLSTM with the naïve residual variant, we observe similar performance w.r.t. A T V on both validation and test sets. AUC and MSE results on the validation set are similar as well, w.r.t. both best and mean performance. At the same time, the test performance of our proposed model with the best validation performance is measurably better than the naïve residual variant, which indicates an improved generalisation ability of our proposed ConvLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Qualitative</head><p>In <ref type="figure" target="#fig_9">Fig. 9</ref>, we show three example horizon line trajectories from the KITTI Horizon dataset. In the first example, <ref type="figure" target="#fig_9">Fig. 9a</ref>, the single frame estimation fluctuates heavily, while our proposed temporally consistent approach remains much more stable throughout the sequence. The second example, <ref type="figure" target="#fig_9">Fig. 9b</ref>, contains segments where the single frame estimation is moderately better, e.g. between frames 150 and 200, but also segments where the single frame estimation shows severe fluctuations, e.g. frames 200 to 300. Besides that, the results of the two algorithms are mostly very similar. Lastly, in <ref type="figure" target="#fig_9">Fig. 9c</ref>, we can observe a failure case of our approach. In the middle section between frames 100 and 300, both algorithms perform similarly, but at the beginning and at the end, our proposed method exhibits a relatively constant but large error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. KITTI Horizon Dataset</head><p>We provide a few examples from the KITTI Horizon dataset with ground truth horizons in <ref type="figure" target="#fig_10">Fig. 10</ref>, in order to give an impression of the variety of scenes it contains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Horizon Lines in the Wild</head><p>As mentioned in the paper, we noticed that some of the horizon line labels provided by the Horizon Lines in the   <ref type="table">Table 4</ref>: Horizon estimation results on the KITTI Horizon validation and test sets. We compare our proposed temporally consistent approach with two ablation studies and our single-frame CNN, see Sec. C.1. We present the results of the training run with the best validation AUC out of four runs. The numbers in brackets are (mean ± standard deviation) over all four runs.</p><p>Wild (HLW) <ref type="bibr" target="#b42">[43]</ref> dataset are visibly inaccurate. In <ref type="figure" target="#fig_11">Fig. 11</ref>, we show a few examples which convey the severity of the problem. This is by no means an exhaustive analysis, but we hypothesise that the HLW ground truth contains noticeable errors, which should be kept in mind when using this dataset. However, a more detailed analysis is required to quantify and test our hypothesis.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Cropped images from a KITTI sequence with annotated horizon lines (top), and a sketch of the trajectory of the car with gravity vector g and plane normal n (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of horizon line types (Sec. 1.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example frames from KITTI with annotated horizon line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>have shown, residual connections improve gradient flow in deep neural networks, which ConvLSTM with residual paths as described in Sec. 4.2. The [·, ·]-operator denotes concatenation along the channel axis. Left: Our proposed ConvLSTM with residual paths and dense connections. Changes w.r.t. a standard ConvLSTM: residual connection from X t to Y t in green; dense connection from X t ,Ĥ t and H t−1 to Y t in orange; reversal of operation order in blue. Right: Part of a standard ConvLSTM, with a naïve implementation of a residual connection in purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Proposed neural network structure employing ConvLSTM layers as described in Sec. 4.3. Two ConvLSTM layers are inserted between the last convolutional layer and the global average pooling layer of our ResNet18-based CNN. The outputs ω and θ are the offset and slope, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Cumulative horizon error histograms with AUC values for KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Training and validation loss curves for our proposed ConvLSTM based CNN (green) and the 3-3-5-7 TCN (yellow, Sec. B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Three example horizon line trajectories from the KITTI Horizon dataset. We compare our proposed temporally consistent approach (green) with the single frame CNN (yellow) and the ground truth (black). Top left: example image from the video sequence. Top right: signed horizon error over time. Bottom: horizon offset and slope over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Examples from the KITTI Horizon dataset with ground truth horizon lines (yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>A selection of images sampled from the HLW dataset. The horizon line labels (yellow) are visibly inaccurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Lezama et al. [28] 34.17% 30.45% &gt;1000 &gt;1000 2397 1537 28.79% 25.28% Kluger et al. [24] 54.27% 48.21% &gt;1000 &gt;1000 188.6 206.4 47.29% 41.47% Simon et al. [37] 57.03% 47.84% 84.26 224.0 65.94 88.71 50.97% 41.98% Zhai et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>AUC (pose)</cell></row><row><cell></cell><cell></cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>[47]</cell><cell></cell><cell cols="7">60.97% 50.98% &gt;1000 &gt;1000 91.56 1575 53.52% 43.47%</cell></row><row><cell cols="2">Workman et al. [43]</cell><cell cols="2">70.32% 66.48%</cell><cell>9.208</cell><cell cols="4">11.19 6.893 8.430 62.36% 58.58%</cell></row><row><cell>Average baseline</cell><cell></cell><cell cols="2">69.40% 64.18%</cell><cell>8.800</cell><cell cols="4">12.20 6.091 5.123 59.45% 54.98%</cell></row><row><cell>single frame</cell><cell>trained on HLW</cell><cell cols="2">71.10% 63.64%</cell><cell>10.41</cell><cell cols="4">14.31 13.90 15.71 64.02% 55.20%</cell></row><row><cell>(Sec. 3)</cell><cell cols="3">trained on KITTI-H 77.42% 74.08%</cell><cell>6.024</cell><cell cols="4">7.025 5.061 5.585 70.51% 66.62%</cell></row><row><cell></cell><cell>w/ exp. smoothing</cell><cell cols="2">77.44% 74.11%</cell><cell>5.986</cell><cell cols="4">6.987 4.337 4.687 70.50% 66.64%</cell></row><row><cell>TCN [2] (3-3-5)</cell><cell></cell><cell cols="2">75.42% 71.80%</cell><cell>6.392</cell><cell cols="4">8.318 4.945 4.937 67.33% 64.21%</cell></row><row><cell cols="2">temporally consistent (Sec. 4)</cell><cell cols="3">78.09% 74.55% 5.427</cell><cell cols="4">6.731 4.619 4.984 71.17% 67.33%</cell></row><row><cell></cell><cell>w/ exp. smoothing</cell><cell cols="3">78.11% 74.68% 5.405</cell><cell cols="4">6.712 4.159 4.404 71.19% 67.49%</cell></row></table><note>Table 1: Horizon estimation results on the KITTI Horizon (Sec. 2) validation and test sets using the metrics described in Sec. 5.2. AUC: higher is better; MSE and A T V : lower is better. Refer to Sec. 5.3 for a detailed discussion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AUC MSE A T V Huber loss (Sec. 5.4.1) 71.96% 7.851 7.051 non-temporal (Sec. 5.4.2) 74.36% 7.266 5.699 w/o residual (Sec. 5.4.3) 64.29% 11.60 5.279 naïve residual (Sec. 5.4.3) 74.01% 7.009 4.967</figDesc><table><row><cell>Ours (Sec. 4)</cell><cell>74.55% 6.731 4.984</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study (Sec. 5.4) results on the KITTI Horizon test set. MSE and A T V scaled by 10 −3 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(S fov = 9) 75.42% 71.80% 6.392 8.318 4.946 4.937 67.33% 64.21% 1-3-5-5 (S fov = 11) 75.82% 71.65% 6.498 8.329 4.997 5.119 68.43% 64.14% 1-3-5-7 (S fov = 13) 75.83% 72.23% 6.383 7.909 4.932 5.043 68.42% 64.64% 3-3-5-7 (S fov = 15) 76.08% 72.25% 6.453 8.185 4.956 5.023 68.66% 64.69% 3-5-5-7 (S fov = 17) 75.49% 71.57% 7.084 8.458 5.117 5.263 68.52% 64.15% 5-5-5-7 (S fov = 19) 75.76% 72.21% 6.573 8.002 5.075 4.980 68.42% 64.73% single frame 77.42% 74.08% 6.024 7.025 5.061 5.585 70.51% 66.62% Ours 78.09% 74.55% 5.427 6.731 4.619 4.984 71.17% 67.33%</figDesc><table><row><cell></cell><cell cols="2">AUC (horizon)</cell><cell cols="2">MSE ×10 −3</cell><cell cols="2">A T V × 10 −3</cell><cell cols="2">AUC (pose)</cell></row><row><cell>configuration</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>1-3-3-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Horizon estimation results on the KITTI Horizon validation and test sets comparing several TCN variants (Sec. B) with our single frame CNN and our proposed temporally consistent approach.We converted our ResNet18 based single-frame CNN into a TCN by replacing the last three respectively four 2D convolutional layers with 3D convolutional layers. We considered various configurations with 1 ≤ M l ≤ 7 and 9 ≤ S fov ≤ 19, which we trained on the KITTI Horizon dataset as described in Sec. A. We named the configurations according to the values set for M l in the last four layers, e.g. 1-3-3-5 means M L−3:L = [1, 3, 3, 5]. In order to avoid zero padding in the temporal dimension, we sampled additional S fov − 1 previous frames for each sequence in a training batch, if possible. Tab. 3 shows the results of our TCNs on the KITTI Horizon validation and test sets. Compared to our ConvLSTM based approach, the TCNs perform poorly w.r.t. all metrics but A T V . They even fall behind the single frame CNN w.r.t. AUC and MSE. We suspect that the TCNs are more susceptible to overfitting. In</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>77.03 ± 0.296) (74.19 ± 0.219) (6.125 ± 0.0635) (7.084 ± 0..19 ± 0.514) (74.43 ± 0.332) (5.926 ± 0.154)</figDesc><table><row><cell></cell><cell cols="2">AUC (horizon)</cell><cell>MSE ×10 −3</cell><cell></cell><cell cols="2">A T V × 10 −3</cell></row><row><cell></cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell><cell>val</cell><cell>test</cell></row><row><cell>single</cell><cell>77.42%</cell><cell>74.08%</cell><cell>6.024</cell><cell>7.025</cell><cell>5.061</cell><cell>5.585</cell></row><row><cell>frame</cell><cell></cell><cell></cell><cell></cell><cell>0886)</cell><cell>(5.276 ± 0.237)</cell><cell>(5.670 ± 0.0893)</cell></row><row><cell>non-</cell><cell>77.63%</cell><cell>74.36%</cell><cell>5.852</cell><cell>7.266</cell><cell>5.368</cell><cell>5.699</cell></row><row><cell cols="5">temporal (77(7.122 ± 0.228)</cell><cell>(5.569 ± 0.234)</cell><cell>(5.982 ± 0.291)</cell></row><row><cell>naïve</cell><cell>78.19%</cell><cell>74.01%</cell><cell>5.534</cell><cell>7.009</cell><cell>4.583</cell><cell>4.967</cell></row><row><cell>residual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by German Research Foundation (DFG) grant Ro 2497 / 12-2.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix is structured as follows: In Sec. A, we state more in-depth details of the implementation of our proposed method. Sec. B describes our implementation of Temporal Convolutional Networks, and provides additional experiments and evaluation. Sec. C.1 provides a more detailed look into the quantitative results of our proposed approach as well as its ablation studies, while Sec. C.2 contains horizon trajectories from KITTI Horizon to highlight a few best-, average-and worst-case examples of our approach. In Sec. C.3, we provide a few examples from the KITTI Horizon dataset which convey an impression of the variety of scenes it contains. Sec. D discusses examples from the Horizon Line in the Wild dataset which have visibly inaccurate horizon line labels.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d scene priors for road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric image parsing in man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tretiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporally consistent motion segmentation from rgb-d video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bertholet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Smoothing, forecasting and prediction of discrete time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single view metrology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="148" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision-guided flight stability and control for micro air vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Nechyba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Ifju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waszak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="617" to="640" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular road mosaicing for urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional convolutional lstm for the detection of violence in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koutilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A perceptual measure for deep single image camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hold-Geoffroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gambaretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video super-resolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning for vanishing point detection using an inverse gnomonic projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic upright adjustment of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding vanishing points via point alignments in image primal and dual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grompone Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video processing from electro-optical sensors for object detection and tracking in a maritime environment: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rachmawati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rajabally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1993" to="2016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporally consistent superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jachalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new approach to vanishing point detection in architectural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Atlanta world: An expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A-contrario horizonfirst vanishing point detection using second-order grouping laws</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporally consistent depth estimation in videos with recurrent architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tananaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-iterative approach for fast and accurate vanishing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tardif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Self-similar sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust camera selfcalibration from monocular images of manhattan worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wildenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Horizon lines in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A minimum error vanishing point detection approach for uncalibrated monocular images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a non-manhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
