<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamir</forename><surname>Mustafa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and whitebox attack scenarios and show significant gains in comparison to state-of-the art defenses 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Adversarial examples contain small, humanimperceptible perturbations specifically designed by an adversary to fool a learned model <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b9">10]</ref>. These examples pose a serious threat for security critical applications, e.g. autonomous cars <ref type="bibr" target="#b0">[1]</ref>, bio-metric identification <ref type="bibr" target="#b33">[34]</ref> and surveillance systems <ref type="bibr" target="#b27">[28]</ref>. Furthermore, if a slight perturbation added to a benign input drastically changes the deep network's output with a high-confidence, it reflects that our current models are not distinctively learning the fundamental visual concepts. Therefore, the design of robust deep networks goes a long way towards developing <ref type="bibr" target="#b0">1</ref> Code and models are available at: https://github.com/ aamir-mustafa/pcl-adversarial-defense  and its adversarial counterpart (PGD attack) for standard softmax trained model and our method on MNIST (top row) and CIFAR-10 (bottom row) datasets. Note that our method correctly maps the attacked image to its true-class feature space. reliable and trustworthy artificial intelligence systems.</p><p>To mitigate adversarial attacks, various defense methods have recently been proposed. These can be broadly classified into two categories: (a) Reactive defenses that modify the inputs during testing time, using image transformations to counter the effect of adversarial perturbation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26]</ref>, and (b) Proactive defenses that alter the underlying architecture or learning procedure e.g. by adding more layers, ensemble/adversarial training or changing the loss/activation functions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14]</ref>. Proactive defenses are generally more valued, as they provide relatively better robustness against white-box attacks. Nevertheless, both proactive and reactive defenses are easily circumvented by the iterative white-box adversaries <ref type="bibr" target="#b1">[2]</ref>. This paper introduces a new proactive defense based on a novel training procedure, which maximally separates the learned feature representations at multiple depth levels of the deep model. We note that the addition of perturbations in the input domain leads to a corresponding polytope in the high-dimensional manifold of the intermediate features and the output classification space. Based upon this observation, we propose to maximally separate the polytopes for different class samples, such that there is a minimal overlap between any two classes in the decision and intermediate fea-ture space. This ensures that an adversary can no longer fool the network within a restricted perturbation budget. In other words, we build on the intuition that two different class samples, which are visually dissimilar in the input domain, must be mapped to different regions in the output space. Therefore, we must also enforce that their feature representations are well separated along the hierarchy of network layers. This is achieved by improving within-class proximities and enhancing between-class differences of the activation maps, along multiple levels of the deep model. As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, the penultimate layer features learnt by the proposed scheme are well separated and hard to penetrate compared with the easily attacked features learnt using standard loss without any deep supervision. As evidenced with empirical evaluations (Sec. 5), the proposed method provides an effective and robust defense by significantly outperforming current state-of-the-art defenses under both white-box and black-box settings. Also, we experimentally show that our method does not suffer from the obfuscated gradient problem, which is otherwise the case for most existing defenses.</p><p>Our approach provides strong evidence towards the notion that the adversarial perturbations exist not only due to the properties of data (e.g. high-dimensionality) and network architecture (e.g. non-linearity functions) but also are greatly influenced by the choice of objective functions used for optimization. The deeply supervised multilayered loss based defense provides a significant boost in robustness under strictest attack conditions where the balance is shifted heavily towards the adversary. These include white-box attacks and iterative adversaries including the strongest first-order attacks (Projected Gradient Descent). We demonstrate the robustness of the proposed defense through extensive evaluations on five publicly available datasets and achieve a robustness of 46.7% and 36.1% against the strongest PGD attack ( = 0.03) for the CIFAR-10 and CIFAR-100 datasets, respectively. To the best of our knowledge, these are significantly higher levels of robustness against a broad range of strong adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generating adversarial examples to fool a deep network and developing defenses against such examples have gained significant research attention recently. Adversarial perturbations were first proposed by Szegedy et al. <ref type="bibr" target="#b36">[37]</ref> using an L-BFGS based optimization scheme, followed by Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b9">[10]</ref> and its iterative variant <ref type="bibr" target="#b15">[16]</ref>. Moosavi-Dezfooli et al. <ref type="bibr" target="#b24">[25]</ref> then proposed Deep-Fool, which iteratively projects an image across the decision boundary (form of a polyhydron) until it crosses the boundary and is mis-classified. One of the strongest attacks proposed recently is the Projected Gradient Descent (PGD) <ref type="bibr" target="#b21">[22]</ref>, which takes maximum loss increments allowed within a specified l ∞ norm-ball. Other popular attacks in-clude the Carlini and Wagner Attack <ref type="bibr" target="#b2">[3]</ref>, Jacobian-based Saliency Map Approach <ref type="bibr" target="#b29">[30]</ref>, Momentum Iterative Attack <ref type="bibr" target="#b7">[8]</ref> and Diverse Input Iterative Attack <ref type="bibr" target="#b41">[42]</ref>.</p><p>Two main lines of defense mechanisms have been proposed in the literature to counter adversarial attacks. First, by applying different pre-processing steps and transformations on the input image at inference time <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11]</ref>. The second category of defenses improve network's training regime to counter adversarial attacks. An effective scheme in this regards is adversarial training, where the model is jointly trained with clean images and their adversarial counterparts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10]</ref>. Ensemble adversarial training is used in <ref type="bibr" target="#b37">[38]</ref> to soften the classifier's decision boundaries. Virtual Adversarial Training <ref type="bibr" target="#b23">[24]</ref> smoothes the model distribution using a regularization term. Papernot et al. <ref type="bibr" target="#b30">[31]</ref> used distillation to improve the model's robustness by retraining with soft labels. Parsevel Networks <ref type="bibr" target="#b3">[4]</ref> restrict the Lipschitz constant of each layer of the model. Input Gradient Regularizer <ref type="bibr" target="#b32">[33]</ref> penalizes the change in model's prediction w.r.t input perturbations by regularizing the gradient of cross-entropy loss. The Frobenius norm of the Jacobian of the network has been shown to improve model's stability in <ref type="bibr" target="#b12">[13]</ref>. <ref type="bibr" target="#b19">[20]</ref> proposed defensive quantization method to control the Lipschitz constant of the network to mitigate the adversarial noise during inference. <ref type="bibr" target="#b6">[7]</ref> proposed Stochastic Activation Pruning as a defense against adversarial attacks. Currently the strongest defense method is Min-Max optimization <ref type="bibr" target="#b21">[22]</ref> which augments the training data with a first order attacked samples. Despite significant research activity in devising defenses against adversarial attacks, it was recently shown in <ref type="bibr" target="#b1">[2]</ref> that the currently existing state-of-the-art defenses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> are successfully circumvented under white-box settings. Only Min-Max optimization <ref type="bibr" target="#b21">[22]</ref> and Cascade adversarial machine learning <ref type="bibr" target="#b26">[27]</ref> retained 47% and 15% accuracy respectively, and withstood the attacks under whitebox settings. In our experiments (see Sec. 5), we extensively compare our results with <ref type="bibr" target="#b21">[22]</ref> and make a compelling case by achieving significant improvements.</p><p>At the core of our defense are the proposed objective function and multi-level deep supervision, which ensure feature space discrimination between classes. Our training objective is inspired from center loss <ref type="bibr" target="#b39">[40]</ref>, which clusters penultimate layer features. We propose multiple novel constraints (Sec. 3) to enhance between-class distances, and ensure maximal separation of a sample from its non-true classes. Our method is therefore fundamentally different from <ref type="bibr" target="#b39">[40]</ref>, since the proposed multi-layered hierarchical loss formulation and the notion of maximal separation has not been previously explored for adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Prototype Conformity Loss</head><p>Below, we first introduce the notations used, then provide a brief overview of the conventional cross entropy loss  <ref type="figure">(·)</ref> is an auxiliary branch to map features to a low dimensional output, which is then used for loss in Eq. 8</p><p>followed by a detailed description of our proposed method. Notations: Let x ∈ R m and y denote an input-label pair and 1 y be the one-hot encoding of y. We denote a deep neural network (DNN) as a function F θ (x), where θ are the trainable parameters. The DNN outputs a feature representation f ∈ R d , which is then used by a classification layer to perform multi-class classification. Let k be the number of classes; the parameters of the classifier can then be represented as W = [w 1 , . . . , w k ] ∈ R d×k . To train the model, we find the optimal θ and W that minimize a given objective function. Next, we introduce a popular loss function for deep CNNs.</p><p>Cross-entropy Objective: The cross-entropy objective function maximizes the dot product between an input feature f i and its true class representative vector w y , such that w y ∈ W . In other words, cross-entropy loss forces the classifier to learn a mapping from feature to output space such that the projection on to the correct class vector is maximized:</p><formula xml:id="formula_0">L CE (x, y) = m i=1 − log exp(w T yi f i + b yi ) k j=1 exp(w T j f i + b j ) ,<label>(1)</label></formula><p>where, m is the number of images, and f i is the feature of an i th image x i with the class y i . W and b are, respectively, the weights and the bias terms for the classification layer. Adversarial Perspective: The main goal of an attack algorithm is to force a trained DNN F θ to make wrong predictions. Attack algorithms seek to achieve this goal within a minimal perturbation budget. The attacker's objective can be represented by:</p><formula xml:id="formula_1">argmax δ L(x + δ, y), s.t., δ p &lt; ,<label>(2)</label></formula><p>where y is the ground-truth label for an input sample x, δ denotes the adversarial perturbation, L(·) denotes the error function, · p denotes the p-norm, which is generally considered to be an ∞ -ball centered at x, and is the available perturbation budget.</p><p>In order to create a robust model, the learning algorithm must consider the allowed perturbations in the input domain and learn a function that maps the perturbed images to the correct class. This can be achieved through the following min-max (saddle point) objective that minimizes the empirical risk in the presence of perturbations:</p><formula xml:id="formula_2">min θ E (x,y)∼D max δ L(x + δ, y; θ) , s.t., δ p &lt; , (3)</formula><p>where D is the data distribution. CE Loss in Adversarial Setting: The CE loss is the default choice for conventional classification tasks. However, it simply assigns an input sample to one of the pre-defined classes. It therefore does not allow one to distinguish between normal and abnormal inputs (adversarial perturbations in our case). Further, it does not explicitly enforce any margin constraints amongst the learned classification regions. It can be seen from Eq. 3 that an adversary's job is to maximize L(·) within a small perturbation budget . Suppose, the adversarial polytope in the output space 2 with respect to an input sample x is given by:</p><formula xml:id="formula_3">P (x; θ) = {F θ (x + δ) s.t., δ p ≤ }.<label>(4)</label></formula><p>An adversary's task is easier if there is an overlap between the adversarial polytopes for different input samples belonging to different classes. Definition 1: The overlap O i,j between polytopes for each data sample pair (i, j) can be defined as the volume of intersection between the respective polytopes:</p><formula xml:id="formula_4">O i,j = P (x i yi ; θ) ∩ P (x j yj ; θ).</formula><p>Note that the considered polytopes can be non-convex as well. However, the overlap computation can be simplified for convex polytopes <ref type="bibr" target="#b5">[6]</ref>. Proposition 1: For an i th input sample x i yi with class label y i , reducing the overlap O i,j between its polytope P (x i yi ; θ) and the polytopes of other class samples P (x j yj ; θ), s.t., y j = y i will result in lower adversary success for a bounded perturbation δ p ≤ . Proposition 2: For a given adversarial strength , assume λ is the maximum distance from the center of the polytope to the convex outer bounded polytope. Then, a classifier maintaining a margin m &gt; 2λ between two closest samples belonging to different classes will result in a decision boundary with guaranteed robustness against perturbation within the budget .</p><p>In other words, if the adversarial polytopes for samples belonging to different classes are non-overlapping, the adversary cannot find a viable perturbation within the allowed budget. We propose that an adversary's task can be made difficult by including a simple maximal separation constraint in the objective of deep networks. The conventional CE loss does not impose any such constraint, which makes the resulting models weaker against adversaries. A more principled approach is to define convex category-specific classification regions for each class, where any sample outside all of such regions is considered an adversarial perturbation. Consequently, we propose the prototype conformity loss function, described below.</p><p>Proposed Objective: We represent each class with its prototype vector, which represents the training examples of that class. Each class is assigned a fixed and non-overlapping pnorm ball and the training samples belonging to a class i are encouraged to be mapped close to its hyper-ball center:</p><formula xml:id="formula_5">L PC (x, y) = i f i − w c yi 2 − 1 k − 1 j =yi f i − w c j 2 + w c yi − w c j 2 .<label>(5)</label></formula><p>During model inference, a feature's similarity is computed with all the class prototypes and it is assigned the closest class label if and only if the sample lies within its decision region:</p><formula xml:id="formula_6">ŷ i = argmin j f i − w c j .<label>(6)</label></formula><p>Here, w c denotes the trainable class centroids. Note that the classification rule is similar to the Nearest Class Mean (NCM) classifier <ref type="bibr" target="#b22">[23]</ref>, but we differ in some important aspects: (a) the centroids for each class are not fixed as the mean of training samples, rather learned automatically during representation learning, (b) class samples are explicitly forced to lie within respective class norm-balls, (c) feature representations are appropriately tuned to learn discriminant mappings in an end-to-end manner, and (d) to avoid inter-class confusions, disjoint classification regions are considered by maintaining a large distance between each pair of prototypes. We also experiment with the standard softmax classifier and get equivalent performance compared to nearest prototype rule mentioned above. Deeply Supervised Learning: The overall loss function used for training our model is given by:</p><formula xml:id="formula_7">L(x, y) = L CE (x, y) + L PC (x, y).<label>(7)</label></formula><p>The above loss enforces the intra-class compactness and an inter-class separation using learned prototypes in the output space. In order to achieve a similar effect in the intermediate feature representations, we include other auxiliary loss functions {L n } along the depth of our deep networks, which act as companion objective functions for the final loss. This is achieved by adding an auxiliary branch G φ (·) after the defined network depth, which maps the features to a lower dimension output, and is then used in the loss definition. For illustration, see <ref type="figure" target="#fig_2">Fig. 2</ref>.  These functions avoid the vanishing gradients problem and act as regularizers that encourage features belonging to the same class to come together and the ones belonging to different classes to be pushed apart.</p><formula xml:id="formula_8">L n (x, y) = L CE (f l , y) + L PC (f l , y)<label>(8)</label></formula><formula xml:id="formula_9">s.t., f l = G l φ (F l θ (x)).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adversarial Attacks</head><p>We evaluate our defense model against five recently proposed state-of-the-art attacks, which are summarized below, for completeness.</p><p>Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b9">[10]</ref> generates an adversarial sample x adv from a clean sample x by maximizing the loss in Eq. 2. It finds x adv by moving a single step in the opposite direction to the gradient of the loss function, as:</p><p>x adv = x + · sign(∇ x L(x, y)).</p><p>Here, is the allowed perturbation budget.</p><p>Basic Iterative Method (BIM) <ref type="bibr" target="#b15">[16]</ref> is an iterative variant of FGSM and generates an adversarial sample as:</p><formula xml:id="formula_11">x m = clip (x m−1 + i · sign(∇ xm−1 (L(x m−1 , y))),<label>(11)</label></formula><p>where x 0 is clean image x and i is the iteration number.</p><p>Momentum Iterative Method (MIM) <ref type="bibr" target="#b7">[8]</ref> introduces an additional momentum term to BIM to stabilize the direction of gradient. Eq. 11 is modified as:</p><formula xml:id="formula_12">g m = µ · g m−1 + ∇ xm−1 L(x m−1 , y) ∇ xm−1 (L(x m−1 , y)) 1 (12) x m = clip (x m−1 + i · sign(g m )),<label>(13)</label></formula><p>where µ is the decay factor.</p><p>Carlini &amp; Wagner Attack <ref type="bibr" target="#b2">[3]</ref> defines an auxiliary variable ζ and minimizes the objective function:</p><formula xml:id="formula_13">min ζ 1 2 (tanh (ζ) + 1) − x +c · f ( 1 2 (tanh ζ + 1)),<label>(14)</label></formula><p>where 1 2 (tanh (ζ) + 1) − x is the perturbation δ, c is the constant chosen and f (.) is defined as:</p><formula xml:id="formula_14">f (x adv ) = max(Z(x adv ) y −max{Z(x adv ) k : k = y}, −κ).<label>(15)</label></formula><p>Here, κ controls the adversarial sample's confidence and Z(x adv ) k are the logits values corresponding to a class k.</p><p>Projected Gradient Descent (PGD) <ref type="bibr" target="#b21">[22]</ref> is similar to BIM, and starts from a random position in the clean image neighborhood U(x, ). This method applies FGSM for m iterations with a step size of γ as:</p><formula xml:id="formula_15">x m =x m−1 + γ · sign(∇ xm−1 L(x m−1 , y)). (16) x m = clip(x m , x m − , x m + ).<label>(17)</label></formula><p>It proves to be a strong iterative attack, relying on the first order information of the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets and Models: We extensively evaluate the proposed method on five datasets: MNIST, Fasion-MNIST (F-MNIST), CIFAR-10, CIFAR-100 and Street-View House Numbers (SVHN). For the MNIST and F-MNIST datasets, the CNN model chosen has six layers, as in <ref type="bibr" target="#b39">[40]</ref>. For the CIFAR-10, CIFAR-100 and SVHN datasets, we use a ResNet-110 model <ref type="bibr" target="#b11">[12]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>). The deep features for the prototype conformity loss are extracted from different intermediate layers using an auxiliary branch, which maps the features to a lower dimension output (see <ref type="figure" target="#fig_2">Fig. 2</ref>). We Compute gradients w.r.t θ and x, ∇ θ L(x, y) and ∇xL(x, y) respectively. <ref type="bibr" target="#b7">8</ref> Update model weights, θ := arg min θ L. <ref type="bibr" target="#b8">9</ref> Update class centroids w c j ∀ l <ref type="bibr" target="#b9">10</ref> Generate adversarial examples as: <ref type="bibr" target="#b10">11</ref> if FGSM: then x adv = x + · sign (∇xL(x, y)) <ref type="bibr" target="#b11">12</ref> elif PGD: then x adv = clip (x, x − , x + ) <ref type="bibr" target="#b12">13</ref> Augment x with x adv 14 return θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results and Analysis</head><p>White-Box vs Black-Box Settings: In an adversarial setting, there are two main threat models: white-box attacks where the adversary possesses complete knowledge of the target model, including its parameters, architecture and the training method, and black-box attacks where the adversary  <ref type="table">Table 2</ref> shows our results for the different attacks described in Sec. 4. The number of iterations for BIM, MIM and PGD are set to 10 with a step size of /10. The iteration steps for C&amp;W are 1, 000, with a learning rate of 0.01. We report our model's robustness with and without adversarial training for standard perturbation size i.e. = 0.3 for F/MNIST and and = 0.03 for the CIFAR-10/100 and SVHN datasets. Recent literature has shown transferability amongst deep models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10]</ref>, where adversarial images are effective even for the models they were never generated on. An adversary can therefore exploit this characteristic of deep models and generate generic adversarial samples to attack unseen models. Defense against black-box attacks is therefore highly desirable for secure deployment of machine learning models <ref type="bibr" target="#b29">[30]</ref>. To demonstrate the effectiveness of our proposed defense under black-box settings, we generate adversarial samples using a VGG-19 model, and feed them to the model trained using our proposed strategy. Results in <ref type="table">Table 2</ref> show that black-box settings have negligible attack potential against our model. For example, on the CIFAR-10 dataset, where our model's accuracy on clean images is 91.89%, even the strongest iterative attack (PGD-0.03) fails, and our defense retains an accuracy of 88.8%. Adversarial Training has been shown to enhance many recently proposed defense methods <ref type="bibr" target="#b17">[18]</ref>. We also evaluate the impact of adversarial training (AdvTrain) in conjunction with our proposed defense. For this, we jointly train our model on clean and attacked samples, which are generated <ref type="table">Table 2</ref>: Robustness of our model in white-box and black-box settings. Adversarial samples generated in the black-box settings show negligible attack potential against our models. Here is the perturbation size and c is the initial constant for C&amp;W attack. It can be seen that AdvTrain further complements the robustness of our models.  <ref type="table">Table 2</ref> indicate that AdvTrain further complements our method and provides an enhanced robustness under both black-box and white-box attack settings. Adaptive White-box Settings: Since at inference time, our model performs conventional softmax prediction, we evaluated the robustness against standard white-box attack settings, to be consistent with existing defenses. Now, we also experiment in an adaptive white-box setting where the attack is performed on the joint PC+CE loss (with access to learned prototypes). Negligible performance drop is observed in adaptive settings (see <ref type="table" target="#tab_2">Table 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Existing Defenses</head><p>We compare our method with recently proposed stateof-the art proactive defense mechanisms, which alter the network or use modified training loss functions. To this end, we compare with <ref type="bibr" target="#b16">[17]</ref>, which injects adversarial examples into the training set and generates new samples at each iteration. We also compare with <ref type="bibr" target="#b28">[29]</ref>, which introduces an Adaptive Diversity Promoting (ADP) regularizer to improve adversarial robustness. Further, we compare with an input gradient regularizer mechanism <ref type="bibr" target="#b32">[33]</ref> that penalizes the degree to which input perturbations can change a model's predictions by regularizing the gradient of the cross-entropy loss. Finally, we compare with the current state-of-theart Min-Max optimization based defense <ref type="bibr" target="#b21">[22]</ref>, which augments the training data with adversarial examples, causing the maximum gradient increments to the loss within a specified l ∞ norm. The results in <ref type="table" target="#tab_8">Tables 8 , 9</ref> and 4 in terms of retained classification accuracy on different datasets show that our method significantly outperforms all existing defense schemes by a large margin. The performance gain is more pronounced for the strongest iterative attacks (e.g. C&amp;W and PGD) with large perturbation budget . For example, our method achieves a relative gain of 20.6% (AdvTrain models) and 41.4% (without AdvTrain) compared to the 2 nd best methods on the CIFAR-10 and MNIST datasets respectively for the PGD attack. On CIFAR-100 dataset, for the strongest PGD attack with = 0.01, the proposed method achieves 38.9% compared with 18.3% by ADP <ref type="bibr" target="#b28">[29]</ref>, which, to the best of our knowledge, is the only method in the literature evaluated on the CIFAR-100 dataset. Our results further indicate that adversarial training consistently compliments our method and augments its performance across all evaluated datasets.</p><p>Additionally we compare our model's performance with a close method proposed by Song et al. <ref type="bibr" target="#b35">[36]</ref> in <ref type="table" target="#tab_5">Table 5</ref>, where our approach outperforms them by a significant margin. Besides a clear improvement, we discuss our main distinguishing features below: (a) Our approach is based on a "deeply-supervised" loss that prevents changes to the outputs within the limited perturbation budget. Such supervision paradigm is the main contributing factor towards our improved results (See <ref type="table" target="#tab_7">Table 7</ref>). (b) <ref type="bibr" target="#b35">[36]</ref> focuses on domain adaption between adversarial and natural samples without any constraint on the intermediate feature representations.</p><p>In contrast, we explicitly enforce the hidden layer activations to be maximally separated in our network design. (c) <ref type="bibr" target="#b35">[36]</ref> only considers adversarially trained models, while we demonstrate clear improvements both with and without adversarial training (a more challenging setting). In <ref type="table" target="#tab_5">Table 5</ref> we have followed the exact model settings used in <ref type="bibr" target="#b35">[36]</ref>.   <ref type="bibr" target="#b35">[36]</ref> and blue for our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Transferability Test</head><p>We investigate the transferability of attacks on CIFAR-10 dataset between a standard VGG-19 model, adversarially trained VGG-19 <ref type="bibr" target="#b16">[17]</ref>, Madry et al.'s <ref type="bibr" target="#b21">[22]</ref> and our model. We report the accuracy of target models (columns) on adversarial samples generated from source models (rows) in <ref type="table" target="#tab_6">Table 6</ref>. Our results yield the following findings: Improved black-box robustness: As noted in <ref type="bibr" target="#b1">[2]</ref>, a model that gives a false sense of security due to obfuscated gradients can be identified if the black-box attacks are stronger than white-box. In other words, robustness of such a model under white-box settings is higher than under black-box settings. It was shown in <ref type="bibr" target="#b1">[2]</ref> that most of the existing defenses obfuscate gradients. Madry et al.'s approach <ref type="bibr" target="#b21">[22]</ref> was endorsed by <ref type="bibr" target="#b1">[2]</ref> to not cause gradient masking. The comparison in <ref type="table" target="#tab_6">Table 6</ref> shows that our method outperforms <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similar architectures increase transferability:</head><p>Changing the source and target network architectures decreases the transferability of an attack. The same architectures (e.g. VGG-19 and its AdvTrain counterpart, as in <ref type="table" target="#tab_6">Table 6</ref>) show increased robustness against black-box attacks generated from each other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Analysis</head><p>L PC at Different Layers: We investigate the impact of our proposed prototype conformity loss (L PC ) at different depths of the network. Specifically, as shown in <ref type="table" target="#tab_7">Table 7</ref>, we apply L PC individually after each layer (see <ref type="table" target="#tab_0">Table 1</ref> for architectures) and in different combinations. We report the achieved results on the CIFAR-10 dataset for clean and perturbed samples (using FGSM and PGD attacks) in <ref type="table" target="#tab_7">Table 7</ref>. The network without any L PC loss is equivalent to a standard softmax trained model. It achieves good performance on clean images, but fails under both white-box and blackbox attacks (see <ref type="table">Table 2</ref>). The models with L PC loss at initial layers are unable to separate deep features class-wise, thereby resulting in inferior performance. Our proposed L PC loss has maximum impact in the deeper layers of the network. This justifies our choice of different layers for L PC loss, indicated in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Identifying Obfuscated Gradients</head><p>Recently, Athalye et.al. <ref type="bibr" target="#b1">[2]</ref> were successful in breaking several defense mechanisms in the white-box settings by identifying that they exhibit a false sense of security. They call this phenomenon gradient masking. Below, we discuss   how our defense mechanism does not cause gradient masking on the basis of characteristics defined in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. Iterative attacks perform better than one-step attacks: Our evaluations in <ref type="figure" target="#fig_6">Fig. 4</ref> indicate that stronger iterative attacks (e.g. BIM, MIM, PGD) in the white-box settings are more successful at attacking the defense models than singlestep attacks (FGSM in our case). Robustness against black-box settings is higher than white-box settings: In white-box settings, the adversary has complete knowledge of the model, so attacks should be more successful. In other words, if a defense does not suffer from obfuscated gradients, robustness of the model against white-box settings should be inferior to that in the black-box settings. Our extensive evaluations in <ref type="table">Table 2</ref> show that the proposed defense follows this trend and therefore does not obfuscate gradients.</p><p>Increasing the distortion bound ( ) decreases the robustness of defense: On increasing the perturbation size, the success rate of the attack method should significantly increase monotonically. For an unbounded distortion, the classifier should exhibit 0% robustness to the attack, which again is true in our case (see <ref type="figure" target="#fig_6">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Our findings provide evidence that the adversary's task is made difficult by incorporating a maximal separation constraint in the objective function of DNNs, which conventional cross-entropy loss fails to impose. Our theory and experiments indicate, if the adversarial polytopes for samples belonging to different classes are non-overlapping, the adversary cannot find a viable perturbation within the allowed budget. We extensively evaluate the proposed model against a diverse set of attacks (both single-step and iterative) in black-box and white-box settings and show that the proposed model maintains its high robustness in all cases. Through empirical evaluations, we further demonstrate that the achieved performance is not due to obfuscated gradients, thus the proposed model can provide significant security against adversarial vulnerabilities in deep networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>2D penultimate layer activations of a clean image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of our training with joint supervision of LPC and LCE. G φ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between different training methods. The red circle encompasses the adversarial sample space within a perturbation budget δ p &lt; .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 3 if t &lt; T : 4 6</head><label>1346</label><figDesc>first train for T epochs (T = 50 for F/MNIST, T = 200 for CIFAR-10/100 and SVHN) with L CE and then use the loss in Eq. 8 for 300 epochs. A batch size of 256 and a learning rate of 0.1 (×0.1 at T =200, 250) are used. Further training details are summarized in Algorithm 1. Model training with Prototype Conformity Loss. Input: Classifier F θ (x), training data {x}, ground truth labels {y}, trainable parameters θ, trainable class centroids {w c j : j ∈ [1, k] }, perturbation budget , epochs T , number of auxiliary branches L. Output: Updated parameters θ 1 Initialize θ in convolution layers. 2 for t = 0 to T : Converge softmax objective, θ := arg min θ L CE . Compute joint loss L = L CE + L l L PC 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Robustness of our model (without adversarial training)against white-box attacks for various perturbation budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Two network architectures: CNN-6 (MNIST, FMNIST)and ResNet-110 (CIFAR-10,100 and SVHN). Features are extracted in CNN-6 (after Layer 3 and two FC layers) and ResNet-110 (after Layer 3, 4 and FC layer) to impose the proposed LPC. Auxiliary branches are shown in green color.</figDesc><table><row><cell>Layer #</cell><cell>6-Conv Model</cell><cell></cell><cell>ResNet-110</cell><cell></cell></row><row><cell>1</cell><cell>Conv(32, 5 × 5) PReLu(2 × 2)</cell><cell>×2</cell><cell>Conv(16, 3 × 3) + BN ReLU(2 × 2)</cell><cell></cell></row><row><cell>2</cell><cell>Conv(64, 5 × 5) PReLu(2 × 2)</cell><cell>×2</cell><cell>Conv(16, 1 × 1) + BN Conv(64, 1 × 1) + BN Conv(16, 3 × 3) + BN</cell><cell>×12</cell></row><row><cell>3</cell><cell>Conv(128, 5 × 5) PReLu(2 × 2)</cell><cell>×2</cell><cell>Conv(32, 1 × 1) + BN Conv(128, 1 × 1) + BN Conv(32, 3 × 3) + BN</cell><cell>×12</cell></row><row><cell></cell><cell>GAP → L PC</cell><cell></cell><cell cols="2">(GAP→FC(512) → L PC )</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Conv(64, 1 × 1) + BN</cell><cell></cell></row><row><cell>4</cell><cell>FC(512) → L PC</cell><cell></cell><cell>Conv(256, 1 × 1)+ BN Conv(64, 3 × 3) + BN</cell><cell>×12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GAP → L PC</cell><cell></cell></row><row><cell>5</cell><cell>FC(64) → L PC</cell><cell></cell><cell>FC(1024) → L PC</cell><cell></cell></row><row><cell>6</cell><cell>FC(10) → L CE</cell><cell></cell><cell>FC(100/10) → L CE</cell><cell></cell></row></table><note>feeds perturbed images at test time (which are generated without any knowledge of the target model). We evaluate the robustness of our proposed defense against both white- box and black-box settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Robustness in adaptive white-box attack settings. The performance for conventional attacks (where CE is the adversarial loss) is shown in blue. * indicates adversarially trained models.</figDesc><table><row><cell cols="2">Training No Attack</cell><cell>FGSM</cell><cell>BIM</cell><cell>MIM</cell><cell>PGD</cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR-10 ( = 8/255)</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>90.45</cell><cell cols="4">66.90 (67.7) 31.29 (32.6) 32.84 (33.2) 27.09 (27.2)</cell></row><row><cell>Ours  *  FGSM</cell><cell>91.28</cell><cell cols="4">74.24 (75.8) 44.05 (45.9) 43.77 (44.7) 41.32 (42.5)</cell></row><row><cell>Ours  *  PGD</cell><cell>91.89</cell><cell cols="4">74.31 (74.9) 44.85 (46.0) 47.31 (49.3) 44.75 (46.7)</cell></row><row><cell></cell><cell></cell><cell cols="2">F-MNIST ( = 0.3/1)</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>91.32</cell><cell>28.1 (29.0)</cell><cell>21.7 (22.0)</cell><cell>20.3 (21.8)</cell><cell>19.5 (20.3)</cell></row><row><cell>Ours  *  FGSM</cell><cell>91.03</cell><cell>53.3 (55.1)</cell><cell>36.0 (37.5)</cell><cell>39.3 (40.6)</cell><cell>34.7 (35.3)</cell></row><row><cell>Ours  *  PGD</cell><cell>91.30</cell><cell>46.0 (47.2)</cell><cell>40.1 (40.1)</cell><cell>40.7 (41.3)</cell><cell>39.7 (40.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison on CIFAR-100 dataset for white-box adversarial attacks (numbers shows robustness, higher is better). * sign denotes adversarially trained models. For our model, we report results without adversarial training (Ours) and with adversarially generated images from FGSM (Ours * f ) and PGD (Ours * p ) attacks.</figDesc><table><row><cell>Attacks</cell><cell>Params.</cell><cell cols="2">Baseline ADP [29]</cell><cell cols="2">Ours Ours  *  f</cell><cell>Ours  *  p</cell></row><row><cell>No Attack</cell><cell>-</cell><cell>72.6</cell><cell>70.2</cell><cell>71.9</cell><cell>69.1</cell><cell>68.3</cell></row><row><cell>BIM</cell><cell>= 0.005 = 0.01</cell><cell>21.6 10.1</cell><cell>26.2 14.8</cell><cell>44.8 39.8</cell><cell>55.1 46.2</cell><cell>55.7 46.9</cell></row><row><cell>MIM</cell><cell>= 0.005 = 0.01</cell><cell>24.2 11.2</cell><cell>29.4 17.2</cell><cell>46.1 40.6</cell><cell>56.7 43.8</cell><cell>57.1 45.9</cell></row><row><cell>PGD</cell><cell>= 0.005 = 0.01</cell><cell>26.6 11.7</cell><cell>32.1 18.3</cell><cell>42.2 38.9</cell><cell>53.6 40.1</cell><cell>55.0 44.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Comparison</cell></row><row><cell>of our approach with</cell></row><row><cell>[36] on 4 datasets.</cell></row><row><cell>Green rows show</cell></row><row><cell>results for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Transferability Test on CIFAR-10: PGD adversaries are generated with = 0.03, using the source network, and then evaluated on target model. Underline denotes robustness against white-box attack. Note that adversarial samples generated on our model are highly transferable to other models as black-box attacked images.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell cols="4">VGG-19 AdvTrain [17] Madry et al. [22] Ours</cell></row><row><cell cols="2">VGG-19</cell><cell>0.0</cell><cell>16.20</cell><cell>52.71</cell><cell>88.80</cell></row><row><cell cols="2">AdvTrain [17]</cell><cell>12.43</cell><cell>0.0</cell><cell>49.80</cell><cell>72.53</cell></row><row><cell cols="2">Madry et al. [22]</cell><cell>58.91</cell><cell>67.32</cell><cell>43.70</cell><cell>71.72</cell></row><row><cell cols="2">Ours</cell><cell>50.31</cell><cell>61.02</cell><cell>66.70</cell><cell>49.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation Analysis with LPC applied at different layers of ResNet-110(Table 1)for CIFAR-10 dataset.</figDesc><table><row><cell>Layer #</cell><cell>No Attack = 0</cell><cell>FGSM = 0.03</cell><cell>PGD = 0.03</cell></row><row><cell>None</cell><cell>90.80</cell><cell>21.40</cell><cell>0.01</cell></row><row><cell>Layer 1</cell><cell>74.30</cell><cell>23.71</cell><cell>0.01</cell></row><row><cell>Layer 2</cell><cell>81.92</cell><cell>30.96</cell><cell>8.04</cell></row><row><cell>Layer 3</cell><cell>88.75</cell><cell>33.74</cell><cell>10.47</cell></row><row><cell>Layer 4</cell><cell>90.51</cell><cell>39.90</cell><cell>11.90</cell></row><row><cell>Layer 5</cell><cell>91.11</cell><cell>47.02</cell><cell>13.56</cell></row><row><cell>Layer 4+5</cell><cell>90.63</cell><cell>55.36</cell><cell>20.70</cell></row><row><cell>Layer 3+4+5</cell><cell>90.45</cell><cell>67.71</cell><cell>27.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison on CIFAR-10 dataset for white-box adversarial attacks (numbers shows robustness, higher is better). * sign denotes adversarially trained models. For our model, we report results without adversarial training (Ours) and with adversarially generated images from FGSM (Ours * f ) and PGD (Ours * p ) attacks.Yu  et al. [43] * Ross et al. [33] * Pang et al. [29] * Madry et al. [22] * Ours Ours *</figDesc><table><row><cell>Attacks</cell><cell>Params.</cell><cell>Baseline AdvTrain [17]</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison on MNIST dataset for white-box adversarial attacks (numbers shows robustness, higher is better). * sign denotes adversarially trained models. For our model, we report results without adversarial training (Ours) and with adversarially generated images from FGSM (Ours * f ) and PGD (Ours * p ) attacks.</figDesc><table><row><cell>[29]</cell><cell>Ours Ours  *  f</cell><cell>Ours  *  p</cell></row></table><note>Attacks Params. Baseline AdvTrain [17]* Yu et al. [43] Ross et al. [33] Pang et al.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the output space in our case is not the final prediction space, but the intermediate feature space.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How drive. ai is mastering autonomous driving with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum Magazine</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02900</idno>
		<title level="m">Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computing the maximum overlap of two convex polygons under translations. Theory of computing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Kreveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teillaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="613" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Countering adversarial images using input transformations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving dnn robustness to adversarial attacks using jacobian regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jakubovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<title level="m">Adversarial logit pairing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00851</idno>
		<title level="m">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<title level="m">Adversarial examples in the physical world</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NIPS&apos;17 Competition: Building Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02976</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Defensive quantization: When efficiency meets robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Foveationbased mechanisms alleviate adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06292</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image super-resolution as a defense against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01677</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cascade adversarial machine learning regularized with a unified embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02582</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning applications and challenges in big data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Villanustre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muharemagic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness via promoting ensemble diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08846</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09344</idno>
		<title level="m">Certified defenses against adversarial examples</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Biometric person recognition: Face, speech and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>VDM Publishing</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Certifiable distributional robustness with principled adversarial training. stat, 1050:29</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving the generalization of adversarial training with domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mc-Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<title level="m">The space of transferable adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06978</idno>
		<title level="m">Improving transferability of adversarial examples with input diversity</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Interpreting adversarial robustness: A view from decision surface in input space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
