<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-15">15 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<title level="a" type="main">Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-15">15 Dec 2019</date>
						</imprint>
					</monogr>
					<note>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, SUBMISSION 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Adversarial Training</term>
					<term>Graph-based Learning</term>
					<term>Graph Neural Networks ✦</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (e.g., articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. Adversarial Training (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples. In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose Graph Adversarial Training (GraphAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GraphAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GraphAT, we employ it on a state-of-the-art graph neural network model -Graph Convolutional Network (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GraphAT which outperforms normal training on GCN by 4.51% in node classification accuracy. Codes are available via: https://github.com/fulifeng/GraphAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-based learning makes predictions by accounting for both input features of examples and the relations between examples. It is remarkably effective for a wide range of applications, such as predicting the profiles and interests of social network users <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, predicting the role of a protein in biological interaction graph <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and classifying contents like documents, videos, and webpages based on their interlinks <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. In addition to the supervised loss on labeled examples, graph-based learning also optimizes the smoothness of predictions over the graph structure, that is, closely connected examples are encouraged to have similar predictions <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Recently, owing to the extraordinary representation ability, deep neural networks become prevalent models for graph-based learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>Despite promising performance, we argue that graph neural networks are vulnerable to small but intentional perturbations on the input features <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and this could even be more serious than the standard neural networks that do not model the graph structure. The reasons are twofold: 1) graph neural networks also optimize the supervised loss on labeled data, thus it will face the same vulnerability issue as the standard neural networks <ref type="bibr" target="#b14">[15]</ref>, and 2) the additional smoothness constraint will exacerbate the impact of perturbations, since smoothing across connected nodes 1 would aggregate the impact of perturbations from nodes connected to the target node (i.e., the node that we apply perturbations with the aim of changing its prediction). <ref type="figure">Figure 1</ref> illustrates the impact of perturbations on node features with an intuitive example of a graph with 4 nodes. A graph neural network model predicts node labels (3 in total) for clean input features and features with applied perturbations, respectively. Here perturbations are intentionally applied to the features of nodes 1, 2, 4. Consequently, the graph neural network model is fooled to make wrong predictions on nodes 1 and 2 as with standard neural networks. Moreover, by propagating the node embeddings, the model aggregates the influence of perturbations to node 3, from which its prediction is also affected. In real-world applications, small perturbations like the update of node features may frequently happen, but should not change the predictions much. As such, we believe that there is a strong need to stabilize the graph neural network models during training.</p><p>Adversarial Training (AT) is a dynamic regularization technique that proactively simulates the perturbations during the training phase <ref type="bibr" target="#b14">[15]</ref>. It has been empirically shown to be able to stabilize neural networks, and enhance their robustness against perturbations in standard classification tasks <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Therefore, employing a similar approach to that of AT on a graph neural network model would also be helpful to the model's robustness. However, directly employing AT on graph neural network is insufficient, since  <ref type="figure">Fig. 1</ref>: An intuitive example to illustrate the impact of applying perturbations to the input node features on the prediction of graph neural networks. Here the model implements the graph smoothness constraint via propagating node embeddings over the graph. On the right, the model propagates the applied perturbations on the connected nodes of the target node 3, leading to a wrong prediction. Moreover, the perturbations on node 1 and 2 directly lead to the wrong associated predictions like in the standard neural networks. (Better viewed in color.)</p><p>it treats examples as independent of each other and does not consider the impacts from connected examples. As such, we propose a new adversarial training method, named Graph Adversarial Training (GraphAT), which learns to construct and resist perturbations by taking the graph structure into account.</p><p>The key idea of GraphAT is that, when generating perturbations on a target example, it maximizes the divergence between the prediction of the target example and its connected examples. That is, the adversarial perturbations should attack the graph smoothness constraint as much as possible. Then, GraphAT updates model parameters by additionally minimizing a graph adversarial regularizer, reducing the prediction divergence between the perturbed target example and its connected examples. Through this way, GraphAT can resist the worst-case perturbations on graphbased learning and enhance model robustness. To efficiently calculate the adversarial perturbations, we further devise a linear approximation method based on back-propagation.</p><p>To demonstrate GraphAT, we employ it on a wellestablished graph neural network model, Graph Convolutional Network (GCN) <ref type="bibr" target="#b6">[7]</ref>, which implements the smoothness constraint by performing embedding propagation. We study the method's performance on node classification, one of the most popular tasks on graph-based learning. Extensive experiments on three public benchmarks (two citation graphs and a knowledge graph) verify the strengths of GraphAT -compared to normal training on GCN, GraphAT leads to 4.51% accuracy improvement. Moreover, the improvements on less popular nodes (with a small degree) are more significant, highlighting the necessity of performing AT with the graph structure considered.</p><p>The main contributions of this paper are summarized as: In the remainder of this paper, we first discuss related work in Section 2, followed by the problem formulation and preliminaries in Section 3. In Section 4 and 5, we elaborate the method and experimental results, respectively. We conclude the paper and envision future directions in Section 6.</p><formula xml:id="formula_0">• We</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss the existing research on graphbased learning and adversarial learning, which are closely related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-based Learning</head><p>Graph, a natural representation of relational data, in which nodes and edges represent entities and their relations, is widely used in the analysis of social networks, transaction records, biological interactions, collections of interlinked documents, web pages, and multimedia contents, etc.. On such graphs, one of the most popular tasks is node classification targeting to predicting the label of nodes in the graph by accounting for node features and the graph structure. The existing work on node classification mainly fall into two broad categories: graph Laplacian regularization and graph embedding-based methods. Methods lying in the former category explicitly encode the graph structure as a regularization term to smooth the predictions over the graph, i.e., the regularization incurs a large penalty when similar nodes (e.g., closely connected) are predicted with different labels <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>.</p><p>Recently, graph embedding-based methods, which learn node embeddings that encodes the graph data, have become promising solution. Most of embedding-based methods fall into two broad categories: skip-gram based methods and convolution based methods, depending on how the graph data are modeled. The skip-gram based methods learn node embeddings via using the embedding of a node to predict node context that are generated by performing random walk on the graph so as the embeddings of "connected" nodes are associated to each other <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Inspired by the idea of convolution in computer vision, which aggregates contextual signals in a local window, convolution based methods iteratively aggregate representation of neighbor nodes to learn a node embedding <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b29">[30]</ref>.</p><p>In both of the two categories, methods leveraging the advanced representation ability of deep neural networks (neural graph-based learning methods) have shown remarkably effective in solving the node classification task. However, the neural graph-based learning models are vulnerable to intentionally designed perturbations indicating the unstability in generalization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and little attention has been paid to enhance the robustness of these methods, which is the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Adversarial Training</head><p>In order tackle the vulnerability to intentional perturbations of deep neural networks, researchers proposed adversarial training which is an alternative minimax process <ref type="bibr" target="#b31">[32]</ref>. The adversarial training methods augment the training process by dynamically generating adversarial examples from clean examples with perturbations maximally attacking the training objective, and then learn over these adversarial examples by minimizing an additional regularization term <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b37">[38]</ref>. The adversarial training methods mainly fall into supervised and semi-supervised ones regarding the target of the training objective. In supervised learning tasks such as visual recognition <ref type="bibr" target="#b14">[15]</ref>, supervised loss <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and its surrogates <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref> over adversarial examples are designed as the target of the maximization and minimization. For semi-supervised learning where partial examples are labeled, divergence of predictions for inputs around each examples is adopted as the target. Generally speaking, the philosophy of adversarial training methods is to smooth the prediction around individual inputs in a dynamical fashion.</p><p>Our work is inspired by these adversarial training methods. In addition to the local smoothness of individual examples, our method further accounts for relation between examples (i.e., the graph structure) in the target of the minimax process so as to learn robust classifiers predicting smoothly over the graph structure. To the best of our knowledge, this is the first attempt to incorporate graph structure in adversarial training.</p><p>Another emerging research topic related to our work is generating adversarial perturbations attacking neural graph-based learning models where <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b12">[13]</ref> are the only published work. However, methods in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b12">[13]</ref> are not suitable for constructing adversarial examples in graph adversarial training. This is because these methods generate a new graph as the adversarial example for each individual node, i.e., they would generate N graphs when the number of nodes is N leading to unaffordable memory overhead. In this work, we devise an efficient method to generate adversarial examples for graph adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Generative Adversarial Networks</head><p>Generative adversarial networks (GAN) is a machine learning framework with two different networks as a generator and a discriminator playing minimax game on generating weights and bias to be learned at layer l.</p><formula xml:id="formula_1">y i = f (xi, G|Θ)</formula><p>prediction function. Θ model parameters of the prediction function f . and detecting fake examples. Recently, several GAN-based models are proposed to learn graph embeddings, which either generate fake nodes and edges to augment embedding learning <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> or smooth the leaned embeddings to follow a prior distribution <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref>. However, using two different networks inevitably doubles the computation of model training and the labor of parameter tuning of GAN-based methods. Moreover, for different applications, one may need to build GAN from scratch, whereas our method is a generic solution can be seamlessly applied to enhance the existing graph neural network models with less computing and tuning overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>We first introduce some notations used in the following sections. We use bold capital letters (e.g. X) and bold lowercase letters (e.g. x) to denote matrices and vectors, respectively. Note that all vectors are in a column form if not otherwise specified, and X ij denotes the entry of matrix X at the row i and column j. In <ref type="table" target="#tab_1">Table 1</ref>, we summarize some of the terms and notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Representation</head><p>The nodes and edges of a graph represent the entities of interest and their relations, respectively. First, the edges in a graph with N nodes are typically represented as an adjacency matrix A ∈ R N ×N . In this work, we mainly study unweighted graphs where A is a binary matrix. A ij = 1 if there is an edge between node i and j, otherwise A ij = 0. Moreover, we use a diagonal matrix D ∈ R N ×N to denote the degrees of nodes, i.e., D ii = N j=1 A ij . For an attributed graph, where each node is associated with a feature vector, we use a matrix X = [x 1 , x 2 , · · · , x N ] T ∈ R N ×F to represent the feature vectors of all nodes, where F is the dimension of the features. Finally, an attributed graph is denoted as G = (A, D, X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Classification</head><p>On graph data, node classification is one of the most popular tasks. In the general problem setting of node classification, a graph G with N nodes is given, associated with labels (Y ) of a some portion of nodes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. This setting is transductive since testing nodes are observed (only features and associated edges) during training, and is the focus of this work. Here, Y = [y 1 , y 2 , · · · , y M ] T ∈ R M×L are the labels, where M and L are the numbers of labeled nodes and node classes, respectively, and y i is the one-hot encoding of node i's label. Note that, without loss of generality, we index the labeled nodes and unlabeled nodes in the range of [1, M ] and (M, N ], respectively. The target of node classification is to learn a prediction function (classifier)ŷ i = f (x i , G|Θ), to forecast the label of the node, where Θ includes model parameters to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-based Learning</head><p>Graph-based learning methods have been shown remarkably effective on solving the node classification task <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Generally, most of the graph-based learning models jointly optimize two objectives: 1) supervised loss on labeled nodes and 2) graph smoothness constraint, which can be summarized as:</p><formula xml:id="formula_2">Γ = Ω + λΦ,<label>(1)</label></formula><p>where Ω is a classification loss (e.g., log loss, hinge loss, and cross-entropy loss) that measures the discrepancy between prediction and ground-truth of labeled nodes. Φ encourages smoothness of predictions over the graph structure, which is based on the assumption that closely connected nodes tend to have similar predictions. For instance, Φ could be a graph Laplacian term,</p><formula xml:id="formula_3">N i,j=1</formula><p>A ij ŷ i −ŷ j 2 , which directly regulates the predictions of connected nodes to be similar <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The assumption could also be implicitly implemented by iteratively propagating node embeddings through the graph so that connected nodes obtain close embeddings and are predicted similarly <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Here, λ is a hyperparameter to balance the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first introduce the formulation of Graph Adversarial Training, followed by the introduction of Graph-VAT, an extension of GraphAT, which further incorporates the virtual adversarial regularization <ref type="bibr" target="#b34">[35]</ref>. We then present two solutions for the node classification task, GraphAT and GraphVAT, which employ GraphAT and GraphVAT to train GCN <ref type="bibr" target="#b6">[7]</ref>, respectively. Finally, we analyze the time complexity of the two solutions and present the important implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Adversarial Training</head><p>Recent advances of Adversarial Training has been successful in learning deep neural network-based classifiers, making them robust against perturbations for a wide range of standard classification tasks such as visual recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref> and text classification <ref type="bibr" target="#b16">[17]</ref>. Generally, applying AT would regulate the model parameters to smooth the output distribution <ref type="bibr" target="#b34">[35]</ref>. Specifically, for each clean example in the dataset, AT encourages the model to assign similar outputs to the artificial input (i.e., the adversarial example) derived from the clean example. Inspired by the philosophy of standard AT, we develop graph adversarial training, which trains graph neural network modules in the manner of generating adversarial examples and optimizing additional regularization terms over the adversarial examples, so as to prevent the adverse effects of perturbations. Here the focus is to prevent perturbations propagated through node connections (as illustrated in <ref type="figure">Figure 1</ref>), i.e., accounting for graph structure in adversarial training. Generally, the formulation of GraphAT is:</p><formula xml:id="formula_4">min: Γ GAT = Γ + β N i=1 j∈Ni d(f (x i + r g i , G|Θ), f (x j , G|Θ)), max: r g i = arg max ri, ri ≤ǫ j∈Ni d(f (x i + r i , G|Θ), f (x j , G|Θ)),<label>(2)</label></formula><p>where Γ GAT is the training objective function with two terms: the standard objective function of the origin graphbased learning model (e.g., Equation 1) and graph adversarial regularizer. The second term encourages the graph adversarial examples to be classified similarly as connected examples where Θ denotes the parameters to be learned, and d is a nonnegative function that measures the divergence (e.g., Kullback-Leibler divergence <ref type="bibr" target="#b44">[45]</ref>) between two predictions. r g i denotes the graph adversarial perturbation, which is applied to the input feature of the clean example i to construct a graph adversarial example.</p><p>The graph adversarial perturbation is calculated by maximizing the graph adversarial regularizer under current value of model parameters. That is to say, the graph adversarial perturbation is the direction of changes on the input feature, which can maximally attack the graph adversarial regularizer, i.e., the worst case of perturbations propagated from neighbor nodes. ǫ is a hyperparameter controling the magnitude of perturbations, which is typically set as small values so that the feature distribution of adversarial examples is close to that of clean examples.</p><p>Generally, similar to the standard AT, each iteration of GraphAT can also be viewed as playing a minimax game: As such, the model becomes robust against perturbations propagated through the graph. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the process of GraphAT. While the traditional graph-based regularizations (e.g., the graph Laplacian term) also encourage the smoothness of predictions over the graph structure, GraphAT is believed to be a more advanced regulation for two reasons: 1) the regularization performed by GraphAT is dynamic since the adversarial examples are adaptively generated according to the current parameters and predictions of the model whereas the standard graphbased regularizations are static; and 2) GraphAT to some extent augments the training data, since the generated adversarial examples have not occurred in the training data, which is beneficial to model generalization.</p><p>Approximation. It is non-trivial to obtain the closedform solution of r g i . Inspired by the linear approximation method proposed in <ref type="bibr" target="#b14">[15]</ref> for standard adversarial training, we also design a linear approximation method to calculate the graph adversarial perturbations in GraphAT, of which the formulation is:</p><formula xml:id="formula_5">r g i ≈ ǫ g g , where g = ∇ xi j∈Ni D(f (x i , G|Θ), f (x j , G|Θ)),<label>(3)</label></formula><p>where g is the gradient w.r.t. the input x i . For graph neural network models, the gradient can be efficiently calculated by one backpropagation. Note thatΘ is a constant set denoting the current model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Virtual Graph Adversarial Training</head><p>Considering that node classification is a task of semisupervised learning by nature, we further devise an extended version of GraphAT, named GraphVAT which additionally smooths the distribution of predictions around each clean example to further enhance the model robustness. </p><formula xml:id="formula_6">min: Γ GV AT = Γ + α N i=1 d(f (x i + r v i , G|Θ),ỹ i ) virtual adversarial regularizer + β N i=1 j∈Ni d(f (x i + r g i , G|Θ), f (x j , G|Θ)) graph adversarial regularizer , max: r v i = arg max r ′ i , r ′ i ≤ǫ ′ d(f (x i + r ′ i , G|Θ),ỹ i ),<label>(4)</label></formula><p>where r ′ i denotes the virtual adversarial perturbation, the direction that leads to the largest change on the model prediction of x i . For labeled nodes and unlabeled nodes, y i denotes ground truth label and model prediction, respectively. That is,</p><formula xml:id="formula_7">y i = ŷ i , i ≤ M (labeled node), f (x i , G|Θ), M &lt; i ≤ N (unlabeled node).</formula><p>Note that GraphVAT can be seen as jointly playing two minimax games with three players, where the two maximum players generate virtual adversarial examples and graph adversarial examples, respectively. That is, in each iteration, two types of perturbations and the associated adversarial examples are generated to attack 1) the smoothness of prediction around individual clean example; and 2) the smoothness of connected examples, respectively. By minimizing the additional regularizers over these adversarial examples, the learned model is encouraged to be more smooth and robust, achieving good generalization. Approximation. For labeled nodes, r ′ i can be easily evaluated via linear approximation <ref type="bibr" target="#b14">[15]</ref>, i.e., calculating the gradient of d(f (x i , G|Θ),ỹ i ) w.r.t. x i . For unlabeled nodes, such approximation is infeasible since the gradient will always be zero. This is because d(f (x i , G|Θ),ỹ i ) achieves the minimum value (0) at x i (note thatỹ i = f (x i , G|Θ) for unlabeled data). Realizing that the first-order gradient is always zero, we estimate r ′ i from the second-order</p><formula xml:id="formula_8">Taylor approximation of d(f (x i + r ′ i , G|Θ),ỹ i ). That is, r v i ≈ arg max r ′ i , r ′ i ≤ǫ ′ 1 2 r ′T i Hr ′ i where H is the Hessian matrix of d(f (x i + r ′ i , G|Θ),ỹ i ).</formula><p>For the consideration of efficiency, we calculate r v i via the power iteration approximation <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_9">r v i ≈ ǫ ′ g g , where g = ∇ ri d(f (x i + r i , G|Θ,ỹ i )) | ri=ξd ,<label>(5)</label></formula><p>where d is a random vector. Detailed derivation of the method is referred to <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Convolution Network</head><p>Inspired by the extraordinary representation ability, many neural networks have been used as the predictive model f (x i , G|Θ) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Under the transductive setting, Graph Convolutional Network <ref type="bibr" target="#b6">[7]</ref> is a state-of-the-art model. Specifically, GCN stacks multiple graph convolution layers, which is formulated:</p><formula xml:id="formula_10">H l = σ D − 1 2 A D − 1 2 H l−1 W l + b l .<label>(6)</label></formula><p>Specifically, the l-th graph convolution layer conducts three operations to project H l−1 ∈ R N ×D l−1 (the output of the (l − 1)-th layer or the node features X) into H l ∈ R N ×D l , where D l−1 and D l are the output dimension of layer l − 1 and l, respectively.</p><p>• Similar as the fully connected layer, the graph convolution layer first projects the input (H l−1 ) into latent representations with W l ∈ R D l−1 ×D l and b l ∈ R D l . • It then propagates the latent representations (H l−1 W l +b l ) through the normalizied adjacency matrix</p><formula xml:id="formula_11">D − 1 2 A D − 1 2</formula><p>with self-connections, where D = D + I and A = A + I (I ∈ R N ×N is an identity matrix). Here, the representation of node i in H is the aggregation of latent representations in (H l−1 W l + b l ) of nodes connected to i (including itself due to the self-connection). • Finally, a non-linear activation function σ (e.g., the sigmoid, hyperbolic tangent, and rectifier functions) is applied to allow non-linearity.</p><p>The original objective function of GCN is,</p><formula xml:id="formula_12">M i=1 cross-entropy(f (x i , G|Θ), y i ) + λ Θ 2 F ,<label>(7)</label></formula><p>where the second term is L 2 -norm to prevent overfitting. To train GCN with our proposed GraphAT and GraphVAT, we set the Γ term in Equation 2 and 4 as the cross-entropy loss in <ref type="bibr">Equation 7</ref>, which are minimized to update the parameter of GCN, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Time Complexity and Implementation</head><p>Time Complexity. As compared to GCN with standard training, the additional computation of GraphAT is twofold: 1) generating graph adversarial perturbations ({r g i , i &lt; N }) with Equation 3 and 2) calculating the value of graph adversarial regularizer ( <ref type="figure">G|Θ)</ref>)). Considering that they can be accomplished with a back-propagation (to calculate r g i ) and a forward-propagation (to calculate f (x i + r g i , G|Θ)), the computation overhead of GraphAT is acceptable. Additionally, GraphVAT computes virtual adversarial perturbations and virtual adversarial regularizer, which can also be finished by performing one back-propagation and one forward-propagation <ref type="bibr" target="#b34">[35]</ref>. It indicates that the overhead of GraphVAT is still acceptable <ref type="bibr" target="#b34">[35]</ref>.</p><formula xml:id="formula_13">N i=1 j∈Ni D(f (x i + r g i , G|Θ), f (x j ,</formula><p>Implementation. Noting that number of connected nodes varies a lot across the nodes in the graph, we sample K neighbors for each node to generate adversarial examples and calculate the graph adversarial regularizer to facilitate the calculation. Here, the following sampling strategies <ref type="bibr" target="#b27">[28]</ref> are considered:</p><p>• Uniform: neighbors are selected uniformly. • Degree: the probability of selecting a node is proportional to the normalized node degree. • Degree-Reverse: on the contrary, the probability is the reciprocal of node degree (also normalized to sum to unity). • PageRank: it performs PageRank <ref type="bibr" target="#b45">[46]</ref> on the graph and takes the normalized pagerank score as the sampling probability. Note that advanced but complex sampling strategies (e.g., the one proposed in <ref type="bibr" target="#b27">[28]</ref>) are not considered due to efficiency consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We follow the same experimental settings as in <ref type="bibr" target="#b6">[7]</ref> and conduct experiments on two types of node classification datasets: citation network datasets (Citeseer and Cora <ref type="bibr" target="#b46">[47]</ref>) and knowledge graph (NELL [12]) 2 , of which the statistics are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>• In the citation networks, nodes and edges represent documents and citation links between documents, respectively. Note that the direction of edge is omitted since a citation is assumed to have equally impacts on the prediction of the 2. https://github.com/kimiyoung/planetoid. associated two documents. Each document is associated with a normalized bag-of-words feature vector and a class label. During training, we use features of all nodes, but only 20 labels per class. 500 and 1,000 of the remaining nodes are used as validation and testing, respectively. • NELL is a bipartite graph of 55,864 relation nodes and 9,891 entity nodes, extracted from a knowledge graph which is a set of triplets in the format of (e 1 , r, e 2 ). Here e 1 and e 2 are entities, and r is the connected relation between them. Following <ref type="bibr" target="#b6">[7]</ref>, each relation r is split into two relation nodes (r 1 and r 2 ), from which two edges (e 1 , r 1 ) and (e 2 , r 2 ) are constructed. Entity nodes and relation nodes are described by bag-of-words feature vectors (normalized) and one-hot encodings, respectively. Note that we pad zero values to align the feature vectors of entity and relation nodes. Here only labels of entity nodes are available and only 0.001 of entities under each class are labeled during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines</head><p>We compare the following baselines:</p><p>• LP <ref type="bibr" target="#b7">[8]</ref>: label propagation ignores node features and only propagates labels over the graph structure. • DeepWalk <ref type="bibr" target="#b4">[5]</ref>: it is a skip-gram based graph embedding method, which learns the embedding of a node by predicting its contexts that are generated by performing random walk on the graph. • SemiEmb <ref type="bibr" target="#b47">[48]</ref>: it learns node embeddings from node features and leverages Laplacian regularization to encourage connected nodes have close embeddings. • Planetoid <ref type="bibr" target="#b11">[12]</ref>: similar as DeepWalk, this method learns node embeddings by predicting node context, but additionally accounts for node features. • GCN <ref type="bibr" target="#b6">[7]</ref>: it stacks two graph convolution layers to project node features into labels. It propagates node representations and predictions over the graph structure to smooth the output. • GraphSGAN <ref type="bibr" target="#b39">[40]</ref>: this is a semi-supervised generative adversarial network which encodes the density signal of the graph structure during generation of fake nodes. Note that LP, DeepWalk, SemiEmb, and Planetoid are also baselines in the paper of GCN, we exactly follow their settings in <ref type="bibr" target="#b6">[7]</ref>. In addition, the setting of GraphSGAN is same as the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Parameter Settings</head><p>We implement GraphAT and GraphVAT, which train GCN with different versions of graph adversarial training, respectively, with Tensorflow. GraphAT has six hyperparameters in total: 1) D 1 , the size of hidden layer (GCN); 2) λ, the weight for L 2 -norm (GCN); 3) dropout ratio (GCN); 4) ǫ, the scale of graph adversarial perturbations (GraphAT); 5) β, the weight for graph adversarial regularizer (GraphAT); and 6) K, the number of sampled neighbors (GraphAT). For fair comparison, we set D 1 , λ as the optimal values of standard GCN. But we set dropout ratio as zero in GraphAT for stable training. For the remaining three parameters, ǫ, β, and K, we performed grid-search within the ranges of [0.01, 0.05, 0.1, 0.5, 1], [0.01, 0.05, 0.1, 0.5, 1, 5], <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, respectively. For GraphVAT, for simplicity, we set the six hyperparameters common to that of GraphAT using the optimal values found for GraphAT. Here we only tune its three additional hyperparameters: 1) ǫ ′ , the scale of virtual adversarial perturbations; 2) α, the weight for virtual adversarial regularizer; and 3) ξ, the scale to calculate approximation. In particular, we perform grid-search within the ranges of [0.01, 0.05, 0.1, 0.5, 1], [0.001, 0.005, 0.01, 0.05, 0.1, 0.5], [1e-6, 1e-5, 1e-4], respectively. It should be noted that the Uniform strategy is adopted to sample neighbor nodes if not other specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Model Comparison</head><p>We first investigate how effective is the proposed graph adversarial training via comparing the performance of Graph-VAT (the extended version of GraphAT) with state-of-the-art node classification methods. <ref type="table" target="#tab_3">Table 3</ref> shows the classification performance of the compared methods on the three datasets regarding accuracy. The performance of LP, DeepWalk, SemiEmb, and Planetoid are taken from the GCN paper <ref type="bibr" target="#b6">[7]</ref> since we follow its settings exactly. From the results, we have the following observations:</p><p>• GraphVAT significantly outperforms the standard GCN, exhibiting relative improvements of 6.35%, 1.47%, and 5.72% on the Citeseer, Cora, and NELL datasets, respectively. As the only difference between GraphVAT and GCN is applying the proposed graph adversarial training, the improvements are attributed to the proposed training method which would enhance the stabilization and generalization of GCN. Besides, the results validate that GraphVAT is effective in solving the node classification task. • GraphVAT achieves comparable performance as that of GraphSGAN, which is the state-of-the-art method of node classification. It demonstrates the efficacy of the proposed method. However, our method could offer a more feasible solution for two reasons: 1) GraphSGAN is based on the standard generative adversarial network, which explicitly plays a mini-max game between a discriminator and a generator (two different networks). This, inevitably, will lead to doubling of the computation of model training and the labor of parameter tuning. 2) For different node classification applications, GraphSGAN needs to be built from scratch, whereas our GraphVAT is a generic solution that can be seamlessly applied to enhance the existing models of the applications. • GraphVAT and GraphSGAN achieve better results in all the cases as compared to the other baselines. On the Citeseer, Cora, and NELL datasets, the relative improvements are at least 6.35%, 1.97%, and 4.52%, respectively. This indicates the effectiveness of adversarial learning, i.e., dynamically playing a mini-max game either implicitly (GraphVAT) and explicitly (GraphSGAN) in the training phase. Moreover, the results are consistent with findings in previous work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b48">[49]</ref>. • Among the baselines, 1) the methods that jointly account for the graph structure and node features (in the category of +Node Features) outperform LP and DeepWalk that only consider graph structure. This suggests further exploration of how to combine the connection patterns and node features more appropriately. 2) As compared to SemiEmb that is a shallow model, Planetoid and GCN achieves significant improvements (from 8.56% to 131.8%) in all cases. The improvement is reasonable and attributed to the strong representation ability of neural networks. As such, methods targeting to enhance the graph neural network models, such as the graph adversarial training, will be meaningful and influential in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance w.r.t. Node Degree</head><p>We next study how the graph adversarial training performs on nodes with different densities of connections so as to understand where this regularization technique can be more suitably applied. We empirically split the nodes into three groups according to node degree (i.e., the number of neighbors), where node degrees are in ranges of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, <ref type="bibr">[6, N ]</ref>. <ref type="figure">Figure 3</ref> illustrates the distribution of nodes over the three groups. As can be seen, in all the three datasets, a great number of nodes are sparsely connected (with degrees smaller than three), and only about ten percent of the nodes are densely connected with degrees bigger than five. By separately counting the accuracy of GCN and <ref type="figure">Fig. 3</ref>: Percentage of nodes with degrees in different groups in the three datasets.</p><p>GraphVAT over nodes in different groups, we obtain the group-oriented performance on the three datasets, which is depicted in <ref type="figure" target="#fig_4">Figure 4</ref>. From the results, we observe that:</p><p>• In all the three datasets, both GCN and GraphVAT achieves the best performance on the group of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. The relatively worse performance on the group of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> could be attributed to that the nodes in that group are sparsely  connected and lacks sufficient signals propagated from the neighbors, which are helpful for the classification <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b49">[50]</ref>. In addition, we postulate the reason for the worse performance over nodes with degrees in <ref type="bibr">[6, N ]</ref> as such nodes are harder to classify. This is because such nodes typically represent more general entities, such as those having connections to other entities with different types of relations and are thus harder to be accurately classified into a specific category. • In most cases (except the [6, N ] group of Cora and NELL),</p><p>GraphVAT outperforms GCN, which indicates that graph adversarial training would benefit the prediction of nodes with different degrees and is roughly not sensitive to the density of graph. For one of the exceptions (the [6, N ] group of NELL), we speculate that the reason is the underfitting of standard GCN on such nodes (note that the performance of GCN on [6, N ] is 27.7% on average worse than the other two groups), where additional regularization performed by graph adversarial training worsens the under-fitting problem. • GraphVAT significantly and consistently outperforms GCN on the group of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, with an average improvement of 5.45%. The result indicates that the graph adversarial training would be more effective on sparse part of the graph. It should be noted that most of the graphs are sparse in real world applications <ref type="bibr" target="#b50">[51]</ref>. As such this result further demonstrates the potential of the proposed methods in real world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Method Ablation</head><p>Recall that we design two versions of graph adversarial training: 1) basic GraphAT (Equation 2) and 2) incorporating virtual adversarial training (Equation 4). To evaluate the contribution of these two types of regularizations, we compare the performance of the following solutions built upon GCN:  <ref type="table" target="#tab_4">Table 4</ref> shows the performance of the compared methods on the three datasets w.r.t. accuracy. As can be seen:</p><p>• In most of the cases, GCN performs worse than the other approaches, which indicates that adversarial training could enhance the node classification model as compared to the standard training. That is, by intentionally and dynamically generating perturbations and optimizing additional regularizers, the trained model could by more accurate. • GraphVAT achieves the best performance in all cases, i.e.,</p><p>GraphVAT outperforms both GCN-VAT and GraphAT. It shows that perturbations targeting at both the individual nodes (virtual adversarial perturbations) and neighbor nodes (graph adversarial perturbations) benefit the training of graph neural network model. Moreover, it suggests that it is beneficial to jointly consider both the node features and graph structure in adversarial training of graph neural networks. • Compared to GCN-VAT, GraphAT achieves improvements of 1.38% and 4.04% on the Citeseer and Cora datasets, which signifies the benefit of accounting for the graph structure in adversarial training of graph neural networks. However, on the NELL dataset, the performance of GraphAT is 1.58% worse than GCN-VAT. We speculate that the decrease is mainly because NELL is a bipartite graph where the neighbors of an entity node are all relation nodes without bag-of-words descriptions and labels. Therefore, as compared to standard graph with homogeneous nodes, the generated graph adversarial perturbations according to the predictions of connected relation nodes are less effective. It should be noted that, by resisting such perturbations, GraphAT still implicitly encourages smooth predictions of entity nodes connected by the same relation node, which could be the reason why GraphAT outperforms standard GCN on NELL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Effect of Sampling Strategies</head><p>As mentioned in Section 4.4, different strategies could be adopted to sample neighbor nodes for the generation of graph adversarial perturbations and the calculation of graph adversarial regularizer. Here, we investigate the effect of sampling strategies via comparing the results of GraphAT performing different samplings. Note that we select GraphAT rather than GraphVAT for the reason that we focus on investigating properties of the proposed graph adversarial training. <ref type="table" target="#tab_5">Table 5</ref> shows the corresponding performance, from which we can observe that the performance of different sampling strategies are comparable to each other. As compared to Uniform, the relative improvement (RI) achieved by the other strategies is within a range of [-0.9%, 0.1%]. As such, Uniform would be a suitable selection since it will not bring any additional computation as compared to the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Sensitivity</head><p>We then investigate how the value of hyperparameters effects the performance of the proposed GraphAT. Given a hyperparameter, we evaluate model performance when adjusting its value and fixing the other hyperparameters with optimal values. Since focusing on graph adversarial regularization, we mainly study: a) weight of graph adversarial regularizer (β), b) scale of graph adversarial perturbations (ǫ), and c) number of sampled neighbors (k), and use GraphAT to report the performance. It should be noted that, in the following, we focus on the citation graphs and omit results on NELL, which is a bipartite graph rather than a standard graph. <ref type="figure">Figure 5</ref> illustrates the performance of GraphAT on the validation and testing of the three datasets when varying the value of β, ǫ, and k. From the figures, we have the following observations:</p><p>• Under most cases, the performance of GraphAT changes smoothly near the optimal value of the selected hyperparameter, which indicates that GraphAT is not sensitive to hyperparameters. The only exception is that GraphAT performs significantly worse when k = 3 and k = 5 as compared to the performance with other values of k.</p><p>We check the training procedure and observe that both of them are caused by triggering early stopping at the early stage of the training (dozens of epochs), which is occasional and would converge to an expected performance if disable early stopping. • For each parameter, a) GraphAT achieves best performance with β is around 0.1, which roughly balances the contribution of the supervised loss and the graph adversarial regularizer (note that the supervised loss decreases fast at the early epochs). Larger value of β (stronger regularization) will harm GraphAT since the model could suffer from the underfitting issue. b) GraphAT performs well when ǫ is in the range of [1e-4, 1e-2], but the performance decreases significantly as increasing ǫ. This result supports the assumption that perturbations have to be in a small scale so that the constructed adversarial examples have similar feature distributions as the real data. c) GraphAT performs best when k = 1 or k = 2, which is somehow coherent with the result in <ref type="figure" target="#fig_4">Figure 4</ref> that graph adversarial training are more effective to nodes with degree in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. This result is appealing since the computation cost linearly increases as sampling more neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Tuning ǫ Only</head><p>Considering that the number of candidate combinations exponentially increases with the number of hyperparameters, we explore whether comparable performance could be achieved when tune one hyperparameter alone and fix the others with empirical values. It should be noted that previous work <ref type="bibr" target="#b34">[35]</ref> has shown that tuning ǫ ′ alone could suffice for achieving satisfactory performance of VAT. Similarly, we tune ǫ with β = 1 and k = 1 and summarize the performance of GraphAT in <ref type="table" target="#tab_6">Table 6</ref>. As can be seen, on the citation graphs, tuning ǫ alone achieves satisfactory performance. As such, the overhead of additional hyperparameters of the proposed GraphAT could be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Graph Adversarial Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Training Process</head><p>We next study the effect of GraphAT on the training process of GCN. Specifically, we observe the performance of GCN and GraphAT on the validation and testing of Citeseer and Cora after every training epoch, which is depicted in <ref type="figure">Figure 6</ref>. As can be seen, 1) On the two datasets, the performance of GCN and GraphAT becomes stable after 100 epochs, which indicates that graph adversarial training will not affect the convergence speed of GCN. 2) It is interesting to see that the performance of GraphAT increases faster than standard GCN during the initial several epochs. Note that the supervised loss is typically much larger (about 1e5 times) than the value of graph adversarial regularizer at the initial epochs. This is because all nodes are assigned predictions close to random at the beginning which leads to tiny divergence between connected nodes. As such, the  acceleration of performance increase at the initial epochs is believed to be the effect of data augmentation (additional adversarial examples) rather then a better regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Robustness against Adversarial Perturbations</head><p>Recall that our target is to enhance the robustness of graph neural networks. <ref type="table" target="#tab_7">Table 7</ref> shows relative performance decrease of GCN and GraphAT on adversarial examples as compared to clean examples. As can be seen, by training GCN with graph adversarial training, the model becomes less sensitive to graph adversarial perturbations. Graph adversarial perturbations in the scale of 0.01 (i.e., ǫ = 0.01) decreases accuracy of GCN by 13.9% on average, while the number is only 2.9% for GraphAT. It validates that the graph adversarial training technique could enhance the robustness of a GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Effect of Graph Adversarial Training on Divergence of Neighbor Nodes</head><p>We retrospect that the intuition of graph adversarial regularizer is to encourage connected nodes to be predicted similarly. <ref type="table" target="#tab_8">Table 8</ref> shows the effect of applying graph adversarial training to train GCN, from which we can see that graph adversarial training reduces the divergence between connected nodes as expected. These results show that the predictions of GraphAT are more smooth over the graph structure, which indicates the stronger generalization ability and robustness of the trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we proposed a new learning method, named graph adversarial training, which additional accounts for relation between examples as compared to standard adversarial training. By iteratively generating adversarial examples attacking the graph smoothness constraint and learning over adversarial examples, the proposed method encourages the smoothness of predictions over the given graph, a property indicating good generalization of the model. As can be seen as a dynamic regularization technique, our method is generic to be applied to train most graph neural network models. We trained one well-established model, GCN, with the proposed method to solve the node classification task. By conducting experiments on three benchmark datasets, we demonstrated that training GCN with our method is remarkably effective, achieving an average improvement of 4.51%. Moreover, it also beats GCN trained with VAT, indicating the necessity of performing AT with graph structure considered In future, we will explore we are interested to explore the effectiveness of GAD on more graph neural network models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Moreover, we are interested to investigate the effect of GAD on other graph-based learning tasks such as link prediction and community detection. As focusing on graph-based learning with only one graph in this paper, one potential future work is to investigate the effectiveness of graph adversarial training for graph-based learning methods simultaneously handling multiple graphs. In addition, we are interested in testing the performance of graph adversarial training on graphs with specifical structures, for instance, hyper-graphs and heterogeneous information graphs. Moreover, we would like to incorporate techniques like robust optimization <ref type="bibr" target="#b51">[52]</ref> and adversarial dropout <ref type="bibr" target="#b52">[53]</ref> into the proposed method to further enhance its ability of stabilizing graph neural network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.1/-&amp;)*';/-&amp;(&lt;3*=+/!"#$%&amp;' -34&amp;/5&amp;'()*&amp;+/9-(3/&amp;78&amp;449-0+/ 2/!"#!()('% &amp;78&amp;449-.1/-&amp;)*';/-&amp;(&lt;3*=+/!"#$%&amp;' -34&amp;/5&amp;'()*&amp;+/9-(3/&amp;78&amp;449-0+/ 2/!"#!()('% &amp;78&amp;449-0+/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>)(/0*'.1/2/-34&amp;/ 5&amp;'()*&amp;+/6!"#$%&amp;#'$()"#7 ! " # $ %&amp;'()*&amp;+ ,-.)(/0*'.1/2/-34&amp;/5&amp;'()*&amp;+/ 6$*+#,-$,.$"&amp;#'$()"#7 8&amp;*()*9'(:3-+'*:'&gt;/ *&amp;0)&gt;'*:?&amp;* &gt;3++ The training process of GraphAT: 1) constructing graph adversarial example and 2) updating model parameters by minimizing loss and graph adversarial regularizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Maximization: GraphAT generates graph adversarial perturbations from clean examples, which break the smoothness between connected nodes to the maximum extent. and then constructs graph adversarial examples by adding the perturbations to the input of associated clean examples. • Minimization: GraphAT minimizes the objective function of the graph neural network with an additional regularizer over graph adversarial examples, by encouraging smoothness between predictions of adversarial examples and connected examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Inspired by the idea of virtual adversarial training [35], we further add a virtual adversarial regularizer into the training objective function and construct virtual adversarial examples to attack the local smoothness of predictions. Compared to standard AT which only considers labeled clean examples, virtual adversarial training additionally encourages the model to make consistent predictions around the unlabeled clean examples. The formulation of Graph-VAT is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Performance of GCN and GraphVAT on nodes with different degrees in Citeseer (a), Cora (b), and NELL (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>GCN: It learns the parameters of GCN with standard training, i.e., it optimizes Equation 7. • GCN-VAT: Virtual adversarial training, which performs perturbations by considering node features only, is employed to train GCN, i.e., optimizing Equation 4 with β = 0. • GraphAT: It trains GCN by the basic GraphAT, of which the perturbations only focus on graph structure. That is optimizing Equation 4 with α = 0. • GraphVAT: It accounts for both the virtual and graph adversarial regularizations during the training of GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Performance of GraphAT with different values of hyperparameters: (a) weight of graph adversarial regularizer (β), (b) scale of graph adversarial perturbations (ǫ), and (c) number of sampled neighbors (k) on the validation and testing of the three datasets (When investigating the effect of a hyperparameter, the other two are set as the optimal values). Training curves of the GCN and GraphAT on the validation and testing of Citeseer and Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Terms and notations. , x N ] T ∈ R N×F features of N nodes. A, D ∈ R N×N adjacency matrix and degree matrix of a graph. Y = [y1, y2, · · · , y M ] T ∈ R M ×L labels of M nodes.</figDesc><table><row><cell>Symbol</cell><cell>Definition</cell></row><row><cell>X = [x1, x2, · · · r g i r v i W l , b l</cell><cell>graph adversarial perturbation of node i. virtual graph adversarial perturbation of node i.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Statistics of the experiment datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell>#Classes</cell><cell cols="2">#Features Label rate</cell></row><row><cell>Citeseer</cell><cell>3,312</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell>NELL</cell><cell>65,755</cell><cell>266,144</cell><cell>210</cell><cell>5,414</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Performance of the compared methods on the three datasets w.r.t. accuracy.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell cols="2">Citeseer Cora</cell><cell>NELL</cell></row><row><cell>Graph</cell><cell>LP DeepWalk</cell><cell>45.3 43.2</cell><cell>68.0 67.2</cell><cell>26.5 58.1</cell></row><row><cell>+Node Features</cell><cell>SemiEmb Planetoid GCN</cell><cell>59.6 64.7 69.3</cell><cell>59.0 75.7 81.4</cell><cell>26.7 61.9 61.2</cell></row><row><cell>+Adversarial</cell><cell>GraphSGAN GraphVAT</cell><cell>73.1 73.7</cell><cell>83.0 82.6</cell><cell>-64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Effect of graph adversarial regularization and virtual graph adversarial regularization.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell cols="2">Citeseer Cora</cell><cell>NELL</cell></row><row><cell>Standard Training</cell><cell>GCN</cell><cell>69.3</cell><cell>81.4</cell><cell>61.2</cell></row><row><cell>Adversarial Training</cell><cell>GCN-VAT GraphAT GraphVAT</cell><cell>72.4 73.4 73.7</cell><cell>79.3 82.5 82.6</cell><cell>63.3 62.3 64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Performance comparison of GraphAT with different neighbor sampling strategies during adversarial example generation. RI denotes the relative improvement over the Uniform strategy.</figDesc><table><row><cell cols="2">Sampling Strategy Citeseer</cell><cell>Cora</cell><cell>NELL</cell><cell>RI</cell></row><row><cell>Uniform</cell><cell>73.4</cell><cell>82.5</cell><cell>62.3</cell><cell>-</cell></row><row><cell>Degree</cell><cell>73.0</cell><cell>82.9</cell><cell>61.8</cell><cell>-0.9%</cell></row><row><cell>Degree-Reverse</cell><cell>73.8</cell><cell>82.4</cell><cell>62.1</cell><cell>0.1%</cell></row><row><cell>PageRank</cell><cell>72.6</cell><cell>83.1</cell><cell>62.0</cell><cell>-0.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Performance of GraphAT as tuning all hyperparameters (i.e., β, ǫ, and k) and tuning ǫ with fixed β = 1.0 and k = 1.</figDesc><table><row><cell>Hyperparameter</cell><cell>Citeseer</cell><cell>Cora</cell></row><row><cell>{β, ǫ, k}</cell><cell>73.4</cell><cell>82.5</cell></row><row><cell>{ǫ}</cell><cell>73.6</cell><cell>82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 :</head><label>7</label><figDesc>The impact of adding graph adversarial perturbations to GCN and GraphAT. The number shows the relative decrease of testing accuracy.</figDesc><table><row><cell>Method</cell><cell>Citeseer</cell><cell>Cora</cell></row><row><cell>GCN</cell><cell>-21.1%</cell><cell>-6.6%</cell></row><row><cell>GraphAT</cell><cell>-4.1%</cell><cell>-1.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 :</head><label>8</label><figDesc>Average Kullback-Leibler divergence between connected node pairs calculated from predictions of GCN and GraphAT (small value indicates close predictions).</figDesc><table><row><cell>Method</cell><cell cols="2">Citeseer</cell><cell cols="2">Cora</cell></row><row><cell></cell><cell>Test</cell><cell>All</cell><cell>Test</cell><cell>All</cell></row><row><cell>GCN</cell><cell>0.132</cell><cell cols="2">0.137 0.345</cell><cell>0.333</cell></row><row><cell>GraphAT</cell><cell>0.127</cell><cell cols="2">0.130 0.308</cell><cell>0.299</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is part of NExT++ reserach, supported by the National Research Foundation Singapore under its AI Singapore Programme (AISG-100E-2018-012), Prime Minister's Office under its IRC@SG Funding Initiative, and National Natural Science Foundation of China (61972372). We thank the anonymous reviewers and the associated editor for their reviewing efforts.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuli</head><p>Feng is a Ph.D. student in the School of Computing, National University of Singapore. He received the B.E. degree in School of Computer Science and Engineering from Baihang University, Beijing, in 2015. His research interests include information retrieval, data mining, and multi-media processing. He has over 10 publications appeared in several top conferences such as SIGIR, WWW, and MM. His work on Bayesian Personalized Ranking has received the Best Poster Award of WWW 2018. Moreover, he has been served as the PC member and external reviewer for several top conferences including SIGIR, ACL, KDD, IJCAI, AAAI, WSDM etc. Xiangnan He is currently a research fellow with School of Computing, National University of Singapore (NUS). He received his Ph.D. in Computer Science from NUS. His research interests span recommender system, information retrieval, natural language processing and multimedia. His work on recommender system has received the Best Paper Award Honorable Mention in WWW 2018 and SIGIR 2016. Moreover, he has served as the PC member for top-tier conferences including SIGIR, WWW, MM, KDD, WSDM, CIKM, AAAI, and ACL, and the invited reviewer for prestigious journals including TKDE, TOIS, TKDD, TMM, and WWWJ. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-regularized deep multi-network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing stock movement prediction with adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5843" to="5849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial active learning for unsupervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial substructured representation learning for mobile user profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial training towards robust multimedia recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New regularized algorithms for transductive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="442" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning on partialorder hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1115" to="1124" />
			<date type="published" when="2018" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graphgan: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aaane: Attention-based adversarial autoencoder for multi-scale network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep network representations with adversarially regularized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">International encyclopedia of statistical science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Joyce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="720" to="722" />
		</imprint>
	</monogr>
	<note>Kullback-leibler divergence</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stanford InfoLab, Tech. Rep</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial personalized ranking for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SI-GIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="3917" to="3924" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
