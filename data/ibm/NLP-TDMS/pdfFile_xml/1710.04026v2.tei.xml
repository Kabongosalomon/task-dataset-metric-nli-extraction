<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wangmeng</forename><forename type="middle">Zuo</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image denoising</term>
					<term>convolutional neural networks</term>
					<term>Gaussian noise</term>
					<term>spatially variant noise</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled subimages, achieving a good trade-off between inference speed and denoising performance. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including (i) the ability to handle a wide range of noise levels (i.e., [0,  75]) effectively with a single network, (ii) the ability to remove spatially variant noise by specifying a non-uniform noise level map, and (iii) faster speed than benchmark BM3D even on CPU without sacrificing denoising performance. Extensive experiments on synthetic and real noisy images are conducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The results show that FFDNet is effective and efficient, making it highly attractive for practical denoising applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE importance of image denoising in low level vision can be revealed from many aspects. First, noise corruption is inevitable during the image sensing process and it may heavily degrade the visual quality of acquired image. Removing noise from the observed image is an essential step in various image processing and computer vision tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Second, from the Bayesian perspective, image denoising is an ideal test bed for evaluating image prior models and optimization methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Last but not least, in the unrolled inference via variable splitting techniques, many image restoration problems can be addressed by sequentially solving a series of denoising subproblems, which further broadens the application fields of image denoising <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>As in many previous literature of image denoising <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, in this paper we assume that the noise is additive white Gaussian noise (AWGN) and the noise level is given. In order to handle practical image denoising problems, a flexible image denoiser is expected to have the following desirable properties: (i) it is able to perform denoising using a single model; (ii) it is efficient, effective and user-friendly; and (iii) it can handle spatially variant noise. Such a denoiser can be directly deployed to recover the clean image when the noise level is known or can be well estimated. When the noise level is unknown or is difficult to estimate, the denoiser should allow the user to adaptively control the trade-off between noise reduction and detail preservation. Furthermore, the noise can be spatially variant and the denoiser should be flexible enough to handle spatially variant noise.</p><p>However, state-of-the-art image denoising methods are still limited in flexibility or efficiency. In general, image denoising methods can be grouped into two major categories, modelbased methods and discriminative learning based ones. Modelbased methods such as BM3D <ref type="bibr" target="#b10">[11]</ref> and WNNM <ref type="bibr" target="#b4">[5]</ref> are flexible in handling denoising problems with various noise levels, but they suffer from several drawbacks. For example, their optimization algorithms are generally time-consuming, and cannot be directly used to remove spatially variant noise. Moreover, model-based methods usually employ hand-crafted image priors (e.g., sparsity <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and nonlocal selfsimilarity <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>), which may not be strong enough to characterize complex image structures.</p><p>As an alternative, discriminative denoising methods aim to learn the underlying image prior and fast inference from a training set of degraded and ground-truth image pairs. One approach is to learn stage-wise image priors in the context of truncated inference procedure <ref type="bibr" target="#b16">[17]</ref>. Another more popular approach is plain discriminative learning, such as the MLP <ref type="bibr" target="#b17">[18]</ref> and convolutional neural network (CNN) based methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, among which the DnCNN <ref type="bibr" target="#b19">[20]</ref> method has achieved very competitive denoising performance. The success of CNN for image denoising is attributed to its large modeling capacity and tremendous advances in network training and design. However, existing discriminative denoising methods are limited in flexibility, and the learned model is usually tailored to a specific noise level. From the perspective of regression, they aim to learn a mapping function x = F(y; Θ σ ) between the input noisy observation y and the desired output x. The model parameters Θ σ are trained for noisy images corrupted by AWGN with a fixed noise level σ, while the trained model with Θ σ is hard to be directly deployed to images with other noise levels. Though a single CNN model (i.e., DnCNN-B) is trained in <ref type="bibr" target="#b19">[20]</ref> for Gaussian denoising, it does not generalize well to real noisy images and works only arXiv:1710.04026v2 [cs.CV] 22 May 2018 if the noise level is in the preset range, e.g., <ref type="bibr">[0,</ref><ref type="bibr" target="#b55">55]</ref>. Besides, all the existing discriminative learning based methods lack flexibility to deal with spatially variant noise.</p><p>To overcome the drawbacks of existing CNN based denoising methods, we present a fast and flexible denoising convolutional neural network (FFDNet). Specifically, our FFDNet is formulated as x = F(y, M; Θ), where M is a noise level map. In the DnCNN model x = F(y; Θ σ ), the parameters Θ σ vary with the change of noise level σ, while in the FFDNet model, the noise level map is modeled as an input and the model parameters Θ are invariant to noise level. Thus, FFDNet provides a flexible way to handle different noise levels with a single network.</p><p>By introducing a noise level map as input, it is natural to expect that the model performs well when the noise level map matches the ground-truth one of noisy input. Furthermore, the noise level map should also play the role of controlling the trade-off between noise reduction and detail preservation. It is found that heavy visual quality degradation may be engendered when setting a larger noise level to smooth out the details. We highlight this problem and adopt a method of orthogonal initialization on convolutional filters to alleviate this. Besides, the proposed FFDNet works on downsampled sub-images, which largely accelerates the training and testing speed, and enlarges the receptive field as well.</p><p>Using images corrupted by AWGN, we quantitatively compare FFDNet with state-of-the-art denoising methods, including model-based methods such as BM3D <ref type="bibr" target="#b10">[11]</ref> and WNNM <ref type="bibr" target="#b4">[5]</ref> and discriminative learning based methods such as TNRD <ref type="bibr" target="#b16">[17]</ref> and DnCNN <ref type="bibr" target="#b19">[20]</ref>. The results clearly demonstrate the superiority of FFDNet in terms of both denoising performance and computational efficiency. In addition, FFDNet performs favorably on images corrupted by spatially variant AWGN. We further evaluate FFDNet on real-world noisy images, where the noise is often signal-dependent, non-Gaussian and spatially variant. The proposed FFDNet model still achieves perceptually convincing results by setting proper noise level maps. Overall, FFDNet enjoys high potentials for practical denoising applications.</p><p>The main contribution of our work is summarized as follows:</p><p>• A fast and flexible denoising network, namely FFDNet, is proposed for discriminative image denoising. By taking a tunable noise level map as input, a single FFDNet is able to deal with noise on different levels, as well as spatially variant noise. • We highlight the importance to guarantee the role of the noise level map in controlling the trade-off between noise reduction and detail preservation. • FFDNet exhibits perceptually appealing results on both synthetic noisy images corrupted by AWGN and realworld noisy images, demonstrating its potential for practical image denoising.</p><p>The remainder of this paper is organized as follows. Sec. II reviews existing discriminative denoising methods. Sec. III presents the proposed image denoising model. Sec. IV reports the experimental results. Sec. V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review and discuss the two major categories of relevant methods to this work, i.e., maximum a posteriori (MAP) inference guided discriminative learning and plain discriminative learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MAP Inference Guided Discriminative Learning</head><p>Instead of first learning the prior and then performing the inference, this category of methods aims to learn the prior parameters along with a compact unrolled inference through minimizing a loss function <ref type="bibr" target="#b20">[21]</ref>. Following the pioneer work of fields of experts <ref type="bibr" target="#b2">[3]</ref>, Barbu <ref type="bibr" target="#b20">[21]</ref> trained a discriminative Markov random field (MRF) model together with a gradient descent inference for image denoising. Samuel and Tappen <ref type="bibr" target="#b21">[22]</ref> independently proposed a compact gradient descent inference learning framework, and discussed the advantages of discriminative learning over model-based optimization method with MRF prior. Sun and Tappen <ref type="bibr" target="#b22">[23]</ref> proposed a novel nonlocal range MRF (NLR-MRF) framework, and employed the gradient-based discriminative learning method to train the model. Generally speaking, the methods above only learn the prior parameters in a discriminative manner, while the inference parameters are stage-invariant.</p><p>With the aid of unrolled half quadratic splitting (HQS) techniques, Schmidt et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> proposed a cascade of shrinkage fields (CSF) framework to learn stage-wise inference parameters. Chen et al. <ref type="bibr" target="#b16">[17]</ref> further proposed a trainable nonlinear reaction diffusion (TNRD) model through discriminative learning of a compact gradient descent inference step. Recently, Lefkimmiatis <ref type="bibr" target="#b26">[26]</ref> and Qiao et al. <ref type="bibr" target="#b27">[27]</ref> adopted a proximal gradient-based denoising inference from a variational model to incorporate the nonlocal self-similarity prior. It is worth noting that, apart from MAP inference, Vemulapalli et al. <ref type="bibr" target="#b28">[28]</ref> derived an end-to-end trainable patch-based denoising network based on Gaussian Conditional Random Field (GCRF) inference.</p><p>MAP inference guided discriminative learning usually requires much fewer inference steps, and is very efficient in image denoising. It also has clear interpretability because the discriminative architecture is derived from optimization algorithms such as HQS and gradient descent <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, the learned priors and inference procedure are limited by the form of MAP model <ref type="bibr" target="#b24">[25]</ref>, and generally perform inferior to the state-of-the-art CNN-based denoisers. For example, the inference of CSF <ref type="bibr" target="#b23">[24]</ref> is not very flexible since it is strictly derived from the HQS optimization under the field of experts (FoE) framework. The capacity of FoE is however not large enough to fully characterize image priors, which in turn makes CSF less effective. For these reasons, Kruse et al. <ref type="bibr" target="#b29">[29]</ref> generalized CSF for better performance by replacing some modular parts of unrolled inference with more powerful CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Plain Discriminative Learning</head><p>Instead of modeling image priors explicitly, the plain discriminative learning methods learn a direct mapping function to model image prior implicitly. The multi-layer perceptron 01234 5678 9ÿ 6594 ÿ 1 4 9ÿ 998 ÿ 57 138 395ÿ 577 3 0931 4 9ÿ 6594 ÿ ÿ ÿ ÿ !" # ÿ ÿ !" # ÿ ÿ ÿ ÿ !" # (MLP) and CNNs have been adopted to learn such priors. The use of CNN for image denoising can be traced back to <ref type="bibr" target="#b18">[19]</ref>, where a five-layer network with sigmoid nonlinearity was proposed. Subsequently, auto-encoder based methods have been suggested for image denoising <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. However, early MLP and CNN-based methods are limited in denoising performance and cannot compete with the benchmark BM3D method <ref type="bibr" target="#b10">[11]</ref>. The first discriminative denoising method which achieves comparable performance with BM3D is the plain MLP method proposed by Burger et al. <ref type="bibr" target="#b17">[18]</ref>. Benefitted from the advances in deep CNN, Zhang et al. <ref type="bibr" target="#b19">[20]</ref> proposed a plain denoising CNN (DnCNN) method which achieves state-of-the-art denoising performance. They showed that residual learning and batch normalization <ref type="bibr" target="#b32">[32]</ref> are particularly useful for the success of denoising. For a better trade-off between accuracy and speed, Zhang et al. <ref type="bibr" target="#b8">[9]</ref> introduced a 7-layer denoising network with dilated convolution <ref type="bibr" target="#b33">[33]</ref> to expand the receptive field of CNN. Mao et al. <ref type="bibr" target="#b34">[34]</ref> proposed a very deep fully convolutional encoding-decoding network with symmetric skip connection for image denoising. Santhanam et al. <ref type="bibr" target="#b35">[35]</ref> developed a recursively branched deconvolutional network (RBDN) for image denoising as well as generic image-to-image regression. Tai et al. <ref type="bibr" target="#b36">[36]</ref> proposed a very deep persistent memory network (MemNet) by introducing a memory block to mine persistent memory through an adaptive learning process.</p><p>Plain discriminative learning has shown better performance than MAP inference guided discriminative learning; however, existing discriminative learning methods have to learn multiple models for handling images with different noise levels, and are incapable to deal with spatially variant noise. To the best of our knowledge, it remains an unaddressed issue to develop a single discriminative denoising model which can handle noise of different levels, even spatially variant noise, in a speed even faster than BM3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED FAST AND FLEXIBLE DISCRIMINATIVE CNN DENOISER</head><p>We present a single discriminative CNN model, namely FFDNet, to achieve the following three objectives:</p><p>• Fast speed: The denoiser is expected to be highly efficient without sacrificing denoising performance. • Flexibility: The denoiser is able to handle images with different noise levels and even spatially variant noise.</p><p>• Robustness: The denoiser should introduce no visual artifacts in controlling the trade-off between noise reduction and detail preservation. In this work, we take a tunable noise level map M as input to make the denoising model flexible to noise levels. To improve the efficiency of the denoiser, a reversible downsampling operator is introduced to reshape the input image of size W ×H×C into four downsampled sub-images of size</p><formula xml:id="formula_0">W 2 × H 2 × 4C.</formula><p>Here C is the number of channels, i.e., C = 1 for grayscale image and C = 3 for color image. In order to enable the noise level map to robustly control the trade-off between noise reduction and detail preservation by introducing no visual artifacts, we adopt the orthogonal initialization method to the convolution filters.</p><p>A. Network Architecture <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates the architecture of FFDNet. The first layer is a reversible downsampling operator which reshapes a noisy image y into four downsampled sub-images. We further concatenate a tunable noise level map M with the downsampled sub-images to form a tensorỹ of size W 2 × H 2 × (4C + 1) as the inputs to CNN. For spatially invariant AWGN with noise level σ, M is a uniform map with all elements being σ.</p><p>With the tensorỹ as input, the following CNN consists of a series of 3 × 3 convolution layers. Each layer is composed of three types of operations: Convolution (Conv), Rectified Linear Units (ReLU) <ref type="bibr" target="#b37">[37]</ref>, and Batch Normalization (BN) <ref type="bibr" target="#b32">[32]</ref>. More specifically, "Conv+ReLU" is adopted for the first convolution layer, "Conv+BN+ReLU" for the middle layers, and "Conv" for the last convolution layer. Zero-padding is employed to keep the size of feature maps unchanged after each convolution. After the last convolution layer, an upscaling operation is applied as the reverse operator of the downsampling operator applied in the input stage to produce the estimated clean imagex of size W × H × C. Different from DnCNN, FFDNet does not predict the noise. The reason is given in Sec. III-F. Since FFDNet operates on downsampled sub-images, it is not necessary to employ the dilated convolution <ref type="bibr" target="#b33">[33]</ref> to further increase the receptive field.</p><p>By considering the balance of complexity and performance, we empirically set the number of convolution layers as 15 for grayscale image and 12 for color image. As to the channels of feature maps, we set 64 for grayscale image and 96 for color image. The reason that we use different settings for grayscale and color images is twofold. First, since there are high dependencies among the R, G, B channels, using a smaller number of convolution layers encourages the model to exploit the inter-channel dependency. Second, color image has more channels as input, and hence more feature (i.e., more channels of feature map) is required. According to our experimental results, increasing the number of feature maps contributes more to the denoising performance on color images. Using different settings for color images, FFDNet can bring an average gain of 0.15dB by PSNR on different noise levels. As we shall see from Sec. IV-F, 12-layer FFDNet for color image runs slightly slower than 15-layer FFDNet for grayscale image. Taking both denoising performance and efficiency into account, we set the number of convolution layers as 12 and the number of feature maps as 96 for color image denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noise Level Map</head><p>Let's first revisit the model-based image denoising methods to analyze why they are flexible in handling noises at different levels, which will in turn help us to improve the flexibility of CNN-based denoiser. Most of the model-based denoising methods aim to solve the following problem</p><formula xml:id="formula_1">x = arg min x 1 2σ 2 y − x 2 + λΦ(x),<label>(1)</label></formula><p>where 1 2σ 2 y − x 2 is the data fidelity term with noise level σ, Φ(x) is the regularization term associated with image prior, and λ controls the balance between the data fidelity and regularization terms. It is worth noting that in practice λ governs the compromise between noise reduction and detail preservation. When it is too small, much noise will remain; on the opposite, details will be smoothed out along with suppressing noise.</p><p>With some optimization algorithms, the solution of Eqn. (1) actually defines an implicit function given bŷ</p><formula xml:id="formula_2">x = F(y, σ, λ; Θ).<label>(2)</label></formula><p>Since λ can be absorbed into σ, Eqn. (2) can be rewritten aŝ</p><formula xml:id="formula_3">x = F(y, σ; Θ).<label>(3)</label></formula><p>In this sense, setting noise level σ also plays the role of setting λ to control the trade-off between noise reduction and detail preservation. In a word, model-based methods are flexible in handling images with various noise levels by simply specifying σ in Eqn. (3). According to the above discussion, it is natural to utilize CNN to learn an explicit mapping of Eqn. (3) which takes the noise image and noise level as input. However, since the inputs y and σ have different dimensions, it is not easy to directly feed them into CNN. Inspired by the patch based denoising methods which actually set σ for each patch, we resolve the dimensionality mismatching problem by stretching the noise level σ into a noise level map M. In M, all the elements are σ. As a result, Eqn. (3) can be further rewritten aŝ</p><formula xml:id="formula_4">x = F(y, M; Θ).<label>(4)</label></formula><p>It is worth emphasizing that M can be extended to degradation maps with multiple channels for more general noise models such as the multivariate (3D) Gaussian noise model N (0, Σ) with zero mean and covariance matrix Σ in the RGB color space <ref type="bibr" target="#b38">[38]</ref>. As such, a single CNN model is expected to inherit the flexibility of handling noise model with different parameters, even spatially variant noises by noting M can be non-uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Denoising on Sub-images</head><p>Efficiency is another crucial issue for practical CNN-based denoising. One straightforward idea is to reduce the depth and number of filters. However, such a strategy will sacrifice much the modeling capacity and receptive field of CNN <ref type="bibr" target="#b19">[20]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, dilated convolution is introduced to expand receptive field without the increase of network depth, resulting in a 7layer denoising CNN. Unfortunately, we empirically find that FFDNet with dilated convolution tends to generate artifacts around sharp edges.</p><p>Shi et al. <ref type="bibr" target="#b39">[39]</ref> proposed to extract deep features directly from the low-resolution image for super-resolution, and introduced a sub-pixel convolution layer to improve computational efficiency. In the application of image denoising, we introduce a reversible downsampling layer to reshape the input image into a set of small sub-images. Here the downsampling factor is set to 2 since it can largely improve the speed without reducing modeling capacity. The CNN is deployed on the sub-images, and finally a sub-pixel convolution layer is adopted to reverse the downsampling process.</p><p>Denoising on downsampled sub-images can also effectively expand the receptive field which in turn leads to a moderate network depth. For example, the proposed network with a depth of 15 and 3 × 3 convolution will have a large receptive field of 62 × 62. In contrast, a plain 15-layer CNN only has a receptive field size of 31×31. We note that the receptive field of most state-of-the-art denoising methods ranges from 35 × 35 to 61 × 61 <ref type="bibr" target="#b19">[20]</ref>. Further increase of receptive field actually benefits little in improving denoising performance <ref type="bibr" target="#b40">[40]</ref>. What is more, the introduction of subsampling and sub-pixel convolution is effective in reducing the memory burden.</p><p>Experiments are conducted to validate the effectiveness of downsampling for balancing denoising accuracy and efficiency on the BSD68 dataset with σ = 15 and 50. For grayscale image denoising, we train a baseline CNN which has the same depth as FFDNet without downsampling. The comparison of average PSNR values is given as follows: (i) when σ is small (i.e., 15), the baseline CNN slightly outperforms FFDNet by 0.02dB; (ii) when σ is large (i.e., 50), FFDNet performs better than the baseline CNN by 0.09dB. However, FFDNet is nearly 3 times faster and is more memory-friendly than the baseline CNN. As a result, by performing denoising on sub-images, FFDNet significantly improves efficiency while maintaining denoising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Examining the Role of Noise Level Map</head><p>By training the model with abundant data units (y, M; x), where M is exactly the noise level map of y, the model is expected to perform well when the noise level matches the ground-truth one (see <ref type="figure" target="#fig_2">Fig. 2(a)</ref>). On the other hand, in practice, we may need to use the learned model to smooth out some details with a higher noise level map than the ground-truth one (see <ref type="figure" target="#fig_2">Fig. 2(b)</ref>). In other words, one may take advantage of the role of λ to control the trade-off between noise reduction and detail preservation. Hence, it is very necessary to further examine whether M can play the role of λ.</p><p>Unfortunately, the use of both M and y as input also increases the difficulty to train the model. According to our experiments on several learned models, the model may give rise to visual artifacts especially when the input noise level is much higher than the ground-truth one (see <ref type="figure" target="#fig_2">Fig. 2</ref>(c)), which indicates M fails to control the trade-off between noise reduction and detail preservation. Note that it does not mean all the models suffer from such problem. One possible solution to avoid this is to regularize the convolution filters. As a widely-used regularization method, orthogonal regularization has proven to be effective in eliminating the correlation between convolution filters, facilitating gradient propagation and improving the compactness of the learned model. In addition, recent studies have demonstrated the advantage of orthogonal regularization in enhancing the network generalization ability in applications of deep hashing and image classification <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>. According to our experiments, we empirically find that the orthogonal initialization of the convolution filters <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b46">[46]</ref> works well in suppressing the above mentioned visual artifacts.</p><p>It is worth emphasising that this section aims to highlight the necessity of guaranteeing the role of M in controlling the trade-off between noise reduction and detail preservation rather than proposing a method to avoid the possible visual artifacts caused by noise level mismatch. In practice, one may retrain the model until M plays its role and results in no visual artifacts with a lager noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. FFDNet vs. a Single Blind Model</head><p>So far, we have known that it is possible to learn a single model for blind and non-blind Gaussian denoising, respectively. And it is of significant importance to clarify their differences.</p><p>First, the generalization ability is different. Although the blind model performs favorably for synthetic AWGN removal without knowing the noise level, it does not generalize well to real noisy images whose noises are much more complex than AWGN (see the results of DnCNN-B in <ref type="figure" target="#fig_8">Fig. 8</ref>). Actually, since the CNN model can be treated as the inference of Eqn. <ref type="bibr" target="#b0">(1)</ref> and the data fidelity term corresponds to the degradation process (or the noise model), the modeling accuracy of the degradation process is very important for the success of a denoising model. For example, a model trained for AWGN removal is not expected to be still effective for Poisson noise removal. By contrast, the non-blind FFDNet model can be viewed as multiple denoisers, each of which is anchored with a noise level. Accordingly, it has the ability to control the trade-off between noise removal and detail preservation which in turn facilitates the removal of real noise to some extent (see the results of DnCNN and FFDNet in <ref type="figure" target="#fig_8">Fig. 8)</ref>.</p><p>Second, the performance for AWGN removal is different. The non-blind model with noise level map has moderately better performance for AWGN removal than the blind one (about 0.1dB gain on average for the BSD68 dataset), possibly because the noise level map provides additional information to the input. Similar phenomenon has also been recognized in the task of single image super-resolution (SISR) <ref type="bibr" target="#b47">[47]</ref>.</p><p>Third, the application range is different. In the variable splitting algorithms for general image restoration tasks, the prior term involves a denoising subproblem with a current noise level <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Thus, the non-blind model can be easily plugged into variable splitting algorithms to solve various image restoration tasks, such as image deblurring, SISR, and image inpainting <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b48">[48]</ref>. However, the blind model does not have this merit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Residual vs. Non-residual Learning of Plain CNN</head><p>It has been pointed out that the integration of residual learning for plain CNN and batch normalization is beneficial to the removal of AWGN as it eases the training and tends to deliver better performance <ref type="bibr" target="#b19">[20]</ref>. The main reason is that the residual (noise) output follows a Gaussian distribution which facilitates the Gaussian normalization step of batch normalization. The denoising network gains most from such a task-specific merit especially when a single noise level is considered.</p><p>In FFDNet, we instead consider a wide range of noise level and introduce a noise level map as input. Thus, it is interesting to revisit the integration of residual learning and batch normalization for plain CNN. According to our experiments, batch normalization can always accelerate the training of denoising network regardless of the residual or non-residual learning strategy of plain CNN. In particular, with batch normalization, while the residual learning enjoys a faster convergence than non-residual learning, their final performances after fine-tuning are almost exactly the same. Some recent works have proposed to train very deep plain networks with nearly the same performance to that with residual learning <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b49">[49]</ref>. In fact, when a network is moderately deep (e.g., less than 20), it is feasible to train a plain network without the residual learning strategy by using advanced CNN training and design techniques such as ReLU <ref type="bibr" target="#b37">[37]</ref>, batch normalization <ref type="bibr" target="#b32">[32]</ref> and Adam <ref type="bibr" target="#b50">[50]</ref>. For simplicity, we do not use residual learning for network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Un-clipping vs. Clipping of Noisy Images for Training</head><p>In the AWGN denoising literature, there exist two widelyused settings, i.e., un-clipping <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and clipping <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>, of synthetic noisy image to evaluate the performance of denoising methods. The main difference between the two settings lies in whether the noisy image is clipped into the range of 0-255 (or more precisely, quantized into 8-bit format) after adding the noise.</p><p>On the one hand, the un-clipping setting which is also the most widely-used setting serves an ideal test bed for evaluating the denoising methods. This is because most denoising methods assume the noise is ideal AWGN, and the clipping of noisy input would make the noise characteristics deviate from being AWGN. Furthermore, in the variable splitting algorithms for solving general image restoration problems, there exists a subproblem which, from a Bayesian perspective, corresponds to a Gaussian denoising problem <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b48">[48]</ref>. This further broadens the use of the un-clipping setting. Thus, unless otherwise specified, FFDNet in this work refers to the model trained on images without clipping or quantization.</p><p>On the other hand, since real noisy images are always integer-valued and range-limited, it has been argued that the clipping setting of noisy image makes the data more realistic <ref type="bibr" target="#b23">[24]</ref>. However, when the noise level is high, the noise will be not zero-mean any more due to clipping effects <ref type="bibr" target="#b51">[51]</ref>. This in turn will lead to unreliable denoiser for plugging into the variable splitting algorithms to solve other image restoration problems.</p><p>To thoroughly evaluate the proposed method, we also train an FFDNet model with clipping setting of noisy image, namely FFDNet-Clip, for comparison. During training and testing of FFDNet-Clip, the noisy images are quantized into 8-bit format. Specifically, for a clean image x, we use the matlab function imnoise(x, 'gaussian', 0, ( σ 255 ) 2 ) to generate the quantized noisy y with noise level σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Generation and Network Training</head><p>To train the FFDNet model, we need to prepare a training dataset of input-output pairs {(y i ,</p><formula xml:id="formula_5">M i ; x i )} N i=1 .</formula><p>Here, y i is obtained by adding AWGN to latent image x i , and M i is the noise level map. The reason to use AWGN to generate the training dataset is two-fold. First, AWGN is a natural choice when there is no specific prior information on noise source. Second, real-world noise can be approximated as locally AWGN <ref type="bibr" target="#b52">[52]</ref>. More specifically, FFDNet model is trained on the noisy images y i = x i + v i without quantization to 8-bit integer values. Though the real noisy images are generally 8bit quantized, we empirically found that the learned model still works effectively on real noisy images. For FFDNet-Clip, as mentioned in Sec. III-G, we use the matlab function imnoise to generate the quantized noisy image from a clean one. We collected a large dataset of source images, including 400 BSD images, 400 images selected from the validation set of ImageNet <ref type="bibr" target="#b53">[53]</ref>, and the 4,744 images from the Waterloo Exploration Database <ref type="bibr" target="#b54">[54]</ref>. In each epoch, we randomly crop N = 128 × 8, 000 patches from these images for training. The patch size should be larger than the receptive field of FFDNet, and we set it to 70×70 and 50×50 for grayscale images and color images, respectively. The noisy patches are obtained by adding AWGN of noise level σ ∈ [0, 75] to the clean patches. For each noisy patch, the noise level map is uniform. Since FFDNet is a fully convolutional neural network, it inherits the local connectivity property that the output pixel is determined by the local noisy input and local noise level. Hence, the trained FFDNet naturally has the ability to handle spatially variant noise by specifying a non-uniform noise level map. For clarity, in <ref type="table" target="#tab_0">Table I</ref> we list the main specifications of the FFDNet models.</p><p>The ADAM algorithm <ref type="bibr" target="#b50">[50]</ref> is adopted to optimize FFDNet by minimizing the following loss function,</p><formula xml:id="formula_6">L(Θ) = 1 2N N i=1 F(y i , M i ; Θ) − x i 2 .<label>(5)</label></formula><p>The learning rate starts from 10 −3 and reduces to 10 −4 when the training error stops decreasing. When the training error keeps unchanged in five sequential epochs, we merge the parameters of each batch normalization into the adjacent convolution filters. Then, a smaller learning rate of 10 −6 is adopted for additional 50 epochs to fine-tune the FFDNet model. As for the other hyper-parameters of ADAM, we use their default settings. The mini-batch size is set as 128, and the rotation and flip based data augmentation is also adopted during training. The FFDNet models are trained in Matlab (R2015b) environment with MatConvNet package <ref type="bibr" target="#b55">[55]</ref> and an Nvidia Titan X Pascal GPU. The training of a single model can be done in about two days. To evaluate the proposed FFDNet denoisers on grayscale image denoising, we use BSD68 <ref type="bibr" target="#b2">[3]</ref> and Set12 datasets to test FFDNet for removing AWGN noise, and use the "RNI6" dataset <ref type="bibr" target="#b56">[56]</ref> to test FFDNet for removing real noise. The BSD68 dataset consists of 68 images from the separate test set of the BSD300 dataset <ref type="bibr" target="#b57">[57]</ref>. The Set12 dataset is a collection of widely-used testing images. The RNI6 dataset contains 6 real noisy images without ground-truth. In particular, to evaluate FFDNet-Clip, we use the quantized "Clip300" dataset which comprises the 100 images of test set from the BSD300 dataset <ref type="bibr" target="#b57">[57]</ref> and 200 images from PASCALVOC 2012 <ref type="bibr" target="#b58">[58]</ref> dataset. Note that all the testing images are not included in the training dataset.</p><p>As for color image denoising, we employ four datasets,  <ref type="bibr" target="#b60">[60]</ref>. The RNI15 dataset consists of 15 real noisy images. We note that RNI6 and RNI15 cover a variety of real noise types, such as camera noise and JPEG compression noise. Since the ground-truth clean images are unavailable for real noisy images, we thus only provide the visual comparisons on these images. The source codes of FFDNet and its extension to multivariate Gaussian noise are available at https://github.com/cszn/FFDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on AWGN Removal</head><p>In this subsection, we test FFDNet on noisy images corrupted by spatially invariant AWGN. For grayscale image denoising, we mainly compare FFDNet with state-of-the-art methods BM3D <ref type="bibr" target="#b10">[11]</ref>, WNNM <ref type="bibr" target="#b4">[5]</ref>, MLP <ref type="bibr" target="#b17">[18]</ref>, TNRD <ref type="bibr" target="#b16">[17]</ref>, and DnCNN <ref type="bibr" target="#b19">[20]</ref>. Note that BM3D and WNNM are two representative model-based methods based on nonlocal self-similarity prior, whereas TNRD, MLP and DnCNN are discriminative learning based methods. Tables II and III report the PSNR results on BSD68 and Set12 datasets, respectively. We also use two CNN-based denoising methods, i.e., RED30 <ref type="bibr" target="#b34">[34]</ref> and MemNet <ref type="bibr" target="#b36">[36]</ref>, for further comparison. Their PSNR results on BSD68 dataset with noise level 50 are 26.34dB and 26.35dB, respectively. Note that RED30 and MemNet are trained on a specific noise level and are less efficient than DnCNN. From Tables II and III, one can have the following observations.</p><p>First, FFDNet surpasses BM3D by a large margin and outperforms WNNM, MLP and TNRD by about 0.2dB for a wide range of noise levels on BSD68. Second, FFDNet is slightly inferior to DnCNN when the noise level is low (e.g., σ ≤ 25), but gradually outperforms DnCNN with the increase of noise level (e.g., σ &gt; 25). This phenomenon may be resulted from the trade-off between receptive field size and modeling capacity. FFDNet has a larger receptive field than DnCNN, thus favoring for removing strong noise, while DnCNN has better modeling capacity which is beneficial for denoising images with lower noise level. Third, FFDNet outperforms WNNM on images such as "House", while it is inferior to WNNM on image "Barbara". This is because "Barbara" has a rich amount of repetitive structures, which can be effectively exploited by nonlocal self-similarity based WNNM method. The visual comparisons of different methods are given in <ref type="figure">Fig. 3</ref>. Overall, FFDNet produces the best perceptual quality of denoised images.</p><p>To evaluate FFDNet-Clip, <ref type="table" target="#tab_0">Table IV</ref> shows the PSNR comparison with DCGRF <ref type="bibr" target="#b28">[28]</ref> and RBDN <ref type="bibr" target="#b35">[35]</ref> on the Clip300 dataset. It can be seen that FFDNet-Clip with matched noise level achieves better performance than DCGRF and RBDN, showing that FFDNet performs well under the clipping setting. We also tested FFDNet-Clip on BSD68 dataset with clipping setting, it has been found that the PSNR result is similar to that of FFDNet with un-clipping setting.</p><p>For color image denoising, we compare FFDNet with CBM3D <ref type="bibr" target="#b10">[11]</ref> and CDnCNN <ref type="bibr" target="#b19">[20]</ref>. <ref type="table" target="#tab_4">Table V</ref> reports the performance of different methods on CBSD68, Kodak24, and McMaster datasets, and <ref type="figure" target="#fig_3">Fig. 4</ref> presents the visual comparisons. It can be seen that FFDNet consistently outperforms CBM3D on different noise levels in terms of both quantitative and qualitative evaluation, and has competing performance with CDnCNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Spatially Variant AWGN Removal</head><p>We then test the flexibility of FFDNet to deal with spatially variant AWGN. To synthesize spatially variant AWGN, we first generate an AWGN image v 1 with unit standard deviation and a noise level map M of the same size. Then, element-wise multiplication is applied on v 1 and M to produce the spatially variant AWGN, i.e., v = v 1 M. In the denoising stage, we take the bilinearly downsampled noise level map as the input to FFDNet. Since the noise level map is spatially smooth, the use of downsampled noise level map generally has very little effect on the final denoising performance. <ref type="figure" target="#fig_4">Fig. 5</ref> gives an example to show the effectiveness of FFDNet on removing spatially variant AWGN. We do not compare FFDNet with other methods because no state-of-theart AWGN denoising method can be readily extended to handle spatially variant AWGN. From <ref type="figure" target="#fig_4">Fig. 5</ref>, one can see that FFDNet with non-uniform noise level map is flexible and powerful to remove spatially variant AWGN. In contrast, FFDNet with uniform noise level map would fail to remove strong noise at the region with higher noise level while smoothing out the details at the region with lower noise level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Noise Level Sensitivity</head><p>In practical applications, the noise level map may not be accurately estimated from the noisy observation, and mismatch between the input and real noise levels is inevitable. If the input noise level is lower than the real noise level, the noise cannot be completely removed. Therefore, users often prefer to set a higher noise level to remove more noise. However, this may also remove some image details together with noise. A practical denoiser should tolerate certain mismatch of noise levels. In this subsection, we evaluate FFDNet in comparison with benchmark BM3D and DnCNN by varying different input noise levels for a given ground-truth noise level. <ref type="figure">Fig. 6</ref> illustrates the noise level sensitivity curves of BM3D, DnCNN and FFDNet. Different methods with different input noise levels (e.g., "FFDNet-15" represents FFDNet with input noise level fixed as 15) are evaluated on BSD68 images with noise level ranging from 0 to 50. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the visual comparisons between BM3D/CBM3D and FFDNet by setting  • On all noise levels, FFDNet achieves similar denoising results to BM3D and DnCNN when their input noise levels are the same. • With the fixed input noise level, for all the three methods, the PSNR value tends to stay the same when the groundtruth noise level is lower, and begins to decrease when the ground-truth noise level is higher. • The best visual quality is obtained when the input noise level matches the ground-truth one. BM3D and FFDNet produce similar visual results with lower input noise levels, while they exhibit certain difference with higher input noise levels. Both of them will smooth out noise in flat regions, and gradually smooth out image structures with the increase of input noise levels. Particularly, FFDNet may wipe out some low contrast line structure, whereas BM3D can still preserve the mean patch regardless of the input noise levels due to its use of nonlocal information. • Using a higher input noise level can generally produce better visual results than using a lower one. In addition, there is no much visual difference when the input noise level is a little higher than the ground-truth one. According to above observations, FFDNet exhibits similar noise level sensitivity performance to BM3D and DnCNN in balancing noise reduction and detail preservation. When the ground-truth noise level is unknown, it is more preferable to set a larger input noise level than a lower one to remove noise with better perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on Real Noisy Images</head><p>In this subsection, real noisy images are used to further assess the practicability of FFDNet. However, such an evaluation is difficult to conduct due to the following reasons. (i) Both the ground-truth clean image and noise level are unknown for real noisy image. (ii) The real noise comes from various sources such as camera imaging pipeline (e.g., shot noise, amplifier noise and quantization noise), scanning, lossy compression and image resizing <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, and it is generally non-Gaussian, spatially variant, and signal-dependent. As a result, the AWGN assumption in many denoising algorithms does not hold, and the associated noise level estimation methods do not work well for real noisy images.</p><p>Instead of adopting any noise level estimation methods, we adopt an interactive strategy to handle real noisy images. First of all, we empirically found that the assumption of spatially invariant noise usually works well for most real noisy images. We then employ a set of typical input noise levels to produce multiple outputs, and select the one which has best trade-off between noise reduction and detail preservation. Second, the spatially variant noise in most real-world images is signaldependent. In this case, we first sample several typical regions of distinct colors. For each typical region, we apply different noise levels with an interval of 5, and choose the best noise level by observing the denoising results. The noise levels at other regions are then interpolated from the noise levels of the typical regions to constitute an approximated non-uniform noise level map. Our FFDNet focuses on non-blind denoising and assumes the noise level map is known. In practice, some advanced noise level estimation methods <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b63">[64]</ref> can be adopted to assist the estimation of noise level map. In our following experiments, unless otherwise specified, we assume spatially invariant noise for the real noisy images.</p><p>Since there is no ground-truth image for a real noisy image, visual comparison is employed to evaluate the performance of FFDNet. We choose BM3D for comparison because it is widely accepted as a benchmark for denoising applications. Given a noisy image, the same input noise level is used for BM3D and FFDNet. Another CNN-based denoising method DnCNN and a blind denoising method Noise Clinic <ref type="bibr" target="#b56">[56]</ref> are also used for comparison. Note that, apart from the non-blind DnCNN models for specific noise levels, the blind DnCNN model (i.e., DnCNN-B) trained on noise level range of [0, 55] is also used for grayscale image denoising. For color image denoising, the blind CDnCNN-B is used for comparison. <ref type="figure" target="#fig_8">Fig. 8</ref> compares the grayscale image denoising results of Noise Clinic, BM3D, DnCNN, DnCNN-B and FFDNet on RNI6 images. As one can see, Noise Clinic reduces much the noise, but it also generates many algorithm-induced artifacts. BM3D, DnCNN and FFDNet produce more visually pleasant results. While the non-blind DnCNN models perform favorably, the blind DnCNN-B model performs poorly in removing the non-AWGN real noise. This phenomenon clearly demonstrates the better generalization ability of non-blind model over blind one for controlling the trade-off between noise removal and detail preservation. It is worth noting that, for image "Building" which contains structured noise, Noise Clinic and BM3D fail to remove those structured noises since the structured noises fit the nonlocal self-similarity prior adopted in Noise Clinic and BM3D. In contrast, FFDNet and DnCNN successfully remove such noise without losing underlying image textures. <ref type="figure" target="#fig_9">Fig. 9</ref> shows the denoising results of Noise Clinic, CBM3D, CDnCNN-B and FFDNet on five color noisy images from RNI15. It can be seen that CDnCNN-B yields very pleasing results for noisy image with AWGN-like noise such as image "Frog", and is still unable to handle non-AWGN noise.</p><p>Notably, from the denoising results of "Boy", one can see that CBM3D remains the structured color noise unremoved whereas FFDNet removes successfully such kind of noise. We can conclude that while the nonlocal self-similarity prior helps to remove random noise, it hinders the removal of structured noise. In comparison, the prior implicitly learned by CNN is able to remove both random noise and structured noise.  <ref type="figure" target="#fig_1">Fig. 11</ref> shows a more challenging example to demonstrate the advantage of FFDNet for denoising noisy images with spatially variant noise. We select five typical regions to estimate the noise levels, including two background regions, the coffee region, the milk-foam region, and the specular reflection region. In our experiment, we manually and interactively set σ = 10 for the milk-foam and specular reflection regions, σ = 35 for the background region with high noise (i.e., green region), and σ = 25 for the other regions. We then interpolate the non-uniform noise level map for the whole image based on the estimated five noise levels. As one can see, while FFDNet with a small uniform input noise level can recover the details of regions with low noise level, it fails to remove strong noise. On the other hand, FFDNet with a large uniform input noise level can remove strong noise but it will also smooth out the details in the region with low noise level. In contrast, the denoising result with a proper non-uniform noise level map not only preserves image details but also removes the strong noise.</p><p>Finally, according to the above experiments on real noisy images, we can see that the FFDNet model trained with unquantized image data performs well on 8-bit quantized real noisy images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Running Time</head><p>Table VI lists the running time results of BM3D, DnCNN and FFDNet for denoising grayscale and color images with size 256×256, 512×512 and 1,024×1,024. The evaluation was performed in Matlab (R2015b) environment on a computer with a six-core Intel(R) Core(TM) i7-5820K CPU @ 3.3GHz, 32 GB of RAM and an Nvidia Titan X Pascal GPU. For BM3D, we evaluate its running time by denoising images with noise level 25. For DnCNN, the grayscale and color image denoising models have 17 and 20 convolution layers, respectively. The Nvidia cuDNN-v5.1 deep learning library is used to accelerate the computation of DnCNN and FFDNet. The memory transfer time between CPU and GPU is also counted. Note that DnCNN and FFDNet can be implemented with both single-threaded (ST) and multi-threaded (MT) CPU computations.</p><p>From <ref type="table" target="#tab_0">Table VI</ref>, we have the following observations. First, BM3D spends much more time on denoising color images than grayscale images. The reason is that, compared to gray-BM3D, CBM3D needs extra time to denoise the chrominance components after luminance-chrominance color transformation. Second, while DnCNN can benefit from GPU computation for fast implementation, it has comparable CPU time to BM3D. Third, FFDNet spends almost the same time for processing grayscale and color images. More specifically, FFDNet with multi-threaded implementation is about three times faster than DnCNN and BM3D on CPU, and much faster than DnCNN on GPU. Even with single-threaded implementation, FFDNet is also faster than BM3D. Taking denoising performance and flexibility into consideration, FFDNet is very competitive for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a new CNN model, namely FFD-Net, for fast, effective and flexible discriminative denoising. To achieve this goal, several techniques were utilized in network design and training, such as the use of noise level map as input and denoising in downsampled sub-images space. The results on synthetic images with AWGN demonstrated that FFDNet can not only produce state-of-the-art results when input noise level matches ground-truth noise level, but also have the ability to robustly control the trade-off between noise reduction and detail preservation. The results on images with spatially variant AWGN validated the flexibility of FFDNet for handing inhomogeneous noise. The results on real noisy images further demonstrated that FFDNet can deliver perceptually appealing denoising results. Finally, the running time comparisons showed the faster speed of FFDNet over other competing methods such as BM3D. Considering its flexibility, efficiency and effectiveness, FFDNet provides a practical solution to CNN denoising applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This project is partially supported by the National Natural Scientific Foundation of China (NSFC) under Grant No. 61671182 and 61471146, and the HK RGC GRF grant (under no. PolyU 152124/15E). K. Zhang is with the School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China, and also with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail: cskaizhang@gmail.com). W. Zuo is with the School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China (e-mail: cswmzuo@gmail.com). L. Zhang is with the Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail: cslzhang@comp.polyu.edu.hk).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of the proposed FFDNet for image denoising. The input image is reshaped to four sub-images, which are then input to the CNN together with a noise level map. The final output is reconstructed by the four denoised sub-images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An example to show the importance of guaranteeing the role of noise level map in controlling the trade-off between noise reduction and detail preservation. The input is a noisy image with noise level 25. (a) Result without visual artifacts by matched noise level 25. (b) Result without visual artifacts by mismatched noise level 60. (c) Result with visual artifacts by mismatched noise level 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) CBM3D (25.49dB) (b) CDnCNN (26.19dB) (c) FFDNet (26.28dB) Color image denoising results by CBM3D, CDnCNN and FFDNet on noise level σ = 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of FFDNet on removing spatially variant AWGN. (a) Noisy image (20.55dB) with spatially variant AWGN. (b) Ground-truth noise level map and corresponding denoised image (30.08dB) by FFDNet; (c) Uniform noise level map constructed by using the mean value of ground-truth noise level map and corresponding denoised image (27.45dB) by FFDNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparisons between FFDNet and BM3D/CBM3D by setting different input noise levels to denoise a noisy image. (a) From top to bottom: ground-truth image, four clean zoom-in regions, and the corresponding noisy regions (AWGN, noise level 15). (b) From top to bottom: denoising results by BM3D with input noise levels 5, 10, 15, 20, 50, and 75, respectively. (c) Results by FFDNet with the same settings as in (b). (d) From top to bottom: ground-truth image, four clean zoom-in regions, and the corresponding noisy regions (AWGN, noise level 25). (e) From top to bottom: denoising results by CBM3D with input noise levels 10, 20, 25, 30, 45 and 60, respectively. (f) Results by FFDNet with the same settings as in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 further</head><label>10</label><figDesc>shows more visual results of FFDNet on the other nine images from RNI15. It can be seen that FFDNet can handle various kinds of noises, such as JPEG lossy compression noise (see image "Audrey Hepburn"), and video noise (see image "Movie").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>(a) David Hilbert (b) Old Tom Morris (c) Chupa Chups (d) Vinegar (e) Building (f) Marilyn Grayscale image denoising results by different methods on real noisy images. From top to bottom: noisy images, denoised images by Noise Clinic, denoised images by BM3D, denoised images by DnCNN, denoised images by DnCNN-B, denoised images by FFDNet. (a) σ = 14 (15 for DnCNN); (b) σ = 15; (c) σ = 10; (d) σ = 20; (e) σ = 20; (f) σ = 7 (10 for DnCNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Color image denoising results by different methods on real noisy images. From top to bottom: noisy images, denoised images by Noise Clinic, denoised images by CBM3D, denoised images by CDnCNN-B, denoised images by FFDNet. (a) σ = 28; (b) σ = 15; (c) σ = 12; (d) σ = 40; (e) σ = 45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>More denoising results of FFDNet on real image denoising. (a) σ = 70; (b) σ = 15; (c) σ = 10; (d) σ = 15; (e) σ = 18; (f) σ = 15; (g) σ = 30; (h) σ = 12; (i) σ = 25. An example of FFDNet on image "Glass" with spatially variant noise. (a) Noisy image; (b) Denoised image by Noise Clinic; (c) Denoised image by FFDNet with σ = 10; (d) Denoised image by FFDNet with σ = 25; (e) Denoised image by FFDNet with σ = 35; (f) Denoised image by FFDNet with non-uniform noise level map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MAIN</head><label>I</label><figDesc>SPECIFICATIONS OF THE PROPOSED FFDNET</figDesc><table><row><cell>FFDNet</cell><cell>Number</cell><cell>Number of</cell><cell>Noise level</cell><cell>Training</cell></row><row><cell>models</cell><cell>of layers</cell><cell>channels</cell><cell>range</cell><cell>patch size</cell></row><row><cell>Grayscale</cell><cell>15</cell><cell>64</cell><cell>[0, 75]</cell><cell>70×70</cell></row><row><cell>Color</cell><cell>12</cell><cell>96</cell><cell>[0, 75]</cell><cell>50×50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>PSNR(DB) RESULTS OF DIFFERENT METHODS ON SET12 DATASET WITH NOISE LEVELS 15, 25 35, 50 AND 75. THE BEST TWO RESULTS ARE HIGHLIGHTED IN RED AND BLUE COLORS, RESPECTIVELY Images</figDesc><table><row><cell></cell><cell>C.man</cell><cell>House</cell><cell>Peppers</cell><cell>Starfish</cell><cell cols="2">Monarch Airplane</cell><cell>Parrot</cell><cell>Lena</cell><cell>Barbara</cell><cell>Boat</cell><cell>Man</cell><cell>Couple</cell><cell>Average</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D</cell><cell>31.91</cell><cell>34.93</cell><cell>32.69</cell><cell>31.14</cell><cell>31.85</cell><cell>31.07</cell><cell>31.37</cell><cell>34.26</cell><cell>33.10</cell><cell>32.13</cell><cell>31.92</cell><cell>32.10</cell><cell>32.37</cell></row><row><cell>WNNM</cell><cell>32.17</cell><cell>35.13</cell><cell>32.99</cell><cell>31.82</cell><cell>32.71</cell><cell>31.39</cell><cell>31.62</cell><cell>34.27</cell><cell>33.60</cell><cell>32.27</cell><cell>32.11</cell><cell>32.17</cell><cell>32.70</cell></row><row><cell>TNRD</cell><cell>32.19</cell><cell>34.53</cell><cell>33.04</cell><cell>31.75</cell><cell>32.56</cell><cell>31.46</cell><cell>31.63</cell><cell>34.24</cell><cell>32.13</cell><cell>32.14</cell><cell>32.23</cell><cell>32.11</cell><cell>32.50</cell></row><row><cell>DnCNN</cell><cell>32.61</cell><cell>34.97</cell><cell>33.30</cell><cell>32.20</cell><cell>33.09</cell><cell>31.70</cell><cell>31.83</cell><cell>34.62</cell><cell>32.64</cell><cell>32.42</cell><cell>32.46</cell><cell>32.47</cell><cell>32.86</cell></row><row><cell>FFDNet</cell><cell>32.42</cell><cell>35.01</cell><cell>33.10</cell><cell>32.02</cell><cell>32.77</cell><cell>31.58</cell><cell>31.77</cell><cell>34.63</cell><cell>32.50</cell><cell>32.35</cell><cell>32.40</cell><cell>32.45</cell><cell>32.75</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D</cell><cell>29.45</cell><cell>32.85</cell><cell>30.16</cell><cell>28.56</cell><cell>29.25</cell><cell>28.42</cell><cell>28.93</cell><cell>32.07</cell><cell>30.71</cell><cell>29.90</cell><cell>29.61</cell><cell>29.71</cell><cell>29.97</cell></row><row><cell>WNNM</cell><cell>29.64</cell><cell>33.22</cell><cell>30.42</cell><cell>29.03</cell><cell>29.84</cell><cell>28.69</cell><cell>29.15</cell><cell>32.24</cell><cell>31.24</cell><cell>30.03</cell><cell>29.76</cell><cell>29.82</cell><cell>30.26</cell></row><row><cell>MLP</cell><cell>29.61</cell><cell>32.56</cell><cell>30.30</cell><cell>28.82</cell><cell>29.61</cell><cell>28.82</cell><cell>29.25</cell><cell>32.25</cell><cell>29.54</cell><cell>29.97</cell><cell>29.88</cell><cell>29.73</cell><cell>30.03</cell></row><row><cell>TNRD</cell><cell>29.72</cell><cell>32.53</cell><cell>30.57</cell><cell>29.02</cell><cell>29.85</cell><cell>28.88</cell><cell>29.18</cell><cell>32.00</cell><cell>29.41</cell><cell>29.91</cell><cell>29.87</cell><cell>29.71</cell><cell>30.06</cell></row><row><cell>DnCNN</cell><cell>30.18</cell><cell>33.06</cell><cell>30.87</cell><cell>29.41</cell><cell>30.28</cell><cell>29.13</cell><cell>29.43</cell><cell>32.44</cell><cell>30.00</cell><cell>30.21</cell><cell>30.10</cell><cell>30.12</cell><cell>30.43</cell></row><row><cell>FFDNet</cell><cell>30.06</cell><cell>33.27</cell><cell>30.79</cell><cell>29.33</cell><cell>30.14</cell><cell>29.05</cell><cell>29.43</cell><cell>32.59</cell><cell>29.98</cell><cell>30.23</cell><cell>30.10</cell><cell>30.18</cell><cell>30.43</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D</cell><cell>27.92</cell><cell>31.36</cell><cell>28.51</cell><cell>26.86</cell><cell>27.58</cell><cell>26.83</cell><cell>27.40</cell><cell>30.56</cell><cell>28.98</cell><cell>28.43</cell><cell>28.22</cell><cell>28.15</cell><cell>28.40</cell></row><row><cell>WNNM</cell><cell>28.08</cell><cell>31.92</cell><cell>28.75</cell><cell>27.27</cell><cell>28.13</cell><cell>27.10</cell><cell>27.69</cell><cell>30.73</cell><cell>29.48</cell><cell>28.54</cell><cell>28.33</cell><cell>28.24</cell><cell>28.69</cell></row><row><cell>MLP</cell><cell>28.08</cell><cell>31.18</cell><cell>28.54</cell><cell>27.12</cell><cell>27.97</cell><cell>27.22</cell><cell>27.72</cell><cell>30.82</cell><cell>27.62</cell><cell>28.53</cell><cell>28.47</cell><cell>28.24</cell><cell>28.46</cell></row><row><cell>DnCNN</cell><cell>28.61</cell><cell>31.61</cell><cell>29.14</cell><cell>27.53</cell><cell>28.51</cell><cell>27.52</cell><cell>27.94</cell><cell>30.91</cell><cell>28.09</cell><cell>28.72</cell><cell>28.66</cell><cell>28.52</cell><cell>28.82</cell></row><row><cell>FFDNet</cell><cell>28.54</cell><cell>31.99</cell><cell>29.18</cell><cell>27.58</cell><cell>28.54</cell><cell>27.47</cell><cell>28.02</cell><cell>31.20</cell><cell>28.29</cell><cell>28.82</cell><cell>28.70</cell><cell>28.68</cell><cell>28.92</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D</cell><cell>26.13</cell><cell>29.69</cell><cell>26.68</cell><cell>25.04</cell><cell>25.82</cell><cell>25.10</cell><cell>25.90</cell><cell>29.05</cell><cell>27.22</cell><cell>26.78</cell><cell>26.81</cell><cell>26.46</cell><cell>26.72</cell></row><row><cell>WNNM</cell><cell>26.45</cell><cell>30.33</cell><cell>26.95</cell><cell>25.44</cell><cell>26.32</cell><cell>25.42</cell><cell>26.14</cell><cell>29.25</cell><cell>27.79</cell><cell>26.97</cell><cell>26.94</cell><cell>26.64</cell><cell>27.05</cell></row><row><cell>MLP</cell><cell>26.37</cell><cell>29.64</cell><cell>26.68</cell><cell>25.43</cell><cell>26.26</cell><cell>25.56</cell><cell>26.12</cell><cell>29.32</cell><cell>25.24</cell><cell>27.03</cell><cell>27.06</cell><cell>26.67</cell><cell>26.78</cell></row><row><cell>TNRD</cell><cell>26.62</cell><cell>29.48</cell><cell>27.10</cell><cell>25.42</cell><cell>26.31</cell><cell>25.59</cell><cell>26.16</cell><cell>28.93</cell><cell>25.70</cell><cell>26.94</cell><cell>26.98</cell><cell>26.50</cell><cell>26.81</cell></row><row><cell>DnCNN</cell><cell>27.03</cell><cell>30.00</cell><cell>27.32</cell><cell>25.70</cell><cell>26.78</cell><cell>25.87</cell><cell>26.48</cell><cell>29.39</cell><cell>26.22</cell><cell>27.20</cell><cell>27.24</cell><cell>26.90</cell><cell>27.18</cell></row><row><cell>FFDNet</cell><cell>27.03</cell><cell>30.43</cell><cell>27.43</cell><cell>25.77</cell><cell>26.88</cell><cell>25.90</cell><cell>26.58</cell><cell>29.68</cell><cell>26.48</cell><cell>27.32</cell><cell>27.30</cell><cell>27.07</cell><cell>27.32</cell></row><row><cell>Noise Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>σ = 75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM3D</cell><cell>24.32</cell><cell>27.51</cell><cell>24.73</cell><cell>23.27</cell><cell>23.91</cell><cell>23.48</cell><cell>24.18</cell><cell>27.25</cell><cell>25.12</cell><cell>25.12</cell><cell>25.32</cell><cell>24.70</cell><cell>24.91</cell></row><row><cell>WNNM</cell><cell>24.60</cell><cell>28.24</cell><cell>24.96</cell><cell>23.49</cell><cell>24.31</cell><cell>23.74</cell><cell>24.43</cell><cell>27.54</cell><cell>25.81</cell><cell>25.29</cell><cell>25.42</cell><cell>24.86</cell><cell>25.23</cell></row><row><cell>MLP</cell><cell>24.63</cell><cell>27.78</cell><cell>24.88</cell><cell>23.57</cell><cell>24.40</cell><cell>23.87</cell><cell>24.55</cell><cell>27.68</cell><cell>23.39</cell><cell>25.44</cell><cell>25.59</cell><cell>25.02</cell><cell>25.07</cell></row><row><cell>DnCNN</cell><cell>25.07</cell><cell>27.85</cell><cell>25.17</cell><cell>23.64</cell><cell>24.71</cell><cell>24.03</cell><cell>24.71</cell><cell>27.54</cell><cell>23.63</cell><cell>25.47</cell><cell>25.64</cell><cell>24.97</cell><cell>25.20</cell></row><row><cell>FFDNet</cell><cell>25.29</cell><cell>28.43</cell><cell>25.39</cell><cell>23.82</cell><cell>24.99</cell><cell>24.18</cell><cell>24.94</cell><cell>27.97</cell><cell>24.24</cell><cell>25.64</cell><cell>25.75</cell><cell>25.29</cell><cell>25.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>AVERAGE PSNR(DB) RESULTS OF DIFFERENT METHODS ON BSD68 WITH NOISE LEVELS 15, 25 35, 50 AND 75</figDesc><table><row><cell>Methods</cell><cell cols="3">BM3D WNNM MLP</cell><cell>TNRD</cell><cell cols="2">DnCNN FFDNet</cell></row><row><cell>σ = 15</cell><cell>31.07</cell><cell>31.37</cell><cell>-</cell><cell>31.42</cell><cell>31.72</cell><cell>31.63</cell></row><row><cell>σ = 25</cell><cell>28.57</cell><cell>28.83</cell><cell>28.96</cell><cell>28.92</cell><cell>29.23</cell><cell>29.19</cell></row><row><cell>σ = 35</cell><cell>27.08</cell><cell>27.30</cell><cell>27.50</cell><cell>-</cell><cell>27.69</cell><cell>27.73</cell></row><row><cell>σ = 50</cell><cell>25.62</cell><cell>25.87</cell><cell>26.03</cell><cell>25.97</cell><cell>26.23</cell><cell>26.29</cell></row><row><cell>σ = 75</cell><cell>24.21</cell><cell>24.40</cell><cell>24.59</cell><cell>-</cell><cell>24.64</cell><cell>24.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell></row><row><cell cols="7">THE AVERAGE PSNR(DB) RESULTS OF DIFFERENT METHODS ON CLIP300</cell></row><row><cell></cell><cell cols="5">WITH NOISE LEVELS 15, 25 35, 50 AND 60</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">σ = 15</cell><cell>σ = 25</cell><cell>σ = 35</cell><cell>σ = 50</cell><cell>σ = 60</cell></row><row><cell>DCGRF</cell><cell></cell><cell>31.35</cell><cell>28.67</cell><cell>27.08</cell><cell>25.38</cell><cell>24.45</cell></row><row><cell>RBDN</cell><cell></cell><cell>31.05</cell><cell>28.77</cell><cell>27.31</cell><cell>25.80</cell><cell>23.25</cell></row><row><cell cols="2">FFDNet-Clip</cell><cell>31.68</cell><cell>29.25</cell><cell>27.75</cell><cell>26.25</cell><cell>25.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V THE</head><label>V</label><figDesc>AVERAGE PSNR(DB) RESULTS OF CBM3D, CDNCNN AND FFDNET ON CBSD68, KODAK24 AND MCMASTER DATASETS WITH NOISE LEVELS 15, 25 35, 50 AND 75</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell>σ=15</cell><cell>σ=25</cell><cell>σ=35</cell><cell>σ=50</cell><cell>σ=75</cell></row><row><cell></cell><cell>CBM3D</cell><cell>33.52</cell><cell>30.71</cell><cell>28.89</cell><cell>27.38</cell><cell>25.74</cell></row><row><cell>CBSD68</cell><cell cols="2">CDnCNN 33.89</cell><cell>31.23</cell><cell>29.58</cell><cell>27.92</cell><cell>24.47</cell></row><row><cell></cell><cell>FFDNet</cell><cell>33.87</cell><cell>31.21</cell><cell>29.58</cell><cell>27.96</cell><cell>26.24</cell></row><row><cell></cell><cell>CBM3D</cell><cell>34.28</cell><cell>31.68</cell><cell>29.90</cell><cell>28.46</cell><cell>26.82</cell></row><row><cell cols="3">Kodak24 CDnCNN 34.48</cell><cell>32.03</cell><cell>30.46</cell><cell>28.85</cell><cell>25.04</cell></row><row><cell></cell><cell>FFDNet</cell><cell>34.63</cell><cell>32.13</cell><cell>30.57</cell><cell>28.98</cell><cell>27.27</cell></row><row><cell></cell><cell>CBM3D</cell><cell>34.06</cell><cell>31.66</cell><cell>29.92</cell><cell>28.51</cell><cell>26.79</cell></row><row><cell cols="3">McMaster CDnCNN 33.44</cell><cell>31.51</cell><cell>30.14</cell><cell>28.61</cell><cell>25.10</cell></row><row><cell></cell><cell>FFDNet</cell><cell>34.66</cell><cell>32.35</cell><cell>30.81</cell><cell>29.18</cell><cell>27.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Fig. 6. Noise level sensitivity curves of BM3D, DnCNN and FFDNet. The averaged PSNR results are evaluated on BSD68. different input noise levels to denoise a noisy image. Four typical image structures, including flat region, sharp edge, line with high contrast, and line with low contrast, are selected for visual comparison to investigate the noise level sensitivity of BM3D and FFDNet. From Figs. 6 and 7, we have the following observations.</figDesc><table><row><cell></cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFDNet-5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FFDNet-15</cell></row><row><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FFDNet-25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FFDNet-50</cell></row><row><cell></cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BM3D-5</cell><cell></cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BM3D-15 BM3D-25</cell><cell></cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BM3D-50 DnCNN-15</cell></row><row><cell>PSNR (dB)</cell><cell>26 28 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DnCNN-25 DnCNN-50</cell></row><row><cell></cell><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Image Noise Level</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI RUNNING</head><label>VI</label><figDesc>TIME (IN SECONDS) OF DIFFERENT METHODS FOR DENOISING IMAGES WITH SIZE 256×256, 512×512 AND 1,024×1,024</figDesc><table><row><cell>Methods</cell><cell>Device</cell><cell cols="5">256×256 Gray Color Gray 512×512 Color Gray 1,024×1,024 Color</cell></row><row><cell>BM3D</cell><cell cols="2">CPU(ST) 0.59</cell><cell>0.98</cell><cell>2.52</cell><cell>3.57</cell><cell>10.77 20.15</cell></row><row><cell></cell><cell cols="2">CPU(ST) 2.14</cell><cell>2.44</cell><cell>8.63</cell><cell>9.85</cell><cell>32.82 38.11</cell></row><row><cell cols="3">DnCNN CPU(MT) 0.74</cell><cell>0.98</cell><cell>3.41</cell><cell>4.10</cell><cell>12.10 15.48</cell></row><row><cell></cell><cell>GPU</cell><cell cols="5">0.011 0.014 0.033 0.040 0.124 0.167</cell></row><row><cell></cell><cell cols="2">CPU(ST) 0.44</cell><cell>0.62</cell><cell>1.81</cell><cell>2.51</cell><cell>7.24</cell><cell>10.17</cell></row><row><cell cols="3">FFDNet CPU(MT) 0.18</cell><cell>0.21</cell><cell>0.73</cell><cell>0.98</cell><cell>2.96</cell><cell>3.95</cell></row><row><cell></cell><cell>GPU</cell><cell cols="5">0.006 0.008 0.012 0.017 0.038 0.057</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Digital image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Englewood Cliffs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Is denoising dead?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="895" to="911" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast image recovery using variable splitting and constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2345" to="2356" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FlexISP: A flexible camera image processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">231</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The little engine that could: Regularization by denoising (RED)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">submitted to SIAM Journal on Imaging Sciences</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of gaussians in the wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training an active random field for real-time image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2451" to="2462" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning optimized MAP estimates in continuously-valued MRF models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="477" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning non-local range markov random field for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2745" to="2752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Half-quadratic inference and learning for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Darmstadt</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technische Universität</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="http://tuprints.ulb.tu-darmstadt.de/6044/" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3587" to="3596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning non-local image diffusion for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1847" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep gaussian conditional random field network: A model-based deep network for discriminative denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to push the limits of efficient FFT-based image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust image denoising with multi-column deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized deep image to image regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5609" to="5619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A holistic approach to cross-channel image noise modeling and its application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Natural image denoising: Optimality and inherent bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2833" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multimodal hashing with orthogonal regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2291" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient orthogonal parametrisation of recurrent neural networks using householder reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00188</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving training of deep neural networks via singular value bounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4344" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6176" to="6185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05693</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conditioned regression models for non-blind single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Plug-and-Play ADMM for image restoration: Fixed-point convergence and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Elgendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="98" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00388</idno>
		<title level="m">Diracnets: training very deep neural networks without skip-connections</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Refined filtering of image noise using local statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics and image processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="380" to="389" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Waterloo exploration database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The noise clinic: A blind image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<ptr target="http://demo.ipol.im/demo/125/" />
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Kodak lossless true color image suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Franzen</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak" />
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Color demosaicking by local directional interpolation and nonlocal adaptive thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatic estimation and removal of noise from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A non-parametric approach for the estimation of intensity-frequency dependent noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4261" to="4265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Gaussian-cauchy mixture modeling for robust signal-dependent noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5357" to="5361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
