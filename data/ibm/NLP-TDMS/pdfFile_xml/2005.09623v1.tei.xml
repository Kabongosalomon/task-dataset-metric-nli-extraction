<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focus on defocus: bridging the synthetic to real domain gap for depth estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Maximov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Galim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focus on defocus: bridging the synthetic to real domain gap for depth estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-driven depth estimation methods struggle with the generalization outside their training scenes due to the immense variability of the real-world scenes. This problem can be partially addressed by utilising synthetically generated images, but closing the synthetic-real domain gap is far from trivial. In this paper, we tackle this issue by using domain invariant defocus blur as direct supervision. We leverage defocus cues by using a permutation invariant convolutional neural network that encourages the network to learn from the differences between images with a different point of focus. Our proposed network uses the defocus map as an intermediate supervisory signal. We are able to train our model completely on synthetic data and directly apply it to a wide range of real-world images. We evaluate our model on synthetic and real datasets, showing compelling generalization results and state-of-the-art depth prediction. The dataset and code are available at https: //github.com/dvl-tum/defocus-net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, we have seen an increase in the number of smartphone photography users, bringing the need for image editing tools to a wider audience. Most of these tools are still limited to color adjustments and simple image transformations. More advanced post-capture changes such as focus and depth-of-field adjustments are not commonly available due to the need for depth maps of the captured scene. While there exist specialized hardware solutions to compute depth, e.g., light-field cameras <ref type="bibr" target="#b14">[14]</ref>, nowadays, data-driven machine learning makes it possible to tackle the problem from the software side, predicting depth maps <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">36</ref>] from a single image. However, monocular depth estimation methods do not generalize well to unseen data/scenes, e.g., different viewing angles, geometry and objects types. They heavily rely on perspective, size, texture and shading cues to measure distance. Those cues are dependent on the type of scene and objects, texture and illumination, which easily leads to overfitting to those memorized settings <ref type="bibr" target="#b20">[20]</ref>. Other works on depth estimation show better generalization by relying on comparison-based depth cues, such as depth from motion <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b19">19]</ref> or stereo <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b29">29]</ref>.</p><p>An under-explored cue for depth estimation is defocus, given that an objects depth dictates how sharp it will appear in the image. Depth-from-focus (or defocus) is defined as the task of obtaining the depth of a scene from a focal stack, i.e., a set of images taken by the same camera but focused on different focal planes. Analytical approaches <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b33">33]</ref> compute depth based on the sharpness of each pixel. Such approaches are time-consuming, and perform especially poorly for texture-less objects. Recent deep learning approaches address these challenges in a datadriven way by learning to directly regress depth from a focal stack <ref type="bibr" target="#b11">[12]</ref>. Their main drawback is that they do not consider the underlying image formation process, therefore, such methods are also prone to overfitting to the specific training conditions. Another challenge towards achieving generalization is the lack of high-quality and diverse training sets. Collecting focal stack images with registered depth maps is an extremely time-consuming task, not to mention the imperfect depth ground truth data obtained from hardware solutions like time-of-flight sensors <ref type="bibr" target="#b11">[12]</ref>. One can rely on synthetic data as used in inverse-graphics tasks <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">9]</ref>, but not without addressing the problem of bridging the domain gap  between synthetic and real images <ref type="bibr" target="#b25">[25]</ref>. Defocus blur is a well-modeled physical effect, and as such, straightforward to simulate in a realistic way. The main insight of our work is that, while appearance features greatly differ from synthetic to real images, blur does not. Such domain invariant measurement effectively aids in bridging the domain gap between synthetic and real data. We therefore propose to leverage defocus in a data-driven model to predict depth from focal stacks. By breaking the depth prediction into two steps, and using defocus maps as intermediate representations, we can train a neural network that generalizes from synthetic to real data without finetuning. Additionally, we show our architecture works for an arbitrary number of input images and propose an extension to dynamic focal stacks <ref type="bibr" target="#b15">[15]</ref>, where camera motion or scene motion is present. Our contribution is three-fold:</p><p>• We propose to use defocus blur as intermediate supervision to train data-driven models to predict depth from focal stacks. We show that this is key towards generalization from synthetic to real images, and show stateof-the-art results.</p><p>• We generate a new synthetic dataset with multiple objects, textures and varying illumination with depth, blur and all-in-focus information.</p><p>• We propose architectures for static scenes that can work with a varying number of inputs. In dynamic focal stacks, our model can handle scene or camera motions within the stack. We show their robustness in a comprehensive ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Depth estimation from defocus. Depth estimation is a popular topic and is being explored from multiple directions. The vast majority of work focuses on monocu-lar <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">36]</ref> or stereo <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b29">29]</ref> depth estimation. Using defocus information for depth prediction is less common. Several optimization-based works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b32">32]</ref> estimate depth from a focal stack, while <ref type="bibr" target="#b15">[15]</ref> extends such methods to videos. These are general approaches that work on a variety of scenes, though they struggle on texture-less surfaces, and produce compelling depth measurements, but they are highly time-consuming and require careful calibration. It takes up to minutes for these methods to estimate the sharpness of the image regions and compose a depth image. Recent methods leverage deep learning to bring this process closer to real-time. <ref type="bibr" target="#b11">[12]</ref> uses convolutional neural networks (CNNs) to estimate depth directly from input focal stacks, without considering the underlying image formation process. Such a method is bound to have generalization problems unless train and test conditions are very similar. Additionally, it can only take a pre-defined number of inputs and does not incorporate any distance measurement.</p><p>Other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b4">5]</ref> implicitly use defocus information for monocular depth estimation. <ref type="bibr" target="#b30">[30]</ref> proposes to use defocus as a part of the loss function to estimate depth from an all-in-focus image. Nonetheless, they still use an all-infocus monocular image as input, hence they do not leverage defocus blur during inference. Similarly, <ref type="bibr" target="#b10">[11]</ref> uses a differentiable loss layer that uses focus as a cue for depth prediction. <ref type="bibr" target="#b1">[2]</ref> uses CNNs for image deblurring and depth estimation from a single out-of-focus image. <ref type="bibr" target="#b4">[5]</ref> uses outof-focus images for direct depth estimation. Their findings indicate that training on defocus images gives better depth measurements than training on sharp in-focus images. These methods use single images as input, therefore failing to leverage the much richer focus information present in a focal stack. As a consequence, they face difficulties when predicting depth in the wild, i.e., for completely different scenes and/or cameras than the ones used at training time. In contrast, we propose to combine the power of data-driven approaches with knowledge of the image formation process, so that depth estimation can be computer by relying on focus differences between images in a focal stack.</p><p>Synthetic-to-real. There are several previous works that show domain generalization from synthetic to real data. According to <ref type="bibr" target="#b31">[31]</ref>, all of them can be divided into three main strategies: domain adaptation, photo-realistic rendering, and domain randomization. Domain adaptation approaches usually convert samples from one domain to another <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b38">38]</ref>, or use model fine-tuning on real data after training on synthetic <ref type="bibr" target="#b9">[10]</ref>. Several works show compelling results on photo-realistic synthetic training and real test sets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. However, photo-realism consumes a lot of time with physically based rendering (PBR) computation and hand-modelling of the entire environment. Domain randomization, similar to data augmentation, introduces a variety of random attributes to make a model invariant to small changes and force the network to focus on the main features of the image. It requires simplistic modeling by assuming a random environment while being able to incorporate real data, e.g., in the background. The main issue is to select the extent and type of attribute randomization. Several works show promising results in this direction <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>We believe there is a fourth strategy for domain generalization, i.e., domain invariance, which involves training a model on features that are invariant for the two domains. One good example is stereo depth estimation. Models trained on synthetic data <ref type="bibr" target="#b8">[9]</ref> are successful since they focus on the difference between two input images rather than the appearance and shape of objects. This approach can also be embedded into a network architecture <ref type="bibr" target="#b0">[1]</ref>, where a permutation invariant architecture is proposed in an image translation context. It combines information across a random unordered number of input images.</p><p>In our work, we rely on a domain invariant cue, defocus, as the main strategy for bridging the synthetic and real domain. Additionally, we also use domain randomization, and, to some extent, a photo-realistic rendering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning depth through defocus</head><p>In this section, we detail our model for depth estimation using defocus cues. We show that decomposing the problem into defocus estimation and later depth estimation is critical to close the domain gap between synthetic and real data. We show state-of-the-art results on real data while training our models only with synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method overview</head><p>We show a diagram of our method in <ref type="figure" target="#fig_0">Fig. 1</ref>, which shows the three main elements of our model: DefocusNet. Instead of directly estimating depth from a stack of RGB images, we first estimate a defocus map using DefocusNet (Section 3.2). In particular, we estimate the amount of defocus per pixel. DepthNet. This defocus map is used as input to Depth-Net, which estimates the scene depth map (Section 3.3). To have sharper and better structured output depth, skip connections are used between the encoder of DefocusNet and the decoder of DepthNet. Both DefocusNet and DepthNet are trained jointly and in an end-to-end manner. AiFNet. While obtaining a depth map is our end goal, image post-processing applications, e.g., refocusing, further require an all-in-focus (AiF) image, aside from the predicted depth map. We can easily predict the all-in-focus image with the focal stack and the defocus map, therefore, it is trivial to extend our architecture with a head, AiFNet, to predict all-in-focus images (Section 3.4).</p><p>In the following sections, we introduce the necessary concepts related to depth and defocus, and proceed to describe the three modules in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Defocus (blur) estimation</head><p>Circle of Confusion. In order to compute a defocus map, we first need to establish a measure for sharpness. When a light source passes through the camera lens, the light rays converge to form a focal point, which is found on the image plane of the camera. The circle of confusion (CoC) measures the diameter of the focal point. For a given point in front of the camera, if all rays of light flowing out of it converge into single location in the image plane, then the point will have sharpest projection possible ( <ref type="figure" target="#fig_2">Fig. 3</ref>). Hence, the CoC is a direct translation of the amount of sharpness, equivalently, the amount of defocus. The CoC can be computed using the following equation:</p><formula xml:id="formula_0">c = | S 2 − S 1 | S 2 f 2 N (S 1 − f ) ,<label>(1)</label></formula><p>where f is the focal length of the lens, S 1 is the focus distance, S 2 is the distance from the lens to the object, and N is the f-number. The f-number is the ratio of focal length to effective aperture diameter, essentially indicating aperture size. An illustration of a lens system is shown in <ref type="figure" target="#fig_2">Fig.3</ref>. The range of acceptable values of CoC, that we consider sharp, depends on the image format and camera model, and it is typically decided based on visual acuity. This range is referred as depth of field (DoF). On the right of <ref type="figure" target="#fig_2">Fig. 3</ref>, we show a graph of the evolution of CoC values as the object distance S 2 increases. Each line represents a different value of focus distance S 1 . The more variation lines produces in observation, the easier it is to estimate depth. Once the value surpasses the minimum diameter, the ambiguity of the CoC increases with the distance. After after a certain depth, the CoC no longer changes, indicating we can no longer rely on defocus cues to compute the depth of objects. Thus, the defocus information is useful in a short range which depends on the camera properties.</p><p>In this paper, we use the term defocus (blur) and CoC interchangeably. In fact, to construct a defocus map, we compute the CoC values for all pixels in an image, clip all values inside a chosen upper limit, and normalize all values in a range between 0 to 1 (from sharp to blurry).</p><p>Depth-from-Defocus limitations. The problem is inherently limited by design, as depth-from-focus works better on short ranges. Nonetheless, ubiquitous depth-fromstereo methods (and by extension, conventional depth cameras with a separate IR projector and an IR camera) are less effective in a short range due to part of the scene being not visible by both cameras. Depth-from-focus can be seen as a solution for short ranges. In our camera settings, the effective range in which we can use defocus to predict depth is within 2 meters, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Data-driven defocus estimation. A key design choice of our work is to use defocus estimation as an intermediate step, or supervisory signal, to estimate depth from a focal stack. This allows us to obtain a model that generalizes from synthetic to real images. Therefore, we begin by training a model, DefocusNet, to estimate a defocus map from a set of images. <ref type="figure" target="#fig_3">Fig. 4</ref> shows an example of RGB images from our synthetic dataset and their corresponding defocus map, where the pixel values represent their sharpness level.</p><p>The focal stack is processed by an autoencoder convolutional neural network (CNN), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The encoder contains one branch per image in the focal stack, where all branches share the weights of the CNN. Our goal is to encourage the network to perform comparisons between the features extracted at each branch, as to better establish focused and defocused regions. We do this comparison at every layer of the CNN, inspired by <ref type="bibr" target="#b0">[1]</ref>. Layer-wise global pooling. The network computes the output of the convolution layer for every input image, then all output feature maps are pooled by a symmetric operation, i.e., we compute the maximum value of each feature map cell across all branches. The globally pooled features, or global feature map, are then concatenated to the local feature map coming from each branch. The combined output is then passed to the next convolution layer, and the process is repeated. This way, each CNN branch will contain both local as well as global features. Intuitively, this allows the network to compare local features with globally pooled features, finding out the sharpest regions of the image by comparison, and passing those to the next layer of the CNN. The main advantage of using this layer-wise pooling across inputs is that our model can handle an arbitrary number of images as input, making our model extremely flexible.</p><p>The CNN is rather shallow with only 4 layers. Since sharpness is a local property, we do not need a large receptive field. The decoder then propagates the estimated focus information from the edges to the center of the objects, where there might not be enough texture to properly estimate sharpness. We also make use of skip connections (by concatenation) to properly recover boundaries in the regressed defocus maps. Global pooling is used only in the encoder, while the decoder has separate branches for each output. The main idea is for the encoder to learn to detect sharp regions by comparison, and for the decoder to regress a defocus map independently for each input. We use an L2 loss to train DefocusNet without additional regularization.</p><p>As we can see in <ref type="figure" target="#fig_0">Fig. 1</ref>, once defocus is estimated, we can use it as an input to estimate depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth estimation.</head><p>A depth map can be constructed from defocus maps by using the camera capture settings for each image in the focal stack. However, our network architecture is by design unaware of image order. Therefore, we also include a focus distance map together with the previously predicted defocus map as input to DepthNet, our neural network that is trained for depth regression. The focus distance map is a single channel and single value image, where every pixel takes the value of the focus distance.</p><p>We obtain focus distances from our dataset rendering script (Section 4) and then rescale them in the range between 0 and 1. For real images, if we know the order in a focal stack, we can assign focus distances consecutively with computed increments (based on a number of images) in the range 0 to 1. We can also extract the focus distance from EXIF properties (camera settings used to take an image) and rescale the values to the required range. While our architecture needs this additional input, it comes at a minimal cost, does not affect generalization and allows us to have an arbitrary number of input images.</p><p>The network architecture for DepthNet is similar to De-focusNet, except we have a single branch also in the decoder to get one depth map as output. Additionally, in the DepthNet decoder we use skip connections from the Defo-cusNet encoder to combine information from the RGB input image. This helps especially to improve the depth prediction around object boundaries. During training, we use the L2 loss between estimated and ground-truth depth.</p><p>The full loss function is shown below:</p><formula xml:id="formula_1">L = λ a I def − E def 2 + λ b I dep − E dep 2 (2)</formula><p>where I are ground truth images, E are estimated images, for depth and defocus, λ a,b are weight coefficients. Dynamic stacks. As we mentioned in previous paragraphs, the goal of global pooling across inputs is to encourage the network to compare different input branches. However, such approach has a problem with focal stacks taken with a moving camera or a static camera but a moving scene. In those scenarios, each part of the image will contain drastically different information, and comparison across inputs will not be informative to compute defocus. To handle such scenarios, we propose to use a recurrent autoencoder <ref type="bibr" target="#b5">[6]</ref> for DepthNet and DefocusNet, instead of the global pooling autoencoder. Such a recurrent autoencoder concatenates the local features from one branch to the next sequentially, taking order into account. This allows us to gradually incorpo-rate changes, also changes in the scene, and implicitly compare the amount of defocus in them. Still, such recurrent architecture has its own drawbacks: (i) it has a short memory <ref type="bibr" target="#b2">[3]</ref>, and (ii) the number and order of images on the focal stack is fixed due to the architecture. For these reasons, we use it only when dealing with dynamic stacks. More architecture details are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">All-in-Focus estimation</head><p>In our work, we use a stack of differently focused images to estimate a depth map. However, for image postprocessing applications such as refocusing, we additionally need an all-in-focus (AiF) image where all pixels are appropriately sharp. We can estimate such image given a focal stack by combining different image parts according to their sharpness. For this reason, we propose to incorporate such estimation inside our network, reusing the focal stack processing as well as the defocus map. The AiF image is computed by an additional CNN head, the AiFNet. Since AiF prediction is not the main focus of our work, the model description and results are included in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Synthetic Training Data</head><p>We use only synthetic data to train our full model for depth prediction from focal stacks. As we will show in the experimental section, our network will generalize to depth prediction on real images without the need to fine-tune our model on real data. We create our synthetic dataset using Blender <ref type="bibr" target="#b3">[4]</ref> Cycles renderer with reflection turned on, but without shadows. Examples from the dataset are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. For training, we render a total of 1000 scenes, each of them with 5 RGB images per focal stack, 5 defocus maps, 1 depth image and 1 all-in-focus image (taken with a wide aperture). Each defocus map was calculated using Equation (1) based on depth and camera parameters. Each image was rendered at a resolution of 256 × 256. Dataset characteristics. We acquired CAD 3D objects from a model repository containing a total of 400 objects <ref type="bibr" target="#b40">[40]</ref>, and place between 20 to 30 objects per scene, all randomly chosen. Objects are assigned a random size, location and rotation in each scene to have a random spatial arrangement. Locations are limited to the effective range of defocus (Sec. 3.2) and camera field of view. Some objects might not be fully in the camera field of view or can be occluded by other objects. We choose a non-realistic scene composition on purpose due to its simplicity to model, but primarily to avoid overfitting on spatial cues, and instead force the model to focus on defocus cues.</p><p>Each object is assigned one random material. Before rendering, we randomize the hue of the diffuse and specular components, glossiness and roughness are chosen within a range that produces a realistic appearance. All materials use physically based rendering (PBR) shaders. For illumi- nation, we used 20 different HDR environment maps (EM), both indoor and outdoor. We used fixed camera parameters with fixed focus distances for all scenes. The f-number is set to 1 in order to have a shallow depth-of-field, therefore making depth changes more observable in terms of blur. Dynamic stacks. To handle camera or scene motion, we additionally create dynamic stacks with 4 images, where the position of the object changes for each image. We assign a random direction and magnitude of translation and rotation for each object at the beginning of sequence rendering. More details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We present a comprehensive ablation study on a diverse set of synthetic scenes. We further show qualitative and quantitative results for depth prediction on real datasets and also on real images with synthetic blur. We show generalization by training only on synthetic data and obtaining state-of-the-art results on real images with synthetic blur. We provide in the supplementary material additional results on real/synthetic images and all-in-focus image prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>The method is implemented in PyTorch <ref type="bibr" target="#b24">[24]</ref>. We train the model using the Adam optimization algorithm <ref type="bibr" target="#b16">[16]</ref> with a learning rate of 0.0001. We assign λ d = 0.02 and λ a,b = 1. We provide a detailed description of all network architectures in the supplementary material. Run-Time Performance. On an Nvidia Titan X, a forward pass of our network takes 70ms on a focal stack with 10 images and 150ms with 20 images. For <ref type="bibr" target="#b33">[33]</ref> reported time is 20mins and for [32] -6.7s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Metrics and Datasets</head><p>Evaluation metrics. For depth comparison, we use root mean squared error (RMSE) in <ref type="table">Table 3</ref>  The third test is identical to the second set, but with smaller camera apertures (f-number is from 3 to 10) to show that we rely on defocus cue. Having a larger DoF negatively affects blur estimation because there is less blur information.</p><p>(iv) Medium DoF test. The fourth test is rendered with the same options as the second set, but with slightly smaller camera apertures (we randomly choose an f-number from 1 to 3 whereas a training set was trained on f-number of 1) to test the generalization to slightly different camera settings. Real datasets. There are very few datasets that provide focal stacks. We use these datasets for quantitative and qualitative experiments. (i) DDFF 12-Scene benchmark <ref type="bibr" target="#b11">[12]</ref>. Obtaining focal stacks with standard cameras is a time-consuming task. To speed up the process, <ref type="bibr" target="#b11">[12]</ref> uses plenoptic cameras that can capture 4D light-fields. With a single light-field, we can generate a focal stack and an all-in-focus image. This dataset consists of 1200 focal stacks with 10 images each. The dataset is challenging due to the type of scene recorded: many flat and texture-less surfaces such as walls and desks, and other texture-less objects such as monitors, doors and cabinets. Furthermore, their capture settings are not optimal for defocus blur, they shoot with wide DoF and capture scenes with far distances. We use the same training/test split as <ref type="bibr" target="#b11">[12]</ref>.</p><p>(ii) Mobile Depth <ref type="bibr" target="#b33">[33]</ref>. This dataset consists of 13 scenes, each scene has a different number of images in the range between 13 and 32. All images were taken with a mobile phone and aligned using optical flow. Since there are not enough images to train a deep learning approach, we show the results of our model trained only on our synthetic training set. Synthetically blurred real datasets. Due to lack of datasets with focal stacks for quantitative experiments, we propose to use popular indoor datasets and create focal stacks from RGB images and ground-truth depth. We apply synthetic blur following <ref type="bibr" target="#b10">[11]</ref>. (i) NYU Depth Dataset v2 <ref type="bibr" target="#b23">[23]</ref>. This dataset consists of 1449 pairs of aligned RGB and depth frames. We use the regular split between test and train sets.</p><p>(ii) 7-Scenes <ref type="bibr" target="#b27">[27]</ref>. This dataset consists of around 43000 images of aligned RGB and depth frames scenes. Due to large size of the dataset, we randomly sample total of 890 images from all 7 sequences for our tests. (iii) Middlebury stereo dataset <ref type="bibr" target="#b26">[26]</ref>. This dataset consists of 46 images of stereo RGB pairs and disparity frames. Based on provided camera calibration parameters, we compute the depth map for the RGB image corresponding to the right camera.</p><p>(iv) SUN RGB-D <ref type="bibr" target="#b39">[39]</ref>. This dataset is a combination of NYU depth v2 <ref type="bibr" target="#b23">[23]</ref>, Berkeley B3DO <ref type="bibr" target="#b12">[13]</ref> and SUN3D <ref type="bibr" target="#b35">[35]</ref> datasets with improved depth maps. It consists of 10,000 images of aligned RGB and depth frames. Similarly, we randomly sample total of 490 images for our tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study on Synthetic Dataset</head><p>We quantitatively analyze our methods generalization capabilities to new environments and camera settings on 4 different synthetic test modalities. <ref type="table" target="#tab_2">Table 1</ref> shows estimation results between different models on the depth estimation task. We explain and compare all models below. In the tables, "All" indicates that all N images of the focal stack were used for prediction, while "Random" indicates the use of a randomly chosen number of images r ≤ N . Architecture choice. We consider two architecture types: (i) FixedAE, our baseline with single autoencoder without global pooling layers that is trained on a single input with fixed-sized focal stack with N images, and (ii) PoolAE, the autoencoder with global pooling layers, which can take any number of images as input. In <ref type="table" target="#tab_2">Table 1</ref>, the first two rows show a direct comparison of the two models. FixedAE was trained on a specific number of inputs and shows good performance only when tested on the same number of images (column All). In contrast, PoolAE shows generalization to the column Random, where the number of input images varies but accuracy is maintained. From these tests, we can summarize that PoolAE architecture is robust, generalizes better than just stacking all inputs in single AE, and has the added value of processing any number of inputs without the need to retrain the model. Is defocus needed? We compare models that directly estimate depth from the input RGB image and our proposed approach, where we first estimate a defocus map and then depth. In <ref type="table" target="#tab_2">Table 1</ref>, with our PoolAE architecture, we achieve a much better result when going over the defocus map (row 3.) compared to a direct estimation (row 2.). On the Wide DoF test, PoolAE with defocus map (row 3.) performs worse due to having less defocus blur to rely on. It confirms that our model uses defocus as the primary signal to estimate depth. If that signal is inexistent due to a too wide DoF, performance drops as expected. Overall, we can clearly conclude that using defocus information is beneficial for depth estimation, and is cue that allows us to generalize to a diverse set of object shapes and appearances. Single image vs. Focal stack. We also train our method on a single image input setting. Such network is expected   to perform worse when predicting depth, as it is not able to rely on blur comparison between images in the focal stack.</p><p>Results are presented on the first row of <ref type="table">Table 2</ref>. We clearly see the error in depth prediction is much larger than all models that use a focal stack ( <ref type="table" target="#tab_2">Table 1)</ref>. All-in-Focus vs. Out-of-Focus. Out-of-focus images give more information related to depth, as shown in <ref type="bibr" target="#b4">[5]</ref>. We also  <ref type="figure">Figure 7</ref>. Qualitative results on Mobile Depth dataset. We compare our method with a model-based approach. <ref type="bibr" target="#b33">[33]</ref>.</p><p>perform a similar test in <ref type="table">Table 2</ref>, where we compare to a model trained on all-in-focus images. We can see from the Shape and Appearance tests, that defocus gives more information for depth even without explicitly computing the defocus map. The model in Row 1. also performs worse in both camera aperture tests, since they both have wider DoF. Wider DoF has less defocus blur and does not give any information to the model. The Row 2 model does not rely on defocus and therefore shows similar results on all tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation on Real data</head><p>Synthetically blurred NYU, 7 scenes, Middlebury and SUN RGB-D. In this section, we show quantitative results on cross-dataset and domain generalization task. Since defocus blur is only effective on close distances, we conduct experiments on 4 different versions of each dataset: (i) regular, no modifications, (ii) less than 2m, only depths within 2 meters are taken into account, since this is the functioning range for our method, (iii) normalized version, depth is rescaled from 0 to 10 meters to a range from 0 to 1 meters, and (iv) 45 degrees, the normalized version with images rotated 45 degrees. The last test of rotating input images by 45 degrees is a simple yet effective way to show that current datasets have photographic bias, which leads to overfitting on the training settings. We compare our method with the state-of-the art single image depth estimation model VNL <ref type="bibr" target="#b36">[36]</ref> in <ref type="table">Table 3</ref>. We trained our method on the regular synthetic dataset, "Ours", and its normalized version, "Ours*". We additionally show a version fine-tuned on the NYU dataset. Due to the difference in datasets, we use the median to rescale estimated depth for all models to match ground-truth depth as in <ref type="bibr" target="#b7">[8]</ref>. VNL was trained on the NYU dataset and performs well  <ref type="table">Table 3</ref>. Regular -no modifications, &lt;2m -same as regular but counting results only for depth less than 2 meters, and normalized version -depth was rescaled to range from 0 to 1. All models with * were trained for normalized sets. 45 degrees set is a version with images rotated 45 degrees. Our models trained first on synthetic dataset then tested with and without finetuning on NYU dataset.  on that dataset. However, on other datasets, its performance drops in comparison to our methods that use defocus cues. Our fine-tuned versions, Ours(Synth.+NYU) and Ours*(Synth.+NYU), perform best across all test but fail the 45 degree test. The purely synthetically-trained model shows similar or better performance to the method trained purely on real data, and generalizes much better in the case of the 45 degree experiment. This clearly shows the bias of the real dataset that does not allow the networks to generalize to any scene configuration.</p><p>We use focal stacks with 4 images which is to some degree unfair to single image methods. Nonetheless, capturing a focal stack is straightforward: (i) it takes just slightly longer than a single shot, (ii) we do not need additional camera hardware, and (iii) we do not need to move the camera to satisfy stereo requirements. The benefits in depth prediction accuracy come at a very little cost during caption. DDFF 12-Scene. We compare our approach to a CNNbased method <ref type="bibr" target="#b11">[12]</ref> and a classic method (VDFF) <ref type="bibr" target="#b22">[22]</ref>. As explained in Section 4, this dataset is not ideal for Defocus-Net due its wide DoF. Our synthetically trained models did not directly perform well, but after fine-tuning on the provided training data, we were able to achieve state-of-the-art results, shown in <ref type="table">Table 4</ref>. As we can see, PoolAE network with DefocusNet shows better performance. We conclude that our approach generalizes well within similar types of real images and is able to handle texture-less surfaces. Mobile Depth from Focus. We compare our method with traditional methods <ref type="bibr" target="#b33">[33]</ref> that take focal stack images as in-puts. The dataset does not have ground truth depth, but the authors provide their depth estimations. We compare our models to their depth to test the generalization capabilities of our approach. <ref type="table">Table 5</ref> shows that a direct approach does not generalize from synthetic to real images while our method does. We also show qualitative results in <ref type="figure">Fig. 7</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref>. Note that our models are trained on synthetic data only, and are not fine-tuned on this dataset. Additionally, we show visual results with the increasing number of input images in <ref type="figure" target="#fig_4">Fig. 5</ref>. We can see that it gradually improves the depth estimates thanks to our pooling architecture.</p><p>There are several aspects that work in favor of a better generalization in our work: (i) we use down-scaled images, e.g., original images from Mobile Depth are 360 x 640, which makes the out-of-focus blur details similar for most conventional cameras; (ii) the method is based on the comparison between differently focused inputs rather than analysis of blur shape/size; (iii) <ref type="bibr" target="#b0">[1]</ref> showed that with enough randomness in synthetic noise, invariance to various real noise can be achieved.</p><p>Dynamic stacks. Since we lack real test data to show our model on dynamic stacks, we implemented a smartphone application to capture focal stacks. <ref type="figure" target="#fig_5">Fig. 6</ref> shows qualitative results of the recurrent approach on real data recorded with a moving camera. Note, the models were trained with synthetically moving sequences. We show more qualitative results for all datasets in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a data-driven approach for estimating depth using defocus cues from a focal stack as a supervisory signal. Our key design decision is to use domain invariant defocus information as supervision for the depth prediction. This allows our model to generalize from synthetic to real images. Our permutation-invariant network allows us to correctly estimate depth with any focal stack size, and we further show a simple extension to process stacks with either moving camera or moving scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The pipeline of our approach. Our proposed end-toend learned model combines depth and all-in-focus estimation from a focal stack using intermediate defocus map estimation and permutation-invariant networks, leading to a better generalization from a synthetic training to real photos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>DefocusNet architecture. The proposed architecture takes a focal stack with an arbitrary size as an input and estimates corresponding defocus maps. The network uses an autoencoder as a basis and shares weights across all branches. Global pooling is used as a communication tool between separate branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Lens diagram on the left. Circle of confusion plot on the right. Each line in the plot corresponds to a different focus distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>On the left is a schematic for the random scene generation. On the right side are the examples from the synthetic dataset (pairs of RGB images and defocus maps from focal stacks). Sharper regions are darker in defocus maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Our sequential estimates on a real focal stack. The first row images are inputs to our pipeline. The other horizontal sequences show our outputs for a growing number of input frames. So the results on the left use only the first input image and on the right use all four inputs. Note how adding more inputs quickly improves the depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Our estimates on a real focal stack on a dynamic sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2005.09623v1 [cs.CV] 19 May 2020</figDesc><table><row><cell>Input 1</cell><cell>Input 2</cell><cell>Input N</cell><cell>Focal Stack</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Convolution Blocks</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Global Pools</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Skip Connections</cell></row><row><cell>Output 1</cell><cell>Output 2</cell><cell>Output N</cell><cell>Defocus maps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and mean squared error (MSE) in all other tables in order to compare to existing methods. Synthetic dataset. Synthetic test data was rendered in the same way as the training data in Section 4, but we use a new set of 10 environment maps, 20 new textures, and 300 new objects. We rendered 4 test sets to evaluate generalization: (i) Shape test. The first test set contains only new objects, while environments and textures are the same as used for training. This is a test on shape generalization. (ii) Appearance test. The second test set contains new objects, textures and environment maps to test appearance generalization. (iii) Wide DoF test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Results on the synthetic data test sets for depth estimation models with focal stacks (FS) as input. Row 1. Direct depth prediction with a fixed-sized AE. Row 2. Direct depth prediction with the proposed AE with global pooling. Row 3. Depth prediction for our method, using the predicted defocus map as supervisory signal.</figDesc><table><row><cell cols="2">Models</cell><cell></cell><cell>All</cell><cell cols="2">Shape Random</cell><cell cols="2">Appearance All Random</cell><cell cols="2">Wide DoF All Random</cell><cell>Medium DoF All Random</cell></row><row><cell cols="2">1. FS → Depth (FixedAE)</cell><cell></cell><cell cols="2">0.014</cell><cell>0.097</cell><cell>0.012</cell><cell>0.095</cell><cell>0.103</cell><cell>0.111</cell><cell>0.083</cell><cell>0.112</cell></row><row><cell cols="2">2. FS → Depth (PoolAE)</cell><cell></cell><cell cols="2">0.031</cell><cell>0.036</cell><cell>0.034</cell><cell>0.039</cell><cell>0.049</cell><cell>0.050</cell><cell>0.047</cell><cell>0.048</cell></row><row><cell cols="5">3. FS → Defocus → Depth (PoolAE) 0.008</cell><cell>0.013</cell><cell>0.007</cell><cell>0.012</cell><cell>0.042</cell><cell>0.078</cell><cell>0.014</cell><cell>0.030</cell></row><row><cell>Models</cell><cell cols="5">Shape App. W. DoF M. DoF</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">1. RGB → Depth 0.032 0.031</cell><cell>0.134</cell><cell cols="2">0.108</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. AiF → Depth</cell><cell cols="2">0.049 0.050</cell><cell>0.050</cell><cell cols="2">0.054</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 2. Results of depth estimation on the synthetic test set using</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">only one image as input, either one of the out-of-focus images of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">the focal stack (Row 1), or the all-in-focus image (Row 2).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input 1</cell><cell>Input 2</cell><cell>Input 3</cell><cell cols="2">Input 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Focal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Defocus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Depth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Synth. NYU Norm.* 45deg.* Regular &lt;2m Norm.* 45deg.* Regular &lt;2m Norm.* 45deg.* Regular &lt;2m Norm.* 45deg.* Regular &lt;2m</figDesc><table><row><cell>Models</cell><cell>Training data</cell><cell>NYU</cell><cell></cell><cell></cell><cell></cell><cell cols="2">7 scenes</cell><cell></cell><cell></cell><cell cols="2">Middlebury</cell><cell></cell><cell></cell><cell cols="2">SUN RGB-D</cell><cell></cell></row><row><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>1.054</cell><cell>0.272</cell><cell>-</cell><cell>-</cell><cell>0.504</cell><cell>0.282</cell><cell>-</cell><cell>-</cell><cell>0.803</cell><cell>0.384</cell><cell>-</cell><cell>-</cell><cell>0.721</cell><cell>0.259</cell></row><row><cell>Ours*</cell><cell>0.056</cell><cell>0.073</cell><cell>-</cell><cell>-</cell><cell>0.030</cell><cell>0.037</cell><cell>-</cell><cell>-</cell><cell>0.052</cell><cell>0.063</cell><cell>-</cell><cell>-</cell><cell>0.037</cell><cell>0.052</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>-</cell><cell>-</cell><cell>0.493</cell><cell>0.181</cell><cell>-</cell><cell>-</cell><cell>0.277</cell><cell>0.189</cell><cell>-</cell><cell>-</cell><cell>0.544</cell><cell>0.351</cell><cell>-</cell><cell>-</cell><cell>0.360</cell><cell>0.196</cell></row><row><cell>Ours*</cell><cell>0.013</cell><cell>0.111</cell><cell>-</cell><cell>-</cell><cell>0.010</cell><cell>0.045</cell><cell>-</cell><cell>-</cell><cell>0.025</cell><cell>0.079</cell><cell>-</cell><cell>-</cell><cell>0.014</cell><cell>0.073</cell><cell>-</cell><cell>-</cell></row><row><cell>VNL [36]</cell><cell>0.040</cell><cell>0.100</cell><cell>0.395</cell><cell>0.206</cell><cell>0.033</cell><cell>0.050</cell><cell>0.328</cell><cell>0.244</cell><cell>0.064</cell><cell>0.071</cell><cell>0.645</cell><cell>0.400</cell><cell>0.037</cell><cell>0.068</cell><cell>0.370</cell><cell>0.289</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Results of depth estimation on DDFF-12. Results of depth estimation on Mobile Depth dataset.</figDesc><table><row><cell>Models</cell><cell>MSE</cell></row><row><cell>FS → Depth</cell><cell>0.184</cell></row><row><cell cols="2">FS → Defocus → Depth 0.045</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was funded by the Sofja Kovalevskaja Award of the Humboldt Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Burst image deblurring using permutation invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="764" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth estimation and blur removal from a single out-offocus image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih Murat</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blender -a 3D modelling and rendering package</title>
	</analytic>
	<monogr>
		<title level="m">Blender Foundation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On regression losses for deep depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Trouvé-Peloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Champagnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive reconstruction of monte carlo image sequences using a recurrent denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Alla</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><forename type="middle">S</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">E</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aila</surname></persName>
		</author>
		<idno>98:1-98:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="523" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="506" to="523" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image depth estimation trained via depth from defocus cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7683" to="7692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep depth from focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A category-level 3-d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">IEEE International Conference on Computer Vision Workshops, ICCV 2011 Workshops</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate depth map estimation from a lenslet light field camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Hae-Gon Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video depth-from-defocus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cgintrinsics: Better intrinsic image decomposition through physically-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="399" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Materials for masses: SVBRDF acquisition with a single mobile phone image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="74" to="90" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward domain independence for learning-based monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1778" to="1785" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational depth from focus reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola</forename><surname>Schönlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5369" to="5378" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational depth from focus reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carola</forename><surname>Schonlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5369" to="5378" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1806.09755</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Nesic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition -36th German Conference</title>
		<meeting><address><addrLine>Münster, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-02" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the importance of stereo for accurate depth estimation: An efficient semi-supervised deep neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aperture supervision for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6393" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="712" to="729" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sunghoon Im, Hyowon Ha, and In So Kweon. Noise robust depth from focus using a ring difference filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeheung</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunwon</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth from focus with your mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5622" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometry-aware symmetric domain adaptation for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9788" to="9798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04797</idno>
		<title level="m">Thingi10k: A dataset of 10,000 3d-printing models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
