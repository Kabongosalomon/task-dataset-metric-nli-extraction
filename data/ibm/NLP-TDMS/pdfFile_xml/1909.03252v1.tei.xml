<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Networks for Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Lab. for Information Science and Technology (TNList)</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<email>mingkuitan@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang1990@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Lab 4 Peng Cheng Laboratory</orgName>
								<orgName type="institution">MIT-IBM Watson AI</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Networks for Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using Graph Convolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding human actions in videos has been becoming a prominent research topic in computer vision, owing to its various applications in security surveillance, human behavior analysis and many other areas <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>. Despite the fruitful progress in this vein, there are still some challenging tasks demanding further exploration -temporal action localization is such an example. To deal with real videos that are untrimmed and usually contain the background of irrelevant activities, temporal action localization requires the machine to not only classify the actions of interest but also localize the start and end time of every action instance. Consider a sport video as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the detector should find out the frames where the action event is happening and identify the category of the event.</p><p>Temporal Action localization has attracted increasing attention in the last several years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Inspired by the success of object detection, most current action detection methods resort to the two-stage pipeline: they first generate a set of 1D temporal proposals and then perform classification and temporal boundary regression on each proposal individually. However, processing each proposal separately in the prediction stage will inevitably neglect the semantic relations between proposals.</p><p>We contend that exploiting the proposal-proposal relations in the video domain provides more cues to facilitate the recognition of each proposal instance. To illustrate this, we revisit the example in <ref type="figure" target="#fig_0">Figure 1</ref>, where we have generated four proposals. On the one hand, the proposals p 1 , p 2 and p 3 overlapping with each other describe different parts of the same action instance (i.e., the start period, main body and end period). Conventional methods perform prediction on p 1 by using its feature alone, which we think is insufficient to deliver complete knowledge for the detection. If we additionally take the features of p 2 and p 3 into account, we will obtain more contextual information around p 1 , which is advantageous especially for the temporal boundary regression of p 1 . On the other hand, p 4 describes the background (i.e., the sport field), and its content is also helpful in identifying the action label of p 1 , since what is happening on the sport field is likely to be sport action (e.g. "discus throwing") but not the one happens elsewhere (e.g. "kissing"). In other words, the classification of p 1 can be partly guided by the content of p 4 even they are temporally disjointed.</p><p>To model the proposal-proposal interactions, one may employ the self-attention mechanism <ref type="bibr" target="#b38">[39]</ref> -as what has been conducted previously in language translation <ref type="bibr" target="#b38">[39]</ref> and object detection <ref type="bibr" target="#b21">[22]</ref> -to capture the pair-wise similarity between proposals. A self-attention module can affect an individual proposal by aggregating information from all other proposals with the automatically learned aggregation weights. However, this method is computationally expensive as querying all proposal pairs has a quadratic complexity of the proposal number (note that each video could contain more than thousands of proposals). On the contrary, Graph Convolutional Networks (GCNs) , which generalize convolutions from grid-like data (e.g. images) to non-grid structures (e.g. social networks), have received increasing interests in the machine learning domain <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b46">47]</ref>. GCNs can affect each node by aggregating information from the adjacent nodes, and thus are very suitable for leveraging the relations between proposals. More importantly, unlike the self-attention strategy, applying GCNs enables us to aggregate information from only the local neighbourhoods for each proposal, and thus can help decrease the computational complexity remarkably.</p><p>In this paper, we regard the proposals as nodes of a specific graph and take advantage of GCNs for modeling the proposal relations. Motivated by the discussions above, we construct the graph by investigating two kinds of edges between proposals, including the contextual edges to incorporate the contextual information for each proposal instance (e.g., detecting p 1 by accessing p 2 and p 3 in <ref type="figure" target="#fig_0">Figure 1</ref>) and the surrounding edges to query knowledge from nearby but distinct proposals (e.g., querying p 4 for p 1 in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>We then perform graph convolutions on the constructed graph. Although the information is aggregated from local neighbors in each layer, message passing between distant nodes is still possible if the depth of GCNs increases. Be-sides, we conduct two different GCNs to perform classification and regression separately, which is demonstrated to be effective by our experiments. Moreover, to avoid the overwhelming computation cost, we further devise a sampling strategy to train the GCNs efficiently while still preserving desired detection performance. We evaluate our proposed method on two popular benchmarks for temporal action detection, i.e., THUMOS14 <ref type="bibr" target="#b23">[24]</ref> and AcitivityNet1.3 <ref type="bibr" target="#b3">[4]</ref>.</p><p>To sum up, our contributions are as follow:</p><p>• To the best of our knowledge, we are the first to exploit the proposal-proposal relations for temporal action localization in videos.</p><p>• To model the interactions between proposals, we construct a graph of proposals by establishing the edges based on our valuable observations and then apply GCNs to do message aggregation among proposals.</p><p>• We have verified the effectiveness of our proposed method on two benchmarks. On THUMOS14 especially, our method obtains the mAP of 49.1% when tIoU = 0.5, which significantly outperforms the stateof-the-art, i.e. 42.8% by <ref type="bibr" target="#b5">[6]</ref>. Augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Temporal action localization. Recently, great progress has been achieved in deep learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53]</ref>, which facilitates the development of temporal action localization. Approaches on this task can be grouped into three categories:</p><p>(1) methods performing frame or segment-level classification where the smoothing and merging steps are required to obtain the temporal boundaries <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref>; (2) approaches employing a two-stage framework involving proposal generation, classification and boundary refinement <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>;</p><p>(3) methods developing end-to-end architectures integrating the proposal generation and classification <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26]</ref>. Our work is built upon the second category where the action proposals are first generated and then used to perform classification and boundary regression. Following this paradigm, Shou et al. <ref type="bibr" target="#b33">[34]</ref> propose to generate proposals from sliding windows and classify them. Xu et al. <ref type="bibr" target="#b45">[46]</ref> exploit the 3D ConvNet and propose a framework inspired by Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>. The above methods neglect the context information of proposals, and hence some attempts have been developed to incorporate the context to enhance the proposal feature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b5">6]</ref>. They show encouraging improvements by extracting features on the extended receptive field (i.e., boundary) of the proposal. Despite their success, they all process each proposal individually. In contrast, our method has considered the proposal-proposal interactions and leveraged the relations between proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Layer 1 Layer 2 GCN2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Layer 1 Layer 2 <ref type="figure">Figure 2</ref>. Schematic of our P-GCN model. Given a set of proposals from the input untrimmed video, we instantiate the nodes in the graph by each proposal. Then, edges are established among nodes to model the relations between proposals. We employ two separate GCNs on the same constructed graph with different input features (i.e., the original feature and the extended feature). Finally, P-GCN model outputs the predicted action category, completeness and boundary regression results for all proposals simultaneously.</p><p>Graph Convolutional Networks. Kipf et al. <ref type="bibr" target="#b24">[25]</ref> propose the Graph Convolutional Networks (GCNs) to define convolutions on the non-grid structures <ref type="bibr" target="#b36">[37]</ref>. Thanks to its effectiveness, GCNs have been successfully applied to several research areas in computer vision, such as skeleton-based action recognition <ref type="bibr" target="#b46">[47]</ref>, person re-identification <ref type="bibr" target="#b31">[32]</ref>, and video classification <ref type="bibr" target="#b44">[45]</ref>. For real-world applications, the graph can be large and directly using GCNs is inefficient. Therefore, several attempts are posed for efficient training by virtue of the sampling strategy, such as the node-wise method SAGE <ref type="bibr" target="#b19">[20]</ref>, layer-wise model FastGCN <ref type="bibr" target="#b6">[7]</ref> and its layer-dependent variant AS-GCN <ref type="bibr" target="#b22">[23]</ref>. In this paper, considering the flexibility and implementability, we adopt SAGE method as the sampling strategy in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation and Preliminaries</head><p>We denote an untrimmed video as V = {I t ∈ R H×W ×3 } T t=1 , where I t denotes the frame at the time slot t with height H and width W . Within each video V , let</p><formula xml:id="formula_0">P = {p i | p i = (x i , (t i,s , t i,e ))} N i=1</formula><p>be the action proposals of interest, with t i,s and t i,e being the start and end time of a proposal, respectively. In addition, given proposal p i , let x i ∈ R d be the feature vector extracted by certain feature extractor (e.g., the I3D network <ref type="bibr" target="#b4">[5]</ref>) from frames between I ti,s and I ti,e .</p><p>Let G(V, E) be a graph of N nodes with nodes v i ∈ V and edge e ij = (v i , v j ) ∈ E. Furthermore, let A ∈ R N ×N be the adjacency matrix associated with G. In this paper, we seek to exploit graphs G(P, E) on action proposals in P to better model the proposal-proposal interactions in videos. Here, each action proposal is treated as a node and the edges in E are used to represent the relations between proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">General Scheme of Our Approach</head><p>In this paper, we use a proposal graph G(P, E) to present the relations between proposals and then apply GCN on the graph to exploit the relations and learn powerful representations for proposals. The intuition behind applying GCN is that when performing graph convolution, each node aggregates information from its neighborhoods. In this way, the feature of each proposal is enhanced by other proposals, which helps boost the detection performance eventually.</p><p>Without loss of generality, we assume the action proposals have been obtained beforehand by some methods (e.g., the TAG method in <ref type="bibr" target="#b51">[52]</ref>). In this paper, given an input video V , we seek to predict the action categoryŷ i and temporal position (t i,s ,t i,e ) for each proposal p i by exploiting proposal relations. Formally, we compute</p><formula xml:id="formula_1">{(ŷ i , (t i,s ,t i,e ))} N i=1 = F (GCN({x i } N i=1 , G(P, E)),<label>(1)</label></formula><p>where F denotes any mapping functions to be learned. To exploit GCN for action localization, our paradigm takes both the proposal graph and the proposal features as input and perform graph convolution on the graph to leverage proposal relations. The enhanced proposal features (i.e., the outputs of GCN) are then used to jointly predict the category label and temporal bounding box. The schematic of our approach is shown in <ref type="figure">Figure 2</ref>. For simplicity, we denote our model as P-GCN henceforth. In the following sections, we aim to answer two questions: (1) how to construct a graph to represent the relations between proposals; (2) how to use GCN to learn representations of proposals based on the graph and facilitate the action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposal Graph Construction</head><p>For the graph G(P, E) of each video, the nodes are instantiated as the action proposals, while the edges E between proposals are demanded to be characterized specifically to better model the proposal relations.</p><p>One way to construct edges is linking all proposals with each other, which yet will bring in overwhelming computations for going through all proposal pairs. It also incurs redundant or noisy information for action localization, as some unrelated proposals should not be connected. In this paper, we devise a smarter approach by exploiting the temporal relevance/distance between proposals instead. Specifically, we introduce two types of edges, the contextual edges and surrounding edges, respectively. Contextual Edges. We establish an edge between proposal p i and p j if r(p i , p j ) &gt; θ ctx , where θ ctx is a certain threshold. Here, r(p i , p j ) represents the relevance between proposals and is defined by the tIoU metric, i.e.,</p><formula xml:id="formula_2">r(p i , p j ) = tIoU (p i , p j ) = I(p i , p j ) U (p i , p j ) ,<label>(2)</label></formula><p>where I(p i , p j ) and U (p i , p j ) compute the temporal intersection and union of the two proposals, respectively. If we focus on the proposal p i , establishing the edges by computing r(p i , p j ) &gt; θ ctx will select its neighbourhoods as those have high overlaps with it. Obviously, the non-overlapping portions of the highly-overlapping neighbourhoods are able to provide rich contextual information for p i . As already demonstrated in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>, exploring the contextual information is of great help in refining the detection boundary and increasing the detection accuracy eventually. Here, by our contextual edges, all overlapping proposals automatically share the contextual information with each other, and these information are further processed by the graph convolution. Surrounding Edges. The contextual edges connect the overlapping proposals that usually correspond to the same action instance. Actually, distinct but nearby actions (including the background items) could also be correlated, and the message passing among them will facilitate the detection of each other. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, the background proposal p 4 will provide a guidance on identifying the action class of proposal p 1 (e.g., more likely to be sport action). To handle such kind of correlations, we first utilize r(p i , p j ) = 0 to query the distinct proposals, and then compute the following distance</p><formula xml:id="formula_3">d(p i , p j ) = |c i − c j | U (p i , p j ) ,<label>(3)</label></formula><p>to add the edges between nearby proposals if d(p i , p j ) &lt; θ sur , where θ sur is a certain threshold. In Eq. (3), c i (or c j ) represents the center coordinate of p i (or p j ). As a complement of the contextual edges, the surrounding edges en-able the message to pass across distinct action instances and thereby provides more temporal cues for the detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph Convolution for Action Localization</head><p>Given the constructed graph, we apply the GCN to do action localization. We build K-layer graph convolutions in our implementation. Specifically for the k-th layer (1 ≤ k ≤ K), the graph convolution is implemented by</p><formula xml:id="formula_4">X (k) = AX (k−1) W (k) .<label>(4)</label></formula><p>Here, A is the adjacency matrix; W (k) ∈ R d k ×d k is the parameter matrix to be learned; X (k) ∈ R N ×d k are the hidden features for all proposals at layer k; X (0) ∈ R N ×d are the input features. We apply an activation function (i.e., ReLU) after each convolution layer before the features are forwarded to the next layer. In addition, our experiments find it more effective by further concatenating the hidden features with the input features in the last layer, namely,</p><formula xml:id="formula_5">X (K) := X (K) X (0) ,<label>(5)</label></formula><p>where denotes the concatenation operation.</p><p>Joining the previous work <ref type="bibr" target="#b51">[52]</ref>, we find that it is beneficial to predict the action label and temporal boundary separately by virtue of two GCNs-one conducted on the original proposal features x i and the other one on the extended proposal features x i . The first GCN is formulated as</p><formula xml:id="formula_6">{ŷ i } N i=1 = softmax(F C 1 (GCN 1 ({x i } N i=1 , G(P, E)))),<label>(6)</label></formula><p>where we apply a Fully-Connected (FC) layer with softmax operation on top of GCN 1 to predict the action label y i . The second GCN can be formulated as</p><formula xml:id="formula_7">{(t i,s ,t i,e )} N i=1 = F C 2 (GCN 2 ({x i } N i=1 , G(P, E))), (7) {ĉ i } N i=1 = F C 3 (GCN 2 ({x i } N i=1 , G(P, E))),<label>(8)</label></formula><p>where the graph structure G(P, E) is the same as that in Eq. (6) but the input proposal feature is different. The extended feature x i is attained by first extending the temporal boundary of p i with 1 2 of its length on both the left and right sides and then extracting the feature within the extended boundary. Here, we adopt two FC layers on top of GCN 2 , one for predicting the boundary (t i,s ,t i,e ) and the other one for predicting the completeness labelĉ i , which indicates whether the proposal is complete or not. It has been demonstrated by <ref type="bibr" target="#b51">[52]</ref> that, incomplete proposals that have low tIoU with the ground-truths could have high classification score, and thus it will make mistakes when using the classification score alone to rank the proposal for the mAP test; further applying the completeness score enables us to avoid this issue.</p><p>Adjacency Matrix. In Eq. (4), we need to compute the adjacency matrix A. Here, we design the adjacency matrix by assigning specific weights to edges. For example, we can apply the cosine similarity to estimate the weights of edge e ij by</p><formula xml:id="formula_8">A ij = x T i x j x i 2 · x j 2 .<label>(9)</label></formula><p>In the above computation, we compute A ij relying on the feature vector x i . We can also map the feature vectors into an embedding space using a learnable linear mapping function as in <ref type="bibr" target="#b43">[44]</ref> before the cosine computation. We leave the discussion in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Efficient Training by Sampling</head><p>Typical proposal generation methods usually produce thousands of proposals for each video. Applying the aforementioned graph convolution (Eq. (4)) on all proposals demands hefty computation and memory footprints. To accelerate the training of GCNs, several approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref> have been proposed based on neighbourhood sampling. Here, we adopt the SAGE method <ref type="bibr" target="#b19">[20]</ref> in our method for its flexibility.</p><p>The SAGE method uniformly samples the fixed-size neighbourhoods of each node layer-by-layer in a top-down passway. In other words, the nodes of the (k − 1)-th layer are formulated as the sampled neighbourhoods of the nodes in the k-th layer. After all nodes of all layers are sampled, SAGE performs the information aggregation in a bottom-up fashion. Here we specify the aggregation function to be a sampling form of Eq. (4), namely,</p><formula xml:id="formula_9">x (k) i =   1 N s Ns j=1 A ij x (k−1) j + x (k−1) i   W (k) ,<label>(10)</label></formula><p>where node j is sampled from the neighbourhoods of node i, i.e., j ∈ N (i); N s is the sampling size and is much less than the total number N . The summation in Eq. <ref type="formula" target="#formula_1">(10)</ref> is further normalized by N s , which empirically makes the training more stable. Besides, we also enforce the self addition of its feature for node i in Eq. (10). We do not perform any sampling when testing. For better readability, Algorithm 1 depicts the algorithmic Flow of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><formula xml:id="formula_10">= {p i | p i = (x i , (t i,s , t i,e ))} N i=1 ; original proposal features {x (0) i } N i=1 ; extended proposal features {x (0) i } N i=1</formula><p>; graph depth K; sampling size N s Parameter: Weight matrices W (k) , ∀k ∈ {1, . . . , K} 1: instantiate the nodes by the proposals p i , ∀p i ∈ P 2: establish edges between nodes 3: obtain a proposal graph G(P, E) 4: calculate adjacent matrix using Eq. (9) 5: while not converges do <ref type="bibr">6:</ref> for k = 1 . . . K do <ref type="bibr">7:</ref> for p ∈ P do perform boundary regression using Eq. <ref type="formula">(7)</ref> 14:</p><p>predict completeness {ĉ i } N i=1 using Eq. (8) 15: end while Output: Trained P-GCN model is challenging since each video has more than 15 action instances and its 71% frames are occupied by background items. Following the common setting in <ref type="bibr" target="#b23">[24]</ref>, we apply 200 videos in the validation set for training and conduct evaluation on the 213 annotated videos from the testing set.</p><p>ActivityNet <ref type="bibr" target="#b3">[4]</ref> is another popular benchmark for action localization on untrimmed videos. We evaluate our method on ActivityNet v1.3, which contains around 10K training videos and 5K validation videos corresponded to 200 different activities. Each video has an average of 1.65 action instances. Following the standard practice, we train our method on the training videos and test it on the validation videos. In our experiments, we contrast our method with the state-of-the-art methods on both THUMOS14 and Activi-tyNet v1.3, and perform ablation studies on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Evaluation Metrics. We use mean Average Precision (mAP) as the evaluation metric. A proposal is considered to be correct if its temporal IoU with the groundtruth instance is larger than a certain threshold and the predicted category is the same as this ground-truth instance. On THUMOS14, the tIOU thresholds are chosen from {0.1, 0.2, 0.3, 0.4, 0.5}; on ActivityNet v1.3, the IoU thresholds are from {0.5, 0.75, 0.95}, and we also report the average mAP of the IoU thresholds between 0.5 and 0.95 with the step of 0.05. Features and Proposals. Our model is implemented under the two-stream strategy <ref type="bibr" target="#b34">[35]</ref>: RGB frames and optical flow fields. We first uniformly divide each input video into 64frame segments. We then use a two-stream Inflated 3D Con-vNet (I3D) model pre-trained on Kinetics <ref type="bibr" target="#b4">[5]</ref> to extract the segment features. In detail, the I3D model takes as input the RGB/optical-flow segment and outputs a 1024-dimensional feature vector for each segment. Upon the I3D features, we further apply max pooling across segments to obtain one 1024-dimensional feature vector for each proposal that is obtained by the BSN method <ref type="bibr" target="#b26">[27]</ref>. Note that we do not finetune the parameters of the I3D model in our training phase. Besides the I3D features and BSN proposals, our ablation studies in § 5 also explore other types of features (e.g. 2-D features <ref type="bibr" target="#b26">[27]</ref>) and proposals (e.g. TAG proposals <ref type="bibr" target="#b51">[52]</ref>). Proposal Graph Construction. We construct the proposal graph by fixing the values of θ ctx as 0.7 and θ sur as 1 for both streams. More discussions on choosing the values of θ ctx and θ sur could be found in the supplementary material. We adopt 2-layer GCN since we observed no clear improvement with more than 2 layers but the model complexity is increased. For more efficiency, we choose N s = 4 in Eq. (10) for neighbourhood sampling unless otherwise specified.</p><p>Training. The initial learning rate is 0.001 for the RGB stream and 0.01 for the Flow stream. During training, the learning rates will be divided by 10 every 15 epochs. The dropout ratio is 0.8. The classificationŷ i and completenesŝ c i are trained with the cross-entropy loss and the hinge loss, respectively. The regression term (t i,s ,t i,e ) is trained with the smooth L 1 loss. More training details can be found in the supplementary material. Testing. We do not perform neighbourhood sampling (i.e. Eq. (10)) for testing. The predictions of the RGB and Flow steams are fused using a ratio of 2:3. We multiply the classification score with the completeness score as the final score for calculating mAP. We then use Non-Maximum Suppression (NMS) to obtain the final predicted temporal proposals for each action class separately. We use 600 and 100 proposals per video for computing mAPs on THUMOS14 and ActivityNet v1.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art results</head><p>THUMOS14. Our P-GCN model is compared with the state-of-the-art methods in <ref type="table" target="#tab_0">Table 1</ref>. The P-GCN model reaches the highest mAP over all thresholds, implying that our method can recognize and localize actions much more accurately than any other method. Particularly, our P-GCN model outperforms the previously best method (i.e. TAL-Net <ref type="bibr" target="#b5">[6]</ref>) by 6.3% absolute improvement and the second-best result <ref type="bibr" target="#b26">[27]</ref> by more than 12.2%, when tIoU = 0.5. ActivityNet v1.3. <ref type="table" target="#tab_7">Table 2</ref> reports the action localization results of various methods. Regarding the average mAP, P-GCN outperforms SSN <ref type="bibr" target="#b51">[52]</ref>, CDC <ref type="bibr" target="#b32">[33]</ref>, and TAL-Net <ref type="bibr" target="#b5">[6]</ref> by 3.01%, 3.19%, and 6.77%, respectively. We observe that the method by Lin et al. <ref type="bibr" target="#b26">[27]</ref> (called LIN below) performs promisingly on this dataset. Note that LIN is originally designed for generating class-agnostic proposals, and thus relies on external video-level action labels (from Untrimmed-Net <ref type="bibr" target="#b40">[41]</ref>) for action localization. In contrast, our method is self-contained and is able to perform action localization without any external label. Actually, P-GCN can still be modified to take external labels into account. To achieve this, we assign the top-2 video-level classes predicted by UntrimmedNet to all the proposals in that video. We provide more details about how to involve external labels in P-GCN in the supplementary material. As summarized in   <ref type="table" target="#tab_7">Table 2</ref>, our enhanced version P-GCN* consistently outperforms LIN, hence demonstrating the effectiveness of our method under the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>In this section, we will perform complete and in-depth ablation studies to evaluate the impact of each component of our model. More details about the structures of baseline methods (such as MLP and MP) can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">How do the proposal-proposal relations help?</head><p>As illustrated in § 3.4, we apply two GCNs for action classification and boundary regression separately. Here, we implement the baseline with a 2-layer MultiLayer-Perceptron (MLP). The MLP baseline shares the same structure as GCN except that we remove the adjacent matrix A in Eq. (4). To be specific, for the k-th layer, the propagation in Eq. (4) becomes X k = X k−1 W k , where W k are the trainable parameters. Without using A, MLP processes each proposal feature independently. By comparing the performance of MLP with GCN, we can justify the importance of message passing along proposals. To do so, we replace each GCN with an MLP and have the following variants of our model including: (1) MLP 1 + GCN 2 where GCN 1 is replaced; (2) GCN 1 + MLP 2 where GCN 2 is replaced; and (3) MLP 1 + MLP 2 where both GCNs are replaced. Table 3 reads that all these variants decrease the performance of our model, thus verifying the effectiveness of GCNs for both action classification and boundary regression. Overall, our model P-GCN significantly outperforms the MLP protocol (i.e. MLP 1 + MLP 2 ), validating the importance of considering proposal-proposal relations in temporal action localization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">How does the graph convolution help?</head><p>Besides graph convolutions, performing mean pooling among proposal features is another way to enable information dissemination between proposals. We thus conduct another baseline by first adopting MLP on the proposal features and then conducting mean pooling on the output of MLP over adjacent proposals. The adjacent connections are formulated by using the same graph as GCN. We term this baseline as MP below. Similar to the setting in § 5.1, we have three variants of our model including: (1) MP 1 + MP 2 ; (2) MP 1 + GCN 2 ; and (3) GCN 1 + MP 2 . We report the results in <ref type="table" target="#tab_7">Table 4</ref>. Our P-GCN outperforms all MP variants, demonstrating the superiority of graph convolution over mean pooling on capturing between-proposal connections. The protocol MP 1 + MP 2 in <ref type="table" target="#tab_7">Table 4</ref> performs better than MLP 1 + MLP 2 in <ref type="table" target="#tab_2">Table 3</ref>, which again reveals the benefit of modeling the proposal-proposal relations, even we pursue it using the naive mean pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Influences of different backbones</head><p>Our framework is general and compatible with different backbones (i.e., proposals and features). Beside the backbones applied above, we further perform experiments on TAG proposals <ref type="bibr" target="#b51">[52]</ref> and 2D features <ref type="bibr" target="#b26">[27]</ref>. We try different combinations: (1) BSN+I3D; (2) BSN+2D; (3) TAG+I3D; (4) TAG+2D, and report the results of MLP and P-GCN in <ref type="figure" target="#fig_3">Figure 3</ref>. In comparison with MLP, our P-GCN leads to significant and consistent improvements in all types of features and proposals. These results conclude that, our method is generally effective and is not limited to the specific feature or proposal type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The weights of edge and self-addition</head><p>We have defined the weights of edges in Eq. (9), where the cosine similarity (cos-sim) is applied. This similarity can be further extended by first embedding the features before the cosine computation. We call the embedded version as embed-cos-sim, and compare it with cos-sim in <ref type="table" target="#tab_7">Table 5</ref>. No obvious improvement is attained by replacing cos-sim with embed-cos-sim (the mAP difference between them is less than 0.4%). Eq. (10) has considered the self-addition of the node feature. We also investigate the importance of this term in <ref type="table" target="#tab_7">Table 5</ref>. It suggests that the self-addition leads to at least 1.7% absolute improvements on both RGB and Flow streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Is it necessary to consider two types of edges?</head><p>To evaluate the necessity of formulating two types of edges, we perform experiments on two variants of our P-GCN, each of which considers only one type of edge in the graph construction stage. As expected, the result in <ref type="table" target="#tab_3">Table 6</ref> drops remarkably when either kind of edge is removed. Another crucial point is that our P-GCN still boosts MLP when only the surrounding edges are remained. The rationale behind this could be that, actions in the same video are correlated and exploiting the surrounding relation will enable more accurate action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">The efficiency of our sampling strategy</head><p>We train P-GCN efficiently based on the neighbourhood sampling in Eq. <ref type="bibr" target="#b9">(10)</ref>. Here, we are interested in how the sampling size N s affects the final performance. <ref type="table" target="#tab_7">Table 7</ref> reports the testing mAPs corresponded to different N s varying from 1 to 5 (and also 10). The training time per iteration is also added in <ref type="table" target="#tab_7">Table 7</ref>. We observe that when N s = 4 the model achieves higher mAP than the full model (i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>High Jump  <ref type="figure">Figure 4</ref>. Qualitative results on THUMOS14 dataset. N s = 10) while reducing 76% of training time for each iteration. This is interesting, as sampling fewer nodes even yields better results. We conjecture that, the neighbourhood sampling could bring in more stochasticity and guide our model to escape from the local minimal during training, thus delivering better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Qualitative Results</head><p>Given the significant improvements, we also attempt to find out in what cases our P-GCN model improves over MLP. We visualize the qualitative results on THUMOS14 in <ref type="figure">Figure 4</ref>. In the top example, both MLP and our P-GCN model are able to predict the action category correctly, while P-GCN predicts a more precise temporal boundary. In the bottom example, due to similar action characteristic and context, MLP predicts the action of "Shotput" as "Throw Discus". Despite such challenge, P-GCN still correctly predicts the action category, demonstrating the effectiveness of our method. More qualitative results could be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have exploited the proposal-proposal interaction to tackle the task of temporal action localization. By constructing a graph of proposals and applying GCNs to message passing, our P-GCN model outperforms the stateof-the-art methods by a large margin on two benchmarks, i.e., THUMOS14 and ActivithNet v1.3. It would be interesting to extend our P-GCN for object detection in image and we leave it for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proposal Features</head><p>We have two types of proposal features and the process of feature extraction is shown in <ref type="figure" target="#fig_4">Figure A</ref>. Proposal features. For the original proposal, we first obtain a set of segment-level features within the proposal and then apply max-pooling across segments to obtain one 1024dimensional feature vector. Extended proposal features. The boundary of the original proposal is extended with 1 2 of its length on both the left and right sides, resulting in the extended proposal. Thus, the extended proposal has three portions: start, center and end. For each portion, we follow the same feature extraction process as the original proposal. Finally, the extended proposal feature is obtained by concatenating the feature of three portions. B. Network Architectures P-GCN. The network architecture of our P-GCN model is shown in <ref type="figure">Figure B</ref>. Let N and N class be the number of proposals in one video and the total number of action categories, respectively. On the top of GCN, we have three fully-connected (FC) layers for different purposes. The one with N class × 2 outputs is for boundary regression and the other two with N class outputs are designed for action classification and completeness classification, respectively. MLP baseline. The network architecture of MLP baseline is shown in <ref type="figure" target="#fig_5">Figure C</ref>. We replace each of GCNs with a 2layer multilayer perceptron (MLP). We set the number of parameters in MLP the same as GCN's for a fair compari- ReLu ReLu son. Note that MLP processes each proposal independently without exploiting the relations between proposals. Mean-Pooling baseline. As shown in <ref type="figure" target="#fig_6">Figure D</ref>, the network architecture of Mean-Pooling baseline is the same as the MLP baseline's except that we conduct mean-pooling on the output of MLP over the adjacent proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>We have three types of training samples chosen by two criteria, i.e., the best tIoU and best overlap. For each proposal, we calculate its tIoU with all the ground truth in that video and choose the largest tIoU as the best tIoU (similarly for best overlap). For simplicity, we denote the best tIoU and best overlap as tIoU and OL. Then, three types of training samples can be described as: <ref type="bibr" target="#b0">(1)</ref>   (3) Background sample: tIoU ≤ θ 4 . These certain thresholds are slightly different on two datasets as shown in <ref type="table" target="#tab_7">Table  A</ref>. We consider all foreground proposals as the complete proposals. For more efficiency, we fix the number of neighborhoods for each node to be 10 by selecting contextual edges with the largest relevance scores and surrounding edges with the smallest distances, where the ratio of contextual and surrounding edges is set to 4:1.</p><p>In addition, we empirically found that setting A i,j to 0 (when A i,j &lt; 0) leads to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss function</head><p>Multi-task Loss. Our P-GCN model can not only predict action category but also refine the proposals temporal boundary by location regression. With the action classifier, completeness classifier and location regressors, we define a multi-task loss by:</p><formula xml:id="formula_11">L = i L cls (y i ,ŷ i ) + λ 1 i [y i ≥ 1, e i = 1]L reg (o i ,ô i ) + λ 2 i [y i ≥ 1]L com (e i ,ĉ i ),<label>(11)</label></formula><p>whereŷ i and y i ∈ {0, . . . , N class } is the predicted probability and ground truth action label of the i-th proposal in a mini-batch, respectively. Here, 0 represents the background class. e i is the completeness label.ô i and o i are the predicted and ground truth offset, which will be detailed below. In all experiments, we set λ 1 = λ 2 = 0.5. Completeness Loss. Here, the completeness term L com is used only when y i ≥ 1, i.e., the proposal is not considered as part of the background. Regression Loss. We devise a set of location regressors {R m } N class m=1 , each for an activity category. For a proposal, we regress the boundary using the closest ground truth instance as the target. Our P-GCN model does not predict the start time and end time of each proposal directly. Instead, it predicts the offsetô i = (ô i,c ,ô i,l ) relative to the proposal, whereô i,c andô i,l are the offset of center coordinate and length, respectively. The ground truth offset is denoted as o i = (o i,c , o i,l ) and parameterized by:</p><formula xml:id="formula_12">o i,c = (c i − c gt )/l i , o i,l = log(l i /l gt ),<label>(12)</label></formula><p>where c i and l i denote the original center coordinate and length of the proposal, respectively. c gt and l gt account for the center coordinate and length of the closest ground truth, respectively. L reg is the smooth L1 loss and used when y i ≥ 1 and e i = 1, i.e., the proposal is a foreground sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Details of Augmentation Experiments on ActivityNet</head><p>Our P-GCN model can be further augmented by taking the external video-level labels into account. To achieve this, we replace the predicted action classes in Eq. (6) with the external action labels. Specifically, given an input video, we use UntrimmedNet to predict the top-2 video-level classes and assign these classes to all the proposals in this video. In this way, each proposal has two action classes.</p><p>To further compute mAP, the score of each proposal is required. In our implementation, we follow the settings in BSN by calculating s prop = s act * s com * s bsn * s unet ,</p><p>where s act and s com are the action score and completeness score associated with the action class. s bsn represents the confidence score produced by BSN and s unet denotes for the action score predicted by UntrimmedNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Explanation and ablation study of θ ctx</head><p>The parameter θ ctx is a threshold value for constructing contextual edges, i.e. r(p i , p i ) &gt; θ ctx . Since r(p i , p i ) ∈ [0, 1], θ ctx can be chosen from [0, 1). An ablation study is shown in <ref type="table" target="#tab_7">Table B</ref>. Our method performs well when θ ctx = 0.7, 0.8, 0.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablation study of boundary regression</head><p>We conducted an ablation study on boundary regression in <ref type="table" target="#tab_7">Table C</ref>, whose results validate the necessity of using boundary regression. H. Additional runtime compared to <ref type="bibr" target="#b51">[52]</ref> The MLP baseline is indeed a particular implementation of <ref type="bibr" target="#b51">[52]</ref>, and it shares the same amount of parameters with our P-GCN. We compare the runtime between P-GCN and MLP in <ref type="table" target="#tab_7">Table D</ref>. It reads that GCN only incurs a relatively small additional runtime compared to MLP but is able to boost the performance significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Schematic depiction of our approach. We apply graph convolutional networks to model the proposal-proposal interactions and boost the temporal action localization performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>24 ]</head><label>24</label><figDesc>is a standard benchmark for action localization. Its training set known as the UCF-101 dataset consists of 13320 videos. The validation, testing and background set contain 1010, 1574 and 2500 untrimmed videos, respectively. Performing action localization on this dataset Algorithm 1 The training process of P-GCN model. Input: Proposal set P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>action categories {ŷ i } N i=1 using Eq. (6) 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Action localization results on THUMOS14 with different backbones, measured by mAP@tIoU=0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A .</head><label>A</label><figDesc>The illustration of (extended) proposal feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure C .</head><label>C</label><figDesc>The network architecture of the MLP baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure D .</head><label>D</label><figDesc>The network architecture of the Mean-Pooling baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Action localization results on THUMOS14, measured by mAP (%) at different tIoU thresholds α.</figDesc><table><row><cell>tIoU</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>Oneata et al. [29]</cell><cell cols="5">36.6 33.6 27.0 20.8 14.4</cell></row><row><cell>Wang et al. [40]</cell><cell cols="5">18.2 17.0 14.0 11.7 8.3</cell></row><row><cell>Caba et al. [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.5</cell></row><row><cell cols="6">Richard et al. [31] 39.7 35.7 30.0 23.2 15.2</cell></row><row><cell>Shou et al. [34]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell>Yeung et al. [48]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell></row><row><cell>Yuan et al. [49]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell>Escorcia et al. [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.9</cell></row><row><cell>Buch et al. [2]</cell><cell>-</cell><cell>-</cell><cell>37.8</cell><cell>-</cell><cell>23.0</cell></row><row><cell>Shou et al. [33]</cell><cell>-</cell><cell>-</cell><cell cols="3">40.1 29.4 23.3</cell></row><row><cell>Yuan et al. [50]</cell><cell cols="5">51.0 45.2 36.5 27.8 17.8</cell></row><row><cell>Buch et al. [1]</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell></row><row><cell>Gao et al. [18]</cell><cell cols="5">60.1 56.7 50.1 41.3 31.0</cell></row><row><cell>Hou et al. [21]</cell><cell>51.3</cell><cell>-</cell><cell>43.7</cell><cell>-</cell><cell>22.0</cell></row><row><cell>Dai et al. [8]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.3 25.6</cell></row><row><cell>Gao et al. [17]</cell><cell cols="5">54.0 50.9 44.1 34.9 25.6</cell></row><row><cell>Xu et al. [46]</cell><cell cols="5">54.5 51.5 44.8 35.6 28.9</cell></row><row><cell>Zhao et al. [52]</cell><cell cols="5">66.0 59.4 51.9 41.0 29.8</cell></row><row><cell>Lin et al. [27]</cell><cell>-</cell><cell>-</cell><cell cols="3">53.5 45.0 36.9</cell></row><row><cell>Chao et al. [6]</cell><cell cols="5">59.8 57.1 53.2 48.5 42.8</cell></row><row><cell>P-GCN</cell><cell cols="5">69.5 67.8 63.6 57.8 49.1</cell></row><row><cell>tIoU</cell><cell>0.5</cell><cell cols="4">0.75 0.95 Average</cell></row><row><cell cols="2">Singh et al. [36] 34.47</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Wang et al. [43] 43.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="4">Shou et al. [33] 45.30 26.00 0.20</cell><cell cols="2">23.80</cell></row><row><cell>Dai et al. [8]</cell><cell cols="3">36.44 21.15 3.90</cell><cell>-</cell><cell></cell></row><row><cell>Xu et al. [46]</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="4">Zhao et al. [52] 39.12 23.48 5.49</cell><cell cols="2">23.98</cell></row><row><cell>Chao et al. [6]</cell><cell cols="3">38.23 18.30 1.30</cell><cell cols="2">20.22</cell></row><row><cell>P-GCN</cell><cell cols="3">42.90 28.14 2.47</cell><cell cols="2">26.99</cell></row><row><cell cols="4">Lin et al. [27] * 46.45 29.96 8.02</cell><cell cols="2">30.03</cell></row><row><cell>P-GCN*</cell><cell cols="3">48.26 33.16 3.27</cell><cell cols="2">31.11</cell></row></table><note>Table 2. Action localization results on ActivityNet v1.3 (val), mea- sured by mAP (%) at different tIoU thresholds and the average mAP of IoU thresholds from 0.5 to 0.95. (*) indicates the method that uses the external video labels from UntrimmedNet [41].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison between our P-GCN model and MLP on THUMOS14, measured by mAP (%). + GCN 2 35.94 1.19 44.59 0.91 GCN 1 + MLP 2 35.82 1.07 45.26 1.58 P-GCN (GCN 1 + GCN 2 ) 37.27 2.52 46.53 2.85 GCN 1 + GCN 2 ) 37.27 1.95 46.53 2.56</figDesc><table><row><cell>mAP@tIoU=0.5</cell><cell cols="4">RGB Gain Flow Gain</cell></row><row><cell>MLP 1 + MLP 2</cell><cell>34.75</cell><cell>-</cell><cell>43.68</cell><cell>-</cell></row><row><cell cols="5">MLP 1 Table 4. Comparison between our P-GCN model and mean-</cell></row><row><cell cols="4">pooling (MP) on THUMOS14, measured by mAP (%).</cell><cell></cell></row><row><cell>mAP@tIoU=0.5</cell><cell cols="4">RGB Gain Flow Gain</cell></row><row><cell>MP 1 + MP 2</cell><cell>35.32</cell><cell>-</cell><cell>43.97</cell><cell>-</cell></row><row><cell>MP 1 + GCN 2</cell><cell cols="4">36.50 1.18 45.78 1.81</cell></row><row><cell>GCN 1 + MP 2</cell><cell cols="4">36.22 0.90 44.42 0.45</cell></row><row><cell cols="5">P-GCN (Table 5. Comparison of different types of edge functions on THU-</cell></row><row><cell>MOS14, measured by mAP (%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP@tIoU=0.5</cell><cell></cell><cell cols="2">RGB Flow</cell><cell></cell></row><row><cell>MLP</cell><cell></cell><cell cols="2">34.75 43.68</cell><cell></cell></row><row><cell>P-GCN(cos-sim)</cell><cell></cell><cell cols="2">35.55 44.83</cell><cell></cell></row><row><cell cols="2">P-GCN(cos-sim, self-add)</cell><cell cols="2">37.27 46.53</cell><cell></cell></row><row><cell cols="4">P-GCN(embed-cos-sim, self-add) 36.81 46.89</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison of two types of edge on THUMOS14, measured by mAP (%).</figDesc><table><row><cell cols="2">mAP@tIoU=0.5</cell><cell></cell><cell cols="4">RGB Gain Flow Gain</cell></row><row><cell cols="4">w/ both edges (P-GCN) 37.27</cell><cell>-</cell><cell>46.53</cell><cell>-</cell></row><row><cell cols="3">w/o surrounding edges</cell><cell cols="4">35.84 -1.43 45.89 -0.64</cell></row><row><cell cols="3">w/o contextual edges</cell><cell cols="4">36.81 -0.46 45.57 -0.96</cell></row><row><cell cols="3">w/o both edges (MLP)</cell><cell cols="4">34.75 -2.52 43.68 -2.85</cell></row><row><cell cols="7">Table 7. Comparison of different sampling size and training time</cell></row><row><cell cols="7">for each iteration on THUMOS14, measured by mAP@tIoU=0.5.</cell></row><row><cell>N s</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>10</cell></row><row><cell>RGB</cell><cell cols="6">36.0 36.92 35.68 37.27 36.11 36.37</cell></row><row><cell>Flow</cell><cell cols="6">46.15 45.06 45.13 46.53 46.28 46.14</cell></row><row><cell cols="7">Time(s) 0.10 0.23 0.33 0.41 0.48 1.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Foreground sample: tIoU ≥ θ 1 ; (2) Incomplete sample: OL ≥ θ 2 , tIoU ≤ θ 3 ;</figDesc><table><row><cell>Original Proposal</cell><cell cols="3">Extended Proposal</cell></row><row><cell>Feature</cell><cell></cell><cell>Feature</cell><cell></cell></row><row><cell>× 1024</cell><cell></cell><cell cols="2">× 3072</cell></row><row><cell>FC, 512</cell><cell></cell><cell cols="2">FC, 512</cell></row><row><cell>× 512</cell><cell></cell><cell></cell><cell>× 512</cell></row><row><cell>ReLu</cell><cell></cell><cell>ReLu</cell><cell></cell></row><row><cell>FC, 1024</cell><cell></cell><cell cols="2">FC, 3072</cell></row><row><cell>× 1024</cell><cell></cell><cell></cell><cell>× 3072</cell></row><row><cell>ReLu</cell><cell></cell><cell>ReLu</cell><cell>× 3072</cell></row><row><cell>Mean-Pooling</cell><cell></cell><cell cols="2">Mean-Pooling</cell></row><row><cell>Concatenate</cell><cell></cell><cell cols="2">Concatenate</cell></row><row><cell>× 2048</cell><cell></cell><cell></cell><cell cols="2">× 6144</cell></row><row><cell>FC,</cell><cell>FC,</cell><cell cols="2">FC,</cell><cell>2</cell></row><row><cell>×</cell><cell>×</cell><cell></cell><cell>×</cell><cell>×2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>The thresholds on different datasets.</figDesc><table><row><cell>Dataset</cell><cell>θ 1</cell><cell>θ 2</cell><cell>θ 3</cell><cell>θ 4</cell></row><row><cell>THUMOS14</cell><cell cols="3">0.7 0.7 0.3</cell><cell>0</cell></row><row><cell cols="5">ActivityNet v1.3 0.7 0.7 0.6 0.1</cell></row><row><cell cols="5">Each mini-batch contains examples sampled from a sin-</cell></row><row><cell cols="5">gle video. The ratio of three types of samples is fixed to</cell></row><row><cell cols="5">(1):(2):(3)=1:6:1. We set the mini-batch size to 32 on THU-</cell></row><row><cell cols="3">MOS14 and 64 on ActivityNet v1.3.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B .</head><label>B</label><figDesc>Results on THUMOS14 (Flow) with different θctx. .5) 45.31 45.29 45.37 45.61 45.65 45.82 45.79 46.53 46.64 46.45</figDesc><table><row><cell>θctx</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell>mAP(tIoU=0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table C .</head><label>C</label><figDesc>Ablation results of boundary regression on THUMOS14.Table D. Runtime/computation complexity in FLOPs/action localization mAP on THUMOS14. We train each model with 200 iterations on a Titan X GPU and report the average processing time per video per iteration (note that proposal generation and feature extraction are excluded for each model). For P-GCN, we choose the number of sampling neighbourhoods as Ns = 4.</figDesc><table><row><cell cols="2">mAP@tIoU=0.5</cell><cell></cell><cell cols="2">RGB Flow</cell></row><row><cell cols="5">without boundary regression 36.4 45.4</cell></row><row><cell cols="3">with boundary regression</cell><cell cols="2">37.3 46.5</cell></row><row><cell>Method</cell><cell cols="2">Runtime FLOPs</cell><cell cols="2">mAP@tIoU=0.5 RGB Flow</cell></row><row><cell>MLP baseline</cell><cell>0.376s</cell><cell cols="2">16.57M 34.8</cell><cell>43.7</cell></row><row><cell>P-GCN</cell><cell>0.404s</cell><cell cols="2">17.70M 37.3</cell><cell>46.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially supported by National Natural Science Foundation of China (NSFC) 61602185, 61836003 (key project), Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, Guangdong Provincial Scientific and Technological Funds under Grants 2018B010107001, and Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201902).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7746" to="7755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised dense event captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3059" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="77" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auto-embedding generative adversarial networks for high resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time temporal action localization in untrimmed videos by subaction discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<title level="m">Temporal activity detection in untrimmed videos with recurrent neural networks. 1st NIPS Workshop on Large Scale Computer Vision Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ActivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning graph structure for multi-label image classification via clique generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4100" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Uts at activitynet 2016. Ac-tivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discrimination-aware channel pruning for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangwei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
