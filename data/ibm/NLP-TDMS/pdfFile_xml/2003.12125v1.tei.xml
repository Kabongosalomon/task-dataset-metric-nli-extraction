<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SaccadeNet: A Fast and Accurate Object Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
							<email>sylan@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ganghua@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SaccadeNet: A Fast and Accurate Object Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection is an essential step towards holistic scene understanding. Most existing object detection algorithms attend to certain object areas once and then predict the object locations. However, neuroscientists have revealed that humans do not look at the scene in fixed steadiness. Instead, human eyes move around, locating informative parts to understand the object location. This active perceiving movement process is called saccade.</p><p>Inspired by such mechanism, we propose a fast and accurate object detector called SaccadeNet. It contains four main modules, the Center Attentive Module, the Corner Attentive Module, the Attention Transitive Module, and the Aggregation Attentive Module, which allows it to attend to different informative object keypoints, and predict object locations from coarse to fine. The Corner Attentive Module is used only during training to extract more informative corner features which brings free-lunch performance boost. On the MS COCO dataset, we achieve the performance of 40.4% mAP at 28 FPS and 30.5% mAP at 118 FPS. Among all the real-time object detectors, our SaccadeNet achieves the best detection performance, which demonstrates the effectiveness of the proposed detection mechanism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human visual system is accurate and fast. As the first gate to perceive the physical world, our visual system glances at a scene and immediately understands what objects are there and where they are. This efficient and effective vision system enables human to perceive the visual world with little conscious thought. In machine intelligence, similarly a fast and accurate object detector is essential, which can allow machines to perceive the physical world efficiently and effectively, and unlock subsequent processes such as understanding the holistic scene and interacting within it. * This work was done when Shiyi Lan was a research intern at Wormpex AI Research.  <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>. Best viewed in color.</p><p>Many recent algorithms have been proposed to advance object detection. On the one hand, anchor-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref> proposed to pre-define a large amount of anchor locations, and then either directly regress object bounding box locations, or generate region proposals based on anchors and decide whether each region contains a certain object category. These methods usually achieve competitive performance since they aggregate detailed image features within each region. However, the time-consuming region proposal stage is an bottleneck of inference speed.</p><p>On the other hand, researchers proposed anchor-free detectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref>. This type of methods proposed to directly regress object locations by utilizing features at certain pre-defined object keypoints, either in the object center or on the bounding box edges. Most edge keypoints based methods are not fast because of the time-consuming grouping process that combines multiple detected keypoints to form a single object bounding box. The recent proposed center keypoint based detectors <ref type="bibr" target="#b28">[29]</ref> avoid the complex grouping process and run much faster.</p><p>Most existing object detection algorithms steadily attend to certain object areas only once and then predict the object locations. During this one time of scanning for objects, different algorithms attend to different areas, either to the anchor boxes, to the proposed object regions, to the center keypoint, or to the edge keypoints. However, neuroscientists have revealed that <ref type="bibr" target="#b3">[4]</ref>, to understand an object's location, human do not look at the scene steadily. Instead, our eyes move around, locating informative parts to understand the object location.</p><p>Inspired by such mechanism, we propose a fast and accurate object detector, named SaccadeNet, which effectively attends to informative object keypoints, and predicts object locations from coarse to fine. Our SaccadeNet contains four main modules: the Center Attentive Module, the Corner Attentive Module, the Attention Transitive Module, and the Aggregation Attentive Module. The Center Attentive Module predicts the object center location and category. Meanwhile, for each predicted object center, Attention Transitive Module is used to predict the rough location of corresponding bounding box corners. To extract informative corner features, the Corner Attentive Module is used to enforce the CNN backbone to pay more attention to object boundaries, so that the regressed bounding boxes are more accurate. Finally, the Aggregation Attentive Module utilizes the features aggregated from both the center and the corners to refine the object bounding boxes.</p><p>SaccadeNet adopts multiple object keypoints including the center point and the corners, which encode and extract multiple levels of rich-detailed objects features. Moreover, it barely has speed loss comparing to the fastest center keypoint based detectors, since we predict object center and its corresponding corners jointly. Thus we do not need a grouping algorithm to combine them. Extensive experiments on the PASCAL VOC and MS COCO datasets have shown that SaccadeNet is fast and accurate. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, on COCO dataset when using ResNet-18 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> as the backbone SaccadeNet achieves mAP of 30.5% at 118 FPS. With DLA-34 <ref type="bibr" target="#b27">[28]</ref>, SaccadeNet achieves 40.4% mAP at 28 FPS, which is much better than other real-time detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Modern object detectors can be roughly divided into two categories: anchor-based object detectors and anchor-free object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anchor-based Detectors</head><p>After the seminal work of Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>, anchors have been widely used in modern detectors. It usually contains two stages. The first-stage module is a region proposal network (RPN), which estimates the objectness probabilities of all anchors and regresses the offsets between object boundaries and anchors. The second stage is R-CNN, which predicts the category probability and refines the boundary of bounding box.</p><p>Recently, anchor-based one-stage approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7]</ref> have drawn much attention in object detection because the architectures are simpler and usually run faster <ref type="bibr" target="#b22">[23]</ref>. They remove the RPN and directly predict the categories and regress the boxes of candidate anchors. However, the performance of anchor-based one-stage detectors are usually lower than multi-stage detectors due to the extreme imbalance between positive and negative anchors during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anchor-free Detectors</head><p>Recently, anchor-free detectors have become more and more popular <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. They avoid the complex design of anchors and usually run faster. The object detection is usually formulated as a keypoint detection problem so that the techniques of fully convolutional network (FCN) used in semantic segmentation <ref type="bibr" target="#b19">[20]</ref> and pose estimation <ref type="bibr" target="#b20">[21]</ref> can be applied for detection <ref type="bibr" target="#b28">[29]</ref>.</p><p>YOLOv1 <ref type="bibr" target="#b21">[22]</ref> is one of the most popular anchor-free detectors. On each location of final layer of network, it predicts the bounding box, confidence of the box, and the class probability. In DenseBox <ref type="bibr" target="#b9">[10]</ref>, Huang et.al extend the FCN <ref type="bibr" target="#b19">[20]</ref> for face and car detection. The ground truth is a 5channel map where the first one is a binary mask for the center of object and the other four are for the bounding box size.</p><p>After the seminal work of CornerNet <ref type="bibr" target="#b12">[13]</ref>, keypoint based anchor-free object detectors have drawn much attention. In CornerNet, the FCN directly predicts the corner heatmap, an embedding and a group of offsets for each corner. The embeddings are used to group the pairs of corner to form bounding boxes and the offsets remap the corners from low-resolutional heatmap to the high-resolutional input image. A corner pooling layer is proposed to better localize corners. ExtremeNet <ref type="bibr" target="#b29">[30]</ref> introduces a method that predicts the extreme points instead of the corners of bounding box, and the centerness heatmap is introduced for grouping step. In <ref type="bibr" target="#b4">[5]</ref>, Duan et.al. extend CornerNet by adding a center keypoint. The center keypoint is used to define a central region heuristically and then they use this region to refine the grouped corners.</p><p>To avoid the complex grouping process, CenterNet <ref type="bibr" target="#b28">[29]</ref> directly predicts the center keypoint and the size of object. Furthermore, it replaces IoU-based Non-Maximum Suppression (NMS) by peak keypoint extraction which can be run on GPU to reduce inference time. In <ref type="bibr" target="#b25">[26]</ref> centerness is used to represent the objectiveness of the bounding box predicted at each location. In RepPoints <ref type="bibr" target="#b1">[2]</ref>, a set of sample points is learned to bound the spatial extent of an object  <ref type="figure">Figure 2</ref>. In SaccadeNet, we utilize 5 keypoints as informative parts for detection: the object center and 4 bounding box corners. After the CNN backbone, as in the middle branch, the Center Attentive Module focuses at predicting the object center keypoint; then the Attention Transitive Module in the bottom switches the attention from object center to estimate rough location of object corners. After that, the Aggregation Attentive Module uses information aggregated from both center and corner keypoints, and predicts a refined location of objects. Moreover, in order to obtain informative corner features, the Corner Attentive Module is used (in training only) to enforce the CNN backbone to pay more attention to object boundaries, as shown in the top branch.</p><p>under the keypoint prediction framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SaccadeNet</head><p>It has been discovered that human eyes pick up informative parts to understand object locations instead of looking at every detail of objects <ref type="bibr" target="#b3">[4]</ref>, which makes it fast and accurate. To balance the trade-off between speed and accuracy, on top of the object center point, we use four object bounding box corner points as the informative keypoints in SaccadeNet since it naturally defines the bounding box position. SaccadeNet attends to these informative keypoints sequentially and then aggregates their features to infer accurate object locations. In this section, we will introduce four main modules of SaccadeNet respectively: the Center Attentive Module (Center-Attn), the Attention Transitive Module (Attn-Trans), the Aggregation Attentive Module (Aggregation-Attn), and the Corner Attentive Module (Corner-Attn) used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Center Attentive Module</head><p>Center-Attn provides SaccadeNet the first sight of an object at its center and predicts object center keypoints. It takes the feature from CNN backbone as input and predicts the centerness heatmap. The centerness heatmap is used to estimate the categories and the center locations of all objects in the image. The number of channels in centerness heatmap is the number of categories. <ref type="figure">Figure 2</ref> shows Center-Attn together with its output. In Center-Attn, it contains 2 convolutional layers. This 2-convolutional structure is called head module. It is a basic component for building other modules of SaccadeNet. We will describe it in details in Section 4.</p><p>We use the Gaussian heatmap as ground truth <ref type="bibr" target="#b12">[13]</ref>. The ground-truth heatmap for keypoints is not defined as either 0 or 1 because locations near the target keypoint should get less penalization than locations far away. Suppose the keypoint is at location X k , the value at location X on the ground-truth heatmap is defined as e X−X k 2 2σ 2</p><p>. σ is set to 1/3 of the radius, which is determined by the size of objects to ensure that all locations inside the area could generate a bounding box with at least t IoU with the ground-truth annotations. We follow the previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref> and set t as 0.3.</p><p>Besides, a variant of focal loss <ref type="bibr" target="#b15">[16]</ref> is applied to assist the Gaussian heatmap:</p><formula xml:id="formula_0">L hm i,j = (1 − p i,j ) α log(p i,j ), if y i,j = 1 (1 − y i,j ) β (p i,j ) α log(1 − p i,j ) otherwise</formula><p>where p i,j is the score at location (i, j) of heatmap and y i,j is the corresponding ground truth value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Transitive Module</head><p>Attn-Transpredicts the corners for all locations of the deep feature map. The output shape is w f × h f × 2 for a single image, where w f , h f indicate the width and the height of feature map, respectively. The last dimension is designed to be 2 meaning the width and height of the bounding box. After we get the width and height of bounding box for each center at location (i, j), we can compute the corresponding corners as</p><formula xml:id="formula_1">(i−w i,j /2, j −h i,j /2), (i−w i,j /2, j + h i,j /2), (i + w i,j /2, j − h i,j /2), (i + w i,j /2, j + h i,j /2).</formula><p>In training, we adopt the L1 regression loss. With Center-Attn and Attn-Trans, SaccadeNet can generate object detections with coarse boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Aggregation Attentive Module</head><p>Aggregation-Attn is proposed to attend to object center and bounding box corners again to predict a refined loca-tion. As shown in <ref type="figure">Figure 2</ref>, it aggregates CNN features from corner and center keypoints using bilinear interpolation and outputs more accurate object bounding boxes. As shown in the experiments Section 4.3.1, Aggregation-Attn is essential for us to obtain more accurate boundary.</p><p>Aggregation-Attn is a light-weight module for object boundary refinement. Let w i,j , h i,j indicate the width and height prediction at (i, j). Then, we calculate the</p><formula xml:id="formula_2">corresponding top-left, top-right, bottom-left, bottom-right corners centering at position (i, j) by (i − w i,j /2, j − h i,j /2), (i+w i,j /2, j −h i,j /2), (i−w i,j /2, j +h i,j /2), (i+ w i,j /2, j + h i,j /2).</formula><p>Since previous work <ref type="bibr" target="#b7">[8]</ref> shows that bilinear sampling is helpful for the downsampled feature map, Aggregation-Attn takes the corners and center from the output of Attn-Trans, Center-Attn and samples features from the backbone output by bilinear interpolation. The structure of Aggregation-Attn is a revised head module. We change the input of the first convolutional layer and let it take features of center and corners of object as input.</p><p>Finally, Aggregation-Attn regresses the residual offsets to refine the boundary of objects by incorporating both the features from the corners and the center. The output of Aggregation-Attn consists of residual width and residual height. We adopt L1 loss to train this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Corner Attentive Module in Training</head><p>To extract informative corner features, we propose an auxiliary Corner-Attn branch (only in training) to enforce the CNN backbone to learn discriminative corner features. As shown in <ref type="figure">Figure 2</ref>, Corner-Attn uses one head module to process feature and output 4-channel heatmap including top-left, top-right, bottom-left, bottom-right corners. Note that this branch is used only during training so that it is a free lunch for the increased inference accuracy.</p><p>The training of Corner-Attn is also based on the focal loss and Gaussian heatmap. We tried agnostic and nonagnostic heatmaps, meaning whether different object categories share the same corner heatmap output or not. In our experiments, there is no significant difference between their performance. For shorter training time and easier implementation, we use agnostic heatmaps for Corner-Attn in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Relation to existing methods</head><p>We will compare our work with other related work to address one of our contributions: SaccadeNet solves the issue of lacking holistic perception existed in edge-keypointbased detectors and the issue of missing local details presented in center-keypoint-based detectors.</p><p>Edge-keypoint-based detectors infer objects by assembling edge-keypoints, like corners <ref type="bibr" target="#b12">[13]</ref> or extreme keypoints <ref type="bibr" target="#b29">[30]</ref>. They first predict edge keypoints and then use the grouping algorithm to generate object proposals.</p><p>There are two possible problems that may make cornerkeypoint-based fail to model holistic information: (a) Feature of corner encodes less holistic information since most corner-keypoint-based detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5]</ref> still need feature of centers to assemble corner keypoints. (b) Corner keypoints often locate at background pixels which may encode less information than center keypoints do. Although SaccadeNet also utilizes corner keypoints for bounding box estimation, it is still able to capture holistic by inferring bounding boxes directly from center keypoints. Meanwhile, SaccadeNet is very fast since it avoids the time-consuming grouping.</p><p>Center-keypoint-based detectors propose objects from center points <ref type="bibr" target="#b28">[29]</ref>. It outputs center heatmap and regresses boundary directly. However, center point may be far from the boundary of object so they may fail to estimate accurate boundary on some cases, especially for the large objects (as shown in <ref type="figure" target="#fig_2">Figure 3</ref>). On the other hand, corner keypoints are naturally proximal to the boundaries, so it may encode more local accurate information. Lack of modeling corners may be harmful for the center-keypoint-based detectors. Therefore, SaccadeNet utilizes corner keypoints to alleviate this issue so that it can estimate more accurate boundary.</p><p>SaccadeNet bridges the gap between edge-keypointbased detectors and center-keypoint-based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The experiments are conducted on 2 datasets, PASCAL VOC 2012 <ref type="bibr" target="#b5">[6]</ref> and MS COCO <ref type="bibr" target="#b16">[17]</ref>. MS COCO dataset contains 80 categories, including 105k images for training (train2017) and 5k images for validation (val2017). Pascal VOC consists of 20 categories and it contains a training set of 17k images and a validation set of 5k images. This setting is the same as previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>Backbone. Our backbone consists of down-sampling layers and up-sampling layers. The down-sampling layers are from the CNN for image recognition, e.g. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref>. The up-sampling layers use a couple of convolutional layers and skip connections to fuse high-level and low-level feature, e.g. <ref type="bibr" target="#b14">[15]</ref>. We choose DLA-34 <ref type="bibr" target="#b27">[28]</ref> and ResNet-18 <ref type="bibr" target="#b8">[9]</ref> as the down-sampling backbone and use the up-sampling layers adopted in CenterNet <ref type="bibr" target="#b28">[29]</ref>, where deformable convolutions <ref type="bibr" target="#b31">[32]</ref> are used. The size of the backbone output is 1/4 of the input. The high-resolution output help SaccadeNet recognize and locate small objects. For fair comparison and to illustrate the effectiveness of SaccadeNet, we keep all the settings of backbone the same as <ref type="bibr" target="#b28">[29]</ref>.</p><p>Head module. The head module is the basic component of building four modules of SaccadeNet as illustrated in <ref type="figure">Figure 2</ref>. We use the unified structure of 2 convolutional layers for all the head modules. The first convolutional layer is followed by a ReLU layer with a kernel size of 3 × 3 and 256-dimension output channels. The second convolutional layer uses a 1 × 1 kernel without activation function. Center-Attn contains one head module. The number of output channels of this module depends on the number of categories, e.g. 20 for Pascal VOC, 80 for MS COCO. Corner-Attn contains one head module which outputs a 4-channel heatmap representing the agnostic heatmap of 4 corner keypoints. Corner-Attn contains 2 head modules with 2-channel output, indicating the two directional center offset and the width and height of object, respectively. Aggregation-Attn contains one module with output of 2 channels denoting the residual offsets of width and height of object. The number of parameters of each head module is less than 200k.</p><p>Training. Our experiments were conducted on a machine with 4 GPUs of Geforce RTX 2080 Ti. It takes 10 days to train SaccadeNet-DLA34 and 5 days to train SaccadeNet-Res18. We use Adam <ref type="bibr" target="#b10">[11]</ref> for network optimization. For data augmentation, we apply random flipping, random scaling (range from 0.6 to 1.3), cropping and color jittering. On MS COCO dataset, the size of input to the network is 512 × 512. We use a batch size of 32 (8 images on each GPU) with the initial learning rate of 1.25 × 10 −4 for 210 epochs. The learning rate is dropped to 1.25 × 10 −5 at the 181-th epoch. The same training settings are used for CenterNet <ref type="bibr" target="#b28">[29]</ref>. We use different loss weights for the losses. The loss weights for L Corner−Attn , L Center−Attn and L Aggregation−Attn are 1, 1, 0.1, respectively. Corner-Attn outputs center offsets and the centercorner offsets. We use 0.1 for the loss weight of centercorner offsets and 1 for the loss weight of center offsets. On PASCAL VOC 2012, we use a batch size of 32 on single GPU for training and the input shape of the network is 384×384. We set the initial learning rate to 1.25×10 −4 for 70 epochs. The learning rate is decreased to 1.25 × 10 −5 , 1.25 × 10 −6 at the 46-th epoch, 61-th epoch, respectively. All the other settings are kept the same as our experiments on MS COCO dataset for training. We use the parameters pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref> dataset to initialize the downsampling layers. The parameters of up-sampling layers of backbone and head modules are randomly initialized.</p><p>Inference. On MS COCO dataset, the size of input image is 512 × 512. Flipped testing is optional for better performance. When the flipped and the original images are both used as inputs, we average the outputs of Center-Attn, Corner-Attn, Aggregation-Attn. For higher speed, we use peak-picking NMS proposed in <ref type="bibr" target="#b28">[29]</ref> instead of IoU-based NMS for post-processing. Peak-picking NMS is a 3 × 3 pooling-like operator, which eliminates all non-peak activation. After NMS, we select the object proposals with top-100 centerness scores provided by Center-Attn. For Pascal VOC, we do not apply data augmentation for testing. We use Peak-picking NMS instead of IoU-based NMS. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison results of our approaches with previous work. SaccadeNet achieves state-of-the-art performance with higher speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>SaccadeNet-DLA34 achieves 40.4 mAP at 28 FPS. It outperforms CenterNet-DLA34 [29] by 1.2% AP without visible speed loss due to the light-weight head modules. Besides, our approach outperforms the classic two-stage detector, MaskRCNN <ref type="bibr" target="#b7">[8]</ref>. Meanwhile, we achieve approximately 3 times speed of it. Compared with Reti-naNet <ref type="bibr" target="#b15">[16]</ref>, SaccadeNet-DLA34 performs approximately 4 times faster with only 0.4% drop in accuracy. As shown in <ref type="table" target="#tab_0">Table 1</ref>, SaccadeNet-DLA34 is faster and much more accurate than YOLOv3 <ref type="bibr" target="#b22">[23]</ref>. We compare the results of SaccadeNet-DLA34 and CenterNet-DLA34 <ref type="bibr" target="#b28">[29]</ref> with different IoU thresholds and of different sizes. The average precision gains +0.5, +0.7 of IoU@0.5, IoU@0.7 and gains +0.5, +0.8, +1.4 of objects with small, medium, large size, respectively. SaccadeNet benefits more for high-IoU and large object proposals than others. We will study how Aggregation-Attn and Corner-Attn affect the object proposals of different quality and various size in Section 4.3.1. <ref type="figure" target="#fig_2">Figure 3</ref> shows the qualitative results of SaccadeNet and CenterNet. With the help of Aggregation-Attn, Sacca-deNet is able to locate more accurate boundaries of objects.</p><p>Another version of our approach is based on ResNet-18 with deformable convolutions. SaccadeNet-Res18 is the first real-time anchor-free detector that achieves more than 30% mAP on MS COCO val2017 with speed faster than 100 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Efficiency Study</head><p>We will discuss 4 main factors of efficiency: backbone, head modules, data augmentation, non-maximum suppression.</p><p>Backbone. We use DLA-34 <ref type="bibr" target="#b27">[28]</ref> and ResNet-18 <ref type="bibr" target="#b8">[9]</ref> with additional up-sampling layers used in CenterNet <ref type="bibr" target="#b28">[29]</ref> as backbone. DLA-34 runs at 18.4 ms per image. ResNet-18 runs at 6.8 ms per image. The total inference time of SaccadeNet with DLA-34 and ResNet-18 is 20 ms, 8.5 ms per image, respectively. The efficiency of backbone is the major bottleneck of speed.</p><p>Head modules. There are 64 × 256 × 3 × 3 + 256 × C out parameters for each head module, where C out denotes the number of output channels. There are only 3 head modules during inference. The largest head module is the predictor of Center-Attn, which only contains 168k parameters. The only concern is that the inputs of Aggregation-Attn depend on the outputs of Center-Attn and Corner-Attn. It may cause sequential execution that may increase the inference time. Fortunately, the execution turns out to be very fast. The inference time of all the head modules is much smaller  than the backbone, which only cost 1.5 ms and 1.6 ms for SaccadeNet-DLA34 and SaccadeNet-Res18. The performance of SaccadeNet with and without Aggregation-Attn is illustrated in <ref type="table">Table 3</ref>. Obviously, Aggregation-Attn is important for the performance improvement.</p><p>Data augmentation. For better performance, we feed the network with both the flipped image and the original image. Although this technique will double the inference time theoretically, it significantly improves the performance. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the performance of SaccadeNet with and without flip testing.</p><p>Non-maximum Suppression. In SaccadeNet, we replace the popular IoU-based NMS with peak-picking NMS. Peak-picking NMS performs 3 × 3 pooling on the output heatmap of Center-Attn. The inference time of it is less than 0.1ms. In comparison, the IoU-based NMS needs 2 ms for post-procession. <ref type="table">Table 3</ref> shows the comparison between IoU-based NMS and peak-picking NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we will study the characteristics of Sac-cadeNet. We conduct the experiments with SaccadeNet- Res18 on Pascal VOC. Evaluation metrics. For detailed evaluation, we use 6 metrics for different IoU thresholds and size: AP@50, AP@70, AP@90, AP@S, AP@M, AP@L. AP@50, AP@70, AP@90 represent the average precision using IoU thresholds of 50%, 70%, 90%, respectively. For evaluating objects of different size, we define AP@S, AP@M, AP@L as the average precision of small objects, medium objects, and large objects. Small, medium, large objects contain objects with area of [0, 64 2 ], [64 2 , 128 2 ], and [128 2 , ∞], respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Benefits of Aggregation-Attn and Corner-Attn</head><p>Our proposed Aggregation-Attn and Corner-Attn are designed to improve the quality of boundary. To study how much they affect high-quality/low-quality and large/small object proposals, we use different IoU thresholds to compute the mean average precision and evaluate it on the objects of different sizes. As shown in <ref type="table" target="#tab_1">Table 2</ref>, larger objects and high-quality bounding boxes gain more benefits with Aggregation-Attn and Corner-Attn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Keypoint Selection</head><p>Although our proposed SaccadeNet reveals that corners are very important for accurate boundary localization, it is still unknown whether other keypoints are helpful for bounding box regression. We try different kinds of points: middleedge points and other inner-box points.</p><p>The middle-edge points of an object are the 4 points in the middle of 4 edges of a bounding box. We also re- </p><formula xml:id="formula_3">1 − t) + p ml * t,</formula><p>where pct and p ml indicate center points and middle points of an edge of object bounding box. <ref type="figure" target="#fig_3">Figure 4</ref> describes the position of all points mentioned above.</p><p>place corners with points on the orthogonal lines of the bounding box. <ref type="figure" target="#fig_3">Figure 4</ref> describes the keypoints mentioned above. We change the corners to other keypoints as inputs of Aggregation-Attn and the annotations from corners to other keypoints for Corner-Attn. <ref type="table">Table 4</ref> illustrates the results on Pascal VOC. We find that the corners are the most helpful keypoints for SaccadeNet among all other keypoints except centers. We also find that keypoints closer to corners leads to higher performance for both Aggregation-Attn and Corner-Attn. One possible reason is that corners define the extent of the object and we use the bounding box for loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Does Iterative Refinement Help?</head><p>An intuitive idea for improving SaccadeNet is to apply Aggregation-Attn iteratively. In the experiments, we use a couple of sequential modules of Aggregation-Attn. The outputs of the previous module are used as inputs in the next module. <ref type="table">Table 5</ref> shows the results on PASCAL VOC.</p><p>The results show that iterative refinement works for more accurate boundary. The finer bounding boxes get more improvement by iterative refinement. However, as a result of more sequential execution, the iterative refinement is not very efficient. Due to speed-accuracy trade-off, we only use  one Aggregation-Attn in all the other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Does Aggregation-Attn also Help Classification?</head><p>Object detection is the step to understand "what is where". We have validated that Aggregation-Attn improves the localization of object by fusing feature of corner and center keypoints, namely it helps in terms of "where". Now we want to study whether such information aggregation also helps in terms of "what". We add another module, namely Aggregation Attentive Classifier (Aggregation-Attn-Cls) to refine classification scores. Its structure is the same as Corner Attentive Module. We use the classification scores to replace the original object classifier output. <ref type="table">Table 6</ref> illustrates the results. Unfortunately, the performance is degraded by Aggregation-Attn-Cls. One possible reason is that the feature of corner keypoints encode little high-level discriminative information for classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Impact of the Center and Corner Keypoints in Aggregation-Attn module</head><p>The experimental results in Section 4.3.1 have shown that the aggregation of features from corners and center in Aggregation-Attn is of great importance for the performance improvement. However, is the feature fusion of the corners and center necessary and helpful? How much improvement does it gain by using center-only or corner-only feature?</p><p>To address these questions, we change the inputs of Aggregation-Attn into feature of center keypoints or feature of corner keypoints. <ref type="table" target="#tab_4">Table 7</ref> shows that it is useful to fuse feature of corner and center keypoints together. Comparing to the first row where Aggregation-Attn module is not used, by using the center feature alone it barely improves the performance since previous Center-Attn module already use center feature. By using corner features alone, the performance is improved significantly. By incorporating feature of both corner and center keypoints, the detection result is further improved, especially in high-IOU thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Performance comparison on COCO test-dev. Sacca-deNet outperforms all previous fast detectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative Results of SaccadeNet and CenterNet<ref type="bibr" target="#b28">[29]</ref>. The images on the left 3 columns are the results of SaccadeNet-DLA34. The right column includes the results of CenterNet-DLA34<ref type="bibr" target="#b28">[29]</ref>. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Purple points and yellow points denote centers and corners, respectively. On the left, the green lines denote the diagonal lines of bounding box. The blue point represents a Diag Pts computed by bilinear interpolation. On the right, the yellow points are middle points of bounding-box sides. The pink lines denote the middle line of the bounding box. The two end of middle line are two opposite yellow points. The blue point represents a Mid-edge Pts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The experiments are conducted on MS COCO test-dev. SaccadeNet-DLA outperforms CenterNet-DLA by 1.2% mAP with little overhead. This is the first detector that achieves more than 40% mmAP on MS COCO test-dev with more than 25 FPS. SaccadeNet-Res18 outperforms CenterNet-Res18 by 2.4% mAP with small overhead. We show naive/flip testing results of CenterNet and SaccadeNet. A dash indicates the method doesn't provide the result. * means the experiments are conducted on MS COCO val2017.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>FPS</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell cols="2">TridentNet [14] ResNet-101-DCN</cell><cell>0.7</cell><cell>48.4</cell><cell>69.7</cell><cell>53.5</cell><cell>31.8</cell><cell>51.3</cell><cell>60.3</cell></row><row><cell>SNIPER [25]</cell><cell>DPN-98</cell><cell>2.5</cell><cell>46.1</cell><cell>67.0</cell><cell>51.6</cell><cell>29.6</cell><cell>48.9</cell><cell>58.1</cell></row><row><cell>MaskRCNN [8]</cell><cell>ResNeXt-101</cell><cell>11</cell><cell>39.8</cell><cell>62.3</cell><cell>43.4</cell><cell>22.1</cell><cell>43.2</cell><cell>51.2</cell></row><row><cell cols="2">RetinaNet [16] ResNeXt-101-FPN</cell><cell>5.4</cell><cell>40.8</cell><cell>61.1</cell><cell>44.1</cell><cell>24.1</cell><cell>44.2</cell><cell>51.2</cell></row><row><cell>YOLOv3 [23]</cell><cell>DarkNet-53</cell><cell>20</cell><cell>33.0</cell><cell>57.9</cell><cell>34.4</cell><cell>18.3</cell><cell>25.4</cell><cell>41.9</cell></row><row><cell>HSD [1]</cell><cell>ResNet101</cell><cell>21</cell><cell>40.2</cell><cell>58.2</cell><cell>44.0</cell><cell>20.0</cell><cell>44.4</cell><cell>54.9</cell></row><row><cell>HSD [1]</cell><cell>VGG16</cell><cell>23</cell><cell>38.8</cell><cell>58.2</cell><cell>42.5</cell><cell>21.8</cell><cell>41.9</cell><cell>50.2</cell></row><row><cell cols="2">ExtremeNet [30] Hourglass-104</cell><cell>3.1</cell><cell>40.2</cell><cell>55.5</cell><cell>43.2</cell><cell>20.4</cell><cell>43.2</cell><cell>53.1</cell></row><row><cell>CornerNet [13]</cell><cell>Hourglass-104</cell><cell>4.1</cell><cell>40.5</cell><cell>56.5</cell><cell>43.1</cell><cell>19.4</cell><cell>42.7</cell><cell>53.9</cell></row><row><cell>CenterNet [29]</cell><cell>DLA-34-DCN</cell><cell cols="2">52/28 37.4/39.2</cell><cell>-/57.1</cell><cell>-/42.8</cell><cell>-/19.9</cell><cell>-/43.0</cell><cell>-/51.4</cell></row><row><cell cols="6">*  CenterNet [29] ResNet-18-DCN 142/71 28.1/30.0 44.9/47.5 29.6/31.6</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell></row><row><cell>SaccadeNet</cell><cell>DLA-34-DCN</cell><cell cols="7">50/28 38.5/40.4 55.6/57.6 41.4/43.5 19.2/20.4 42.1/43.8 50.6/52.8</cell></row><row><cell>*  SaccadeNet</cell><cell cols="8">ResNet-18-DCN 118/67 30.5/32.5 46.7/48.9 32.6/34.7 12.0/13.9 33.9/36.2 45.8/47.9</cell></row><row><cell></cell><cell></cell><cell cols="2">SaccadeNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>a CenterNet [31]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>+0.33 56.42/+0.92 13.51/-2.97 9.75/+1.60 24.45/+0.71 58.84/+0.98 Aggregation-Attn 70.64/-0.05 55.85/+0.35 17.34/+0.86 8.30/+0.15 24.30/+0.56 58.39/+0.53 Corner-Attn + Aggregation-Attn 70.94/+0.25 57.84/+2.34 21.07/+4.59 9.69/+1.54 25.17/+1.43 60.40/+2.54 This table shows the results of SaccadeNet with or without Aggregation-Attn and Corner-Attn. We use 6 metrics of different IoU thresholds and object sizes. All experiments are conducted on Pascal VOC. For our approaches, we show both the mAP and the mAP gain (+) or loss (-) compared with the baseline.</figDesc><table><row><cell></cell><cell cols="2">mAP@50</cell><cell>mAP@70</cell><cell>mAP@90</cell><cell>mAP@S</cell><cell>mAP@M</cell><cell>mAP@L</cell></row><row><cell>Baseline</cell><cell>70.69</cell><cell></cell><cell>55.50</cell><cell>16.48</cell><cell>8.15</cell><cell>23.74</cell><cell>57.86</cell></row><row><cell cols="4">Corner-Attn 71.02/Backbone Aggregation-Attn Flip NMS FPS mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell>PP</cell><cell cols="2">52 37.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell>PP</cell><cell cols="2">50 38.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell>PP</cell><cell cols="2">28 39.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell>PP</cell><cell cols="2">28 40.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell cols="3">IoU 45 39.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLA</cell><cell cols="3">IoU 27 40.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 3. All experiments are conducted on MS COCO val2017.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">PP and IoU represent peak-picking NMS and IoU-based NMS,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>The table shows the results of applying iterative refinement on SaccadeNet with different IoU thresholds. All the experiments are based on ResNet-18 on PASCAL VOC. Num of iter means the number of iterations used for boundary refinement This table shows the results of using Aggregation-Attn-Cls for classification with different IoU thresholds. All Experiments are performed on Pascal VOC with ResNet-18.</figDesc><table><row><cell>Num of iter.</cell><cell cols="3">mAP@50 mAP@70 mAP@90</cell></row><row><cell>0</cell><cell>71.02</cell><cell>56.42</cell><cell>18.96</cell></row><row><cell>1</cell><cell>70.94</cell><cell>57.84</cell><cell>21.07</cell></row><row><cell>2</cell><cell>71.09</cell><cell>58.18</cell><cell>21.32</cell></row><row><cell>3</cell><cell>71.12</cell><cell>58.42</cell><cell>20.70</cell></row><row><cell cols="4">Aggregation-Attn-Cls mAP@50 mAP@70 mAP@90</cell></row><row><cell></cell><cell>70.92</cell><cell>57.49</cell><cell>18.96</cell></row><row><cell></cell><cell>52.26</cell><cell>43.23</cell><cell>19.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>This table shows the results of using different inputs for Aggregation-Attn with different IoU thresholds. All Experiments are performed on Pascal VOC with ResNet-18.</figDesc><table><row><cell cols="3">Corner Center mAP@50 mAP@70 mAP@90</cell></row><row><cell>71.02</cell><cell>56.42</cell><cell>18.96</cell></row><row><cell>70.89</cell><cell>56.55</cell><cell>19.01</cell></row><row><cell>71.04</cell><cell>57.53</cell><cell>19.78</cell></row><row><cell>70.94</cell><cell>57.84</cell><cell>21.07</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce SaccadeNet, a fast and accurate object detection algorithm. Our model actively attends to informative object keypoints from the center to the corners, and predicts the object bounding boxes from coarse to fine. SaccadeNet runs extremely fast, because these object keypoints are predicted jointly so that we do not need a grouping algorithm to combine them. We extensively evaluate SaccadeNet on PASCAL VOC and MS COCO datasets, which both demonstrates its effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>Gang Hua was supported partly by National Key R&amp;D Program of China Grant 2018AAA0101400 and NSFC Grant 61629301. We deeply appreciate the help of Xingyi Zhou and Zuxuan Wu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Saccade target selection and object recognition: Evidence for a common attentional mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Deubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient Multi-Scale Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<title level="m">Objects as Points. arXiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00621</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
