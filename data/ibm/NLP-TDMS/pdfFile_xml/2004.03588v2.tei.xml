<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event, Ireland. ACM</publisher>
				<availability status="unknown"><p>Copyright Virtual Event, Ireland. ACM</p>
				</availability>
				<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianda</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE &amp; Ingenuity Labs</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<email>zhling@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
							<email>siwei@iflytek.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<email>xiaodan.zhu@queensu.ca</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE &amp; Ingenuity Labs</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianda</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE &amp; Ingenuity Labs</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ECE &amp; Ingenuity Labs</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM In-ternational Conference on Information and Knowledge Management (CIKM &apos;20)</title>
						<meeting>the 29th ACM In-ternational Conference on Information and Knowledge Management (CIKM &apos;20) <address><addrLine>New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event, Ireland. ACM</publisher>
							<biblScope unit="volume">7</biblScope>
							<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340531.3412330</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of employing pre-trained language models for multi-turn response selection in retrievalbased chatbots. A new model, named Speaker-Aware BERT (SA-BERT), is proposed in order to make the model aware of the speaker change information, which is an important and intrinsic property of multi-turn dialogues. Furthermore, a speaker-aware disentanglement strategy is proposed to tackle the entangled dialogues. This strategy selects a small number of most important utterances as the filtered context according to the speakers' information in them. Finally, domain adaptation is performed to incorporate the in-domain knowledge into pre-trained language models. Experiments on five public datasets show that our proposed model outperforms the present models on all metrics by large margins and achieves new state-of-the-art performances for multi-turn response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Retrieval models and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KEYWORDS</head><p>Speaker-aware BERT, multi-turn response selection, retrieval-based chatbot ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Chatbots aim to engage users in open-domain human-computer conversations and are currently receiving increasing attention. The existing work on building chatbots includes generation-based methods and retrieval-based methods. The first type of methods synthesize a response with a natural language generation model <ref type="bibr" target="#b10">[11]</ref>. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '20, October 19-23, 2020, Virtual Event, Ireland © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-6859-9/20/10. . . $15.00 https://doi.org <ref type="bibr">/10.1145/3340531.3412330</ref> In this paper, we focus on the second type and study the problem of multi-turn response selection. This task aims to select the bestmatched response from a set of candidates, given the context of a conversation which is composed of multiple utterances <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15</ref>]. An example of this task is illustrated in Appendix.</p><p>Previous work has kept utterances separated and performs matching within a representation-interaction-aggregation framework. These methods further extended the matching and attention architectures which improved the performance on this task, such as SMN <ref type="bibr" target="#b14">[15]</ref>, DAM <ref type="bibr" target="#b19">[20]</ref>, IMN <ref type="bibr" target="#b3">[4]</ref>, IoI <ref type="bibr" target="#b12">[13]</ref> and MSN <ref type="bibr" target="#b16">[17]</ref>. We elaborate these related work in Appendix. Recently, pre-trained language models have shown to achieve state-of-the-art performance on a wide range of NLP tasks <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b4">[5]</ref> made the first attempt to employ pretrained models for multi-turn response selection. It adopted a simple strategy by concatenating the context utterances and the response literally, and then sending them into the model for classification. However, this shallow concatenation has three main drawbacks. First, it neglects the fact that speakers are always changing in turn as a conversation progresses. Second, it weakens the relationships between the context utterances as they are organized in the chronological order. Third, due to the maximum sequence length limit (e.g., 512 for BERT-Base), pre-trained language models are unable to tackle sequences that are composed of thousands of tokens, which, however, is a typical setup in multi-turn conversations.</p><p>In this paper, we attempt to employ the pre-trained language model and adjust it to fit the task of multi-turn response selection, in which BERT <ref type="bibr" target="#b1">[2]</ref> is adopted as the basis of our work. We propose a new model, named Speaker-Aware BERT (SA-BERT). First, to make the pre-trained language model aware of the speaker change information during the conversation, the model is enhanced by adding the speaker embeddings to the token representation and adding the special segmentation tokens between the context utterances. These two strategies are designed to improve the conversation understanding capability of multi-turn dialogue systems. Furthermore, to tackle the entangled dialogues which are mixed with multiple conversation topics and are composed of hundreds of utterances, we propose a heuristic speaker-aware disentanglement strategy, which helps to select a small number of most important utterances according to the speaker information in them. Finally, domain adaptation is designed to incorporate specific in-domain knowledge into pre-trained language models. We perform the adaptation process with the same domain but different sets under the same setting. We can conclude that adaptation on a domain-specific corpus can help to incorporate more domain-specific knowledge, and the more similar to the task this adaptation corpus is, the more improvement it can help to achieve. We test our model on five datasets, Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b8">[9]</ref>, Ubuntu Dialogue Corpus V2 <ref type="bibr" target="#b9">[10]</ref>, Douban Conversation Corpus <ref type="bibr" target="#b14">[15]</ref>, E-commerce Dialogue Corpus <ref type="bibr" target="#b17">[18]</ref>, and DSTC 8-Track 2-Subtask 2 Corpus <ref type="bibr" target="#b7">[8]</ref>. Experimental results show that the proposed model outperforms the existing models on all metrics by large margins. Specifically, 5.5% R 10 @1 on Ubuntu Dialogue Corpus V1, 5.9% R 10 @1 on Ubuntu Dialogue Corpus V2, 3.2% MAP and 2.7% MRR on Douban Conversation Corpus, 8.3% R 10 @1 on Ecommerce Corpus, and 15.5% R 100 @1 on DSTC 8-Track 2-Subtask 2 Corpus, leading to new state-of-the-art performances for multi-turn response selection.</p><p>In summary, our contributions in this paper are three-fold:</p><p>1) A new model, named Speaker-Aware BERT (SA-BERT), is designed by employing speaker embeddings and speakeraware disentanglement strategy, to make BERT aware of the speaker change information as the conversation progresses. 2) We make further analysis on the effect of adaptation to the performance of response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Experimental results show that our model achieves new</head><p>state-of-the-art performances on five datasets for multi-turn response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>Given a dialogue dataset D, an example of the dataset is denoted as (c, r, y), where c = {u 1 , u 2 , ..., u n } represents a conversation context with {u k } n k =1 as the utterances, r is a response candidate, and y ∈ {0, 1} denotes a label. Specifically, y = 1 indicates that r is a proper response for c; otherwise y = 0. Our goal is to learn a matching model д(c, r ) by minimizing a cross-entropy loss function from D. For any context-response pair (c, r ), д(c, r ) measures the matching degree between c and r .</p><p>We present here our proposed model, named Speaker-Aware BERT (SA-BERT), and a visual architecture of our input representation is illustrated in Appendix. We omit an exhaustive background description of BERT. Readers can refer to <ref type="bibr" target="#b1">[2]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speaker Embeddings &amp; Segmentations</head><p>In order to distinguish utterances in a context and model the speaker change in turn as the conversation progresses, we use two strategies to construct the input sequence for multi-turn response selection as follows.</p><p>First, in order to model the speaker change, we propose to add additional speaker embeddings to token representations. The embedding functions as indicating the speaker's identity for each utterance. For conversations with two speakers, two speaker embedding vectors need to be estimated during the training process. The first vector is added to each token of utterances of the first speaker. When the speaker changes, the second vector is employed. This is performed alternatively and can be extended to conversations with more speakers.</p><p>Second, empirical results in <ref type="bibr" target="#b2">[3]</ref> show that segmentation tokens play an important role for multi-turn response selection. To model conversation, it is natural to extend that to further model turns and utterances. In this work we propose and empirically show that using an [EOU] token at the end of an utterance and an [EOT] token at the end of a turn model interactions between utterances in a context implicitly and improve the performance consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Speaker-Aware Disentanglement Strategy</head><p>When more than two speakers are communicating in a common channel, there are often multiple conversation topics occurring concurrently. In terms of a specific conversation topic, utterances relevant to it are useful and other utterances could be considered as noise for them. Note that BERT is not good at dealing with sequences which are composed of more tokens than the limit (i.e., length of time steps is set to be 512). In order to select a small number of most important utterances, in this paper, we propose a heuristic speaker-aware disentanglement strategy as follows.</p><p>First, we define the speaker who is uttering an utterance as the spoken-from speaker, and define the speaker who is receiving an utterance as the spoken-to speaker. Each utterance usually has the labels of both spoken-from and spoken-to speakers, which can be extracted from the utterance itself. But some utterances may have only the spoken-from speaker label while the spoken-to speaker is unknown which is set to None in our experiments. Second, given the spoken-from speaker of the response, we select the utterances which have the same spoken-from or spoken-to speaker as the spoken-from speaker of the response. Third, these selected utterances are then organized in their original chronological order and used to form the filtered context. Finally, the utterances selected according to their spoken-from or spoken-to speaker labels are assigned with the two speaker embedding vectors respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptation</head><p>The original BERT is trained on a large text corpus to learn general language representations. To incorporate specific in-domain knowledge, adaptation on in-domain corpora are designed. In our experiments, we employ the training set of each dataset for domain adaptation without additional external knowledge. Furthermore, domain adaptation is done by performing the multi-task learning that optimizing a combination of two loss functions: (1) a next sentence prediction (NSP) loss, and <ref type="formula">(2)</ref> a masked language model (MLM) loss <ref type="bibr" target="#b1">[2]</ref>. Specifically, the speaker embeddings can be pre-trained in the task of NSP. If there is no any adaptation processes, the speaker  Ubuntu Corpus V1 Ubuntu Corpus V2 R 2 @1 R 10 @1 R 10 @2 R 10 @5 R 2 @1 R 10 @1 R 10 @2 R 10 @5 SMN <ref type="bibr" target="#b14">[15]</ref> 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Output Representation</head><p>The first token of each concatenated sequence is the [CLS] token, with its embedding being used as the aggregated representation for a context-response pair classification. This embedding captures the matching information between a context-response pair, which is sent into a classifier with a sigmoid output layer. Finally, the classifier returns a score to denote the matching degree of this pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Datasets</head><p>We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b8">[9]</ref>, Ubuntu Dialogue Corpus V2 <ref type="bibr" target="#b9">[10]</ref>, Douban Conversation Corpus <ref type="bibr" target="#b14">[15]</ref>, E-commerce Dialogue Corpus <ref type="bibr" target="#b17">[18]</ref> and DSTC 8-Track 2-Subtask 2 Corpus <ref type="bibr" target="#b7">[8]</ref>. The first four datasets have been disentangled in advance by their publishers and our proposed speaker-aware disentanglement strategy is applied to only the last DSTC 8-Track 2-Subtask 2 Corpus. Some statistics of these datasets are provided in <ref type="table" target="#tab_0">Table 1</ref>. Readers can refer to Appendix for more details of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We used the same evaluation metrics as those used in previous work <ref type="bibr">[8-10, 15, 18]</ref>. Each model was tasked with selecting the k best-matched responses from n available candidates for the given conversation context c, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k.</p><p>In addition to R n @k, we considered mean average precision (MAP), mean reciprocal rank (MRR) and precision-at-one (P@1), especially for the Douban corpus, following settings of previous work. <ref type="table" target="#tab_1">Table 2</ref>, <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref> present the evaluation results of SA-BERT and previous methods on the five datasets. All the results except ours are from the existing literature. Due to previous methods did not make use of pre-trained language models, we reproduced the results of BERT baseline by fine-tuning on the training set for reference, denoted as BERT for fair comparisons. As we can see that, BERT has already outperformed the present models on most metrics, except R 10 @5  We make some further analysis on the effect of adaptation corpus to the performance of multi-turn response selection. We performed the adaptation process with the same domain but different sets. Here, three different sets of Ubuntu were employed: DSTC 8-Track 2, Ubuntu Dialogue Corpus V1, and Ubuntu Dialogue Corpus V2. And then the fine-tuning process was all performed on the training set of Ubuntu Dialogue Corpus V2. The results on the test set of Ubuntu Dialogue Corpus V2 were shown in <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS 4.1 Adaptation Corpus</head><p>As we can see that, the adaptation process can help to improve the performance no matter which adaptation corpus was used. Furthermore, adaptation and fine-tuning on the same corpus achieved the best performance. One explanation may be that although pretrained language models are designed to provide general linguistic knowledge, some domain-specific knowledge is also necessary for a specific task. Thus, adaptation on a domain-specific corpus can help to incorporate more domain-specific knowledge, and the more similar to the task this adaptation corpus is, the more improvement it can help to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speaker Embeddings</head><p>The speaker embeddings were ablated and the results were reported in <ref type="table" target="#tab_7">Table 6</ref>. The first two lines discussed the situation in which the 1 https://github.com/JasonForJoy/SA-BERT adaptation process were omitted, and the last two lines discussed the adaptation process were equipped with. The performance drop verified the effectiveness of speaker embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Speaker-Aware Disentanglement Strategy</head><p>To show the effectiveness of the speaker-aware disentanglement strategy, we also applied it to the existing model, such as IMN <ref type="bibr" target="#b3">[4]</ref>. The original IMN did not employ any disentanglement strategy and selected the last 70 utterances as the context, which achieved a performance of 32.2% R 100 @1. After employing the strategy, about 25 utterances were selected to form the context, which achieved a performance of 37.5% R 100 @1. Similar results can also be observed by employing this strategy to BERT and ablating this strategy in SA-BERT, as shown in <ref type="table" target="#tab_4">Table 4</ref>, which verified the effectiveness of the speaker-aware disentanglement strategy again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we study the problem of employing pre-trained language models for multi-turn response selection in retrieval-based chatbots. A speaker-aware model and a speaker-aware disentanglement strategy are proposed. Experiments on five public datasets show that our proposed method achieves a new state-of-the-art performance for multi-turn response selection. Adjusting pre-trained language models to fit multi-turn response selection and designing new disentanglement strategies will be a part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDICES A.1 Task Definition</head><p>An example of this task is illustrated in <ref type="table" target="#tab_8">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Related Work</head><p>The existing methods used to build an open domain dialogue system can be generally categorized into generation-based methods and retrieval-based methods. The generation-based methods synthesize a response with a natural language generation model by maximizing its generation probability given the previous conversation context. This approach enables the incorporation of rich context when mapping between consecutive dialogue turns <ref type="bibr" target="#b10">[11]</ref>.</p><p>Our work belongs to the retrieval-based methods, which learn a matching model for a pair of a conversational context and a response candidate. This approach has the advantage of providing informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>. Previous work on retrieval-based chatbots focused on single-turn response selection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>. Recently, researchers have extended the focus to the multiturn conversation, which is more practical for real applications. Some earlier work on multi-turn response selection matched a response with concatenating the context utterances literally into a single long sequence, and calculating its matching score with a response candidate <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Recent work has kept utterances separated and performed matching within a representation-interactionaggregation framework, which improved the performance on this task. For example, <ref type="bibr" target="#b18">[19]</ref> proposed a multi-view model, including an utterance view and a word view. <ref type="bibr" target="#b14">[15]</ref> proposed the sequential matching network (SMN) which first matched the response with each utterance and then accumulated the matching information by recurrent neural network. <ref type="bibr" target="#b17">[18]</ref> proposed the deep utterance aggregation network (DUA) which refined utterances and employed self-matching attention to route the vital information in each utterance. <ref type="bibr" target="#b19">[20]</ref> proposed the deep attention matching network (DAM) which constructed representations at different granularities with stacked self-attention and cross-attention. <ref type="bibr" target="#b11">[12]</ref> proposed the multirepresentation fusion network (MRFN) with multiple types of representations. <ref type="bibr" target="#b3">[4]</ref> proposed the interactive matching network (IMN) which performed the global and bidirectional interactions between the context and response. <ref type="bibr" target="#b12">[13]</ref> proposed the interaction over interaction (IoI) model which performed matching by stacking multiple interaction blocks. <ref type="bibr" target="#b16">[17]</ref> proposed the multi-hop selector network (MSN) which utilized a multi-hop selector to select the relevant utterances as context. <ref type="bibr" target="#b4">[5]</ref> made the first attempt to employ pretrained language models for multi-turn response selection which concatenated the context utterances and the response literally and sent into the model for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Input Representation</head><p>A visual architecture of our input representation is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Adaptation Tasks</head><p>Here, the masked language model (MLM) and the next sentence prediction (NSP) <ref type="bibr" target="#b1">[2]</ref> are employed.</p><p>MLM. We follow the experimental settings in the original BERT by masking some percentage of the input tokens at random and then predicting only those masked tokens to train a deep bidirectional representation. In more detail, we replace the word with the [MASK] token at 80% of the time, with a random word at 10% of the time, and with the original word at 10% of the time.</p><p>NSP. If there is no pre-training process, the speaker embeddings have to be initialized at random at the beginning of the fine-tuning process. To achieve a better performance, the speaker embeddings can be pre-trained with the help of NSP. Here, the sentence A and sentence B are constructed with the same method as that used in the fine-tuning process. The positive responses are true responses that follow the context, and the negative responses are randomly sampled. The embedding of the [CLS] token is used as the aggregated representation for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Datasets</head><p>We tested SA-BERT on five public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 <ref type="bibr" target="#b8">[9]</ref>, Ubuntu Dialogue Corpus V2 <ref type="bibr" target="#b9">[10]</ref>, Douban Conversation Corpus <ref type="bibr" target="#b14">[15]</ref>, E-commerce Dialogue Corpus <ref type="bibr" target="#b17">[18]</ref> and DSTC 8-Track 2-Subtask 2 Corpus <ref type="bibr" target="#b7">[8]</ref>. The first four datasets have been disentangled in advance by their publishers and our proposed speaker-aware disentanglement strategy is applied to only the last DSTC 8-Track 2-Subtask 2 Corpus. Here, we adopted the version of Ubuntu Dialogue Corpus V1 shared in Xu et al. <ref type="bibr" target="#b15">[16]</ref>, in which numbers, paths and URLs were replaced by placeholders. Compared with Ubuntu Dialogue Corpus V1, the training, validation and test dialogues in the V2 dataset were generated in different periods without overlap. In the DSTC 8-Track 2-Subtask 2 Corpus, the candidate pool may not contain the correct response, so we need to choose a threshold. When the probability of positive labels was smaller than the threshold, we predicted that candidate pool did not contain the correct response. The threshold was selected among [0.6, 0.65, .., 0.95] based on the validation set. In all of the Ubuntu corpora, the positive responses are true   <ref type="table" target="#tab_0">E0  E0  E0  E0  E0  E0  E0  E0  E1  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1  E1  E0  E1  E1  E1  E1</ref>  responses from humans, and the negative responses are randomly sampled. The Douban Conversation Corpus was crawled from a Chinese social network on open-domain topics. It was constructed in a similar way to the Ubuntu corpus. The Douban Conversation Corpus collected responses via a small inverted-index system, and labels were manually annotated. The Douban Conversation Corpus is different from the other three datasets in that it includes multiple correct candidates for a context in the test set, which leads to low R n @k, e.g., if there are 3 correct responses, the maximum R 10 @1 is 0.33. Hence, MAP and MRR are recommended for reference. The E-commerce Dialogue Corpus collected real-world conversations between customers and customer service staff from the largest e-commerce platform in China. The DSTC 8-Track 2-Subtask 2 Corpus does not release the labels of the test set. Participants should submit their results on the test set to the official and then be evaluated by them. Thus, we submitted only one result to the official and we provide other results on the validation set for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Training Details</head><p>Most hyper-parameters of the original BERT were followed <ref type="bibr" target="#b1">[2]</ref> except the following configurations. The initial learning rate was set to 2e-5 and was linearly decayed by L2 weight decay. The maximum sequence length of the concatenation of a context-response pair was set to 512. The training batch size was set to 25. The maximum number of training epochs was set to 3. We used the validation set to set the stop condition in order to select the best model for testing. All codes were implemented in the TensorFlow framework <ref type="bibr" target="#b0">[1]</ref> and have be published to help replicate our results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>+</head><label></label><figDesc>+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>+ + + + + + + + + + + + + + + + + + + + + + + The input representation of SA-BERT. The final input embeddings are the sum of the token embeddings, the segmentation embeddings, the position embeddings and the speaker embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets that our model is tested on.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">Train Valid</cell><cell>Test</cell></row><row><cell>Ubuntu V1</cell><cell>pairs positive:negative</cell><cell>1M 1: 1</cell><cell>0.5M 1: 9</cell><cell>0.5M 1: 9</cell></row><row><cell>Ubuntu V2</cell><cell>pairs positive:negative</cell><cell>1M 1: 1</cell><cell>195k 1: 9</cell><cell>189k 1: 9</cell></row><row><cell>Douban</cell><cell>pairs positive:negative</cell><cell>1M 1: 1</cell><cell cols="2">50k 1: 1 1.2: 8.8 10k</cell></row><row><cell>E-commerce</cell><cell>pairs positive:negative</cell><cell>1M 1: 1</cell><cell>10k 1: 1</cell><cell>10k 1: 9</cell></row><row><cell>DSTC 8</cell><cell cols="3">pairs positive:negative 1: 99 1: 99 11M 1M</cell><cell>1M 1: 99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of SA-BERT and previous methods on the Ubuntu Dialogue Corpus V1 and V2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results of SA-BERT and previous methods on the Douban Corpus and E-commerce Corpus. 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R</figDesc><table><row><cell></cell><cell cols="3">Douban Conversation Corpus</cell><cell></cell><cell cols="3">E-commerce Corpus</cell></row><row><cell></cell><cell cols="7">MAP MRR P@1 R 10 @5</cell></row><row><cell>SMN [15]</cell><cell>0.529 0.569 0.397</cell><cell>0.233</cell><cell>0.396</cell><cell>0.724</cell><cell>0.453</cell><cell>0.654</cell><cell>0.886</cell></row><row><cell>DUA [18]</cell><cell>0.551 0.599 0.421</cell><cell>0.243</cell><cell>0.421</cell><cell>0.780</cell><cell>0.501</cell><cell>0.700</cell><cell>0.921</cell></row><row><cell>DAM [20]</cell><cell>0.550 0.601 0.427</cell><cell>0.254</cell><cell>0.410</cell><cell>0.757</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MRFN [12] 0.571 0.617 0.448</cell><cell>0.276</cell><cell>0.435</cell><cell>0.783</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IMN [4]</cell><cell>0.570 0.615 0.433</cell><cell>0.262</cell><cell>0.452</cell><cell>0.789</cell><cell>0.621</cell><cell>0.797</cell><cell>0.964</cell></row><row><cell>IoI [13]</cell><cell>0.573 0.621 0.444</cell><cell>0.269</cell><cell>0.451</cell><cell>0.786</cell><cell>0.563</cell><cell>0.768</cell><cell>0.950</cell></row><row><cell>MSN [17]</cell><cell>0.587 0.632 0.470</cell><cell>0.295</cell><cell>0.452</cell><cell>0.788</cell><cell>0.606</cell><cell>0.770</cell><cell>0.937</cell></row><row><cell>BERT</cell><cell>0.591 0.633 0.454</cell><cell>0.280</cell><cell>0.470</cell><cell>0.828</cell><cell>0.610</cell><cell>0.814</cell><cell>0.973</cell></row><row><cell cols="3">SA-BERT 0.619 0.659 0.496 0.313</cell><cell>0.481</cell><cell>0.847</cell><cell>0.704</cell><cell>0.879</cell><cell>0.985</cell></row></table><note>embeddings have to be initialized randomly at the beginning of the fine-tuning. Readers can refer to Appendix for more details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Set</cell><cell>Model</cell><cell cols="2">MRR R 100 @1</cell></row><row><cell></cell><cell>IMN [4]</cell><cell>0.443</cell><cell>0.322</cell></row><row><cell></cell><cell>IMN [4] + SDS</cell><cell>0.504</cell><cell>0.375</cell></row><row><cell></cell><cell>BERT</cell><cell>0.335</cell><cell>0.258</cell></row><row><cell>Valid</cell><cell>BERT + SDS</cell><cell>0.560</cell><cell>0.440</cell></row><row><cell></cell><cell>SA-BERT -SDS</cell><cell>0.344</cell><cell>0.265</cell></row><row><cell></cell><cell>SA-BERT</cell><cell>0.594</cell><cell>0.477</cell></row><row><cell></cell><cell cols="2">SA-BERT (Ensemble) 0.611</cell><cell>0.496</cell></row><row><cell cols="3">Test SA-BERT (Ensemble) 0.621</cell><cell>0.506</cell></row></table><note>Evaluation results of SA-BERT and ablation tests of the speaker-aware disentanglement strategy (SDS) on the DSTC 8-Track 2-Subtask 2 Corpus.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>on Ubuntu Dialogue Corpus V1 and R 10 @1 on E-commerce Corpus. Furthermore, SA-BERT outperformed the present state-of-the-art performance by large margins of 5.5% R 10 @1 on Ubuntu Dialogue Corpus V1, 5.9% R 10 @1 on Ubuntu Dialogue Corpus V2, 3.2% MAP and 2.7% MRR on Douban</figDesc><table><row><cell>Conversation Corpus, 8.3% R 10 @1 on E-commerce Corpus, and</cell></row><row><cell>15.5% R 100 @1 on DSTC 8-Track 2-Subtask 2 Corpus. These results</cell></row><row><cell>show the ability of SA-BERT to select the best-matched response</cell></row><row><cell>and its compatibility across domains (system troubleshooting, so-</cell></row><row><cell>cial network and e-commerce), achieving a new state-of-the-art</cell></row><row><cell>performance for multi-turn response selection. Readers can refer to</cell></row><row><cell>Appendix for more training details. Our code has been published</cell></row><row><cell>to help replicate our results 1 .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>None</cell><cell>0.950</cell><cell>0.786</cell><cell>0.890</cell><cell>0.981</cell></row><row><cell>DSTC8</cell><cell>0.954</cell><cell>0.803</cell><cell>0.902</cell><cell>0.981</cell></row><row><cell>Ubuntu V1</cell><cell>0.961</cell><cell>0.824</cell><cell>0.914</cell><cell>0.985</cell></row><row><cell cols="2">Ubuntu V2 0.963</cell><cell>0.830</cell><cell>0.919</cell><cell>0.985</cell></row></table><note>Results on the test set of Ubuntu Corpus V2, by adapting domain with different corpora and fine-tuning all on the training set of Ubuntu Corpus V2. Corpus R 2 @1 R 10 @1 R 10 @2 R 10 @5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on the test set of Ubuntu Corpus V2, by ablating the speaker embeddings (SE).Pre-Train SE R 2 @1 R 10 @1 R 10 @2 R 10 @5</figDesc><table><row><cell>No</cell><cell>No 0.950</cell><cell>0.781</cell><cell>0.890</cell><cell>0.980</cell></row><row><cell>No</cell><cell>Yes 0.950</cell><cell>0.786</cell><cell>0.890</cell><cell>0.981</cell></row><row><cell>Yes</cell><cell>No 0.961</cell><cell>0.825</cell><cell>0.915</cell><cell>0.984</cell></row><row><cell>Yes</cell><cell cols="2">Yes 0.963 0.830</cell><cell>0.919</cell><cell>0.985</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>An example of the task of multi-turn response selection.</figDesc><table><row><cell>Conversation</cell></row><row><cell>Human How are you doing?</cell></row><row><cell>Chatbot I am going to hold a drum class in Shanghai.</cell></row><row><cell>Anyone wants to join?</cell></row><row><cell>Human Interesting! Do you have coaches who can</cell></row><row><cell>help me practice drum?</cell></row><row><cell>Chatbot Of course.</cell></row><row><cell>Human Can I have a free first lesson?</cell></row><row><cell>Response Candidates</cell></row><row><cell>Chatbot Sure. Have you ever played drum before? " Chatbot What lessons do you want? %</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Gordon</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium of OSDI 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of NAACL-HLT</title>
		<meeting>the 2019 Conference of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enhance word representation for out-ofvocabulary on Ubuntu dialogue corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1802.02614</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference of CIKM 2019</title>
		<meeting>the 28th ACM International Conference of CIKM 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training Neural Response Selection for Task-Oriented Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Coope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of ACL 2019</title>
		<meeting>the 57th Conference of ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5392" to="5404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Information Retrieval Approach to Short Text Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved Deep Learning Baselines for Ubuntu Corpus Dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1510.03753</idno>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulaka</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06394</idno>
		<title level="m">The Eighth Dialog System Technology Challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2015 Conference</title>
		<meeting>the SIGDIAL 2015 Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Thomas Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="31" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference</title>
		<meeting>the Thirtieth AAAI Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference of WSDM 2019</title>
		<meeting>the Twelfth ACM International Conference of WSDM 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One Time of Interaction May Not Be Enough: Go Deep with an Interactionover-Interaction Network for Response Selection in Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of ACL 2019</title>
		<meeting>the 57th Conference of ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Dataset for Research on Short-Text Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of EMNLP</title>
		<meeting>the 2013 Conference of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Conference of ACL 2017</title>
		<meeting>the 55th Conference of ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling Multi-turn Conversation with Deep Utterance Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference of COLING 2018</title>
		<meeting>the 27th International Conference of COLING 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-view Response Selection for Human-Computer Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of EMNLP</title>
		<meeting>the 2016 Conference of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Conference of ACL 2018</title>
		<meeting>the 56th Conference of ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
