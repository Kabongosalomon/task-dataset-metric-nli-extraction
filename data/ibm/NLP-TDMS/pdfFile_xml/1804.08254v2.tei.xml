<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Attention Networks for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
							<email>celi@cumtb.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">China University of Mining &amp; Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Electrical Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computing &amp; Communications</orgName>
								<orgName type="institution">Lancaster University</orgName>
								<address>
									<postCode>LA1 4YW</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
							<email>cqzou@umiacs.umd.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Maryland Institute for Advanced Computer Studies</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
							<email>liu.jianzhuang@huawei.com*</email>
							<affiliation key="aff5">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies Co. Ltd</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Attention Networks for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition task is entangled with complex spatio-temporal variations of skeleton joints, and remains challenging for Recurrent Neural Networks (RNNs). In this work, we propose a temporal-then-spatial recalibration scheme to alleviate such complex variations, resulting in an end-to-end Memory Attention Networks (MANs) which consist of a Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM). Specifically, the TARM is deployed in a residual learning module that employs a novel attention learning network to recalibrate the temporal attention of frames in a skeleton sequence. The STCM treats the attention calibrated skeleton joint sequences as images and leverages the Convolution Neural Networks (CNNs) to further model the spatial and temporal information of skeleton data. These two modules (TARM and STCM) seamlessly form a single network architecture that can be trained in an end-to-end fashion. MANs significantly boost the performance of skeleton-based action recognition and achieve the best results on four challenging benchmark datasets: NTU RGB+D, HDM05, SYSU-3D and UT-Kinect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D skeleton-based human action recognition has recently attracted a lot of research interests due to its high-level representation and robustness to variations of viewpoints, appearances and surrounding distractions <ref type="bibr" target="#b4">[Han et al., 2017;</ref><ref type="bibr" target="#b11">Presti and La Cascia, 2016;</ref><ref type="bibr">Ding and Fan, 2016]</ref>. It is motivated by the biological observations that human beings can <ref type="figure">Figure 1</ref>: Memory Attention Networks use the temporal-then-spatial recalibration scheme. The (TARM) is deployed in the Residual Module (RM) to take advantage of the input features and learned attention information. The (STCM), which treats the skeleton sequences as images and leverages the CNNs, further models the spatial and temporal information of skeleton data to cope with complex spatio-temporal variations in skeleton joints. recognize actions from just the motion of a few joints of the human body, even without appearance information <ref type="bibr">[Johansson, 1973]</ref>. To describe human actions, conventional recognition approaches use relative joint coordinates to overlook the absolute movements of skeleton joints and thus gain partial view-invariant transformation. They include aligned spherical coordinates with person's direction <ref type="bibr" target="#b15">[Xia et al., 2012]</ref>, translated coordinates invariant to absolute position and orientation <ref type="bibr" target="#b7">[Jiang et al., 2015]</ref>, and flexible view invariant transform with principal components <ref type="bibr" target="#b12">[Raptis et al., 2011]</ref>.</p><p>Skeleton sequences are time series of joint coordinate positions. To learn the temporal context of sequences, Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b8">[Li et al., 2017]</ref>, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[Zhu et al., 2016]</ref>, and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014]</ref>, have been successfully applied to skeleton based action recognition. But it still challenging to cope with the complex spatio-temporal variations of skeleton joints caused by a number of factors, such as action speed, jitters, and surrounding distractions. To handle these variations, attention mechanism is introduced arXiv:1804.08254v2 [cs.CV] 3 May 2018 x-coordinate input of MANs X</p><p>x-coordinate output feature map of a TARM F M memory information in a TARM F A attention weight in a TARM F C output feature map of STCM y predicted action label in <ref type="bibr" target="#b10">Liu et al., 2017a;</ref><ref type="bibr" target="#b17">Zhu et al., 2016;</ref><ref type="bibr" target="#b14">Song et al., 2017]</ref> to provide a robust recognition system. For instance, STA-LSTM <ref type="bibr" target="#b14">[Song et al., 2017]</ref> allocates different attention weights for selecting key frames and discriminative joints within one frame. Similarly, GCA-LSTM <ref type="bibr" target="#b10">[Liu et al., 2017a]</ref> selects the global informative joints from a sequence. A few works exploit CNNs to solve the skeleton based action recognition problem. In <ref type="bibr" target="#b7">[Ke et al., 2017]</ref>, skeleton joints after being projected or encoded, are used as the input channels of CNNs, which causes temporal information loss during the conversion of 3D information (x, y, z joint coordinates) into 2D information (images). In <ref type="bibr" target="#b11">[Liu et al., 2017b;</ref><ref type="bibr" target="#b8">Lea et al., 2016]</ref>, skeleton joints in each frame are transformed and expressed as color heat maps in CNNs, where complex data preprocessing gives rise to the loss of distinct spatio-temporal information.</p><p>In this work, our goal is to bring these powerful tools (e.g. RNNs, CNNs and attention learning) under the same umbrella and develop an efficient framework to investigate a new hypothesis of "memory attention + convolution network" for skeleton based action recognition. We propose an end-to-end deep network architecture, termed as Memory Attention Networks (MANs), to perform temporal-then-spatial feature recalibration. It can leverage the state-of-the-art CNNs to enhance the spatio-temporal features . So far, CNNs particularly ResNets <ref type="bibr">[He et al., 2016]</ref> or Wide-ResNets <ref type="bibr" target="#b16">[Zagoruyko and Komodakis, 2016]</ref> have been the most popular tools due to the unique residual module. Inspired by it, we design our temporal-then-spatial recalibration scheme in MANs based on the residual module as shown in <ref type="figure">Fig. 1</ref>. By doing so, both the original input features and the attention information can be fully exploited by subsequent CNNs in a unified framework, leading to a comprehensive and effective feature representation.</p><p>Specifically, as shown in <ref type="figure">Fig. 1</ref>, each input skeleton sequence is denoted as a T × N × 3 matrix, where T is the total number of frames, N is the number of joints, and the 3 indicates x, y and z coordinates for each joint. For each coordinate, we have a T × N matrix. A Temporal Attention Recalibration Module (TARM) is proposed, which consists of (1) a memory cell for extracting memory information features by a Bidirectional Gated Recurrent Unit (BiGRU) <ref type="bibr" target="#b0">[Bahdanau et al., 2015]</ref> and (2) a branch to learn temporal attention for feature recalibration. The three temporally calibrated features X, Y and Z are treated as a 3-channel image and fed to a state-of-the-art CNN in the proposed Spatio-Temporal Convolution Module (STCM). In this way, the modeling ability of MANs is further enhanced by considering the spatial layout of skeleton joints. The resulting feature representations can effectively deal with the spatio-temporal variations among joints in a sequence, due to the robustness of CNNs against deformations. Distinctions between this work and prior art. (1) The state-of-the-art attention network <ref type="bibr" target="#b14">[Song et al., 2017]</ref> uses two LSTMs to model spatial and temporal attentions for each skeleton frame based on the input (frames) at time steps t and t−1. We also use RNN in TARM, but only to model the memory information of skeleton sequences. We design a new attention network to learn the attention weights and then make use of the learned temporal attention to recalibrate the original skeleton sequence in a residual module, which facilitates efficient learning of attention features. (2) The existing CNNs based skeleton action recognition methods <ref type="bibr" target="#b7">[Ke et al., 2017;</ref><ref type="bibr" target="#b11">Liu et al., 2017b]</ref> involve complicated pre-processing. For example, <ref type="bibr" target="#b7">[Ke et al., 2017]</ref> is based on clip generation (skeleton segmentation) and color images transformation; <ref type="bibr" target="#b11">[Liu et al., 2017b]</ref> performs skeleton coordinate transform and generates images with visual enhancement as the input. However, our proposed MANs directly operate on the skeleton sequences without bells and whistles, enabling an end-to-end training of network.</p><p>Contributions. The contributions of this paper are threefold.</p><p>1. We propose an end-to-end framework of Memory Attention Networks (MANs) to demonstrate the powerful capacity of a new "memory attention + convolution network" scheme for modeling the complex spatio-temporal variations in skeleton joints. It is the first time that a "RNNs + CNNs" framework has been developed for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A new attention learning method is presented based on the residual module. It recalibrates temporal features to pay more attention to informative skeleton frames.</p><p>3. MANs achieve the state-of-the-art results on four challenging datasets. We also perform extensive ablation study to show the effectiveness of each unit in MANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Memory Attention Networks</head><p>In this section, we elaborate the two modules: Temporal Attention Recalibration Module (TARM) and Spatio-Temporal Convolution Module (STCM) in MANs. <ref type="table" target="#tab_0">Table 1</ref> summarizes the notations used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Attention Recalibration Module</head><p>The input skeleton data is a sequence of multi-frame 3D joint coordinates forming an action.</p><formula xml:id="formula_0">Let O = {X, Y, Z} ∈ R T ×N ×3 , where X ∈ R T ×N , Y ∈ R T ×N , Z ∈ R T ×N , de-</formula><p>notes N joints along T frames with x, y, and z coordinates. For ease of explanation, X is chosen as an example to describe TARM.  <ref type="bibr">(right)</ref>. TARM is designed based on the residual module as X = X+F(X), which incoporates the input and recalibrated features in a unified framework. F(X) is the recalibrated feature, e.g., the output of our residual attention module, via F(X)=F M (X) F A (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Attention Recalibration Module</head><formula xml:id="formula_1">(TARM) Residual Module (RM) FC BiGRU FC Average Pooling Duplicate FC FC Sigmoid Memory Cell Attention T N  T N  T K  T K  X X ( ) F X X M F A F T N  1 T  T K  T K   T K  T K  Residual BiGRU Memory Cell of TARM Residual Attention T K  GRU forward backward ( 1) t  x BiGRU GRU 1 K  ( 1) t  m GRU GRU 1 K  ( ) t m ( ) t x GRU GRU 1 K  ( 1) t  m ( 1) t  x</formula><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, given a 2D matrix X, the learning of TARM pursuits a specific attention based on the BiGRU in memory cell to capture the temporal memory information across the input action sequence. More specifically, inspired by the original RM in ResNets, we construct TARM via identity mapping with transformation from X ∈ R T ×N to X ∈ R T ×N to capture the richer temporal information, as</p><formula xml:id="formula_2">X = X + F(X),<label>(1)</label></formula><p>where F(X) is the recalibrated feature, e.g., the output of our residual attention module shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, based on two branches: F M (X) and F A (X), which represent the memory information and attention weight, respectively.</p><formula xml:id="formula_3">F(X)=F M (X) F A (X),<label>(2)</label></formula><p>where denotes the element-wise multiplication. For simplicity, F M (X) and F A (X) are denoted as F M ∈ R T ×K and F A ∈ R T ×K , respectively. F A is the weight of F M to recalibrate temporal information. Obviously, for an action sequence, the importance of representative information in each frame is different, and only a few key frames containing important discriminative information deserves to be emphasized for action representation.</p><p>Calculating F M . We implement the memory cell via Bi-GRU. X ∈ R T ×N is resized and updated by the output of a FC layer as X ← F C(X) ∈ R T ×K , as shown at the top of <ref type="figure" target="#fig_0">Fig. 2</ref>. In a slight abuse of notation, we still denote the resized output as X. F M ∈ R T ×K is the memory information made up of two directional combined hidden states in BiGRU, where K denotes the number of neuron units in Bi-GRU.</p><p>For simplicity, we still denote X ∈ R T ×K as</p><formula xml:id="formula_4">X =        x(1) . . . x(t) . . . x(T )        =        x 1 (1) · · · x k (1) · · · x K (1) . . . . . . . . . x 1 (t) x k (t) x K (t) . . . . . . . . . x 1 (T ) · · · x k (T ) · · · x K (T )        ,<label>(3)</label></formula><p>where x(t) is a row vector of X to represent the sequence at t th frame as [x 1 (t), · · · , x K (t)], and t ∈ (1, ..., T ).</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, → GRU (x(t)) and ← GRU (x(t)) are the output hidden states of the forward GRU and backward GRU, respectively. Combining these bidirectional hidden states, the informative vector m(t) ∈ R 1×K at the t th frame is denoted as</p><formula xml:id="formula_5">m(t) = → GRU (x(t)) + ← GRU (x(t)).<label>(4)</label></formula><p>Finally, all the outputs are concatenated across T frames, and the memory information of skeleton joints is represented as</p><formula xml:id="formula_6">F M = [m(1), · · · , m(T )] T ∈ R T ×K ,<label>(5)</label></formula><p>where F M summarizes the memory information in BiGRU for the skeleton joints across the sequence.</p><p>Calculating F A . To recalibrate the memory information F M , the attention weight F A is exploited as shown in Eq.</p><p>(2). In <ref type="figure" target="#fig_0">Fig. 2</ref>, following X ∈ R T ×K , our recalibration scheme can capture global frame-wise dependence across T frames. We first aggregate each row vector of X in Eq. (3) by the average pooling operation to produce a T × 1 vector as</p><formula xml:id="formula_7">X p = 1 K K k=1 x k (1), · · ·, 1 K K k=1 x k (T ) T ∈ R T ×1 . (6)</formula><p>We then duplicate it with K copies as</p><formula xml:id="formula_8">X ← F d (X p , K) ∈ R T ×K ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">F d (X p , K) = [X p , · · · , X p ] K .</formula><p>In the right branch of TARM shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the attention mechanism is represented by a bottleneck with the two FC layers providing the non-linear interaction between frames. We introduce a dimensionality-reduction layer with parameters W 1 and a ratio factor α (empirically set to be 16 in Section 3.1), followed by a ReLU function. And then, we introduce a dimensionality-increasing layer with parameters W 2 and a sigmoid activation function. The dimensionalityreduction and dimensionality-increasing processing can be considered as the denoising and excitation operations respectively, and thus enhance the feature discriminability. Finally, the output of the attention branch F A is calculated as</p><formula xml:id="formula_10">F A = σ (W 2 θ (W 1 X)) ∈ R T ×K ,<label>(8)</label></formula><p>where W 1 ∈ R T α ×T and W 2 ∈ R T × T α . To simplify the notation, the bias terms in Eq. (8) are omitted. θ (·) refers to the ReLU function, and σ (·) denotes the sigmoid function. Finally, F(X) is obtained by the element-wise multiplication of F M and F A .</p><p>Furthermore, to calculate the output feature map of TARM, X, F(X) ∈ R T ×K is resized by a FC layer as F(X) ← F C(F(X)) ∈ R T ×N , shown at the bottom of <ref type="figure" target="#fig_0">Fig. 2</ref>. As a result, the final F(X) describes the temporal information of the entire skeleton sequence. Similar to RM in ResNets, F M and F A in TARM can be jointly learned during training. In a similar way, we can obtain Y and Z based on Y and Z in their corresponding TARM, and O = { X, Y, Z} ∈ R T ×N ×3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatio-Temporal Convolution Module</head><p>Conventional attention methods in skeleton action recognition are limited by the modeling capacity of RNNs <ref type="bibr" target="#b10">Liu et al., 2017a]</ref>. STCM is introduced based on CNNs to extract the enhanced spatio-temporal features from the output ( X, Y and Z) of the TARM. By leveraging the robustness to deformation of CNNs, STCM further extracts high-level feature representations to better cope with spatiotemporal variations of skeleton joints.</p><p>In principal, any CNNs can be used in STCM, e.g., DenseNets <ref type="bibr" target="#b6">[Huang et al., 2017]</ref> and ResNets <ref type="bibr">[He et al., 2016]</ref>. O ∈ R T ×N ×3 denotes the output of TARMs and also the input to STCM, and F C denotes the output of STCM for the softmax classifier. For example, in <ref type="figure">Fig. 1</ref>, the BN-ReLU-Conv blocks in STCM are used to interpret the high-level spatial structures of skeleton joints as</p><formula xml:id="formula_11">F C = Conv ReLU BN ...Conv ReLU BN O .</formula><p>(9) Afterwards, F C is fed to a softmax classifier to predict the class label asŷ</p><formula xml:id="formula_12">= softmax (W C , F C ) ,<label>(10)</label></formula><p>where W C andŷ denote the weights in the softmax layer and the predicted action label, respectively. The cross-entropy loss function <ref type="bibr">[Goodfellow et al., 2016</ref>] is adopted to measure the difference between the true class label y and the prediction resultŷ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation</head><p>NTU RGB+D dataset. The NTU dataset <ref type="bibr" target="#b13">[Shahroudy et al., 2016]</ref> is the largest skeleton-based action recognition dataset, with more than 56000 sequences and 4 million frames. There are 60 classes of actions performed by 40 subjects. In total, there are 80 views for this dataset, and each skeleton has 25 joints. Due to the large viewpoint, intra-class and sequence length variations, the dataset is very challenging. For fair comparisons, we follow the same cross-subject and crossview evaluation protocols in <ref type="bibr" target="#b13">[Shahroudy et al., 2016]</ref>. HDM05 dataset. The HDM05 dataset <ref type="bibr" target="#b11">[Müller et al., 2005]</ref> contains 2,337 skeleton sequences performed by 5 actors (613,377 frames). We use the same experiment setting (65 classes, 10-fold cross validation) in <ref type="bibr" target="#b17">[Zhu et al., 2016]</ref>. SYSU-3D dataset. The SYSU-3D dataset <ref type="bibr" target="#b5">[Hu et al., 2015]</ref> collected with the Microsoft Kinect contains 12 actions performed by 40 subjects. The dataset has 480 skeleton sequences and is very challenging as the motion patterns are quite similar among different action classes. Moreover, there are a lot of viewpoint variations. We evaluate the performance of our method using the standard 30-fold cross-validation protocol <ref type="bibr" target="#b5">[Hu et al., 2015]</ref>, in which half of the subjects are used for training and the rest for testing.</p><p>UT-Kinect dataset. The UT-Kinect dataset <ref type="bibr" target="#b15">[Xia et al., 2012]</ref> is collected using a single stationary Kinect. The skeleton sequences in this dataset are very noisy. 10 action classes are performed by 10 subjects, and each action is performed by the same subject twice. We follow the standard Leave-One-Out-Cross-Validation (LOOCV) protocol in <ref type="bibr" target="#b15">[Xia et al., 2012]</ref>. Implementation details. For all the datasets, the matrices ( X, Y and Z) are generated with all the frames of a skeleton sequence. We use two different scales for the three input matrices of each sequence, i.e., 224 × 224 and 50 × 50, respectively. For the large scale, the number of hidden units of BiGRU in TARM is set to 2 × 128 (K = 128), where 2 indicates bidirectional GRU, 128 is the number of neurons. DenseNet-161 <ref type="bibr" target="#b6">[Huang et al., 2017]</ref> and <ref type="bibr">ResNet-18 [He et al., 2016]</ref> are used in STCM, leading to MANs  or MANs . For the small scale, we set the hidden units of BiGRU to 2×64, and stack multiple BN-ReLU-Conv blocks as STCM, resulting in MANs-n (e.g. n = 9) where n is the number of BN-ReLU-Conv blocks. The architectures of various MANs are illustrated in <ref type="table" target="#tab_1">Table 2</ref>. The number of the units for the last FC layer (i.e., the output layer) is the same as the number of the action classes in each dataset. MANs are trained using the stochastic gradient descent algorithm, and the learning rate, decay, and momentum, are respectively set to 0.1, 0, and 0.9. The minibatches of samples on NTU RGB+D, HDM05, SYSU-3D, and UT-Kinect are constructed by randomly sampling 40, 20, 8, and 8 samples from the training sets, respectively. The training stops after 100 epochs except for NTU RGB+D after 50 epochs. For a fair comparison, the performance of MANs on each dataset is compared with existing methods using the same evaluation protocol. All experiments are performed based on Keras 2 with Tensorflow backend using two NVIDIA Titan X Pascal GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Analysis</head><p>Parameter analysis. To investigate the performance of MANs using different values of the ratio factor α in Eq. (8), the comparison of accuracy on the same trial of HDM05 dataset is conducted by MANs (ResNet-18) in <ref type="table" target="#tab_2">Table 3</ref>. From the second column to the fifth column of <ref type="table" target="#tab_2">Table 3</ref>, the results show that MANs (ResNet-18) consistently keep high training efficiency using α = 4, 8, 16, 32. MANs (ResNet-18) with the ratio factor α = 16 achieve the best accuracy of 99.23% on the HDM05 dataset. The parameter tuning experiment reveals that α = 16 is a proper ratio factor for generating the attention weight. In all the following experiments, we evaluate the performance of MANs by setting α to 16. Ablation study. We conduct extensive ablation study of different units in MANs with the following settings: (A) STCM 3 (i.e. applying CNN on the original skeleton images); (B) MANs (no attention)-MANs without attention in TARM (i.e. no F A ); (C) MANs (other temporal attention)-MANs use the temporal attention scheme in <ref type="bibr" target="#b14">[Song et al., 2017]</ref> for the   TARM. <ref type="table" target="#tab_3">Table 4</ref> shows the results of different architectures on the NTU RGB+D dataset. Note that the STCM of MANs uses DenseNet-161 for the input of 224 × 224, and uses stacked BN-ReLU-Conv blocks with 9 layers for the input of 50 × 50 in <ref type="table" target="#tab_3">Table 4</ref>, respectively. Comparing with our full MANs model, we have these findings.</p><p>(1) Setting A yileds much lower performance, indicating the importance of temporal information modeling for skeleton.</p><p>(2) Setting B reveals the effectiveness of the proposed attention mechanism (i.e. learning F A ).</p><p>(3) Setting C substitutes the attention scheme in <ref type="bibr" target="#b14">[Song et al., 2017]</ref> for our residual attention module in MANs. The recognition accuracy is lower than that of our MANs, which again validates the superiority of our residual attention learning approach. Learning convergence. We plot the training error and testing error curves of the four networks on a same trail of NTU RGB+D dataset in <ref type="figure">Fig. 3</ref>, including STCM-9, MANs-9 (no attention), MANs-9 (other temporal attention), and MANs-9. We can observe that MANs-9 in solid line converges at epoch #26 for training and obtains the best error rate of 16.99% for testing. For training error, STCM-9 stops decreasing at epoch #30, MANs-9 (no attention) converges at epoch #30, and MANs-9 (other temporal attention) converges at epoch #40.</p><p>These curves show that MANs obviously converge faster and gain better performance than others. For example, MANs converge quickly and improve the performance over STCM, which proves that the "memory attention + convolution net- work" scheme in MANs can be used to improve the modeling ability of CNNs. MANs-9 converges much faster than MANs-9 (other temporal attention) (epoch #26 v.s. #40), due to the novel residual attention module which takes the input and attention information into account in the same framework. More specifically, the residual attention module not only uses the temporal attention recalibrated information, but also delivers the spatial structure information of the original input by the identity shortcut. Various CNNs in STCM. The number of stacked layers of CNNs in STCM, i.e., multiple BN-ReLU-Conv blocks used in MANs-9, MANs-33 and MANs-61, is evaluated in terms of the recognition accuracy on all the datasets <ref type="table" target="#tab_4">(Table 5</ref>). We also include MANs (ResNet-18), MANs (DenseNet-161) in the same table. The deeper CNNs have more learnable parameters than the shallower ones. We note that MANs-9 with a similar parameter amount as Deep LSTM <ref type="bibr" target="#b17">[Zhu et al., 2016]</ref> has much better performance than the state-of-thearts. This reveals that our method is more effective if the network complexity should be considered. Interestingly, MANs-9 mostly achieve better performance than other deeper models, which is probably due to its compactness. However, the very deep MANs (DenseNet-161) obtain the best result on the challenging NTU RGB+D dataset in cross-view setting. It is worth noting that because of the "memory attention + convolution network" scheme of MANs, it is quite flexible to deploy different parameter amount in MANs by adjusting the number of CNN layers in STCM to balance between performance and network complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Comparisons</head><p>We show the performance comparison of various MANs architectures with other state-of-the-art approaches in <ref type="table" target="#tab_4">Table 5</ref>  We analyze the best results of various MANs for the NTU RGB+D dataset. MANs perform significantly better than others in both the cross-subject and cross-view protocols. The accuracies of MANs-9 are 83.01% for the cross-subject protocol and 90.66% for the cross-view protocol. MANs (DenseNet-161) achieve 82.67% in the cross-subject test and 93.22% in the cross-view test. Comparing to other methods, MANs-9 increase the accuracy by 3.44% for cross-subject evaluation, and MANs (DenseNet-161) lead to a significant 5.62% improvement on this largest dataset in cross-view evaluation, which demonstrate that MANs can learn more discriminative spatio-temporal features to alleviate the spatial and temporal variations in skeleton joints.</p><p>For the HDM05 dataset, our MANs-33 achieves better result than the state-of-the-art multi-layer RNNs-based models. Our MANs (ResNet-18) achieve even better result up to 99.04%. The improved results of MANs-33 and MANs (ResNet-18) suggest that CNNs in STCM not only enhance the temporal attention information in TARM, but also exhibit better motion modeling ability than deep RNNs-based model by the flexible architecture. For the SYSU-3D dataset, five accuracies of our MANs are higher than all the RNNs-based methods <ref type="bibr" target="#b5">[Hu et al., 2015;</ref><ref type="bibr" target="#b10">Liu et al., 2017a;</ref>; especially MANs-61 outperform the previous best approach GCA-LSTM by 9.03%. It validates the superiority of MANs in skeleton based action recognition, and that the temporal-then-spatial recalibration scheme is effective for this task which suffers from lots of variations.</p><p>For the UT-Kinect datasest, MANs-9 and MANs (ResNet-18) achieve a 100% accuracy, with 1.0% improvement in comparison with the state-of-the-art GCA-LSTM. This experiment shows that compared with the existing RNNs-based methods, the deployed residual module with temporal attention and the temporal-then-spatial scheme can effectively improve the modeling ability of RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose an end-to-end framework, termed Memory Attention Networks (MANs), to enhance the spatiotemporal features for skeleton-based action recognition. In MANs, TARM is designed to recalibrate the temporal attention to skeleton frames in action sequences, and STCM further models the spatial structure and temporal dependence of the skeleton sequence by leveraging the powerful CNNs. Through the unified framework, MANs significantly boost the performance for skeleton-based action recognition. The extensive experiments validate the superiority of MANs, which consistently perform the best on four benchmark datasets and contribute new state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The Residual Module (left), Temporal Attention Recalibration Module (middle), and BiGRU Memory Cell of TARM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>proposed MANs are evaluated on four public skeleton action datasets: NTU RGB+D<ref type="bibr" target="#b13">[Shahroudy et al., 2016]</ref>,HDM05 [Müller et al., 2005], SYSU-3D<ref type="bibr" target="#b5">[Hu et al., 2015]</ref> and UT-Kinect<ref type="bibr" target="#b15">[Xia et al., 2012]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Testing error v.s. epoch Figure 3: Training and testing error curves of STCM-9, MANs-9 (no attention), MANs-9 (other temporal attention) and MANs-9 on the NTU RGB+D dataset (cross-subject setting).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A brief description of notations used in the paper.</figDesc><table><row><cell cols="2">Variable Description</cell></row><row><cell>O</cell><cell>input of MANs</cell></row><row><cell>O</cell><cell>output of three TARMs</cell></row><row><cell>X</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The architectures of various MANs-n (i.e., n = 9, 33, 61).</figDesc><table><row><cell cols="2">Module Output Size</cell><cell>MANs-9</cell><cell></cell><cell>MANs-33</cell><cell></cell><cell>MANs-61</cell><cell></cell></row><row><cell>TARM</cell><cell>50 × 50</cell><cell></cell><cell></cell><cell>64 × 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25 × 25</cell><cell></cell><cell></cell><cell cols="2">5 × 5, 64, stride 2</cell><cell></cell><cell></cell></row><row><cell>STCM</cell><cell>25 × 25 13 × 13</cell><cell>3 × 3, 64 3 × 3, 64 3 × 3, 128 3 × 3, 128</cell><cell>× 2 × 2</cell><cell>3 × 3, 64 3 × 3, 64 3 × 3, 128 3 × 3, 128</cell><cell>× 8 × 8</cell><cell>3 × 3, 64 3 × 3, 64 3 × 3, 128 3 × 3, 128</cell><cell>× 15 × 15</cell></row><row><cell></cell><cell>1 × 1</cell><cell></cell><cell cols="3">Average Pooling, FC, Softmax</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Recognition accuracies of MANs (ResNet-18) on the</cell></row><row><cell cols="3">HDM05 dataset using α = 4, 8, 16, 32.</cell><cell></cell><cell></cell></row><row><cell>α</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="5">Accuracy(%) 98.09 98.38 99.23 98.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of STCM, MANs (no attention), MANs (other temporal attention) and MANs on the NTU RGB+D dataset.</figDesc><table><row><cell>Method</cell><cell>CS.</cell><cell>CV.</cell></row><row><cell>STCM-9 (CNNs)</cell><cell cols="2">81.31 89.78</cell></row><row><cell>MANs-9 (no attention)</cell><cell cols="2">81.41 89.84</cell></row><row><cell>MANs-9 (other temporal attention)</cell><cell cols="2">81.94 90.12</cell></row><row><cell>MANs-9</cell><cell cols="2">83.01 90.66</cell></row><row><cell>STCM (DenseNet-161)</cell><cell cols="2">81.56 90.24</cell></row><row><cell>MANs (DenseNet-161, no attention)</cell><cell cols="2">81.96 92.15</cell></row><row><cell cols="3">MANs (DenseNet-161, other temporal attention) 81.60 92.18</cell></row><row><cell>MANs (DenseNet-161)</cell><cell cols="2">82.67 93.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the results of different units in MANs on four datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NTU RGB+D</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>#param</cell><cell cols="2">Cross Subject Cross View</cell><cell cols="3">HDM05 SYSU-3D UT-Kinect</cell></row><row><cell>Hierarchical RNNs [Du et al., 2015]</cell><cell>-</cell><cell>59.10</cell><cell>64.00</cell><cell>96.92</cell><cell>-</cell><cell>-</cell></row><row><cell>Dynamic skeletons [Hu et al., 2015]</cell><cell>-</cell><cell>60.23</cell><cell>65.22</cell><cell>-</cell><cell>75.50</cell><cell>-</cell></row><row><cell>Deep LSTM [Zhu et al., 2016]</cell><cell>0.6M</cell><cell>-</cell><cell>-</cell><cell>96.80</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-LSTM [Liu et al., 2016]</cell><cell>-</cell><cell>61.70</cell><cell>75.50</cell><cell>-</cell><cell>76.50</cell><cell>97.00</cell></row><row><cell>ST-LSTM + TG [Liu et al., 2016]</cell><cell>-</cell><cell>69.20</cell><cell>77.70</cell><cell>-</cell><cell>76.80</cell><cell>97.50</cell></row><row><cell>Two-stream RNNs [Wang and Wang, 2017]</cell><cell></cell><cell>71.30</cell><cell>79.50</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STA-LSTM [Song et al., 2017]</cell><cell>0.5M</cell><cell>73.40</cell><cell>81.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Adaptive RNN-T [Li et al., 2017]</cell><cell>-</cell><cell>74.60</cell><cell>83.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GCA-LSTM [Liu et al., 2017a]</cell><cell>-</cell><cell>76.10</cell><cell>84.00</cell><cell>-</cell><cell>78.60</cell><cell>99.00</cell></row><row><cell>Clips+CNN+MTLN [Ke et al., 2017]</cell><cell>62M</cell><cell>79.57</cell><cell>84.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VA-LSTM [Zhang et al., 2017]</cell><cell>-</cell><cell>79.40</cell><cell>87.60</cell><cell>-</cell><cell>77.50</cell><cell>-</cell></row><row><cell>MANs-9</cell><cell>0.8M</cell><cell>83.01</cell><cell>90.66</cell><cell>98.46</cell><cell>87.04</cell><cell>100.0</cell></row><row><cell>MANs-33</cell><cell>3.1M</cell><cell>82.40</cell><cell>90.94</cell><cell>98.85</cell><cell>86.81</cell><cell>100.0</cell></row><row><cell>MANs-61</cell><cell>5.7M</cell><cell>82.42</cell><cell>90.97</cell><cell>98.76</cell><cell>87.63</cell><cell>99.50</cell></row><row><cell>MANs (ResNet-18)</cell><cell>12.0M</cell><cell>79.74</cell><cell>91.55</cell><cell>99.04</cell><cell>86.93</cell><cell>100.0</cell></row><row><cell>MANs (DenseNet-161)</cell><cell>27.6M</cell><cell>82.67</cell><cell>93.22</cell><cell>97.69</cell><cell>78.86</cell><cell>99.00</cell></row><row><cell cols="3">for the four datasets, respectively. Note that all our MANs un-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">der different parameter amounts are able to achieve better per-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">formance than the state-of-the-art RNNs-based approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(e.g. VA-LSTM [Zhang et al., 2017], GCA-LSTM [Liu et</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">al., 2017a]) and the state-of-the-art CNNs-based approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(e.g. Clips+CNN+MTLN [Ke et al., 2017]), demonstrating</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the superiority of our "memory attention + convolution net-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>work" architecture.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://keras.io 3 Here STCM takes [X, X, Z] as image input to perform classification via CNNs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work was supported by the Natural Science Foundation of China under Contract 61672079, 61473086, and 61601466, the Open Projects Program of National Laboratory of Pattern Recognition, and Shenzhen Peacock Plan KQTD2016112515134654. Baochang Zhang is also with Shenzhen Academy of Aerospace Technology, Shenzhen, China.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Articulated and generalized gaussian kernel correlation for human pose estimation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">776</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Meng Ding and Guoliang Fan</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Du</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.4" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Aaron Courville. Deep Learning. MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
	<note>Goodfellow et al., 2016] Ian Goodfellow</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Space-time rep-resentation of people based on 3D skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Informative joints based human action recognition using skeleton contexts</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>Wenbo Li, Longyin Wen, Ming-Ching Chang, Ser Nam Lim, and Siwei Lyu</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Adaptive RNN tree for large-scale human action recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global context-aware attention LSTM network for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphic</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2005" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
	<note>3D skeleton-based human action classification</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time classification of dance gestures from skeleton animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raptis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGGRAPH/Eurographics symposium on computer animation</title>
		<meeting>the 2011 ACM SIGGRAPH/Eurographics symposium on computer animation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shahroudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configuration of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017-04-02" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and Nanning Zheng</title>
		<imprint>
			<publisher>Pengfei Zhang</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3697" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
