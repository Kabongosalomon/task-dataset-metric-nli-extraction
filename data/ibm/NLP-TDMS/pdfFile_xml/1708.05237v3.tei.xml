<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S 3 FD: Single Shot Scale-invariant Face Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<email>shifeng.zhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
							<email>xiangyu.zhu@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<email>zlei@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
							<email>hailin.shi@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
							<email>xiaobo.wang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CBSR &amp; NLPR</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S 3 FD: Single Shot Scale-invariant Face Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a real-time face detector, named Single Shot Scale-invariant Face Detector (S 3 FD), which performs superiorly on various scales of faces with a single deep neural network, especially for small faces. Specifically, we try to solve the common problem that anchorbased detectors deteriorate dramatically as the objects become smaller. We make contributions in the following three aspects: 1) proposing a scale-equitable face detection framework to handle different scales of faces well. We tile anchors on a wide range of layers to ensure that all scales of faces have enough features for detection. Besides, we design anchor scales based on the effective receptive field and a proposed equal proportion interval principle; 2) improving the recall rate of small faces by a scale compensation anchor matching strategy; 3) reducing the false positive rate of small faces via a max-out background label. As a consequence, our method achieves state-of-theart detection performance on all the common face detection benchmarks, including the AFW, PASCAL face, FDDB and WIDER FACE datasets, and can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is the key step of many subsequent facerelated applications, such as face alignment <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b60">61]</ref>, face recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b61">62]</ref>, face verification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref> and face tracking <ref type="bibr" target="#b16">[17]</ref>, etc. It has been well developed over the past few decades. Following the pioneering work of Viola-Jones face detector <ref type="bibr" target="#b47">[48]</ref>, most of early works focus on designing robust features and training effective classifiers. But these approaches depend on non-robust hand-crafted features and optimize each component separately, making the face detection pipeline sub-optimal.</p><p>In recent years, convolutional neural network (CNN) has achieved remarkable successes, ranging from image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> to object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> which also inspires face detection. On the one hand, many works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58]</ref> have applied CNN as the feature extractor in the traditional face detection framewrok. On the other hand, face detection is regarded as a special case of generic object detection and lots of methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59]</ref> have inherited valid techniques from generic object detection method <ref type="bibr" target="#b37">[38]</ref>. Following the latter route, we improve the anchor-based generic object detection frameworks and propose a state-of-the-art face detector.</p><p>Anchor-based object detection methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref> detect objects by classifying and regressing a series of pre-set anchors, which are generated by regularly tiling a collection of boxes with different scales and aspect ratios on the image. These anchors are associated with one <ref type="bibr" target="#b37">[38]</ref> or several <ref type="bibr" target="#b25">[26]</ref> convolutional layers, whose spatial size and stride size determine the position and interval of the anchors, respectively. The anchor-associated layers are convolved to classify and align the corresponding anchors. Comparing with other methods, anchor-based detection methods are more robust in complicated scenes and their speed is invariant to object numbers. However, as indicated in <ref type="bibr" target="#b11">[12]</ref>, the performance of anchor-based detectors drop dramatically as the objects becoming smaller. In order to present a scaleinvariant anchor-based face detector, we comprehensively analyze the reasons behind the above problem as follows:</p><p>Biased framework. The anchor-based detection frameworks tend to miss small and medium faces. Firstly, the stride size of the lowest anchor-associated layer is too large (e.g., 8 pixels in <ref type="bibr" target="#b25">[26]</ref> and 16 pixels in <ref type="bibr" target="#b37">[38]</ref>), therefore small and medium faces have been highly squeezed on these layers and have few features for detection, see <ref type="figure" target="#fig_0">Fig. 1</ref>(a). Secondly, small face, anchor scale and receptive field are mutual mismatch: anchor scale mismatches receptive field and both are too large to fit small face, see <ref type="figure" target="#fig_0">Fig. 1</ref>(b). To address the above problems, we propose a scale-equitable face detection framework. We tile anchors on a wide range of layers whose stride size vary from 4 to 128 pixels, which guarantees that various scales of faces have enough features for detection. Besides, we design anchors with scales from 16 to 512 pixels over different layers according to the effective receptive field <ref type="bibr" target="#b28">[29]</ref> and a new equal-proportion interval principle, which ensures that anchors at different layers match their corresponding effective receptive field and different scales of anchors evenly distribute on the image.</p><p>Anchor matching strategy.</p><p>In the anchor-based detection frameworks, anchor scales are discrete (i.e., <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256</ref>, 512 in our method) but face scale is continuous. Consequently, those faces whose scale distribute away from anchor scales can not match enough anchors, such as tiny and outer face in <ref type="figure" target="#fig_0">Fig. 1</ref>(c), leading to their low recall rate. To improve the recall rate of these ignored faces, we propose a scale compensation anchor matching strategy with two stages. The first stage follows current anchor matching method but adjusts a more reasonable threshold. The second stage ensures that every scale of faces match enough anchors through scale compensation.</p><p>Background from small anchors. To detect small faces well, plenty of small anchors have to be densely tiled on the image. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, these small anchors lead to a sharp increase in the number of negative anchors on the background, bringing about many false positive faces. For example, in our scale-equitable framework, over 75% of negative anchors come from the lowest conv3 3 layer, which is used to detect small faces. In this paper, we propose a max-out background label for the lowest detection layer to reduce the false positive rate of small faces.</p><p>For clarity, the main contributions of this paper can be summarized as:</p><p>• Proposing a scale-equitable face detection framework with a wide range of anchor-associated layers and a series of reasonable anchor scales so as to handle different scales of faces well. • Presenting a scale compensation anchor matching strategy to improve the recall rate of small faces. • Introducing a max-out background label to reduce the high false positive rate of small faces. • Achieving state-of-the-art results on AFW, PASCAL face, FDDB and WIDER FACE with real-time speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Face detection has attracted extensive research attention in past decades. The milestone work of Viola-Jones <ref type="bibr" target="#b47">[48]</ref> uses Haar feature and AdaBoost to train a cascade of face/non-face classifiers that achieves a good accuracy with real-time efficiency. After that, lots of works have focused on improving the performance with more sophisticated hand-crafted features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60]</ref> and more powerful classifiers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. Besides the cascade structure, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b62">63]</ref> introduce deformable part models (DPM) into face detection tasks and achieve remarkable performance. However, these methods highly depend on the robustness of hand-crafted features and optimize each component separately, making face detection pipeline sub-optimal.</p><p>Recent years have witnessed the advance of CNN-based face detectors. CascadeCNN <ref type="bibr" target="#b20">[21]</ref> develops a cascade architecture built on CNNs with powerful discriminative capability and high performance. Qin et al. <ref type="bibr" target="#b33">[34]</ref> proposes to jointly train CascadeCNN to realize end-to-end optimization. Faceness <ref type="bibr" target="#b54">[55]</ref> trains a series of CNNs for facial attribute recognition to detect partially occluded faces. MTCNN <ref type="bibr" target="#b57">[58]</ref> proposes to jointly solve face detection and alignment using several multi-task CNNs. UnitBox <ref type="bibr" target="#b56">[57]</ref> introduces a new intersection-over-union loss function.</p><p>Additionally, face detection has inherited some achievements from generic object detection tasks. Jiang et al. <ref type="bibr" target="#b14">[15]</ref> applies Faster R-CNN in face detection and achieves promising results. CMS-RCNN <ref type="bibr" target="#b58">[59]</ref> uses Faster R-CNN in face detection with body contextual information. Convnet <ref type="bibr" target="#b23">[24]</ref> integrates CNN with 3D face model in an endto-end multi-task learning framework. Wan et al. <ref type="bibr" target="#b48">[49]</ref> combines Faster R-CNN with hard negative mining and achieves significant boosts in face detection performance. STN <ref type="bibr" target="#b2">[3]</ref> proposes a new supervised transformer network and a ROI convolution with RPN for face detection. Sun et al. <ref type="bibr" target="#b42">[43]</ref> presents several effective strategies to improve Faster RCNN for resolving face detection tasks. In this paper, inspired by the RPN in Faster RCNN <ref type="bibr" target="#b37">[38]</ref> and the multi-scale mechanism in SSD <ref type="bibr" target="#b25">[26]</ref>, we develop a state-ofthe-art face detector with real-time speed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Single shot scale-invariant face detector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scale-equitable framework</head><p>Our scale-equitable framework is based on the anchorbased detection framework, such as RPN <ref type="bibr" target="#b37">[38]</ref> and SSD <ref type="bibr" target="#b25">[26]</ref>. Despite its great achievement, the main drawback of the framework is that the performance drops dramatically as the face becomes smaller <ref type="bibr" target="#b11">[12]</ref>. To improve the robustness to face scales, we develop a network architecture with a wide range of anchor-associated layers, whose stride size gradually double from 4 to 128 pixels. Hence, our architecture ensures that different scales of faces have adequate features for detection at corresponding anchorassociated layers. After determining the location of anchors, we design the scales of anchors from 16 to 512 pixels based on the effective receptive field and our equalproportion interval principle. The former guarantees that each scale of anchors matches the corresponding effective receptive field well, and the latter makes different scales of anchors have the same density on the image.</p><p>Constructing architecture. Our architecture (see <ref type="figure" target="#fig_1">Fig.2</ref>) is based on the VGG16 <ref type="bibr" target="#b41">[42]</ref> network (truncated before any classification layers) with some auxiliary structures:</p><p>• Base Convolutional Layers: We keep layers of VGG16 from conv1 1 to pool5, and remove all the other layers. • Extra Convolutional Layers: We convert fc6 and fc7 of VGG16 to convolutional layers by subsampling their parameters <ref type="bibr" target="#b3">[4]</ref>, then add extra convolutional layers behind them. These layers decrease in size progressively and form the multi-scale feature maps. • Detection Convolutional Layers: We select conv3 3, conv4 3, conv5 3, conv fc7, conv6 2 and conv7 2 as the detection layers, which are associated with different scales of anchor to predict detections. • Normalization Layers: Comparing to other detection layers, conv3 3, conv4 3 and conv5 3 have different feature scales. Hence we use L2 normalization <ref type="bibr" target="#b26">[27]</ref> to rescale their norm to 10, 8 and 5 respectively, then learn the scale during the back propagation. • Predicted Convolutional Layers: Each detection layer is followed by a p×3×3×q convolutional layer, where p and q are the channel number of input and output, and 3×3 is the kernel size. For each anchor, we predict 4 offsets relative to its coordinates and N s scores for classification, where N s = N m + 1 (N m is the maxout background label) for conv3 3 detection layer and N s = 2 for other detection layers. • Multi-task Loss Layer: We use softmax loss for classification and smooth L1 loss for regression. <ref type="table" target="#tab_1">Anchor RF   conv3 3  4  16  48  conv4 3  8  32  108  conv5 3  16  64  228  conv fc7  32  128  340  conv6 2  64  256  468  conv7 2  128  512  724   Table 1</ref>. The stride size, anchor scale and receptive field (RF) of the six detection layers. The receptive field here is related to 3 × 3 units on the detection layer, since it is followed by a 3×3 predicted convolutional layer to predict detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Layer Stride</head><p>Designing scales for anchors. Each of the six detection layers is associated with a specific scale anchor (i.e., the third column in Tab. 1) to detect corresponding scale faces. Our anchors are 1:1 aspect ratio (i.e., square anchor), because the bounding box of face is approximately square. As listed in the second and fourth column of Tab. 1, the stride size and the receptive field of each detection layer are fixed, which are two base points when we design the anchor scales:</p><p>• Effective receptive field: As pointed out in <ref type="bibr" target="#b28">[29]</ref>, a unit in the CNN has two types of receptive fields. One is the theoretical receptive field, which indicates the input region that can theoretically affect the value of this unit. However, not every pixel in the theoretical receptive field contributes equally to the final output. In general, center pixels have much larger impacts than outer pixels, as shown in <ref type="figure">Fig. 3(a)</ref>. In other words, only a fraction of the area has effective influence on the output value, which is another type of receptive field, named the effective receptive field. According to this theory, the anchor should be significantly smaller than theoretical receptive field in order to match the effective receptive field (see the specific example in <ref type="figure">Fig. 3(b)</ref>).</p><p>• Equal-proportion interval principle: The stride size of a detection layer determines the interval of its anchor on the input image. For example, the stride size of conv3 3 is 4 pixels and its anchor is 16×16, indicating that there is a 16 × 16 anchor for every 4 pixels on the input image. As shown in the second and third column in Tab. 1, the scales of our anchors are 4 times its interval. We call it equal-proportion interval principle (illustrated in <ref type="figure">Fig. 3(c)</ref>), which guarantees that different scales of anchor have the same density on the image, so that various scales face can approximately match the same number of anchors.</p><p>Benefits from the scale-equitable framework, our face detector can handle various scales of faces better, especially for small faces.  <ref type="figure">Figure 3</ref>. (a) Effective receptive field: The whole black box is the theoretical receptive field (TRF) and the white point cloud with Gaussian distribution is the effective receptive field (ERF). ERF only occupies a fraction of TRF. The figure is from <ref type="bibr" target="#b28">[29]</ref>. (b) A specific example: In our framework, conv3 3's TRF is 48 × 48 (the black dotted box) and ERF is the blue dotted circle estimated by (a). Its anchor is 16 × 16 (the red solid line box), which is much smaller than TRF but matches ERF. (c) Equal-proportion interval principle: Assuming n is the anchor scale, so n/4 is the interval of this scale anchor. n/4 also corresponds to the stride size of the layer associated with this anchor. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scale compensation anchor matching strategy</head><p>During training, we need to determine which anchors correspond to a face bounding box. Current anchor matching method firstly matches each face to the anchors with the best jaccard overlap <ref type="bibr" target="#b4">[5]</ref> and then matches anchors to any face with jaccard overlap higher than a threshold (usually 0.5). However, anchor scales are discrete while face scales are continuous, these faces whose scales distribute away from anchor scales can not match enough anchors, leading to their low recall rate. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, we count the average number of matched anchors for different scales of faces. There are two observations: 1) the average number of matched anchors is about 3 which is not enough to recall faces with high scores; 2) the number of matched anchors is highly related to the anchor scales. The faces away from anchor scales tend to be ignored, leading to their low recall rate. To solve these problems, we propose a scale compensation anchor matching strategy with two stages:</p><p>• Stage one: We follow current anchor matching method but decrease threshold from 0.5 to 0.35 in order to increase the average number of matched anchors. • Stage Two: After stage one, some faces still do not match enough anchors, such as tiny and outer faces marked with the gray dotted curve in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>. We deal with each of these faces as follow: firstly picking out anchors whose jaccard overlap with this face are higher than 0.1, then sorting them to select top-N as matched anchors of this face. We set N as the average number from stage one. As shown in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>, our anchor matching strategy greatly increases the matched anchors of tiny and outer faces, which notably improve the recall rate of these faces.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Max-out background label</head><p>Anchor-based face detection methods can be regarded as a binary classification problem, which determines if an anchor is face or background. In our method, it is an extremely unbalanced binary classification problem: according to our statistical results, over 99.8% of the pre-set anchors belong to negative anchors (i.e., background) and only a few of anchors are positive anchors (i.e., face). This extreme imbalance is mainly caused by the detection of small faces. Specifically, we have to densely tile plenty of small anchors on the image to detect small faces, which causes a sharp increase in the number of negative anchors. For example, as listed in Tab. 2, a 640 × 640 image has totally 34, 125 anchors, while about 75% of them come from conv3 3 detection layer which is associated with the smallest anchor <ref type="bibr">(16 × 16)</ref>. These smallest anchors contribute most to the false positive faces. As a result, improving the detection rate of small faces by tiling small anchors will inevitably lead to the high false positive rate of small faces. To address this issue, we propose to apply a more sophisticated classification strategy on the lowest layer to handle the complicated background from small anchors. We apply the max-out background label for the conv3 3 detection layer. For each of the smallest anchors, we predict N m scores for background label and then choose the highest as its final score, as illustrated in <ref type="figure" target="#fig_5">Fig. 4(b)</ref>. Max-out operation integrates some local optimal solutions into our S 3 FD model so as to reduce the false positive rate of small faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>In this subsection, we introduce the training dataset, data augmentation, loss function, hard negative mining and other implementation details.</p><p>Training dataset and data augmentation. Our model is trained on 12, 880 images of the WIDER FACE training set with the following data augmentation strategies:</p><p>• Color distort: Applying some photo-metric distortions similar to <ref type="bibr" target="#b10">[11]</ref>. • Random crop: We apply a zoom in operation to generate larger faces since there are too many small faces in the WIDER FACE training set. Specifically, each image is randomly selected from five square patches, which are randomly cropped from the original image: one is the biggest square patch, and the size of the other four square patches range between [0.3, 1] of the short size of the original image. We keep the overlapped part of the face box if its center is in the sampled patch. • Horizontal flip: After random cropping, the selected square patch is resized to 640 × 640 and horizontally flipped with probability of 0.5.</p><p>Loss function. We employ the multi-task loss defined in RPN [?] to jointly optimize model parameters:</p><formula xml:id="formula_0">L({pi},{ti}) = λ N cls i L cls (pi,p * i )+ 1 Nreg i p * i Lreg(ti,t * i ),</formula><p>where i is the index of an anchor and p i is the predicted probability that anchor i is a face. The ground-truth label p * i is 1 if the anchor is positive, 0 otherwise. As defined in [?], t i is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t * i is that of the groundtruth box associated with a positive anchor. The classification loss L cls (p i , p * i ) is softmax loss over two classes (face vs. background). The regression loss L reg (t i , t * i ) is the smooth L1 loss defined in <ref type="bibr" target="#b7">[8]</ref> and p * i L reg means the regression loss is activated only for positive anchors and disabled otherwise. The two terms are normalized by N cls and N reg , and weighted by a balancing parameter λ. In our implementation, the cls term is normalized by the number of positive and negative anchors, and the reg term is normalized by the number of positive anchors. Because of the imbalance between the number of positive and negative anchors, λ is used to balance these two loss terms.</p><p>Hard negative mining. After anchor matching step, most of the anchors are negative, which introduces a significant imbalance between the positive and negative training examples. For faster optimization and stable training, instead of using all or randomly select some negative samples, we sort them by the loss values and pick the top ones so that the ratio between the negatives and positives is at most 3:1. With hard negative mining, we set above background label N m = 3, and λ = 4 to balance the loss of classification and regression.</p><p>Other implementation details. As for the parameter initialization, the base convolutional layers have the same architecture as VGG16 and their parameters are initialized from the pre-trained <ref type="bibr" target="#b38">[39]</ref> VGG16. The parameters of conv fc6 and conv fc7 are initialized by subsampling parameters from fc6 and fc7 of VGG16 and the other additonal layers are randomly initialized with the "xavier" method <ref type="bibr" target="#b8">[9]</ref>. We fine-tune the resulting model using SGD with 0.9 momentum, 0.0005 weight decay and batch size 32. The maximum number of iterations is 120k and we use 10 −3 learning rate for the first 80k iterations, then continue training for 20k iterations with 10 −4 and 10 −5 . Our method is implemented in Caffe <ref type="bibr" target="#b13">[14]</ref> and the code is available at https://github.com/sfzhang15/SFD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly analyze the effectiveness of our scale-equitable framework, scale compensation anchor matching strategy and max-out background label, then evaluate the final model on common face detection benchmarks, finally introduce the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model analysis</head><p>We analyze our model on the WIDER FACE validation set by extensive experiments. The WIDER FACE validation set has easy, medium and hard subsets, which roughly correspond to large, medium and small faces, respectively. Hence it is suitable to evaluate our model.</p><p>Baseline. To evaluate our contributions, we carry out comparative experiments with our baselines. Our S 3 FD is inspired by RPN <ref type="bibr" target="#b37">[38]</ref> and SSD <ref type="bibr" target="#b25">[26]</ref>, so we directly use them to train two face detectors as the baselines, marked as RPNface and SSD-face, respectively. Different from <ref type="bibr" target="#b37">[38]</ref>, the RPN-face tiles six scales of the square anchor <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256</ref>, 512) on the conv5 3 layer of VGG16 to make the comparison more substantial. The SSD-face inherits the architecture and anchor-setting of SSD. The remainder is set as the same with our S 3 FD.</p><p>Ablative Setting. To understand S 3 FD better, we conduct ablation experiments to examine how each proposed component affects the final performance. We evaluate the performance of our method under three different settings:  From the results listed in Tab. 3, some promising conclusions can be summed up as follows:</p><p>Scale-equitable framework is crucial. Comparing with S 3 FD(F), the only difference with RPN-face and SSD-face are their framework. RPN-face has the same choice of anchors as ours but only tiles on the last convolutional layer of VGG16. Not only its stride size (16 pixels) is too large for small faces, but also different scales of anchors have the same receptive field. SSD-face tiles anchors over several convolutional layers, while its smallest stride size (8 pixels) and smallest anchors are still slightly large for small faces. Besides, its anchors do not match the effective receptive field. The result of S 3 FD(F) in Tab. 3 shows that our framework greatly outperforms SSD-face and RPNface, especially on the hard subsets (rising by 8.6%), which mainly consists of small faces. Comparing the results between different subsets, our S 3 FD(F) handles various scales of faces well, and deteriorates slightly as the faces become smaller, which demonstrates the robustness to face scales.</p><p>Scale compensation anchor matching strategy is better. The comparison between the third and fourth rows in Tab. 3 indicates that our scale compensation anchor matching strategy effectively improves the performance, especially for small faces. The mAP is increased by 0.9%, 0.4%, 2.2% on easy, medium and hard subset, respectively. The increases mainly come from the higher recall rate of various scales of faces, especially for those faces that are ignored by the current anchor matching method.</p><p>Max-out background label is promising. The last contribution of S 3 FD is the max-out background label. It deals with the massive small negative anchors (i.e., background) from the conv3 3 detection layer which is designed to detect small faces. As reported in Tab. 3, the improvements on easy, medium and hard subsets are 0.2%, 0.4%, 0.7%, respectively. It demonstrates that the effectiveness of the maxout background label is positively related to the difficulty of the input image. Since the harder images will generate the more difficult small backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on benchmark</head><p>We evaluate our S 3 FD method on all the common face detection benchmarks, including Annotated Faces in the Wild (AFW) <ref type="bibr" target="#b62">[63]</ref>, PASCAL Face <ref type="bibr" target="#b51">[52]</ref>, Face Detection Data Set and Benchmark (FDDB) <ref type="bibr" target="#b12">[13]</ref> and WIDER FACE <ref type="bibr" target="#b55">[56]</ref>. Due to the limited space, some qualitative results on these dataset will be shown in the supplementary materials.</p><p>AFW dataset. It contains 205 images with 473 labeled faces. We evaluate our model against the well-known works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref> and commercial face detectors (e.g., Face.com, Face++ and Picasa). As illustrated in <ref type="figure" target="#fig_7">Fig.5</ref>, our S 3 FD outperforms all others by a large margin. PASCAL face dataset. It has 1, 335 labeled faces in 851 images with large face appearance and pose variations. It is collected from PASCAL person layout test subset. <ref type="figure" target="#fig_8">Fig.6</ref> shows the precision-recall curves on this dataset, our method significantly outperforms all other methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref> and commercial face detectors (e.g., Sky-Biometry, Face++ and Picasa).  There are two problems for evaluation: 1) FDDB adopts the bounding ellipse while our S 3 FD outputs rectangle bounding box. This inconsistency has a great impact on the continuous score, so we train an elliptical regressor to transform our predicted bounding boxes to bounding ellipses. 2) FDDB has lots of unlabelled faces, which results in many false positive faces with high scores. Hence, we manually review the results and add 238 unlabelled faces (annotations will be released later and some examples are shown in the supplementary materials). Finally, we evaluate our face detector on FDDB against the state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. The results are shown in <ref type="figure" target="#fig_10">Fig. 7(a)</ref> and <ref type="figure" target="#fig_10">Fig.7(b)</ref>. Our S 3 FD achieves the state-of-the-art performance and outperforms all others by a large margin on discontinuous and continuous ROC curves. These results indicate that our S 3 FD can robustly detect unconstrained faces. WIDER FACE dataset. It has 32, 203 images and labels 393, 703 faces with a high degree of variability in scale, pose and occlusion. The database is split into training (40%), validation (10%) and testing (50%) set. Besides, the images are divided into three levels (Easy, Medium and Hard subset) according to the difficulties of the detection. The images and annotations of training and validation set are available online, while the annotations of testing set are not released and the results are sent to the database server for receiving the precision-recall curves. Our S 3 FD is trained only on the training set and tested on both validation and testing set against recent face detection methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. The precision-recall curves and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference time</head><p>During inference, our method outputs a large number of boxes (e.g., 25, 600 boxes for a VGA-resolution image). To speed up the inference time, we first filter out most boxes by a confidence threshold of 0.05 and keep the top 400 boxes before applying NMS, then we perform NMS with jaccard overlap of 0.3 and keep the top 200 boxes. We measure the speed using Titan X (Pascal) and cuDNN v5.1 with Intel Xeon E5-2683v3@2.00GHz. For the VGAresolution image with batch size 1 using a single GPU, our face detector can run at 36 FPS and achieve the real-time speed. Besides, about 80% of the forward time is spent on the VGG16 network, hence using a faster base network could further improve the speed. <ref type="bibr" target="#b0">1</ref> Our latest results on WIDER FACE are shown in <ref type="figure" target="#fig_0">Fig. 16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces a novel face detector by solving the common problem of anchor-based detection methods whose performance decrease sharply as the objects becoming smaller. We analyze the reasons behind this problem, and propose a scale-equitable framework with a wide range of anchor-associated layers and a series of reasonable anchor scales in order to well handle different scales of faces. Besides, we propose the scale compensation anchor matching strategy to improve the recall rate of small faces, and the max-out background label to reduce the false positive rate of small faces. The experiments demonstrate that our three contributions lead S 3 FD to the state-of-the-art performance on all the common face detection benchmarks, especially for small faces. In our future work, we intend to further improve the classification strategy of background patches. We believe that explicitly dividing the background class into some sub-categories is worthy of further study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Precision-recall curves</head><p>In our submitted paper, Tab. 3 in subsection 4.1 only provides the mAP of RPN-face, SSD-face, S 3 FD(F), S 3 FD(F+S) and S 3 FD(F+S+M). Their precision-recall curves on the WIDER FACE validation set are shown in <ref type="figure" target="#fig_12">Fig. 9</ref> for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative results</head><p>In this section, we demonstrate some qualitative results on common face detection benchmarks, including AFW ( <ref type="figure" target="#fig_0">Fig. 10</ref>), PASCAL face ( <ref type="figure" target="#fig_0">Fig. 11</ref>), FDDB ( <ref type="figure" target="#fig_0">Fig. 12</ref>) and WIDER FACE <ref type="figure" target="#fig_0">(Fig. 13</ref>). Besides, another impressive result is shown in <ref type="figure" target="#fig_0">Fig. 14.   Figure 10</ref>. Qualitative results on AFW. The faces in these results have a high degree of variability in scale, pose and occlusion. Our S 3 FD is able to detect these faces with a high confidence, especially for small faces. Please zoom in to see some small detections. <ref type="figure" target="#fig_0">Figure 11</ref>. Qualitative results on PASCAL face. Most faces in these results are small faces, because the image in PASCAL face has a low resolution. Our S 3 FD is able to handle small faces well. Please zoom in to see some small detections. <ref type="figure" target="#fig_0">Figure 12</ref>. Qualitative results on FDDB. These results indicate that our S 3 FD is robust to large appearance, heavy occlusion, scale variance and heavy blur. Please zoom in to see some small detections.</p><p>(a) Scale attribute. Our S 3 FD is able to detect faces at a continuous range of scales.</p><p>(b) Our S 3 FD is robust to pose, occlusion, expression, makeup, illumination and blur. <ref type="figure" target="#fig_0">Figure 13</ref>. Qualitative results on WIDER FACE. We visualize some examples for each attribute. Please zoom in to see small detections. <ref type="figure" target="#fig_0">Figure 14</ref>. Another qualitative result. Our S 3 FD can find 853 faces out of the reportedly 1000 present in the above image. Detector confidence is given by the colorbar on the right. Please zoom in to see some small detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Examples of manually labelled faces on FDDB</head><p>We add 238 unlabelled faces whose height and width are more than 20 pixels. Some examples are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablative analysis of each detection layers</head><p>To examine the contribution of each detection layers on the mAP performance, we progressively remove the detection layers to test their contribution on the WIDER FACE Val set. The detailed experiment results are listed in Tab. 4. After removing Conv3 3 layer, the mAP changes are +0.3%(Easy), +0.5%(Medium) and -24.7%(Hard), showing Conv3 3 is crucial to detect small faces, but tiling plenty of smallest anchors also slightly hurts medium and large face detection performance. Besides, the most contribution of Easy and Medium subset are Conv5 3 (25.8%) and Conv4 3 (20.6%), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection layers</head><p>Ablative analysis  E. Latest results on the WIDER FACE dataset <ref type="figure" target="#fig_0">Fig. 16</ref> shows the latest precision-recall (PR) curves of our S 3 FD (i.e., SFD-F and SFD-C) on WIDER FACE validation and test sets. SFD-F and SFD-C are our upgraded detectors. SFD-F further improves the detection ability of small faces and SFD-C focuses more on big and medium faces. The RP curves of SFD-F and SFD-C can be downloaded from the official website of WIDER FACE dataset 2 , which plots only the RP curves of SFD-F on its figure with the legend "SFD". Our S 3 FD achieves the best average precision on all subsets, i.e. 0.942 (Easy), 0.930 (Medium) and 0.859 (Hard) for validation set, and 0.937 (Easy), 0.925 (Medium) and 0.858 (Hard) for testing set.  <ref type="figure" target="#fig_0">Figure 16</ref>. The latest precision-recall curves on WIDER FACE validation and test sets. <ref type="bibr" target="#b2">3</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Reasons behind the problem of anchor-based methods. (a) Few features: Small faces have few features at detection layer. (b) Mismatch: Anchor scale mismatches receptive field and both are too large to fit small face. (c) Anchor matching strategy: The figure demonstrates the number of matched anchors at different face scales under current anchor matching method. It reflects that tiny and outer faces match too little anchors. (d) Background from small anchors: The two figures have the same resolution. The left one tiles small anchors to detect the small face and the right one tiles big anchors to detect the big face. Small anchors bring about plenty of negative anchors on the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of Single Shot Scale-invariant Face Detector (S 3 FD). It consists of Base Convolutional Layers, Extra Convolutional Layers, Detection Convolutional Layers, Normalization Layers, Predicted Convolutional Layers and Multi-task Loss Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This section introduces our single shot scale-invariant face detector, including the scale-equitable framework (Sec. 3.1), the scale compensation anchor matching strategy (Sec. 3.2), the max-out background label (Sec. 3.3) and the associated training methodology (Sec. 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>(a) The matched number for different scales of faces are compared between current anchor matching method and our scale compensation anchor matching strategy. (b) The illustration of the max-out background label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(i) S 3 FD(F): it only uses the scale-equitable framework (i.e., constructed architecture and designed anchors) and ablates another two components; (ii) S 3 FD(F+S): it applies the scale-equitable framework and the scale compensation anchor matching strategy; (iii) S 3 FD(F+S+M): it is our complete model, consisting of the scale-equitable framework, the scale compensation anchor matching strategy and the max-out background label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Precision-recall curves on AFW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Precision-recall curves on PASCAL face dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FDDB dataset .</head><label>dataset</label><figDesc>It contains 5, 171 faces in 2, 845 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Evaluation on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Precision-recall curves on WIDER FACE validation and test sets. 1 mAP values are shown inFig. 8. Our model outperforms others by a large margin across the three subsets, especially on the hard subset which mainly contains small faces. It achieves the best average precision in all level faces, i.e. 0.937 (Easy), 0.924 (Medium) and 0.852 (Hard) for validation set, and 0.928 (Easy), 0.913 (Medium) and 0.840 (Hard) for testing set.<ref type="bibr" target="#b0">1</ref> These results not only demonstrate the effectiveness of the proposed method but also strongly show the superiority of the proposed model in detecting small and hard faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Precision-recall curves on WIDER FACE validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .</head><label>15</label><figDesc>Examples of our manually labelled faces on the FDDB dataset. Red ellipses are the faces that FDDB has already labelled, green ellipses are the newly added faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 ×</head><label>2</label><figDesc>mAP changes on Easy subset (%) +0.3 -0.6 -25.8 -10.2 -3.2 -1.4 mAP changes on Medium subset (%) +0.5 -20.6 -12.2 -5.0 -1.5 -0.7 mAP changes on Hard subset (%) -24.7 -8.7 -4.1 -1.8 -0.6 -0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The comparative and ablative results of our model on WIDER FACE validation subset. The precision-recall curves of these methods are in the supplementary materials.</figDesc><table><row><cell>mAP(%)</cell><cell>Subsets</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Easy Medium Hard</cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RPN-face</cell><cell></cell><cell>91.0</cell><cell>88.2</cell><cell>73.7</cell></row><row><cell>SSD-face</cell><cell></cell><cell>92.1</cell><cell>89.5</cell><cell>71.6</cell></row><row><cell>S 3 FD(F)</cell><cell></cell><cell>92.6</cell><cell>91.6</cell><cell>82.3</cell></row><row><cell>S 3 FD(F+S)</cell><cell></cell><cell>93.5</cell><cell>92.0</cell><cell>84.5</cell></row><row><cell cols="2">S 3 FD(F+S+M)</cell><cell>93.7</cell><cell>92.4</cell><cell>85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>S 3</head><label>3</label><figDesc>FD: Single Shot Scale-invariant Face Detector -Supplementary Material-Shifeng Zhang Xiangyu Zhu Zhen Lei Hailin Shi Xiaobo Wang Stan Z. Li CBSR &amp; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China University of Chinese Academy of Sciences, Beijing, China {shifeng.zhang,xiangyu.zhu,zlei,hailin.shi,xiaobo.wang,szli}@nlpr.ia.ac.cn</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The ablative results of each detection layers on the WIDER FACE Val set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html 3 Note-worthily, the latest evaluation code and annotation are used to generate these PR curves, while the results of WIDER FACE reported in our above paper are generated from the previous version of evaluation code or annotation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Plan (Grant No.2016YFC0801002), the Chinese National Natural Science Foundation Projects #61473291, #61502491, #61572501, #61572536, #61672521 and AuthenMetric R&amp;D Funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Face detection with a 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gramajo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3596</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="65" to="86" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiview face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICMR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Occlusion coherence: Detecting and localizing occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learned-Miller. Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UMass Amherst Technical Report</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03473</idno>
		<title level="m">Face detection with the faster r-cnn</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted sampling for large-scale boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face tracking and recognition with visual constraints in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual phrases for exemplar face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic elastic part model for unsupervised face detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient boosted exemplar-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1843" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3468" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face detection with end-to-end integration of a convnet and a 3d model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">To boost or not to boost? on the limits of boosted trees for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast training and selection of haar features using statistics in boosting-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint training of cascaded cnn for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3456" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Detecting and aligning faces by image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3460" to="3467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Face detection using deep learning: An improved faster rcnn approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08289</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fast deep convolutional neural network for face detection in big visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Triantafyllidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INNS Conference on Big Data</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bootstrapping face detection with hard negative examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02236</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2497" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face detection by structural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Convolutional channel features. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Cmsrcnn: contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05413</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Highfidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
