<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Student: Breaking the Limits of the Teacher in Semi-supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-03">3 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoye</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Student: Breaking the Limits of the Teacher in Semi-supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-03">3 Sep 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, consistency-based methods have achieved state-of-the-art results in semi-supervised learning (SSL). These methods always involve two roles, an explicit or implicit teacher model and a student model, and penalize predictions under different perturbations by a consistency constraint. However, the weights of these two roles are tightly coupled since the teacher is essentially an exponential moving average (EMA) of the student. In this work, we show that the coupled EMA teacher causes a performance bottleneck. To address this problem, we introduce Dual Student, which replaces the teacher with another student. We also define a novel concept, stable sample, following which a stabilization constraint is designed for our structure to be trainable. Further, we discuss two variants of our method, which produce even higher performance. Extensive experiments show that our method improves the classification performance significantly on several main SSL benchmarks. Specifically, it reduces the error rate of the 13-layer CNN from 16.84% to 12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with 10k labels. In addition, our method also achieves a clear improvement in domain adaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep supervised learning has gained significant success in computer vision tasks, which leads the community to challenge larger and more complicated datasets like Ima-geNet <ref type="bibr" target="#b28">[29]</ref> and WebVision <ref type="bibr" target="#b17">[18]</ref>. However, obtaining full labels for a huge dataset is usually a very costly task. Hence, more attention is now drawn on deep semi-supervised learning (SSL). In order to utilize unlabeled data, many methods in traditional machine learning have been proposed <ref type="bibr" target="#b35">[36]</ref>, and some of them are successfully adapted to deep learning. In addition, some latest techniques like self-training <ref type="bibr" target="#b4">[5]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref> have been utilized for deep SSL with promising results. A primary track of recent deep semi-supervised methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref> can be summarized as consistency-based * kezhanghan@outlook.com methods. In this type of methods, two roles are commonly created, either explicitly or implicitly: a teacher model and a student model (i.e., a Teacher-Student structure). The teacher guides the student to approximate its performance under perturbations. The perturbations could come from the noise of the input or the dropout layer <ref type="bibr" target="#b31">[32]</ref>, etc. A consistency constraint is then imposed on the predictions between two roles, and forces the unlabeled data to meet the smoothness assumption of semi-supervised learning. The teacher in the Teacher-Student structure can be summarized as being generated by an exponential moving average (EMA) of the student model. In the VAT Model <ref type="bibr" target="#b20">[21]</ref> and the Π Model <ref type="bibr" target="#b16">[17]</ref>, the teacher shares the same weights as the student, which is equivalent to setting the averaging coefficient to zero. The Temporal Model <ref type="bibr" target="#b16">[17]</ref> is similar to Π Model except that it also applies an EMA to accumulate the historical predictions. The Mean Teacher <ref type="bibr" target="#b32">[33]</ref> applies an EMA to the student to obtain an ensemble teacher. In this work, we show that the two roles in the Teacher-Student structure are tightly coupled and the degree of the coupling increases as the training goes on. This phenomenon leads to a performance bottleneck since a coupled EMA teacher is not sufficient for the student.</p><p>To overcome this problem, the knowledge coming from another independent model should help. Motivated by this observation, we replace the EMA teacher by another student model. The two students start from different initial states and are optimized through individual paths during training. Hence, their weights will not be tightly coupled and each learns its own knowledge. What remains unclear is how to extract and exchange knowledge between the students. Naively, adding a consistency constraint may lead to the two models collapsing into each other. Thus, we define the stable sample and propose a stabilization constraint for effective knowledge exchange. Our method improves the performance significantly on several main SSL benchmarks. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the Teacher-Student structure and our Dual Student structure.</p><p>In summary, the main contributions of this work include:</p><p>• We demonstrate that the coupled EMA teacher causes a performance bottleneck of the existing Teacher-Student methods. • We define the stable samples of a model and propose a novel stabilization constraint between models. • We propose a new SSL structure, Dual Student, and discuss two variants of Dual Student with higher performances.</p><p>• Extensive experiments are conducted to evaluate the performance of our method on several benchmarks and in different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>Consistency-based SSL methods are derived from the network noise regularization <ref type="bibr" target="#b29">[30]</ref>. Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref> first showed the advantage of adversarial noise over random noise. Miyato et al. <ref type="bibr" target="#b20">[21]</ref> further explored this idea for unlabeled data and generated virtual adversarial samples for the implicit teacher, while Park et al. <ref type="bibr" target="#b24">[25]</ref> proposed a virtual adversarial dropout based on <ref type="bibr" target="#b31">[32]</ref>. In addition to noise, the quality of targets for the consistency constraint is also vital in this process. Bachman et al. <ref type="bibr" target="#b1">[2]</ref> and Rasmus et al. <ref type="bibr" target="#b27">[28]</ref> showed the effectiveness of regularizing the targets. Laine et al. then proposed the internally consistent Π Model and Temporal Model in <ref type="bibr" target="#b16">[17]</ref>. Tarvainen took advantage of averaging model weights <ref type="bibr" target="#b25">[26]</ref> to obtain an explicit ensemble teacher <ref type="bibr" target="#b32">[33]</ref> for generating targets. Some works derived from the traditional methods also improve the consistencybased SSL. Smooth Neighbor by Luo et al. <ref type="bibr" target="#b18">[19]</ref> utilized the connection between data points and built a neighbor graph to cluster data more tightly. Athiwaratkun et al. <ref type="bibr" target="#b0">[1]</ref> modified the stochastic weight averaging (SWA) <ref type="bibr" target="#b13">[14]</ref> to obtain a stronger ensemble teacher faster. Qiao et al. <ref type="bibr" target="#b26">[27]</ref> proposed Deep Co-Training <ref type="bibr" target="#b26">[27]</ref>, by adding a consistency constraint between independent models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Teacher-Student Structure</head><p>The most common structure of recent SSL methods is the Teacher-Student structure. It applies a consistency con-straint between a teacher model and a student model to learn knowledge from unlabeled data. Formally, we assume that a dataset D consists of an unlabeled subset and a labeled subset. Let θ ′ denote the weights of the teacher, and θ denote the weights of the student. The consistency constraint is defined as:</p><formula xml:id="formula_0">L con = E x∈D R(f (θ, x + ζ), T x ) ,<label>(1)</label></formula><p>where f (θ, x + ζ) is the prediction from model f (θ) for input x with noise ζ. T x is the consistency target from the teacher. R(·, ·) measures the distance between two vectors, and is usually set to mean squared error (MSE) or KLdivergence. Previous works have proposed several ways to generate T x . Π Model: In Π Model, the implicit teacher shares parameters with the student. It forwards a sample x twice with different random noise ζ and ζ ′ in each iteration, and treats the prediction of x + ζ ′ as T x .</p><p>Temporal Model: While Π Model needs to forward a sample twice in each iteration, Temporal Model reduces this computational overhead by using EMA to accumulate the predictions over epochs as T x . This approach could reduce the prediction variance and stabilize the training process.</p><p>Mean Teacher: Temporal Model needs to store a record for each sample, and the target T x gets updated only once per epoch while the student is updated multiple times. Hence, Mean Teacher defines an explicit teacher by an EMA of the student and update its weights in each iteration before generating T x .</p><p>VAT Model: Although random noise is effective in previous methods, VAT Model adopts the adversarial noise to generate better T x for the consistency constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Deep Co-Training</head><p>It is known that fusing knowledge from multiple models could improve performance in SSL <ref type="bibr" target="#b34">[35]</ref>. However, directly adding the consistency constraint between models results in the models collapsing into each other. Deep Co-Training addressed this issue by utilizing the Co-Training assumption from the traditional Co-Training algorithm <ref type="bibr" target="#b2">[3]</ref>. It treats the features from the convolutional layers as a view of the input and uses the adversarial samples from other collaborators to ensure that view differences exist among the models. Consistent predictions can then be used for training. However, this strategy requires generating adversarial samples of each model in the whole process, which is complicated and time-consuming.</p><p>Our method also has interactions between models to break the limits of the EMA teacher, but there are two major differences between our method and Deep Co-Training. First, instead of enforcing the consistency constraint and the different-views constraint, we only extract reliable knowledge of the models and exchange them by a more effective stabilization constraint. Second, our method is more efficient since we do not need the adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Limits of the EMA Teacher</head><p>One fundamental assumption in SSL is the smoothness assumption -"If two data points in a high-density region are close, then so should be the corresponding outputs" <ref type="bibr" target="#b3">[4]</ref>. All existing Teacher-Student methods utilize unlabeled data according to this assumption. In practice, if x andx are generated from a sample with different small perturbations, they should have consistent predictions by the corresponding teacher and student. Previous methods achieve this by the consistency constraint and have mainly focused on generating more meaningful targets through ensemble or welldesigned noise.</p><p>However, previous works neglect that the teacher is essentially an EMA of the student. Hence, their weights are tightly coupled. Formally, the teacher weights θ ′ are an ensemble of the student weights θ in a successive training step t with a smoothing coefficient α ∈ [0, 1]:</p><formula xml:id="formula_1">θ ′ t = α θ ′ t−1 + (1 − α) θ t .<label>(2)</label></formula><p>In Π Model and VAT Model, as α is set to zero, θ ′ is equal to θ. Temporal Model improves Π Model by an EMA on historical predictions, but its teacher still shares weights with the student. As for Mean Teacher, the updates of the student weights decreases as the model converges, i.e., |θ t − θ t−1 | becomes smaller and smaller as the number of training steps t increases. Theoretically, it can be proved that the EMA of a converging sequence converges to the same limit as the sequence, which is shown in Appendix A (Supplementary). Thus, the teacher will be very close to the student when the training process converges. In all the above cases, the coupling fact between the teacher and the student is obvious.</p><p>To further visualize it, we train two structures on the CIFAR-10 SSL benchmark. One contains a student and an EMA teacher (named S ema ) while the other contains two independent models (named S split ). We then calculate the Euclidean distance of the weights and predictions between the two models in each structure. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the results. As expected, the EMA teacher in S ema is very close to the student, and their distance approaches zero with increasing epochs. In contrast, the two models in S split always keep a larger distance from each other. These results confirm our conjecture that the EMA teacher is tightly coupled with the student. In addition, they also demonstrate that the two independent models are loosely coupled.</p><p>Due to the coupling effect between the two roles in the existing Teacher-Student methods, the teacher does not have more meaningful knowledge compared to the student. In addition, if the student has biased predictions for specific samples, the EMA teacher is most likely to accumulate the  <ref type="figure">Figure 3</ref>: Our method can alleviate the confirmation bias. f 1 and f 2 are the independent students from our Dual Student, while f s is the student guided by the Mean Teacher. For a misclassified sample (belonging to class1), f 1 can correct it quickly with the knowledge from f 2 . However, f s is unable to correct its prediction due to the wrong guidance from the EMA teacher. mistakes and to enforce the student to follow, making the misclassification irreversible. This is a case of the confirmation bias <ref type="bibr" target="#b32">[33]</ref>. Most methods apply a ramp-up operation for the consistency constraint to alleviate the bias, but it is inadequate to solve the problem. From this perspective, training independent models are also beneficial. <ref type="figure">Fig. 3</ref> visualizes this inability of the EMA teacher. Three models, f 1 , f 2 , and f s , are trained on a two-category task simultaneously. f s is the student from Mean Teacher. f 1 and f 2 are two relatively independent but interactive models, representing the two students from our Dual Student structure (Section 4). They have the same initialization, while f 2 is different from them. The plot shows how the predictions of a sample from class1 changes with epochs for these three models, which demonstrates that our method can alleviate the confirmation bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dual Student</head><p>As analyzed above, the targets from an EMA teacher are not adequate to guide the student when the number of training steps t is large. Therefore, our method gains the loosely coupled targets by training two independent models simultaneously. However, the outputs of these two models  <ref type="figure">Figure 4</ref>: Dual Student structure overview. We train two student models separately. Each batch includes labeled and unlabeled data and is forwarded twice. The stabilization constraint based on the stable samples is enforced between the students. Each student also learns labeled data by the classification constraint and meets the smooth assumption by the consistency constraint.</p><formula xml:id="formula_2">0 +ζ 1 +ζ 2 +ζ 2 +ζ 1 E i E j E j E i</formula><p>may vary widely, and applying the consistency constraint directly will cause them to collapse into each other by exchanging the wrong knowledge. The EMA teacher does not suffer from this problem due to the coupling effect.</p><p>We propose an efficient way to overcome this problem, which is to exchange only reliable knowledge of the models. To put this idea into practice, we need to solve two problems. One is how to define and acquire reliable knowledge of a model. Another is how to exchange the knowledge mutually. To address them, we define the stable sample in Section 4.1 and then elaborate the derived stabilization constraint for training in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Stable Sample</head><p>A model can be regarded as a decision function that can make reliable predictions for some samples but not for the others. We define the stable sample and treat it as the reliable knowledge of a model. A stable sample satisfies two conditions. First, according to the smoothness assumption, a small perturbation should not affect the prediction of this sample, i.e., the model should be smooth in the neighborhood of this sample. Second, the prediction of this sample is far from the decision boundary. This means that this sample has a high probability for the predicted label.  sample is specific to the models. A data point x can be stable with respect to any one model but may not be to the others. This fact is a key to our stabilization constraint, and will be elaborated in Section 4.2. In addition to the criterion of whether a sample point x is stable or not, we would also like to know the degree of stability of a stable sample x. This can be reflected by the prediction consistency in its neighborhood. The more consistent the predictions are, the more stable x is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training by the Stabilization Constraint</head><p>We briefly introduce Dual Student structure before explaining the details on training. It contains two independent student models, which share the same network architecture with different initial states and are updated separately <ref type="figure">(Fig. 4)</ref>. For our structure to be trainable, we derive a novel stabilization constraint from the stable sample.</p><p>In practice, we only utilize two close samples to approximate the conditions of the stable sample to reduce the computational overhead. Formally, we use θ i and θ j to represent weights of the two students. We first define a boolean function {condition} 1 , which outputs 1 when the condition is true and 0 otherwise. Supposex is a noisy augmentation of a sample x. We then check whether x is a stable sample for student i:</p><formula xml:id="formula_3">R i x = {P i x = P ī x } 1 &amp; ({M i x &gt; ξ} 1 {M ī x &gt; ξ} 1 ) , where M i x = || f (θ i , x) || ∞ .<label>(3)</label></formula><p>P i x and P ī x are the predicted labels of x andx, respectively, by student i. Hyperparameter ξ is a confidence threshold in [0, 1). If the maximum prediction probability of sample x exceeds ξ, x is considered to be far enough from the classification boundary. We then use the Euclidean distance to measure the prediction consistency, to indicate the stability of x, as: <ref type="figure">Figure 5</ref>: Illustration of the conditions for a stable sample. Consider three pairs of adjacent data points: (1) x 1 andx 1 do not satisfy the 1 st condition, (2) x 2 andx 2 do not satisfy the 2 nd condition, and (3) x 3 andx 3 satisfy both conditions.</p><formula xml:id="formula_4">E i x = || f (θ i , x) − f (θ i ,x) || 2 .<label>(4)</label></formula><formula xml:id="formula_5">x f( )=0 | ( ) |= f ξ x f( ) 3 x f( ) 1 x f( ) 2 x f( ) 3 x _ f( ) 1 x _ f( ) 2 x _</formula><p>A smaller E i x means that x is more stable to student i. The distance between the predictions of students i and j can be measured using the mean squared error (MSE) as:</p><formula xml:id="formula_6">L mse (x) = || f (θ i , x) − f (θ j , x) || 2 .<label>(5)</label></formula><p>Finally, the stabilization constraint for the student i on sample x is written as:</p><formula xml:id="formula_7">L i sta (x) = {E i x &gt; E j x } 1 L mse (x), R i x = R j x = 1, R j x L mse (x), otherwise.<label>(6)</label></formula><p>We calculate the stabilization constraint for the student j in the same way. As we can see, the stabilization constraint changes dynamically depending on the outputs of the two students. There are three cases: <ref type="bibr" target="#b0">(1)</ref> No constraint is applied if x is unstable for both students. (2) If x is stable only for student i, it can guide student j. (3) If x is stable for both students, the stability is calculated, and the constraint is applied from the more stable one to the other.</p><p>Following previous works, our Dual Student structure also imposes the consistency constraint in each student to meet the smoothness assumption. We also apply the decoupled top layers trick from the Mean Teacher, which splits the constraints for the classification and the smoothness.</p><p>To train Dual Student, the final constraint for student i is a combination of three parts: the classification constraint, consistency constraint in each model, and stabilization constraint between models, as:</p><formula xml:id="formula_8">L i = L i cls + λ 1 L i con + λ 2 L i sta ,<label>(7)</label></formula><p>where λ 1 and λ 2 are hyperparameters to balance the constraints. Algorithm 1 summarizes the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variants of Dual Student</head><p>Here, we briefly discuss two variants of Dual for each unlabeled sample x do <ref type="bibr">8:</ref> for</p><formula xml:id="formula_9">each model in {f (θ), f (θ ′ )} do 9:</formula><p>Determine whether x is stable by Eq. 3 <ref type="bibr">10:</ref> end for <ref type="bibr" target="#b10">11</ref>:</p><p>if both f (θ) and f (θ ′ ) are stable for x then <ref type="bibr">12:</ref> Calculate the stability of x by Eq. 5 <ref type="bibr">13:</ref> end if <ref type="bibr">14:</ref> Calculate Lsta for f (θ) and f (θ ′ ) by Eq. 6 <ref type="bibr">15:</ref> end for <ref type="bibr">16:</ref> Update f (θ) and f (θ ′ ) by the loss in Eq. 7 <ref type="bibr" target="#b16">17</ref>: end for them have higher performances than the standard Dual Student. They do not increase the inference time, even though more computations are required during training.</p><p>Multiple Student: Our Dual Student can be easily extended to Multiple Student. We followed the same strategy as the Deep Co-Training. We assume that our Multiple Student contains 2n student models. At each iteration, we randomly divide these students into n pairs. Each pair is then updated like Dual Student. Since our method does not require models to have view differences, the data stream can be shared among the students. This is different from Deep Co-Training, which requires an exclusive data stream for each pair. In practice, four students (n = 2) achieve a notable improvement over two students. However, having more than four students do not further improve the performance, as demonstrated in Section 5.2.</p><p>Imbalanced Student: Since a well-designed architecture with more parameters usually has better performance, a pre-trained high-performance teacher can be used to improve the light-weight student in knowledge distillation task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Based on the same idea, we extend Dual Student to Imbalanced Student by enhancing the capability of one student. However, we do not consider the sophisticated model as a teacher, since the knowledge will be exchanged mutually. We find that the improvement of the weak student is proportional to the capability of the strong student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first evaluate Dual Student on several common SSL benchmarks, including CIFAR, SVHN, and ImageNet. We then evaluate the performances of the two variants of Dual Student. We further analyze various aspects of our method Unless specified otherwise, the architecture used in our experiments is a same 13-layer convolutional neural network (CNN), following previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>. Its details are described in Appendix B <ref type="figure">(Supplementary)</ref>. As reported in <ref type="bibr" target="#b23">[24]</ref>, the implementations of recent SSL methods are not exactly same, and the training details (e.g., number of training epochs, optimizer and augmentation) may also be different. For a fair comparison, we implement our method following the previous state-of-the-art <ref type="bibr" target="#b0">[1]</ref>, which uses the standard Batch Norm <ref type="bibr" target="#b12">[13]</ref> instead of the meanonly Batch Norm <ref type="bibr" target="#b11">[12]</ref>. The stochastic gradient descent optimizer is adopted with the learning rate adjustment function γ = γ 0 * (0.5 + cos((t − 1) * π/N )), where t is the current training step, N is the total number of steps, and γ 0 is the initial learning rate. These settings provide better baselines for Π Model and Mean Teacher. For other methods, we use the results from the original papers. More training details are provided in Appendix C (Supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">SSL Benchmarks</head><p>We first evaluate Dual Student on the CIFAR benchmark, including CIFAR-10 <ref type="bibr" target="#b14">[15]</ref> and CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>. CIFAR-10 has 50k training samples and 10k testing samples, from 10 categories. Each sample is a 32×32 RGB image. We extract 1k, 2k, and 4k balanced labels randomly. CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> is a more complex dataset including 100 categories. Each category contains only 500 training samples, together with 100 test samples. We extract 10k balanced labels from it randomly. Besides, we also run experiments with full labels on both datasets. We compare our Dual Student (DS) with some recent consistency-based models, including Π Model (Π), Temporal Model (Temp), Mean Teacher (MT), Smooth Neighbor (SN), FastSWA based on Mean Teacher (MT+FSWA), and Deep Co-Training (Deep CT). We also  <ref type="bibr" target="#b26">[27]</ref> 34.63 ± 0.14 -DS (480)</p><p>32.77 ± 0.24 21.79 ± 0.11</p><p>replace the stabilization constraint in our structure with the consistency constraint (CS) as a baseline. <ref type="table" target="#tab_1">Table 1</ref> shows the results on CIFAR-10. All models are trained for 300 epochs, except for those specified with parentheses. Results marked with a † are obtained from other works that published better performances than the original ones. We can see that our Dual Student boosts the performance on all semi-supervised settings. The results reveal that as the number of labeled samples decreases, our method can gain more significant improvements. Specifically, Dual Student improves the result with 1k labels to 14.17% with only half of training epochs comparing to FastSWA. Similar results can also be observed in the experiments with 2k and 4k labels. <ref type="figure">Fig. 6</ref> shows that the accuracy on only the stable samples is higher than that on all samples, which proves that the stable samples represent the relatively more reliable knowledge of a model. This justifies why our DS with stabilization constraint achieves much better results than the CS. Our result on full labels shows less advantages since the labels play a much more important role in the fully supervised case.    <ref type="bibr" target="#b21">[22]</ref> and Im-ageNet <ref type="bibr" target="#b28">[29]</ref>. Street View House Numbers (SVHN) is a dataset containing 73,257 training samples and 26,032 testing samples. Each sample is a 32 × 32 RGB image with a center close-up of a house number. We only experiment with 250 and 500 labels on SVHN. ImageNet contains more than 10 million RGB images belonging to 1k categories. We extract 10% balanced labels and train a 50-layer ResNeXt model <ref type="bibr" target="#b33">[34]</ref>. <ref type="table" target="#tab_4">Tables 3 and 4</ref> show that Dual Student could improve the results on these datasets of various scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance of Variants</head><p>We evaluate Multiple Student and Imbalanced Student on the CIFAR benchmark. <ref type="table" target="#tab_6">Table 5</ref> compares them with the standard Dual Student, all using the same 13-layer CNN trained for 300 epochs. For Multiple Student (MS), we train both the four students and the eight students. The perfor-  mance improvement is limited when more than four students are trained simultaneously. For Imbalanced Student (IS), we replace one student by a ResNet <ref type="bibr" target="#b7">[8]</ref> with Shake-Shake regularization. We then conduct the experiments on two different model sizes. In particular, a small one with 3.53 million parameters and a large one with 11.65 million parameters. The small ResNet has almost no increase in computational cost, as its number of parameters is similar to that of the 13-layer CNN (3.13 million parameters). Imbalanced Student achieves a significant performance improvement by distilling the knowledge from a more powerful student. Notably, the large ResNet improves the result from 15.74% to 12.39% on CIFAR-10 with 1k labels.</p><p>Our structure can also be combined with existing methods easily to further improve the performance. We replace the consistency constraint inside the model by Mean Teacher. <ref type="figure" target="#fig_5">Fig. 7 (left)</ref> shows the accuracy curves. The obvious performance improvement shows the ability of Dual Student in breaking the limits of the EMA teacher. The accuracy of the combination is similar to that using Dual Student only, which means that our method is insensitive to the type of consistency constraint inside each model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Experiments</head><p>We conduct the ablation experiments on CIFAR-10 with 1k labels to analyze the impact of the confidence threshold and various constraints in our structure.</p><p>Confidence threshold: The confidence threshold ξ controls the 2 nd condition in Def. 4.1 of the stable sample by filtering out samples near to the boundary. Its actual value can be set approximately, since our method is robust to it. Typically, ξ is related to the complexity of the task, e.g., the number of categories to predict or the size of the given dataset. More categories or a smaller size would require a smaller ξ. <ref type="table" target="#tab_7">Table 6</ref> compares different ξ values on the CI-FAR benchmark. The results show that ξ is necessary for a better performance, and a meticulous tuning may only help improve the performance slightly.</p><p>Effect of the constraints: Dual Student learns the unlabeled data by both L sta between models and the L con inside each model. We also study their individual impacts. Besides, we compare the results with the experiment where only the consistency constraint is applied between models (named L cs ). <ref type="figure" target="#fig_5">Fig. 7 (right)</ref> shows that L cs reduces the accuracy in the late stage while L sta helps improve the performance continuously. This demonstrates that our L sta is better than L cs . In addition, L con inside the model also plays a role in boosting the performance further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Domain Adaptation</head><p>Domain adaptation aims to transfer knowledge learned from a labeled dataset to an unlabeled one. French et al. <ref type="bibr" target="#b5">[6]</ref> modified Mean Teacher and Temporal Model to enable domain adaptation and showed the effectiveness of the Teacher-Student structure. In this section, we apply Dual Student for adapting the digit recognition model from USPS to MNIST and show that it could be applied to this kind of task with great advantages over the EMA teacher based methods.</p><p>Both USPS and MNIST are greyscale hand-written number dataset. USPS consists of 7,000 images of 16 × 16, and MNIST contains 60,000 images of 28 × 28. To match the image resolution, we resize all images from USPS to 28 × 28 by cubic spline interpolation. <ref type="figure">Fig. 8</ref> shows the domain difference between the two datasets. In our experiments, we set USPS as the source domain and MNIST as the target domain. We compare our method with Mean Teacher,  <ref type="figure">Fig. 9</ref> shows the test accuracy versus the number of epochs. We can see that naively using supervision from USPS would result in overfitting. Mean Teacher avoids it to some extent and improves the top1 accuracy from 69.09% to 80.41%, but it overfits when the number of training epochs is large. Our Dual Student avoids overfitting and boosts the accuracy to 91.50%, which is much closer to the result obtained by supervision from the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have studied the coupling effect of the existing Teacher-Student methods and shown that it sets a performance bottleneck for the structure. We have proposed a new structure, Dual Student, to break limits of the EMA teacher, and a novel stabilization constraint, which provides an effective way to train independent models (either with the same architecture or not). The stabilization constraint is bidirectional overall but is unidirectional for each stable sample. The improved performance is notable across datasets and tasks. Besides, we have also discussed two variants of Dual Student, with even better results. However, our method still shares similar limitations as existing methods, e.g., increased memory usage during training and performance degradation on increasing number of labels. In the future, we plan to address these issues and extend our structure to other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Teacher-Student versus Dual Student. The teacher (T) in Teacher-Student is an EMA of the student (S), imposing a consistency constraint on the student. Their weights are tightly coupled. In contrast, a bidirectional stabilization constraint is applied between the two students (S and S') in Dual Student. Their weights are loosely coupled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: S ema contains two models with similar weights, while the weights of the two models in S split keep a certain distance. Right: The predictions of the two models in S split keep a larger distance than those of S ema .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 . 1 (</head><label>41</label><figDesc>Stable sample). Given a constant ξ ∈ [0, 1), a dataset D ⊆ R m that satisfies the smoothness assumption and a model f : D → [0, 1] n that satisfies ||f (x)|| 1 = 1 for all x ∈ D, x is a stable sample with respect to f if: 1. ∀x ∈ D near x, their predicted labels are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 .</head><label>2</label><figDesc>x satisfies the inequality: ||f (x)|| ∞ &gt; ξ . 1 Def. 4.1 defines the stable sample, and Fig. 5 illustrates its conditions in details. Notice that the concept of the stable 1 || a || 1 := n i=1 | a i |, || a ||∞ := max i=1..n | a i |, a = (a 1 , a 2 , ..., an)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Test accuracy on CIFAR-10 with 1k labels. Left: Combining our method with Mean Teacher can improve its performance. Right: The effectiveness of our stabilization constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Domain difference between USPS and MNIST. The numbers in USPS are in bold font face and span all over the images without border. Test curves of domain adaptation from USPS to MNIST versus the number of epochs. Dual Student avoids overfitting and improves the result remarkably. source domain (USPS) supervised model, and target domain (MNIST) supervised model (trained on 7k balanced labels). All experiments use a small architecture simplified from the above 13-layer CNN. More details are available in Appendix D (Supplementary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Student, named Multiple Student and Imbalanced Student. Both of Algorithm 1 Training of Dual Student for SSL. Batch B containing labeled and unlabeled samples Require: Two independent models f (θ) and f (θ</figDesc><table><row><cell>2:</cell><cell cols="2">Get B1, B2 from B by data augmentation</cell></row><row><cell>3:</cell><cell>for each model in {f (θ), f (θ</cell><cell>′ )} do</cell></row><row><cell>4:</cell><cell cols="2">Calculate L cls on labeled samples</cell></row><row><cell>5:</cell><cell cols="2">Calculate Lcon by Eq. 1 between B1 and B2</cell></row><row><cell>6:</cell><cell>end for</cell><cell></cell></row><row><cell>7:</cell><cell></cell><cell></cell></row></table><note>Require:′ )1: for each batch B do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test error rate on CIFAR-10 averaged over 5 runs. Parentheses show numbers of training epochs (default 300). 31.65 ± 1.20 † 17.57 ± 0.44 † 12.36 ± 0.31 5.56 ± 0.10 Π + SN [19] 21.23 ± 1.27 14.65 ± 0.31 11.00 ± 0.13 5.19 ± 0.14 Temp [17] 23.31 ± 1.01 † 15.64 ± 0.39 † 12.16 ± 0.24 5.60 ± 0.10 ± 0.31 † 14.43 ± 0.20 † 11.41 ± 0.27 † 5.98 ± 0.21 †</figDesc><table><row><cell>Model</cell><cell>1k labels</cell><cell>2k labels</cell><cell>4k labels</cell><cell>all labels</cell></row><row><cell>Π [17]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temp + SN [19]</cell><cell>18.41 ± 0.52</cell><cell>13.64 ± 0.32</cell><cell>10.93 ± 0.34</cell><cell>5.20 ± 0.14</cell></row><row><cell cols="2">MT [33] 18.78 MT + FSWA [1] 16.84 ± 0.62</cell><cell>12.24 ± 0.31</cell><cell>9.86 ± 0.27</cell><cell>5.14 ± 0.07</cell></row><row><cell>CS</cell><cell>17.38 ± 0.52</cell><cell>13.76 ± 0.27</cell><cell>10.24 ± 0.20</cell><cell>5.18 ± 0.11</cell></row><row><cell>DS</cell><cell cols="3">15.74 ± 0.45 11.47 ± 0.14 9.65 ± 0.12</cell><cell>5.20 ± 0.03</cell></row><row><cell cols="2">MT + FSWA (1200) [1] 15.58 ± 0.12</cell><cell>11.02 ± 0.23</cell><cell>9.05 ± 0.21</cell><cell>4.73 ± 0.18</cell></row><row><cell>Deep CT (600) [27]</cell><cell>-</cell><cell>-</cell><cell>9.03 ± 0.18</cell><cell>-</cell></row><row><cell>DS (600)</cell><cell cols="3">14.17 ± 0.38 10.72 ± 0.19 8.89 ± 0.09</cell><cell>4.66 ± 0.07</cell></row><row><cell cols="2">through ablation experiments. Finally, we demonstrate the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">application of Dual Student in a domain adaptation task.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test error rate on CIFAR-100 averaged over 5 runs.</figDesc><table><row><cell>Model</cell><cell>10k labels</cell><cell>all labels</cell></row><row><cell>Temp [17]</cell><cell>38.65 ± 0.51</cell><cell>26.30 ± 0.15</cell></row><row><cell>Π [17]</cell><cell>39.19 ± 0.36</cell><cell>26.32 ± 0.04</cell></row><row><cell>Π + FSWA [1]</cell><cell>35.14 ± 0.71</cell><cell>22.00 ± 0.21</cell></row><row><cell>MT [33]</cell><cell>35.96 ± 0.77  †</cell><cell>23.37 ± 0.16  †</cell></row><row><cell>MT + FSWA [1]</cell><cell>34.10 ± 0.31</cell><cell>21.84 ± 0.12</cell></row><row><cell>DS</cell><cell cols="2">33.08 ± 0.27 21.90 ± 0.14</cell></row><row><cell cols="2">MT + FSWA (1200) [1] 33.62 ± 0.54</cell><cell>21.52 ± 0.12</cell></row><row><cell>Deep CT (600)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>lists the results on CIFAR-100. Especially, in 10k label experiments, Dual Student records a new state-of-the-art 32.77% with</figDesc><table><row><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>75 85 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>all samples</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>stable samples</cell></row><row><cell></cell><cell>air-</cell><cell>auto-</cell><cell>bird</cell><cell>cat</cell><cell>deer</cell><cell>dog</cell><cell>frog horse ship truck</cell></row><row><cell></cell><cell>plane</cell><cell>mobile</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Categories</cell></row><row><cell cols="8">Figure 6: Test accuracy of each category on the stable sam-</cell></row><row><cell cols="8">ples and on all samples of CIFAR-10. The performance gap</cell></row><row><cell cols="8">indicates that the stable samples represent relatively more</cell></row><row><cell cols="8">reliable knowledge of a model. The average ratio of the</cell></row><row><cell cols="8">stable samples on the test set is about 85% w.r.t. the model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test error rate on SVHN averaged over 5 runs.</figDesc><table><row><cell>Model</cell><cell>250 labels</cell><cell>500 labels</cell></row><row><cell cols="3">Supervised [33] 27.77 ± 3.18 16.88 ± 1.30</cell></row><row><cell>MT [33]</cell><cell>4.35 ± 0.50</cell><cell>4.18 ± 0.27</cell></row><row><cell>DS</cell><cell cols="2">4.24 ± 0.10 3.96 ± 0.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test error rate on ImageNet averaged over 2 runs.</figDesc><table><row><cell>Model</cell><cell cols="2">10% labels-top1 10% labels-top5</cell></row><row><cell cols="2">Supervised 42.15 ± 0.09</cell><cell>19.76 ± 0.11</cell></row><row><cell>MT [33]</cell><cell>37.83 ± 0.12</cell><cell>16.65 ± 0.08</cell></row><row><cell>DS</cell><cell>36.48 ± 0.05</cell><cell>16.42 ± 0.07</cell></row><row><cell cols="3">less training epochs than FastSWA and Deep Co-Training.</cell></row><row><cell cols="3">To evaluate the generalization ability of Dual Student,</cell></row><row><cell cols="3">we also conduct experiments on both SVHN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Test error rate of two variants of Dual Student (all using the 13-layer CNN) on the CIFAR benchmark averaged over 3 runs. Parentheses of Multiple Student (MS) indicate the numbers of students. Parentheses of Imbalanced Student (IS) indicate the numbers of parameters for the strong student.</figDesc><table><row><cell>Model</cell><cell>CIFAR-10 1k labels</cell><cell>CIFAR-100 10k labels</cell></row><row><cell>DS</cell><cell>15.74 ± 0.45</cell><cell>33.08 ± 0.27</cell></row><row><cell>MS (4 models)</cell><cell>14.97 ± 0.36</cell><cell>32.89 ± 0.32</cell></row><row><cell>MS (8 models)</cell><cell>14.77 ± 0.33</cell><cell>32.83 ± 0.28</cell></row><row><cell cols="2">IS (3.53M params) 13.43 ± 0.24</cell><cell>32.59 ± 0.27</cell></row><row><cell cols="3">IS (11.6M params) 12.39 ± 0.26 31.56 ± 0.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Mean test error rate on the CIFAR benchmark averaged over 5 runs, with different confidence threshold values, ξ. Parentheses show the numbers of the labeled samples. (Labels) ξ = 0.0 ξ = 0.4 ξ = 0.6 ξ = 0.8</figDesc><table><row><cell>Dataset CIFAR-10 (1k)</cell><cell>16.49</cell><cell>16.12</cell><cell>15.92</cell><cell>15.74</cell></row><row><cell cols="2">CIFAR-100 (10k) 33.67</cell><cell>33.08</cell><cell>33.23</cell><cell>33.54</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A: Convergence of the EMA In our paper, we state that the EMA teacher is coupled with the student in the existing Teacher-Student methods. We provide below a formal proposition for this statement and a simple proof. Proposition 1. Given a sequence { s t } t∈N ⊆ R m and let</p><p>Proof. By the definition of convergence, if { s t } t∈N converges to S, we have: ∀ǫ &gt; 0, ∃T ∈ N such that ∀t &gt; T , |s t − S| &lt; ǫ. First, when t &gt; T , by the formula of the sum of a finite geometric series, we rewrite S and s ′ t as:</p><p>Since T is finite, α T s ′ 0 and T i=1 α T −i s i are bounded. Thus, ∃C ∈ R + such that:</p><p>Since 0 &lt; α &lt; 1, we have lim t→∞ α t = 0. Thus, ∃T ′ &gt; 0 such that ∀t &gt; T ′ , α t &lt; min{ ǫ C , ǫ |S| }. Then, after substituting Eq. 8 into |s ′ t − S| and applying the Triangular Inequality, we have:</p><p>Then ∀t &gt; (T + T ′ ), we have:</p><p>Combining Eq. 9, 10, 11, 12, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Model Architectures</head><p>The model architecture used in our CIFAR-10, CIFAR-100, and SVHN experiments is the 13-layer convolutional network (13-layer CNN), which is the same as previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. We implement it following FastSWA <ref type="bibr" target="#b0">[1]</ref> for comparison. <ref type="table">Table 7</ref> describes its architecture in details. For ImageNet experiments, we use a 50layer ResNeXt <ref type="bibr" target="#b33">[34]</ref> architecture, which includes 3+4+6+3 residual blocks and uses the group convolution with 32 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Semi-supervised Learning Setups</head><p>In our work, all experiments use the SGD optimizer with the nesterov momentum set to 0.9. The learning rate is adjusted by the function γ = γ 0 * (0.5 + cos((t − 1) * π/N )), where t is the current training step, N is the total number of steps, and γ 0 is the initial learning rate. We present the settings of the experiments on each dataset as follows.</p><p>CIFAR-10: On CIFAR-10, we set the batch size to 100 and half of the samples in each batch are labeled. The initial learning rate is 0.1. The weight decay is 1e −4 . For the stabilization constraint, we set its coefficient λ 2 = 100 and ramp it up in the first 5 epochs. We set λ 1 = 10. The confidence threshold for the stable samples is 0.8.</p><p>CIFAR-100: On CIFAR-100, each minibatch contains 128 samples, including 31 labeled samples. We set the initial learning rate to 0.2 and the weight decay to 2e −4 . The confidence threshold is ξ = 0.4. Other hyperparameters are the same as CIFAR-10.</p><p>SVHN: The batch size on SVHN is 100, and each minibatch contains only 10 labeled samples. The initial learning rate is 0.1, and the weight decay is 1e −4 . The stabilization constraint is scaled by 10 (ramp up in 5 epochs). We use the confidence threshold ξ = 0.8.</p><p>ImageNet: We validate our method on ImageNet by the ResNeXt-50 architecture on 8 GPUs with batch size 320 and half of the batch are labeled samples. Each sample is augmented following <ref type="bibr" target="#b10">[11]</ref> and is resized to 224 × 224. We warm-up the learning rate from 0.08 to 0.2 in the first 2 epochs. The model is trained for 60 epochs with the weight decay set to 5e −5 , the stabilization constraint coefficient set to 1000, and a small confidence threshold of 0.01.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D: Domain Adaptation Setups</head><p>We design a small convolutional network for the domain adaptation from USPS (source domain) to MNIST (target domain). The structure is shown in <ref type="table">Table 8</ref>. We train all experiments for 100 epochs by the SGD optimizer with the nesterov momentum set to 0.9 and the weight decay set to 1e −4 . The learning rate declines from 0.1 to 0 by a cosine adjustment. Each batch includes 256 samples while 32 of them are labeled. We randomly extract 7000 balanced samples from MNIST for target-supervised experiments, and other experiments are done by using the training set of USPS. The coefficient of the stabilization constraint is λ 2 = 1.0. We also ramp it up in the first 5 epochs. The confidence threshold is ξ = 0.6. We discover that the input noise with ζ = 0.15 is vital for the Mean Teacher but not for our method in this experiment.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference on Computational Learning Theory</title>
		<meeting>Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tri-net for semi-supervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-ensembling for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient knowledge distillation from an ensemble of teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cifar-100 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Snderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Sren Kaae Snderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Efficient Machine Learning workshop at ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realistic evaluation of semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial dropout for supervised and semisupervised learning</title>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<editor>Sungrae Park, Jun-Keon Park, Su-Jin Shin, and Il-Chul Moon</editor>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. 1</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised and semisupervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. 2017</title>
		<meeting>CVPR. 2017</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">When semi-supervised learning meets ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of Electrical and Electronic Engineering in China</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>TR 1530</idno>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
