<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Kanu</surname></persName>
							<email>jdkanu@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hogan</surname></persName>
							<email>khogan@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>sound propagation</term>
					<term>direction of arrival estimation</term>
					<term>data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel learning-based approach to estimate the direction-of-arrival (DOA) of a sound source using a convolutional recurrent neural network (CRNN) trained via regression on synthetic data and Cartesian labels. We also describe an improved method to generate synthetic data to train the neural network using state-of-the-art sound propagation algorithms that model specular as well as diffuse reflections of sound. We compare our model against three other CRNNs trained using different formulations of the same problem: classification on categorical labels, and regression on spherical coordinate labels. In practice, our model achieves up to 43% decrease in angular error over prior methods. The use of diffuse reflection results in 34% and 41% reduction in angular prediction errors on LOCATA and SOFA datasets, respectively, over prior methods based on image-source methods. Our method results in an additional 3% error reduction over prior schemes that use classification networks, and we use 36% fewer network parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the direction-of-arrival (DOA) of sound sources has been an important problem in terms of analyzing multi-channel recordings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In these applications, the goal is to predict the azimuth and elevation angles of the sound source relative to the microphone, from a sound clip recorded in any multichannel setting. One of the simpler problems is the estimation of the DOA on the horizontal plane <ref type="bibr" target="#b2">[3]</ref>. More complex problems include DOA estimation in three-dimensional space or the identification of both direction and distance of an audio source. Even more challenging problems correspond to performing these goals in noisy and reverberant environments.</p><p>To analyze spatial information from sound recordings, at least two microphones with known relative positions must be used. In practice, various spatial recording formats including binaural, 5.1-channel, 7.1-channel, etc. have been applied to spatial audio related systems <ref type="bibr" target="#b3">[4]</ref>. The Ambisonics format decomposes a soundfield using a spherical harmonic function basis <ref type="bibr" target="#b4">[5]</ref>. Compared with its alternatives, Ambisonics has the advantage of being hardware independent-it does not necessarily encode microphone specifications into the recording.</p><p>Recent work <ref type="bibr" target="#b5">[6]</ref> has applied the Ambisonics format to DOA estimation and trained a CRNN classifier that yields more accurate predictions than a baseline approach using independent component analysis. While a regression formulation seems more natural for the problem of DOA estimation, some recent This work was supported in part by ARO grant W911NF-18-1-0313 and Intel. Project page https://gamma.umd.edu/pro/ speech/doa work <ref type="bibr" target="#b2">[3]</ref> suggests a regression formulation may yield worse performance than that of the classification formulation for multilayer perceptrons. In this work, we present a novel learningbased approach for estimating DOA of a single sound source from ambisonic audio, building on an existing deep learning framework <ref type="bibr" target="#b5">[6]</ref>. We present a CRNN which predicts DOA as a 3-D Cartesian vector. We introduce a method to generate synthetic data using geometric sound propagation that models specular and diffuse reflections, which results in up to 43% error reduction compared with image-source methods. We conduct a four-way comparison between the Cartesian regression network, two classification networks trained with cross-entropy loss, and a regression network trained using angular loss. Finally, we investigate results on two 3rd-party datasets: LO-CATA <ref type="bibr" target="#b6">[7]</ref> and SOFA <ref type="bibr" target="#b7">[8]</ref>, where our best model reduces angular prediction error by 43% compared to prior methods. Section 2 gives an overview of prior work. We propose our method in Section 3. Section 4 presents our results on two benchmarks and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>One classic approach to DOA estimation is to first determine the time delay of arrival (TDOA) between microphone array channels, which can be estimated by generalized cross correlation <ref type="bibr" target="#b8">[9]</ref> or least squares <ref type="bibr" target="#b9">[10]</ref>. The DOA can be computed from known TDOA and the array layout directly. Another approach is to use the signal subspace, as in the MUSIC algorithm <ref type="bibr" target="#b10">[11]</ref>. With some restrictions on operating conditions, these techniques are very effective. However, they do not perform well in highly reverberant and noisy environments, or when the placement of signal sources is arbitrary <ref type="bibr" target="#b11">[12]</ref>. More recently, researchers have applied modern machine learning techniques to speech DOA estimation with the goal of improving performance in noisy, realistic environments, which can be categorized into classification and regression networks. For both networks, angular error for a single example is proportional to the angular distance between the predicted and actual DOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Classification Formulations</head><p>In the classification formulation, DOA is encoded using a categorical representation: an approximately uniform mesh-grid defines the score for each of a finite set of possible categories, where each category corresponds to a unique region of the continuous DOA space. The mesh grid is defined by subdividing the DOA space at a given resolution. The DOA is decoded as the direction associated with the bin with highest score. The categorical formulation uses a discrete encoding, lending itself to a class-based formulation of DOA estimation. Generalized cross correlation (GCC) feature vectors of a microphone array input have been fed to a multilayer perceptron classifier, which predicts a DOA in one angular dimension <ref type="bibr" target="#b2">[3]</ref> and show superior performance over the classic least square method <ref type="bibr" target="#b9">[10]</ref> in both simulated and real rooms of various sizes. Perotin et al. <ref type="bibr" target="#b5">[6]</ref> calculate acoustic intensity vectors using a first-order Ambisonics representation of audio. This representation serves as input to a CRNN, which predicts a DOA in two angular dimensions. Their CRNN yields more accurate predictions than a baseline approach using independent component analysis. CRNNs have also been used in <ref type="bibr" target="#b12">[13]</ref> to identify the DOA for overlapping sound sources, in two angular dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Regression Formulations</head><p>In the regression formulation, two representations of DOA are commonly used, which we refer to as Cartesian and spherical. With the Cartesian representation, DOA is encoded as a threedimensional vector in Cartesian (x, y, z) coordinates, pointing toward the source. With the spherical representation, DOA is encoded as a two-dimensional vector of azimuth (θ) and elevation (φ) angles. Both formulations encode DOA in continuous space, leading to a regression formulation of DOA estimation. In prior work, regression formulations have not shown superior empirical results for DOA estimation. Higher angular errors for regression than for classification is claimed in <ref type="bibr" target="#b2">[3]</ref>. CNN regression has been used in <ref type="bibr" target="#b13">[14]</ref> to estimate the Cartesian coordinates of a sound source in 3-D space. CRNN regression has been used in <ref type="bibr" target="#b12">[13]</ref> and higher angular error is observed for DOA estimates for regression than for classification. Similar to this result, our experiments show a higher angular error for regression on spherical DOA than for classification. However, we discover a lower angular error for regression on Cartesian DOA, than for both classification on categorical DOA and regression on spherical DOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Preparation</head><p>Our DOA estimation network also relies on a large amount of labeled training data. However, collecting Ambisonic recordings and manually labeling them for training is tedious and time-consuming. Therefore, in speech/audio related training, the common practice is to use image-source methods to generate synthetic impulse responses for augmenting the training data <ref type="bibr" target="#b14">[15]</ref>. However, the distribution of synthetic data may not match that of the real data well enough, which can cause large generalization error when applying a synthetically trained DNN to real test data. To overcome this issue of domain mismatch, a more accurate approach for generating training data is needed.</p><p>Sound propagation methods compute the reflection and diffraction paths from the sound sources to a listener in the virtual environment. Image-source methods do not model sound scattering or diffuse reflections, which are important phenomenons in acoustic environments. We utilize the state-of-theart geometric sound propagation method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> to generate synthetic data that is more accurate than image-source methods. Its benefit has also been observed in other speech tasks <ref type="bibr" target="#b18">[19]</ref>.</p><p>Following the suggested procedure in <ref type="bibr" target="#b5">[6]</ref>, we generated 42,000 rectangular room configurations with dimensions uniformly and independently sampled between 2.5m×2.5m×2m and 10m×10m×3m. Under each room configuration, we randomly populate three paired source-listener locations, both at least 0.5m away from walls. The geometric sound propagation method based on path tracing is used on each source-listener <ref type="figure">Figure 1</ref>: General network architecture for each regression network and classification network. The dimensionality of the output vector (shown in green) is 2 for spherical formulation, 3 for Cartesian, and 429 for classification. Note that our implementation of the classifier is equivalent to the implementation in <ref type="bibr" target="#b5">[6]</ref>, but our regression networks differ in the size of the output layer, and use 36% fewer trainable parameters.</p><p>pair to generate its spatial room impulse response (SRIR). Then we convolve each SRIR with a randomly selected one-second clean speech sample from the Libri ASR corpus <ref type="bibr" target="#b19">[20]</ref> to generate realistic reverberant speech recordings in Ambisonic format. Babble and speech shaped noise <ref type="bibr" target="#b20">[21]</ref> are added to the convolved sound at signal-to-noise ratios (SNRs) following a normal distribution centered at 15dB with a standard deviation of 1dB as recommended by <ref type="bibr" target="#b21">[22]</ref>. A short time fourier transform (STFT) is used to convert speech waveforms to spectrogram, and the features are extracted according to Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ambisonic Input Features</head><p>In theory, Ambisonics of an infinite number of bases can reproduce the recorded soundfield with no error. As a practical approximation, we use the first four bases/channels necessary for first-order ambisonics (FOA). The FOA channels are denoted by W, X, Y, Z, where W channel contains the zerothorder coefficients that represents the omnidirectional signal intensity, and X, Y, Z channels contain the first-order coefficients that encode direction modulated information. For a plane wave with azimuth θ and elevation φ creating a sound pressure p, the complex FOA components are:</p><formula xml:id="formula_0">   W (t, f ) X(t, f ) Y (t, f ) Z(t, f )    =     1 √ 3cosθcosφ √ 3sinθcosφ √ 3sinφ     p(t, f ),<label>(1)</label></formula><p>where t and f are time and frequency bins. We follow the approach in <ref type="bibr" target="#b5">[6]</ref> to construct input features from raw FOA audio. The active and reactive intensity vectors are encoded as:</p><formula xml:id="formula_1">I(t, f ) =   W (t, f ) * X(t, f ) W (t, f ) * Y (t, f ) W (t, f ) * Z(t, f )   ,<label>(2)</label></formula><formula xml:id="formula_2">Ia(t, f ) = R{I(t, f )}, Ir(t, f ) = I{I(t, f )},<label>(3)</label></formula><p>where R{·} and I{·} extract the real and imaginary components of a complex signal respectively. Both feature vectors are divided by |W (t, f )</p><formula xml:id="formula_3">| 2 + 1 3 (|X(t, f )| 2 +|Y (t, f )| 2 +|Z(t, f )| 2 )</formula><p>to have a uniform range for deep neural network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Output and Loss Formulation</head><p>We compare Cartesian (x, y, z) and Spherical (θ, φ) output representations in addition to the common Categorical representation in this work. A stacked CRNN formulation using Categorical outputs has been proposed in <ref type="bibr" target="#b5">[6]</ref>. We use this network structure and derive a set of Categorical, Cartesian, and Spherical forms from it, differing only in the size of the output layer, our independent variable. Maintaining a high similarity among the network architecture enables us to conduct well controlled comparisons between output representations for DOA estimation. We visualize our network architecture for the following three output formulations in <ref type="figure">Fig. 1</ref>. Key differences between the formulations are summarized in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Categorical Outputs</head><p>We discretize the continuous DOA space into some number of possible categorical outputs. The angular resolution is chosen to be 10°, which results in 429 direction classes. Each training DOA label is assigned to the direction class that has the smallest angular difference from itself. We use this labeling to train a classifier that outputs a sigmoid vector of the direction class. The model is trained using cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Cartesian Outputs</head><p>We define the output of the Cartesian network as a 3-D vector, representing DOA in Cartesian coordinates. Training labels are unit vectors in R 3 pointing toward the source. We use meansquared error (MSE) as the loss to train our networks. Note that the output of the network is not constrained to lie on the unit sphere. As a result, a hypothetical output that is a large scalar multiple of the DOA label will result in a large loss, despite the perfect alignment of the output vector with the label. In practice, this property does not prevent our formulation from generating an accurate predictor, as shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Spherical Outputs</head><p>In contrast to the Cartesian formulation, the spherical formulation encodes DOA as a 2-D vector representation azimuth (θ) and elevation (φ) angles. This representation has only 2 degrees of freedom in 3-D, which means the Cartesian representation has added a redundant dimension to this learning problem. One issue with using this form is that the periodicity of spherical angles makes distance computation between two angles more complicated than in Cartesian coordinates. Conventional mean squared loss is discontinuous, and therefore non-differentiable, over predicted azimuth and elevation, which eliminates the guarantee of convergence of Gradient Descent. Instead, we compute the great-circle distance on a sphere's surface using the haversine formula, which is differentiable:</p><formula xml:id="formula_4">h = sin 2 φ2 − φ1 2 + cos(φ1) cos(φ2) sin 2 θ2 − θ1 2 , D = 2rarcsin( √ h),<label>(4)</label></formula><p>where r is the radius of the sphere, and D is the great-circle distance between azimuth-elevation angles (θ1, φ1) and (θ2, φ2). By setting r = 1, we define D as our Haversine loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Architecture and Training Procedure</head><p>As described in Section 3.2, the inputs to these networks are active and reactive intensity vectors. The size of the input is The CRNN architecture is shown in <ref type="figure">Fig. 1</ref>. There are three convolutional layers, each consisting of 2-D convolution, a rectified linear unit (ReLU), batch normalization, and max pooling. The outputs of each of the three layers are 64x25x64, 64x25x8, and 64x25x2, respectively. The output of the last layer is flattened to a 128-D vector for each of the 25 frames. Each frame's vector is fed into a two-layer bi-directional LSTM, and the output of the LSTM for each frame is fed through two timedistributed, fully-connected linear layers, generating a DOA estimate for each frame. As described in Section 3.3, we generate three forms of outputs depending on implementation: 2-D azimuth-elevation angles, 3-D Cartesian coordinates, or 429 DOA classes. During training, losses are computed at each frame and error is backpropagated through the network. During evaluation, a single DOA estimate is taken as the uniformly weighted average of all estimates across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks</head><p>We evaluate each model on 1,189 samples from three staticsource microphone signals in the third-party sound localization and tracking (LOCATA) challenge dataset <ref type="bibr" target="#b6">[7]</ref>, and a dataset of ambisonic RIRs accompanying the Spatially Oriented Format for Acoustics (SOFA) convention <ref type="bibr" target="#b7">[8]</ref>. No real RIRs are involved during the training phase. The noise and clean speech used for training and test are from different datasets. We use Eq. (4) as our angular error metric for visualization in <ref type="figure">Fig. 2</ref>.</p><p>Each signal in the LOCATA dataset is a real-world ambisonic speech recording with optically tracked azimuthelevation labels. In theory, ambisonic coefficients up to the 4th order can be captured by an Eigenmike microphone, but we only use its first order components.</p><p>From the SOFA dataset, we extract 225 SRIRs recorded in the Alte Pinakothek museum using Eigenmike®, Sennheiser AMBEO®, and SoundField®microphones. Positions and rotations of all loudspeaker and microphones are provided by measurements using laser meter and pointers. A reverberant test set is generated by convolving each SOFA SRIR with a random 1-second clip from the CMU Arctic speech databases <ref type="bibr" target="#b22">[23]</ref>. Recorded background noise from the LOCATA dataset is added at a mean SNR level of 10dB with a standard deviation of 5dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">LOCATA Dataset</head><p>We compute average error along the temporal axis, for each static-source signal in LOCATA Task 1. Estimates of DOA are generated for each frame in the microphone signal using a sliding window. The resulting estimates are interpolated to the timestamps provided in the LOCATA dataset. Prediction error is computed as the angular distance between the prediction and the ground-truth DOA. Angular tracking error is visualized in Each model must wait for a complete input window to make a prediction, hence the regions are not always identical between waveform and tracking error. Cartesian and Categorical models trained on our synthetic dataset achieve consistently lower tracking error compared with the classifier trained by Perotin et al. <ref type="bibr" target="#b5">[6]</ref> and the MUSIC algorithm <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure">Fig. 2</ref>, for each predictor. Average angular error is computed over 234, 439, and 512 timestamps for Recordings 1, 2, and 3, respectively. Timestamps are selected to compute angular error if each algorithm makes a prediction for that timestamp, and the timestamp is located inside a voice-active region.</p><p>To generate a prediction at frame i using a neural network, we feed in the sequence of 25 frames centered at i, and generate a sequence of 25 outputs of the network. If the output is Cartesian or spherical, we estimate DOA at i as the average of the outputs at each frame. If the output is a classification grid, we average the output grid over the frames, to produce a cumulative score for each DOA, and choose the DOA with highest score as the prediction. When running the MUSIC algorithm, we restrict it to 4-channel recordings, as well, for fair comparisons. e observe that in <ref type="figure">Fig. 2</ref> and Tab. 2, our Cartesian model achieves consistently lowest error on Recording 1 and 2, while our categorical model shows best performance in Recording 3. However, the Spherical model yields higher error than the other two models. We also observe that re-training the original model from Perotin et al. <ref type="bibr" target="#b5">[6]</ref> using data generated by geometric method to augment data results in lower tracking error compared with the use of the image-source method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">SOFA Dataset</head><p>A larger scale test is performed using the SOFA dataset. MU-SIC algorithm is not evaluated because this dataset does not provide microphone hardware configurations required by MUSIC. We compute the percentage of correctly predicted directions under error tolerances of 5°, 10°, and 15°, as well as each model's During our training procedure, we notice that each model is able to converge within dozens of epochs. However, the number of trainable parameters in regression models (i.e. Cartesian and Spherical) is only 64% of that in the classification model. This suggests that regression models tend to have a hypothesis set with lower complexity, which results in lower generalization error when tested on real data. We also tested the option of letting all models have approximately the same amount of trainable parameters, which has degraded the performance of the regression models. In conclusion, we are able to train a regression model that has superior performance over its corresponding classification model, although we do not observe obvious benefits in using the Spherical formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>In this paper, we demonstrate the benefits of using a geometric sound propagation simulator, as compared with image source methods for training DOA estimation networks, by reporting a higher accuracy on evaluation data. We evaluate the performance of a CRNN model in three output formulations: categorical, Cartesian, and spherical. We test them on two 3rd-party datasets and show that our Cartesian regression model achieves superior performance over classification and spherical models.</p><p>Evaluating classification models involves an additional factor: the resolution of the classification grid, which we kept fixed. Further, our work is limited to single-source localization problems, whereas in multi-source localization problems, clas-sification models may have intrinsic advantages over regression models. Lastly, we restricted our simulation to extremely simple room settings to guarantee a fair comparison with the imagesource method. Furture work may involve experimentation on more complex room configurations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Figure 2 :</head><label>32</label><figDesc>Waveforms (top row) and angular tracking error (bottom row) for Recordings 1-3 in LOCATA Task 1. Shaded regions in the waveform indicate voice-active regions, while shaded regions in angular tracking error indicate the intersection of voice activity and regions containing predictions for all models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Three types of output representations studied in this paper. d is the number of classes into which a spherical surface is discretized. In our experiment, d = 429.</figDesc><table><row><cell cols="4">Output Type Activation Dimension Loss Function</cell></row><row><cell>Categorical</cell><cell>Sigmoid</cell><cell>R d</cell><cell>Cross-entropy</cell></row><row><cell>Cartesian</cell><cell>Linear</cell><cell>R 3</cell><cell>MSE</cell></row><row><cell>Spherical</cell><cell>Linear</cell><cell>R 2</cell><cell>Haversine</cell></row><row><cell cols="4">6x25x513, containing 513 frequency bins (16kHz sample rate),</cell></row><row><cell cols="4">for each 6-D intensity vector, computed at each of 25 frames.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average angular tracking error within voice-active regions of LOCATA Task 1 recordings. Best performance in each column is highlighted in bold. All models are trained on data using specular and diffuse reflections in the geometric propagation algorithm, except for the MUSIC algorithm, which does not rely on training data. Perotin et al.<ref type="bibr" target="#b5">[6]</ref> refers to the Categorical model trained on data generated by the image-source method.</figDesc><table><row><cell>Model</cell><cell>Recording 1 Recording 2 Recording 3</cell></row><row><cell>MUSIC</cell><cell>18.6°16.9°17.5°P</cell></row><row><cell>erotin et al.</cell><cell>9.1°6.7°12.5°C</cell></row><row><cell>ategorical</cell><cell>9.3°6.3°3.2°C</cell></row><row><cell>artesian</cell><cell>8.5°5.8°6.8°S</cell></row><row><cell>pherical</cell><cell>9.2°7.9°9.9°W</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the SOFA dataset. First three columns show the percentage of DOA labels correctly predicted within error tolerances, followed by average angular errors, and %improvement on baseline. Best performance in each column is highlighted in bold. Model &lt; 5°&lt; 10°&lt; 15°Error Improv. Perotin et al. 11.9% 35.9% 73.2% 16.9°-Categorical 24.4% 58.2% 88.7% 9.96°41% Cartesian 24.4% 66.3% 88.2% 9.68°43% Spherical 18.2% 55.8% 82.5% 11.2°34% average angular error on the whole dataset. It can be seen from Tab. 3 that our Cartesian model consistently achieves the best performance under each column, outperforming the baseline model by 43% in terms of average prediction error.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The generalized correlation method for estimation of time delay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A robust method for speech signal time-delay estimation in reverberant rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brandstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="375" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A learning-based approach to direction of arrival estimation in noisy and reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2814" to="2818" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surround by sound: A review of spatial audio recording and reproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Abhayapala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">532</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Periphony: With-height sound reproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gerzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crnn-based joint azimuth and elevation localization with the ambisonics intensity vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perotin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guérin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IWAENC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The LOCATA challenge data corpus for acoustic source localization and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Löllmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Barfuss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM)</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ambisonics directional room impulse response as a new convention of the spatially oriented format for acoustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pérez-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Muynke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Audio Engineering Society Convention 144. Audio Engineering Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The generalized correlation method for estimation of time delay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time passive source localization: a practical linear-correction least-squares approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mersereati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple emitter location and signal parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Antennas and Propagation</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust localization in reverberant rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dibiase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brandstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microphone Arrays</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="157" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sound event localization and detection of overlapping sources using convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards end-to-end acoustic localization using deep learning: from audio signal to source position coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Vera-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macias-Guarasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in Sensors</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5220" to="5224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gsound: Interactive sound propagation for games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Engineering Society Conference: 41st International Conference: Audio for Games</title>
		<imprint>
			<publisher>Audio Engineering Society</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-order diffraction and diffuse reflections for interactive sound propagation in large environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive sound propagation and rendering for large multi-source scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving reverberant speech training using diffuse acoustic simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03988</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Noisy speech database for training speech enhancement algorithms and tts models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noisy training for deep neural networks in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tejedor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The cmu arctic speech databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kominek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth ISCA workshop on speech synthesis</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
