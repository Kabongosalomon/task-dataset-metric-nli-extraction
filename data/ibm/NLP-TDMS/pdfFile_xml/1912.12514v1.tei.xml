<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fadel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Tuffaha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
							<email>malayyoub@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our team's effort on the semantic text question similarity task of NSURL 2019. Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data. Then, it takes ELMo pre-trained contextual embeddings of the data and feeds them into an ON-LSTM network with self-attention. This results in sequence representation vectors that are used to predict the relation between the question pairs. The model is ranked in the 1st place with 96.499 F1score (same as the second place F1-score) and the 2nd place with 94.848 F1-score (differs by 1.076 F1-score from the first place) on the public and private leaderboards, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Text Similarity (STS) problems are both real-life and challenging. For example, in the paraphrase identification task, STS is used to predict if one sentence is a paraphrase of the other or not <ref type="bibr" target="#b11">(Madnani et al., 2012;</ref><ref type="bibr" target="#b8">He et al., 2015;</ref><ref type="bibr" target="#b0">AL-Smadi et al., 2017)</ref>. Also, in answer sentence selection task, it is utilized to determine the relevance between question-answer pairs and rank the answers sentences from the most relevant to the least. This idea can also be applied to search engines in order to find documents relevant to a query <ref type="bibr" target="#b19">(Yang et al., 2015;</ref><ref type="bibr" target="#b20">Yang et al., 2019)</ref>.</p><p>A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity (SQS) for the Arabic language <ref type="bibr" target="#b14">(Schwab et al., 2017;</ref><ref type="bibr" target="#b1">Alian and Awajan, 2018)</ref>. SQS is a variant of STS, which aims to compare a pair of questions and determine whether they have the same meaning or not. The SQS in Arabic task is one of the shared tasks of the Workshop on NLP Solutions for Under Resourced Languages <ref type="bibr">(NSURL 2019)</ref> and it consists of 12K questions pairs <ref type="bibr" target="#b15">(Seelawi et al., 2019)</ref>.</p><p>In this paper, we describe our team's efforts to tackle this task. After preprocessing the data, we use four data augmentation steps to enlarge the training data to about four times the size of the original training data. We then build a neural network model with four components. The model uses ELMo (which stands for Embeddings from Language Models) <ref type="bibr" target="#b13">(Peters et al., 2018)</ref> pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs. The task is hosted on Kaggle 2 platform and our model is ranked in the first place with 96.499 F1-score (same as the second place F1-score) and in the second place with 94.848 F1-score (differs by 1.076 F1-score from the first place) on the public and private leaderboards, respectively.</p><p>The rest of this paper is organized as follows. In Section 2, we describe our methodology, including data preprocessing, data augmentation, and model structure, while in Section 3, we present our experimental results and discuss some insights from our model. Finally, the paper is concluded in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we present a detailed description of our model. We start by discussing the preprecessing steps we take before going into the details of the first novel aspect of our work, which is the data augmentation techniques. We then discuss the neural network model starting from the input all the way to the decision step. The implementation is available on a public repository. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preprocessing</head><p>In this work, we only consider one preprocessing step, which is to separate the punctuation marks shown in <ref type="figure" target="#fig_0">Figure 1</ref> from the letters. For example, if the question was: " ", then it will be processed as follows: " ". This is done to preserve as much information as possible in the questions while keeping the words clear of punctuations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>The training data contains 11,997 question pairs: 5,397 labeled as 1 (i.e., similar) and 6,600 labeled as 0 (i.e., not similar). To obtain a larger dataset, we augment the data using the following rules.</p><p>Suppose we have questions A, B and C • Positive Transitive:</p><p>If A is similar to B, and B is similar to C, then A is similar to C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Negative Transitive:</head><p>If A is similar to B, and B is NOT similar to C, then A is NOT similar to C.</p><p>Note: The previous two rules generates 5,490 extra examples (bringing the total up to 17,487).</p><p>• Symmetric:</p><p>If A is similar to B then B is similar to A, and if A is not similar to B then B is not similar to A.</p><p>Note: This rule doubles the number of examples to 34,974 in total.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the growth of the training dataset after each data augmentation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Structure</head><p>We now discuss our model structure, which is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. As the figure shows, the model structure can be divided into the following components/layers: input layer, sequence representation extraction layer, merging layer and decision layer. The following subsections explain each layer/component in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Input</head><p>To build meaningful representations for the input sequences, we use the Arabic ELMo pre-trained model 4 to extract contextual words embeddings with size 1024 and feed them as input to our model. The representations extracted from the ELMo model are the averaged sum of word encoder and both first and second Long Short-Term Memory (LSTM) hidden layers. These representations are affected by the context in which they appear <ref type="bibr" target="#b5">(Cheng et al., 2015;</ref><ref type="bibr" target="#b13">Peters et al., 2018;</ref><ref type="bibr" target="#b17">Smith, 2019)</ref>. For example, the word " " will have different embedding vectors related to the following two sentences as they have different Translation: Ali has a lot of gold.</p><p>Translation: Ali went away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Sequence Representation Extractor</head><p>This component takes the ELMo embeddings related to each word in the question as an input and feeds them into two a special kind of bidirectional LSTM layers called Ordered Neurons LSTM (ON-LSTM) 5 introduced in <ref type="bibr" target="#b16">(Shen et al., 2018)</ref> with 256 hidden units, 20% dropout rate, and 8 as the chunk size for each of them. Then, it applies sequence weighted attention 6 proposed by <ref type="bibr" target="#b7">(Felbo et al., 2017)</ref> on the outputs of the second ON-LSTM layer to get the final question representation. This component uses the same weights to compute representations for each question in the pair. The details of this component are as follows <ref type="bibr" target="#b16">(Shen et al., 2018)</ref>.</p><p>Since NLP data are structured in a hierarchical manner, the authors of ON-LSTM <ref type="bibr" target="#b16">(Shen et al., 2018)</ref> proposed a new form of update and activation functions (in order to enforce a bias towards structuring a hierarchy of the data) to the standard LSTM model reported below:</p><formula xml:id="formula_0">f t = σ g (W f x t + U f h t−1 + b f ) (1) i t = σ g (W i x t + U i h t−1 + b i ) (2) o t = σ g (W o x t + U o h t−1 + b o ) (3) c t = tanh(W c x t + U c h t−1 + b c ) (4) h t = o t • tanh(c t )<label>(5)</label></formula><p>The newly proposed activation function is cumax = cumsum(sof tmax(x)), where cumsum denotes the cumulative sum function. Among the desired properties of this function is to control the updates on the memory cell such that the higher ranking neurons get updated less frequently (storing long-term and global information) compared to the lower ranking neurons, which are updated more frequently (storing short-term and local information). This makes the neurons updates dependent on each other in contrast to the updates on the standard LSTM neurons.</p><p>The following equations define the new master input and forget gates and the new memory cell update function based on the new activation func-</p><formula xml:id="formula_1">tion:f t = cumax(Wf x t + Uf h t−1 + bf ) (6) i t = 1 − cumax(Wĩx t + Uĩh t−1 + bĩ) (7) w t =f t •ĩ t (8) f t = f t • w t + (f t − w t ) (9) i t = i t • w t + (ĩ t − w t ) (10) c t =f t • c t−1 +î t •ĉ t<label>(11)</label></formula><p>The attention mechanism (inspired by <ref type="bibr" target="#b21">Yang et al., 2016)</ref>) allows the model to learn to decide the importance of each word and build the final question representation vector based on important words only, while tuning out less important words. With a single parameter, w a , the attention mechanism can be described as follows:</p><formula xml:id="formula_2">e t = h t w a (12) a t = exp(e t ) T i=1 exp(e i )<label>(13)</label></formula><formula xml:id="formula_3">v = T i=1 a i h i<label>(14)</label></formula><p>The weight matrix w a is the only new trainable parameter which learns the attention mechanism over the outputs of the second ON-LSTM layer.</p><p>To calculate the importance scores, a t , for each time step, it first multiplies each time step output, h t , by the weight matrix, w a , and normalizes the results using a Softmax function. Finally, the final sequence representation, v, is the weighted sum over all ON-LSTM outputs using the importance scores calculated earlier as weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Merging Technique</head><p>After extracting the representations related to each question, we merge them using pairwise squared distance function applied to the representation vectors of the two questions in each question pair. More formally, if V 1 and V 2 are these representation vectors, then, the merged representation vector V m can be expressed as follows:</p><formula xml:id="formula_4">V m =      (V 1 1 − V 2 1 ) 2 (V 1 2 − V 2 2 ) 2 . . . (V 1 512 − V 2 512 ) 2     <label>(15)</label></formula><p>This component allows for the Symmetric augmentation step (Section 2.2) to enhance the results, since the <ref type="figure">(A, B)</ref> examples are computationally different (in the back propagation step) from the (B, A) examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Deep Neural Network</head><p>The final component is a deep neural network that consists of four fully-connected layers with 1024, 512, 256, and 128 units using ReLU activation function and 20% dropout rate applied to each layer. This network takes the merged representation vector, V m, as an input and predicts the label using a Sigmoid function as an output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In this section, we start by discussing our experimental setup. We then discuss all experiments conducted and provide detailed analysis of their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>All experiments discussed in this work have been done on the Google Colab 7 <ref type="bibr" target="#b4">(Carneiro et al., 2018)</ref> environment using Tesla T4 GPU accelerator with the following hyperparameters: The experiments are divided into two sets. The first set aims to explore the effect of the Recurrent Neural Network (RNN) cell type, while the second set aims to explore the effect of the data augmentation techniques mentioned in Section 2.2.</p><p>For each experiment, five models are trained and the following results are reported:</p><p>• Minimum F1 score gained on the test set.</p><p>• Maximum F1 score gained on the test set.</p><p>• Average F1 score gained from the five trained models.</p><p>• Majority Voting F1 score gained by ensembling the five trained models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effect of RNN Cell Type</head><p>In this experiments set, we use the same structure described in Section 2.3 while changing the RNN cell type only. We use all 45,514 examples from the augmented dataset in the training process. The tested RNN cells are: Gated Recurrent Unit (GRU) , LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) and ON-LSTM <ref type="bibr" target="#b16">(Shen et al., 2018)</ref>. The latter one is tested using two chunk sizes, 4 and 8, in order to explore the effect of chunk size on the training process and the size of the model. <ref type="table" target="#tab_0">Table 1</ref> shows the model size in terms of trainable parameters and the training time for each RNN cell type, while <ref type="table" target="#tab_1">Table 2</ref> shows the F1-scores of the model using different RNN cells. Best results are shown in bold. The tables show that while GRU cells are the most efficient, the ON-LSTM cells (with chunk size 8) are the most effective (in terms of all considered measures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effect of Data Augmentation</head><p>In this experiments set, we use the RNN cell type that gives the best results in Section 3.2 (ON-LSTM with chunk size 8) and the same model structure described in Section 2.3 to explore the effect of data augmentation steps mentioned in Section 2.2.</p><p>The data augmentation steps have an effect on two factors, the training time and the accuracy measurement (F1-score). <ref type="table" target="#tab_2">Table 3</ref> shows the av-7 https://colab.research.google.com erage training time over five runs for each data augmentation step. Moreover, <ref type="table" target="#tab_3">Table 4</ref> shows the F1-scores of the trained model using different data augmentation types, best results shown in bold.</p><p>The tables show that each augmentation step affects the model's efficiency negatively. This is expected since each step incrementally increases the size of the dataset. On the other hand, not each increment step has a positive effect on the model's effectiveness. Such trends are worth exploring in a more exhaustive study. Finally, it is worth mentioning that the last experiments in both experiment sets are the same. So, they both have the same results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Attempts</head><p>We test several other techniques to explore how they might affect our model. For example, using pre-trained FastText <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> embeddings as an input to our model yields worse F1score on both public and private leaderboards with 94.254 and 93.118, respectively, compared with the ELMo contextual embeddings model. In another experiment, we use the thought vector outputted from the second ON-LSTM layer as input for the decision component. However, the sequence weighted attention gives better results by about 1 point of the F1-score. Moreover, an attempt to overcome the weakness of the Arabic ELMo model is done by translating the data to   English using Google Translate 8 and treating the problem as an English SQS problem instead, but the results are much worse with 88.868 and 87.504 F1-scores on public and private leaderboards, respectively. This is probably because a lot of information is lost during the translation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>This section briefly analyzes the questions representations learnt by our model. With the sequence weighted attention layer, the model reduces all the information about the sequence extracted using the ON-LSTMs down to a 512 fixed-size vector. By extracting these vectors from our best model and plotting them on a 2D plane using t-SNE (Maaten and Hinton, 2008) dimensionality reduction algorithm, we notice some very useful observations. For example, the model learns to map questions that ask about the same thing to have nearby representations in the vector space such as the questions in <ref type="figure" target="#fig_4">Figure 4</ref> with the form: "How to prepare 'something'?". The same thing goes for the questions in <ref type="figure">Figure 5</ref> with the form: "What is the definition of 'something'?". In a similar manner, in <ref type="figure" target="#fig_5">Figure 6</ref>, the questions ask about different types of languages like "What is the formal language in Portugal?" and "What is PHP language?" are close, as well as, the questions in <ref type="figure">Figure 7</ref> that ask about places like "Where is Sweden?", "Where is the Karak area in Jordan?", and "Where is the Kremlin Castle?".</p><p>To further illustrate the usefulness of the sequence weighted attention layer, <ref type="figure" target="#fig_6">Figure 8</ref> shows that the attention layer learns to focus more on the key words in the questions that would determine what the question is actually asking about. This allows the model to make better decisions for whether the the questions are similar or not, even if the questions have similar words but ask about different things. The first and second questions  <ref type="figure" target="#fig_6">Figure 8</ref> ask about "What is the general manager?". So, the attention layer focuses on "the general manager" which is " ". However, in the third and fourth questions, one asks "What is the most beautiful thing that is said about death?" and the other ones asks "What is death?", although both questions are related to "death" which is "</p><p>" but the attention layer distinguishes them as not similar, where in the former one, the focus is concentrated by order on the words " ", "</p><p>" and " " ("said", "most beautiful" and "death"), while the latter one focuses mostly on " " ("death").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described our team's effort on the semantic text question similarity task of NSURL 2019. Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data. Then, it takes ELMo pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs. The model was ranked in the 1st place with 96.499 F1-score (same as the second place F1-score) and the 2nd place with 94.848 F1-score (differs by 1.076 F1-score from the first place) on the public and private leaderboards, respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Punctuation marks considered in the preprocessing step</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Number of examples per data augmentation step • Reflexive: By definition, a question A is similar to itself. Note: This rule generates 10,540 extra positive examples (45,514 total) which helps balancing the number of positive and negative examples. After the augmentation process, the training data contains 45,514 examples (23,082 positive examples and 22,432 negative ones).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Model Structure meanings ('gold' in the first sentence and 'went' in the second one):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Representations extracted from sequence weighted attention layer for questions of the form: How to prepare 'something'? Figure 5: Representations extracted from sequence weighted attention layer for questions of the form: What is the definition of 'something'?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Representations extracted from sequence weighted attention layer for questions that ask about different language types Figure 7: Representations extracted from sequence weighted attention layer for questions that ask about different places in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Weights per word from sequence weighted attention layer on four different examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model size and training time for each RNN cell type</figDesc><table><row><cell>RNN Cell</cell><cell>#Params</cell><cell>Training Time</cell></row><row><cell>GRU</cell><cell cols="2">4,363K 55.2s/epoch -1.53 hours</cell></row><row><cell>LSTM</cell><cell cols="2">5,413K 58.2s/epoch -1.61 hours</cell></row><row><cell cols="3">ON-LSTM (Chunk: 4) 5,938K 74.2s/epoch -2.06 hours</cell></row><row><cell cols="3">ON-LSTM (Chunk: 8) 5,675K 74.4s/epoch -2.06 hours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model F1-score using different RNN cell types</figDesc><table><row><cell>Leaderboard</cell><cell>RNN Cell</cell><cell>Min</cell><cell>Max</cell><cell>Avg</cell><cell>Vote</cell></row><row><cell></cell><cell>GRU</cell><cell cols="4">94.075 94.793 94.613 95.242</cell></row><row><cell>Public</cell><cell cols="5">LSTM ON-LSTM (Chunk: 4) 94.524 95.601 95.242 96.140 94.614 95.152 94.901 95.062</cell></row><row><cell></cell><cell cols="5">ON-LSTM (Chunk: 8) 95.601 95.780 95.691 96.499</cell></row><row><cell></cell><cell>GRU</cell><cell cols="4">93.271 94.194 93.855 94.579</cell></row><row><cell>Private</cell><cell cols="5">LSTM ON-LSTM (Chunk: 4) 93.810 94.425 94.224 94.732 93.925 94.271 94.040 94.117</cell></row><row><cell></cell><cell cols="5">ON-LSTM (Chunk: 8) 94.002 94.463 94.309 94.848</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model training time for each data augmentation step: O, T, S, and R, which stand for Original, Transitive, Symmetric, and Reflexive, respectively</figDesc><table><row><cell cols="2">Data Augmentation Examples Number</cell><cell>Training Time</cell></row><row><cell>O</cell><cell>11,997</cell><cell>20.0s/epoch -0.55 hours</cell></row><row><cell>O+T</cell><cell>17,487</cell><cell>29.4s/epoch -0.81 hours</cell></row><row><cell>O+T+S</cell><cell>34,974</cell><cell>57.0s/epoch -1.58 hours</cell></row><row><cell>O+T+S+R</cell><cell>45,514</cell><cell>74.4s/epoch -2.06 hours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">: Model F1-score using different data augmentation types: O, T, S, and R, which stand for Original,</cell></row><row><cell cols="2">Transitive, Symmetric, and Reflexive respectively</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Leaderboard Data Aug.</cell><cell>Min</cell><cell>Max</cell><cell>Avg</cell><cell>Vote</cell></row><row><cell></cell><cell>O</cell><cell cols="4">93.626 94.703 94.200 94.973</cell></row><row><cell>Public</cell><cell>O+T O+T+S</cell><cell cols="4">93.177 94.434 93.877 94.793 94.344 94.793 94.631 95.421</cell></row><row><cell></cell><cell cols="5">O+T+S+R 95.601 95.780 95.691 96.499</cell></row><row><cell></cell><cell>O</cell><cell cols="4">93.425 93.810 93.632 94.655</cell></row><row><cell>Private</cell><cell>O+T O+T+S</cell><cell cols="4">92.464 93.771 93.232 94.156 93.579 94.002 93.763 94.655</cell></row><row><cell></cell><cell cols="5">O+T+S+R 94.002 94.463 94.309 94.848</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.mawdoo3.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com arXiv:1912.12514v1 [cs.CL] 28 Dec 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/AliOsm/ semantic-question-similarity</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/HIT-SCIR/ ELMoForManyLangs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/CyberZHG/ keras-ordered-neurons 6 https://github.com/CyberZHG/ keras-self-attention</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://translate.google.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the Deanship of Research at the Jordan University of Science and Technology for supporting this work via Grant #20180193.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paraphrase identification and semantic text similarity analysis in arabic news tweets using lexical, syntactic, and semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Smadi</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zain</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Jararweh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="640" to="652" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arabic semantic similarity approaches-review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwah</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arafat</forename><surname>Awajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Arab Conference on Information Technology (ACIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance analysis of google colaboratory as a tool for accelerating deep learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Victor Medeiros Da Nóbrega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Nepomuceno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Bin</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hugo C De Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Pedrosa Reboucas</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="61677" to="61685" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextual text understanding in distributional semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A text semantic similarity approach for arabic paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnen</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Zrigui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounir</forename><surname>Zrigui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semantic similarity of arabic sentences with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NSURL-2019 task 8: Semantic question similarity in arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><surname>Seelawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Al-Bataineh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Farhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first International Workshop on NLP Solutions for Under Resourced Languages, NSURL &apos;19</title>
		<meeting>the first International Workshop on NLP Solutions for Under Resourced Languages, NSURL &apos;19<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09536</idno>
		<title level="m">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06006</idno>
		<title level="m">Contextual word representations: A contextual introduction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiway attention networks for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4411" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
		<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
