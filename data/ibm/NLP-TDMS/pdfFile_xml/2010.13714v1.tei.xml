<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActiveNet: A computer-vision based approach to determine lethargy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitik</forename><surname>Gupta</surname></persName>
							<email>aitikgupta@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ABV-IIITM</orgName>
								<address>
									<settlement>Gwalior</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadit</forename><surname>Agarwal</surname></persName>
							<email>agarwal.aadit99@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">ABV-IIITM</orgName>
								<address>
									<settlement>Gwalior</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ActiveNet: A computer-vision based approach to determine lethargy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3430984.3430986</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Human Pose Estimation</term>
					<term>Pose Encoding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The outbreak of COVID-19 has forced everyone to stay indoors, fabricating a significant drop in physical activeness. Our work is constructed upon the idea to formulate a backbone mechanism, to detect levels of activeness in real-time, using a single monocular image of a target person. The scope can be generalized under many applications, be it in an interview, online classes, security surveillance, et cetera.</p><p>We propose a Computer Vision based multi-stage approach, wherein the pose of a person is first detected, encoded with a novel approach, and then assessed by a classical machine learning algorithm to determine the level of activeness. An alerting system is wrapped around the approach to provide a solution to inhibit lethargy by sending notification alerts to individuals involved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Activeness, both physical and mental, has been one of the primary healthcare concerns since the inception of smart devices. It has undergone a significant drop, especially since the COVID-19 outbreak <ref type="bibr" target="#b11">[11]</ref>. This is due to lazy and sedentary practices, either during leisure time or while working from home.</p><p>Despite recent advancements in Computer Vision and related domains, reliable security surveillance systems with little to no human intervention <ref type="bibr" target="#b8">[8]</ref> is still a challenge, especially now that these times call for unfortunate motivations and impulses.</p><p>There has been significant research on drowsiness detection using Computer Vision <ref type="bibr" target="#b0">[1]</ref>, but most of them leverage the degree to which the person's eyes are open or closed. While this setting is instrumental for in-car webcams, they fail for long-range distances, which is a standard paradigm in CCTV footages, security webcams, et cetera. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CODS COMAD 2021, January 2-4, 2021, Bangalore, India © 2021 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-8817-7/21/01. https://doi.org <ref type="bibr">/10.1145/3430984.3430986</ref> Moreover, numerous papers related to the study of body language using Computer Vision pertain to emotion detection <ref type="bibr" target="#b12">[12]</ref>; wherein there has been an apparent lack of attention, given to attention.</p><p>With the outlook of a more generalized solution, we demonstrate a multi-stage mechanism to identify activeness of a person, such as in online classes, job interviews, security surveillance systems, et cetera, aiming to rectify diminished activeness in students, interviewees, security guards respectively. The input to the pipeline would just be a single RGB image. To realize lethargy in this type of multistage approach, an intermediate representation of information is essential. At the end of the first stage, we maintain a 2-dimensional Cartesian plane coordinate information of various joints of a person. Just using this representation, it is almost impractical to achieve our goal, which drives the idea of our novel pose encoding stage, where we maintain an angular representation of data, aiming to remove all positional aspects in the data. With abstraction after every stage, the resulting system becomes more robust to the inherent noise of the data, such as visual characteristics of various people, different coordinates for different image views or sizes, et cetera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ARCHITECTURE</head><p>ActiveNet is a machine-learning based system, the proposed architecture for which accepts a camera input. It is then operated through a Human Pose Estimation module to extract keypoints of the joints in human body. The pose encoding module generates a vector, which is calculated by the angles of certain joints. The angles are imputed and scaled via intermediaries generated while training a self-scraped dataset, discussed later. After processing, a traditional machine-learning classifier takes the angles as input. The classifier predicts the activeness level, based on which the alert system is triggered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Human Pose Estimation</head><p>2.1.1 Literature Review. Human Pose Estimation (HPE) is one of the key aspects in computer vision that has undergone tremendous research in the last few years. Its numerous applications are one of the main reasons for the importance it has gained. It estimates the configuration of the body (pose) from a single, typically monocular, image. To study the methods of estimating the position of joints from an image, one must understand some of the most significant challenges:</p><p>• Background noise, lighting, visibility issues • High variance of visual appearance and physique of humans • Partial occlusions due to self-articulation and layering of objects • Complexity of human skeletal structure and its hierarchy • Information loss by projecting a 3D object to a 2D plane arXiv:2010.13714v1 [cs.CV] 26 Oct 2020 Some classical approaches are based on a pictorial-structure framework. Early works introduced a mixture model of parts, which expresses joint relationships <ref type="bibr" target="#b19">[19]</ref>. This approach has the limitation of having the pose model independent of image data. Later on, deep learning based regression approaches were introduced <ref type="bibr" target="#b17">[17]</ref>, which brought a shift in the research paradigm towards them. Most of the later researches operate over convolutional building blocks, and have been universally adopted for image-data driven approaches. Gradually, direct keypoint regression-based methods were replaced by more promising heatmap regression methods <ref type="bibr" target="#b18">[18]</ref>.</p><p>Broadly classified, there are two approaches to the convolutional architectures for Single Person Pose Estimation (SPPE) or Multi-Person Pose Estimation (MPPE). The first approach, the top-down approach, is decoupled into two sub-problems. Firstly, a person detection algorithm is claimed, followed by a pose estimation algorithm for every detected person. State-of-the-art (SOTA) solutions for the sub-problems could potentially be utilized together in the pipeline. The inference speed of this approach strongly depends on the number of detected people inside an image. The second approach, called the bottom-up approach, is more resilient to the number of people in an image, and thereby, could potentially be faster than the first approach. Firstly, all possible keypoints are detected in an image, followed by grouping by human instances.</p><p>2.1.2 Our Work. We based our work on the popular bottom-up method Lightweight OpenPose <ref type="bibr" target="#b13">[13]</ref>, for mainly two reasons. Firstly, this work heavily optimizes the original OpenPose <ref type="bibr" target="#b2">[3]</ref> implementation to reach real-time inference speeds on CPU with negligible accuracy drop, which can further be optimized for Edge Devices using Intel® OpenVINO™ Toolkit, as done in other researches <ref type="bibr" target="#b9">[9]</ref>. Secondly, in the context of finding the levels of alertness in a person, our work is dependent on the angles generated by different body joints. Therefore, the bottom-up approach is well suited for isolation of joints, even in cases where other parts are not detected. We propose a pre-processing step for the pose encoding, which takes care of the joints for which HPE module could not determine the 2D locations of keypoints. It will be discussed later in the paper.</p><p>We leverage the pre-trained weights provided by open-sourced library at: https://github.com/Daniil-Osokin/lightweight-humanpose-estimation.pytorch, which contains the implementation of LightWeight OpenPose <ref type="bibr" target="#b13">[13]</ref>. The weights are trained over the COCO dataset <ref type="bibr" target="#b10">[10]</ref>, consisting of 17 keypoints in the following order: (1) Nose; <ref type="formula">(2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pose Encoding</head><p>We propose a pose encoding technique, with the aim of removing any positional configurations from the detected keypoints. We claim this by taking the angles between different subsets of joints in the upper and lower body. Regardless of the position of a person in an image, the absolute values of angles between the joints remain same for a single type of pose. We do not use the keypoints directly, as <ref type="figure" target="#fig_2">Figure 2</ref> explains that even if the images are just a mirror of each other and the contextual pose of the person remains same, their estimations can be quite different. Those are dependent on the positional aspects in the camera input. A slight change in the camera's offset would lead to an entirely different set of 2D keypoint estimations. Another motivation for angular representation can be explained by <ref type="figure">Figure 3</ref>; as the angle between shoulders, neck and head changes, the abstract activeness of the person changes along with it. We consider the angles made by following subset of joints:</p><p>(1) Nose, Eyes, Ears (2) Neck, Nose, Ears (3) Shoulders, Neck, Nose (4) Neck, Shoulders, Elbows (5) Shoulders, Elbows, Wrists (6) Nose, Neck, Core Joint (7) Neck, Core Joint, Hips Encoding a single pose creates an array of 15 elements, containing angular data in degrees. The joints which are not detected in the pose, are represented by (-1,-1) in the coordinate axes. We give NaN values in the encoding when two out of three keypoints (from A, B and C) have the same 2D coordinates, which includes the (-1,-1) case. This can well occur due to occlusions in the image, or semi-accurate estimations. If this is not handled during encoding, a ZeroDivisionError error is raised. NaN values are later handled in subsection 2.4. Since this module inputs just 2D coordinates of joints detected by HPE algorithm discussed in subsection 2.1, it is invariant to the visual aspects of the image, inherently expanding the generalizability to new image-data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>To train our model on the encoding from the module discussed in subsection 2.2, we scraped images from web using different keywords for different classes. The dataset contains 4 classes for 4 levels of activeness-zones:</p><p>(1) Level 1: Below 25% (2) Level 2: Between 25-50% (3) Level 3: Between 50-75% (4) Level 4: Above 75% There are 40 images for each class, scraped using keywords such as "army soldiers" for Level 4 activeness, while "sleeping while standing" for level 1 activeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pre-Processing</head><p>2.4.1 Treating NaN Values. Pose encoding yields NaN values as discussed in subsection 2.2. Treating these values is a prominent discussion in data science. One approach is to eliminate the encoding which has more than half NaN values. Another approach is to use numpy.ma module <ref type="bibr" target="#b7">[7]</ref> which provides a convenient way to address this issue, by introducing masked arrays, which are either no-mask representing only clean entries or boolean arrays indicating presence of invalid entries, in which case the invalid entries are eliminated, allowing the classifier to train on only valid entries. The approach of pose encoding as described in subsection 2.2 and as shown in <ref type="figure">Figure 3</ref> makes this possible, even with fewer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Scaling</head><p>Features. The raw features from the encodings are scaled down to a similar range across the whole dataset. This helps in many ways during training, as priority can be given to optimizing the weights based on the correlation of features to the target rather than scales of various features. We use the standard scaler provided by scikit-learn for this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training</head><p>To train a classifier using the scaled data discussed in subsection 2.4, we adapt the ensemble algorithms based on Decision Trees <ref type="bibr" target="#b14">[14]</ref>, such as Random Forest Classifier (RFC) <ref type="bibr" target="#b1">[2]</ref> and XGBoost Classifier <ref type="bibr" target="#b3">[4]</ref>. We first train and validate a Logistic Regression algorithm <ref type="bibr" target="#b15">[15]</ref> to establish the benchmark scoring for classification. Subsequently we tune the hyper-parameters using scikit-learn's GridSearchCV. The best scoring results are then chosen for each algorithm, while using K-Fold cross-validation technique. <ref type="table" target="#tab_0">Table 1</ref> provides algorithmwise results.  Based on experimentation, the best results are achieved using RFC with an accuracy of 76.67%. As <ref type="figure">Figure 3</ref> explains, even a small subset of joints can be adequate to assess the overall activeness. Hence a general interpretation can be lined that tree-based ensemble algorithms would do better here, as the individual trees are built on subsets of features. A detailed evaluation of the classifier results is given in <ref type="table" target="#tab_1">Table 2</ref>. Analysing the results, we find that the extreme classes have the best scores compared to the intermediate classes. Our self-scraped dataset does not represent an ideal distribution of the real-world classes, the outcomes so achieved only provide a baseline solution for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Evaluation and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Alert Mechanism</head><p>At the time of inference, an alerting mechanism provides a method to inform people in-concern if diminished levels of activeness is developed in the targeted person. This is done in faith to keep the targeted person agile. To get rid of false positives, and in order to make the pipeline more robust, we raise a notification only when k number of contiguous frames are classified in the lowest class, where k is an arbitrary constant. A reasonable value of k can change with the domain. The alert module relies on Slack Workspaces. Other works have used the same utility, but for different use cases [6] <ref type="bibr" target="#b5">[5]</ref>.</p><p>We configure a notification-alert bot after enabling the Incoming Webhook functionality in Slack Workspace:</p><p>(1) Webhook is a unique URL to send HTTP requests (2) Only organizations with the Webhook URL can send alerts (3) Only organizations in the workspace can receive alerts (4) The alert can be sent with a customized message, along with a time-stamp (5) Works on all platforms, requires Slack to be installed For demonstration purposes, we create a demo Slack Workspace active-networkspace, typically meant for an organization. Here for example, it is meant for active-net organization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEMONSTRATION</head><p>For demonstration, we use a desktop webcam, rigged with a single Nvidia GeForce GTX 1650 graphics card to run the whole pipeline, inferencing at around 34 frames per second. The code, along with screenshots are available at: github.com/aaditagarwal/ActiveNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CURRENT LIMITATIONS</head><p>As there exists no well defined dataset for such type of classification task, in addition to discussion in subsection 2.6, we end up with weights which do not generalize for unseen poses. One of the main complication arises when camera view angle shifts. For instance, a situation where the person is looking sideways, the keypoint estimations will be from a totally different distribution compared to estimations when person is looking directly at the camera. Consecutively, angles between joints do not provide a good encoding solution in this situation. Considering the limitations of the self-scraped dataset, we overlooked the implications of such issues.</p><p>Another limitation to be realized is the loss of information projecting a 3-dimensional object to a 2-dimensional space. The 2D keypoint estimations, and their corresponding encoding, both suffer from this limitation. Considering the recent developments in 3D pose estimation, which are either end-to-end <ref type="bibr" target="#b16">[16]</ref>, i.e. operate on a monocular image, or take 2D estimations from HPE modules and lift them to 3 dimensional space <ref type="bibr" target="#b20">[20]</ref>, theoretically, encodings could be improved further to support angles made in 3 dimensions. Given our aim for a real-time solution, we decided not to address the 3 dimensional approach, but can be approached in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Through this paper, we explained our multi-stage approach to identify levels of activeness with a novel pose encoding stage. Once trained on enough poses, it can potentially be used in any domain without retraining, given that the HPE module is robust enough to generate the keypoints. We demonstrated the idea with a working pipeline, using Slack to alert users in a workspace with custom messages. We welcome future research in this domain with a strong belief that Activeness should be given active attention, both mental and physical.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ActiveNet Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Neck; (3) Right Shoulder; (4) Right Elbow; (5) Right Wrist; (6) Left Shoulder; (7) Left Elbow; (8) Left Wrist; (9) Right Hip; (10) Right Knee; (11) Right Ankle; (12) Left Hip; (13) Left Knee; (14) Left Ankle; (15) Right Eye; (16) Left Eye; (17) Right Ear; (18) Left Ear. The Neck (2) keypoint is just a 2-dimensional mean of Right Shoulder (3) and Left Shoulder (6) keypoints in the Cartesian coordinate system, therefore making it a total of 17 keypoints, along with one additional extrapolated keypoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Nose keypoint estimation with augmentation (8) Hips, Knees, Ankles The subset is chosen such that it captures the possible range of motions in most sections of human body, and thus, numerous poses. We consider Core Joint as the 2D mean of Left Hip and Right Hip joints in the Cartesian plane, and the actual angles are calculated using dot product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: Angular Encoding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 Figure 4 :</head><label>44</label><figDesc>Activeness Levels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Alert Mechanism Flowchart</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classifier-Wise Results</figDesc><table><row><cell>Classifier</cell><cell>Accuracy</cell></row><row><cell>Logistic Regression</cell><cell>56.67%</cell></row><row><cell>Decision Tree Classifier</cell><cell>66.67%</cell></row><row><cell>XGBoost Classifier</cell><cell>63.34%</cell></row><row><cell>Random Forest Classifier (RFC)</cell><cell>76.67%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Class-Wise Results for RFC</figDesc><table><row><cell>Class</cell><cell cols="3">Precision Recall F1-Score</cell></row><row><cell>Level 1</cell><cell>0.989</cell><cell>0.820</cell><cell>0.896</cell></row><row><cell>Level 2</cell><cell>1.000</cell><cell>0.745</cell><cell>0.853</cell></row><row><cell>Level 3</cell><cell>0.914</cell><cell>0.663</cell><cell>0.768</cell></row><row><cell>Level 4</cell><cell>0.989</cell><cell>0.807</cell><cell>0.888</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Driver drowsiness detection using face expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939785" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="785" to="794" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automated Outlier Detection in Crime Data Using Programming. Undergraduate Honors Thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Connolly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>University of Nebraska-Lincoln</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Affordable remote monitoring of plant growth in facilities using Raspberry Pi computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandin</forename><surname>Grindstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makenzie</forename><forename type="middle">E</forename><surname>Mabry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Blischak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheal</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Chris</forename><surname>Pires</surname></persName>
		</author>
		<idno type="DOI">https:/arxiv.org/abs/https:/bsapubs.onlinelibrary.wiley.com/doi/pdf/10.1002/aps3.11280</idno>
		<ptr target="https://bsapubs.onlinelibrary.wiley.com/doi/pdf/10.1002/aps3.11280" />
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Missing data: masked arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uhc Group</surname></persName>
		</author>
		<ptr target="https://currents.soest.hawaii.edu/ocn_data_analysis/_static/masked_arrays.html" />
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on behavior analysis in video surveillance for homeland security applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th IEEE Applied Imagery Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">iSEC: An Optimized Deep Learning Model for Image Classification on Edge Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endah</forename><surname>Kristiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Tung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2971566</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.2971566" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The global macroeconomic impacts of COVID-19: Seven scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshen</forename><surname>Mckibbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorota</forename><surname>Kamińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Sapiński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Anbarjafari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2018.2874986</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2018.2874986" />
		<title level="m">Survey on Emotional Body Gesture Recognition. IEEE Transactions on Affective Computing PP</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decision Trees: An Overview and Their Use in Medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vili</forename><surname>Podgorelec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kokol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Stiglic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Rozman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1016409317640</idno>
		<ptr target="https://doi.org/10.1023/A:1016409317640" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="445" to="463" />
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Logistic Regression</title>
		<idno type="DOI">10.1007/978-0-387-30164-8_493</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-30164-8_493" />
		<editor>Claude Sammut and Geoffrey I. Webb</editor>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="631" to="631" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-View Pose Generator Based on Deep Learning for Monocular 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1116</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepPose: Human Pose Estimation via Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Articulated Human Detection with Flexible Mixtures of Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.261</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2012.261" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic Graph Convolutional Networks for 3D Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
