<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Graph Pooling with Structure Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
							<email>zhenzhang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Graph Pooling with Structure Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep neural networks with convolution and pooling layers have achieved great success in various challenging tasks, ranging from computer vision <ref type="bibr" target="#b8">(He et al. 2016)</ref>, natural language understanding <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> to video processing <ref type="bibr" target="#b9">(Karpathy et al. 2014)</ref>. The data in these tasks are typically represented in the Euclidean space (i.e., modeled as 2-D or 3-D tensors), thus usually containing locality and order information for the convolution operations <ref type="bibr" target="#b1">(Defferrard, Bresson, and Vandergheynst 2016)</ref>. However, in many real-world problems, a large amount of data, such as social networks, chemical molecules and biological networks, are lying on non-Euclidean domains that can be naturally represented as graphs. Due to the neural network's powerful capabilities, it's quite appealing to generalize the convolution and pooling operations to graph-structured data.</p><p>Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>Recently, there have been a myriad of attempts to generalize the convolution operations to arbitrary graphs, referred to as graph neural networks (GNNs for short). In general, these algorithms can be classified into two big categories: spectral and spatial approaches. For the spectral methods, they typically define the graph convolution operations based on graph Fourier transform <ref type="bibr" target="#b1">(Bruna et al. 2013;</ref><ref type="bibr" target="#b1">Defferrard, Bresson, and Vandergheynst 2016;</ref>. For the spatial methods, the graph convolution operations are devised by aggregating the node representations directly from its neighborhood <ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b13">Monti et al. 2017;</ref><ref type="bibr" target="#b16">Veličković et al. 2018;</ref>. Majority of the aforementioned methods mainly involve transforming, propagating and aggregating node features across the graph, which can fit in the message passing scheme <ref type="bibr" target="#b7">(Gilmer et al. 2017)</ref>. GNNs have been applied to different types of graphs <ref type="bibr" target="#b16">(Veličković et al. 2018;</ref><ref type="bibr" target="#b2">Derr, Ma, and Tang 2018)</ref>, and obtained outstanding performance in numerous graph related tasks, including node classification <ref type="bibr">Welling 2017), link prediction (Schlichtkrull et al. 2018;</ref>) and recommendation <ref type="bibr" target="#b17">(Ying et al. 2018a)</ref>, etc.</p><p>Nevertheless, the pooling operations in graphs have not been extensively studied yet, though they act a pivotal part in learning hierarchical representations for the task of graph classification <ref type="bibr" target="#b17">(Ying et al. 2018b</ref>). The goal of graph classification is to predict the label associated with the entire graph by utilizing its node features and graph structure information, i.e., a graph level representation is needed. GNNs are originally designed to learn meaningful node level representations, thus a commonly adopted approach to generate graph level representation is to globally summarize all the node representations in the graph. Although workable, the graph level representation generated via this way is inherently "flat", since the entire graph structure information is neglected during this process. Furthermore, GNNs can only pass messages between nodes through edges, but cannot aggregate node information in a hierarchical way. Meanwhile, graphs often have different substructures and nodes are of different roles, therefore they should contribute differently to the graph level representation. For example, in the proteinprotein interaction graphs, the certain substructures may rep-resent some specific functionalities, which are of great significance to predict the whole graph characteristics. To capture both the graph's local and global structure information, a hierarchical pooling process is demanded.</p><p>There exists some very recent work that focuses on the hierarchical pooling procedure in <ref type="bibr">GNNs (Ying et al. 2018b;</ref><ref type="bibr" target="#b6">Gao and Ji 2019;</ref><ref type="bibr" target="#b4">Diehl 2019;</ref><ref type="bibr" target="#b6">Gao, Chen, and Ji 2019)</ref>. These models usually coarsen the graphs through grouping or sampling nodes into subgraphs level by level, thus the entire graph information is gradually reduced to the hierarchical induced subgraphs. However, the graph pooling operations still have room for improvement. In node grouping approaches, the hierarchical pooling methods <ref type="bibr" target="#b17">(Ying et al. 2018b;</ref><ref type="bibr" target="#b4">Diehl 2019)</ref> suffer from high computational complexity, which require additional neural networks to downsize the nodes. In node sampling approaches, the generated induced subgraph <ref type="bibr" target="#b12">Lee, Lee, and Kang 2019)</ref> might fail to preserve the key substructures and eventually lose the completeness of graph topological information. For instance, two nodes that are not directly connected but sharing many common neighbors in the original graph might become unreachable from each other in the induced subgraph, even if intuitively they ought to be "close" in the subgraph. Therefore, the distorted graph structure will hinder the message passing in subsequent layers.</p><p>To address the aforementioned limitations, we propose a novel graph pooling operator HGP-SL to learn hierarchical graph level representations. Specifically, HGP-SL first adaptively selects a subset of nodes according to our defined node information score, which fully utilizes both the node features and graph topological information. In addition, the proposed graph pooling operation is a non-parametric step, therefore no additional parameters need to be optimized during this procedure. Then, we apply a structure learning mechanism with sparse attention <ref type="bibr" target="#b13">(Martins and Astudillo 2016)</ref> to the pooled graph, aiming to learn a refined graph structure that preserves the key substructures in the original graph. We integrate the pooling operator into graph convolutional neural network to perform graph classification and the whole procedure can be optimized in an end-to-end manner. To summarize, the main contributions of this paper are as follows:</p><p>• We introduce a novel graph pooling operator HGP-SL that can be integrated into various graph neural network architectures. Similarly to the pooling operations in convolutional neural networks, our proposed graph pooling operation is non-parametric 1 and very easy to implement.</p><p>• To the best of our knowledge, we are the first to design a structure learning mechanism for the pooled graph, which has the advantage of learning a refined graph structure to preserve the graph's key substructures.</p><p>• We conduct extensive experiments on six public datasets to demonstrate HGP-SL's effectiveness as well as superiority compared to a range of state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Graph Neural Networks</head><p>GNNs can be generally categorized into two branches: spectral and spatial approaches. The spectral methods typically define the parameterized filters according to graph spectral theory. <ref type="bibr" target="#b1">(Bruna et al. 2013</ref>) first proposed to define convolution operations for graph in the Fourier transform domain. Due to its heavy computation cost, it has difficulty in scaling to large graphs. Later on, (Defferrard, Bresson, and Vandergheynst 2016) improved its efficiency by approximating the K-polynomial filters through Chebyshev expansion. GCN  further simplified the Cheb-Net by truncating the Chebyshev polynomial to the firstorder approximation of the localized spectral filters. The spatial approaches design convolution operations by directly aggregating the node's neighborhood information. Among them, GraphSAGE <ref type="bibr" target="#b7">(Hamilton, Ying, and Leskovec 2017)</ref> proposed an inductive algorithm that can generalize to unseen nodes by aggregating its neighborhood content information. <ref type="bibr">GAT (Veličković et al. 2018)</ref> utilized attention mechanism to aggregate nodes' neighborhood representations with different weights. JK-Net (Xu et al. 2018) leveraged flexible neighborhood ranges to enable better node representations. More details can be found in several comprehensive surveys on graph neural networks <ref type="bibr" target="#b21">(Zhou et al. 2018;</ref><ref type="bibr" target="#b20">Zhang, Cui, and Zhu 2018;</ref><ref type="bibr" target="#b17">Wu et al. 2019</ref>). Nevertheless, the above mentioned two branches of GNNs are mainly designed for learning meaningful node representations, and unable to generate hierarchical graph representations due to the lack of pooling operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Pooling</head><p>Pooling operations in GNNs can scale down the size of inputs and enlarge the receptive fields, thus giving rise to better generalization and performance. DiffPool <ref type="bibr" target="#b17">(Ying et al. 2018b)</ref> proposed to softly assign nodes to a set of clusters using neural networks, which forms a dense cluster assignment matrix and is computation expensive. gPool  and SAGPool (Lee, Lee, and Kang 2019) devised a top-K node selection procedure to form an induced subgraph for the next input layer. Though efficient, it might lose the completeness of the graph structure information and result in isolated subgraphs, which will hamper the message passing process in subsequent layers. EdgePool <ref type="bibr" target="#b4">(Diehl 2019)</ref> designed pooling operation by contracting the edges in the graph, but its flexibility is poor because it will always pool roughly half of the total nodes. iPool <ref type="bibr" target="#b7">(Gao, Xiong, and Frossard 2019)</ref> presented a parameter-free pooling scheme which is invariant to graph isomorphism. EigenPool (Ma et al. 2019) introduced a pooling operator based on the graph Fourier transform, which controls the pooling ratio through spectral clustering and it's also very time consuming.</p><p>In addition, there are also some approaches that perform global pooling. For instance, Set2Set <ref type="bibr" target="#b17">(Vinyals, Bengio, and Kudlur 2015)</ref> implemented the global pooling operation by aggregating information through LSTMs (Hochreiter and Schmidhuber 1997). DGCNN ) pooled the graph according to the last channel of the feature map values which are sorted in the descending order. Graph topological based pooling operations are proposed in <ref type="bibr" target="#b1">(Defferrard, Bresson, and Vandergheynst 2016)</ref> and <ref type="bibr" target="#b15">(Rhee, Seo, and Kim 2017)</ref> as well, where Graclus method <ref type="bibr" target="#b3">(Dhillon, Guan, and Kulis 2007)</ref> is employed as a pooling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations and Problem Formulation</head><p>Given a set of graph data G = {G 1 , G 2 , · · · , G n }, where the number of nodes and edges in each graph might be quite different. For an arbitrary graph G i = (V i , E i , X i ), we have n i and e i denote the number of nodes and edges, respectively. Let A i ∈ R ni×ni be the adjacent matrix describing its edge connection information and X i ∈ R ni×f represents the node feature matrix, where f is the dimension of node attributes. Label matrix Y ∈ R n×c indicates the associated labels for each graph, i.e., if G i belongs to class j, then Y ij = 1, otherwise Y ij = 0. Since the graph structure and node numbers change between layers due to the graph pooling operation, we further represent the i-th graph fed into the k-th layer as G k i with n k i nodes. The adjacent matrix and hidden representation matrix are then denoted as</p><formula xml:id="formula_0">A k i ∈ R n k i ×n k i and H k i ∈ R n k i ×d .</formula><p>With the above notations, we formally define our problem as follows:</p><p>Input: Given a set of graphs G L with its label information Y L , the number of graph neural network layers K, pooling ratio r, and representation dimension d in each layer.</p><p>Output: Our goal is to predict the unknown graph labels of G/G L with graph neural network in an end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Neural Network</head><p>Graph convolutional neural network (or GCN) <ref type="bibr">(Kipf and</ref> Welling 2017) has shown to be very efficient and achieved promising performance in various challenging tasks. Thus, we choose GCN as our model's building block and briefly review its mechanism in this subsection. Please note that our proposed HGP-SL operator can also be integrated into other graph neural network architectures like GraphSAGE (Hamilton, Ying, and Leskovec 2017) and <ref type="bibr">GAT (Veličković et al. 2018</ref>). We will discuss this in the experiment section. For the k-th layer in GCN, it takes graph G's adjacent matrix A and hidden representation matrix H k as input, then the next layer's output will be generated as follows:</p><formula xml:id="formula_1">H k+1 = σ(D − 1 2ÃD − 1 2 H k W k ),<label>(1)</label></formula><p>where σ(·) is the non-linear activation function and H 0 = X,Ã = A + I is the adjacent matrix with self-connections. D is the diagonal degree matrix ofÃ, and W k ∈ R d k ×d k+1 is a trainable weight matrix. For the ease of parameter tuning, we set output dimension d k+1 = d k = d for all layers.</p><p>The Overall Neural Network Architecture <ref type="figure">Figure 1</ref> provides an overview of our proposed Hierarchical Graph Pooling with Structure Learning (HGP-SL) that combines with graph neural network, where graph pooling operations are added between graph convolution operations. The proposed HGP-SL operator is composed of two major components: 1) graph pooling, which preserves a subset of informative nodes and forms a smaller induced subgraph; and 2) structure learning, which learns a refined graph structure for the pooled subgraph. The advantage of our proposed structure learning lies in its capability to preserve the essential graph structure information, which will facilitate the message passing procedure. As in this illustrative example, the pooled subgraph might exist isolated nodes but intuitively ought to be connected, thus it would hinder the information propagation in subsequent layers especially when aggregating information from its neighborhood nodes. The whole architecture is the stacking of convolution and pooling operations, thus making it possible to learn graph representations in a hierarchical way. Then, a readout function is utilized to summarize node representations in each level, and the final graph level representation is the addition of different levels' summarizations. At last, the graph level representation is fed into a Multi-Layer Perceptron (MLP) with softmax layer to perform graph classification task. In what follows, we give the details of graph pooling and structure learning layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Pooling Operation</head><p>In this subsection, we introduce our proposed graph pooling operation to enable down-sampling on graph data. Inspired by <ref type="bibr" target="#b12">Lee, Lee, and Kang 2019;</ref><ref type="bibr" target="#b7">Gao, Xiong, and Frossard 2019)</ref>, the pooling operation identifies a subset of informative nodes to form a new but smaller graph. Here, we design a non-parametric pooling operation, which can fully utilize both the node features and graph structure information.</p><p>The key of our proposed graph pooling operation is to define a criterion that guides the node selection procedure. To perform node sampling, we first introduce a criterion named node information score to evaluate the information that each node contains given its neighborhood. Generally, if a node's representation can be reconstructed by its neighborhood representations, it means this node can probably be deleted in the pooled graph with almost no information loss. Here, we formally define the node information score as the Manhattan distance between the node representation itself and the one constructed from its neighbors:</p><formula xml:id="formula_2">p = γ(G i ) = (I k i − (D k i ) −1 A k i )H k i 1 ,<label>(2)</label></formula><p>where A k i ∈ R n k i ×n k i and H k i ∈ R n k i ×d are the adjacent and node representations matrices. · 1 performs 1 norm rowwisely. D k i represents the diagonal degree matrix of A k i , and I k i is the identity matrix. Therefore, we have p ∈ R ni encode the information score of each node in the graph.</p><p>After having obtained the node information score, we can now select nodes that should be preserved by the pooling operator. To approximate the graph information, we choose to preserve the nodes that can not be well represented by their neighbors, i.e., the nodes with relative larger node information score will be preserved in the construction of the pooled graph, because they can provide more information. In details, we first re-order the nodes in graph according to their node information scores, then a subset of top-ranked  <ref type="figure">Figure 1</ref>: Architecture of proposed HGP-SL operator combined with graph neural network. The dashed box demonstrates the workflow of HGP-SL, which involves graph pooling and structure learning. The learned edges are represented as dashed lines in the graph. This procedure (convolution and pooling operations) is repeated several times. Then, a readout function is applied to aggregate node representations to make a fixed size representation, which goes through MLP layers for graph classification.</p><p>nodes are selected as follows:</p><formula xml:id="formula_3">idx = top-rank(p, r * n k i ) H k+1 i = H k i (idx, :) (3) A k+1 i = A k i (idx, idx)</formula><p>, where r is the pooling ratio and top-rank(·) denotes the function that returns the indices of the top n k+1 i = r * n k i values. H k i (idx, :) and A k i (idx, idx) perform the row or (and) column extraction to form the node representation matrix and adjacent matrix for the induced subgraph. Thus, we havẽ</p><formula xml:id="formula_4">H k+1 i ∈ R n k+1 i ×d and A k+1 i ∈ R n k+1 i ×n k+1 i</formula><p>represent the node feature and graph structure information of next layer .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Learning Mechanism</head><p>In this subsection, we present how our proposed structure learning mechanism learns a refined graph structure in the pooled graph. As we have illustrated in <ref type="figure">Figure 1</ref>, the pooling operation might result in highly related nodes being disconnected in the induced subgraph, which loses the completeness of the graph structure information and further hinders the message passing procedure. Meanwhile, the graph structure obtained from domain knowledge (e.g., social network) or established by human (e.g., KNN graph) are usually nonoptimal for the learning task in graph neural networks, due to the lost or noisy information. To overcome this problem,  proposed to adaptively estimate graph Laplacian using an approximate distance metric learning algorithm, which might lead to local optimal solution. <ref type="bibr" target="#b9">(Jiang et al. 2019)</ref> introduced to learn the constructed graph structure for node label estimation, however it generates dense connected graph and is not applicable in our hierarchical graph level representation learning scenario.</p><p>Here, we develop a novel structure learning layer, which learns sparse graph structure through sparse attention mechanism <ref type="bibr" target="#b13">(Martins and Astudillo 2016)</ref>. For graph G i 's pooled subgraph G k i at its k-th layer, we take its structure information A k i ∈ R n k i ×n k i and hidden representations H k i ∈ R n k i ×d as input. Our target is to learn a refined graph structure that encodes the underlying pairwise relationship between each pair of nodes. Formally, we utilize a single layer neural network parameterized by a weight vector → a ∈ R 1×2d . Then, the similarity score between node v p and v q calculated by the attention mechanism can be expressed as:</p><formula xml:id="formula_5">E k i (p, q) = σ( → a [H k i (p, :)||H k i (q, :)] ) + λ · A k i (p, q), (4) where σ(·)</formula><p>is the activation function like ReLU(·) and || represents the concatenation operation. H k i (p, :) ∈ R 1×d and H k i (q, :) ∈ R 1×d indicate the p-th and q-th row of matrix H k i , which denote the representations of node v p and v q , respectively. Specifically, A k i encodes the induced subgraph structure information, where A k i (p, q) = 0 if node v p and v q are not directly connected. We incorporate A k i into our structure learning layer to bias the attention mechanism to give a relatively larger similarity score between directly connected nodes, and at the same time try to learn the underlying pairwise relationships between disconnected nodes. λ is a trade-off parameter between them.</p><p>To make the similarity score easily comparable across different nodes, we could normalize them across nodes using the softmax function:</p><formula xml:id="formula_6">S k i (p, q) = exp(E k i (p, q)) n k i m=1 exp(E k i (p, m)) .<label>(5)</label></formula><p>However, the softmax transformation always has non-zero values and thus results in dense fully connected graph, which may introduce lots of noise into the learned structure. Hence, we propose to utilize sparsemax function (Martins and Astudillo 2016), which retains most the important properties of softmax function and has in addition the ability of producing sparse distributions. The sparsemax(·) function aims to return the Euclidean projection of input onto the probability simplex and can be formulated as follows: <ref type="formula">(6)</ref> where [x] + = max{0, x}, and τ (·) is the threshold function that returns a threshold according to the procedure shown in Algorithm 1. Thus, sparsemax(·) preserves the values above the threshold and the other values will be truncated to zeros, which brings sparse graph structure. Similarly to softmax function, sparsemax(·) also has the properties of non-negative and sum-to-one, that's to say, S k i (p, q) ≥ 0 and n k i q=1 S k i (p, q) = 1. The proof procedure is available in the supplemental material.</p><formula xml:id="formula_7">S k i (p, q) = sparsemax(E k i (p, q)) sparsemax(E k i (p, q)) = [E k i (p, q) − τ (E k i (p, :))] + ,</formula><p>Algorithm 1 The calculation procedure of function τ (·) Require: input vector z ∈ R n . 1: Sort z into u:</p><formula xml:id="formula_8">u 1 ≥ u 2 ≥ · · · ≥ u n . 2: Get ρ = max{1 ≤ j ≤ n : u j + 1 j (1 − j i=1 u i ) &gt; 0}. 3: Define τ (z) = 1 ρ ( ρ i=1 u i − 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Structure Learning Efficiency</head><p>For large scale graphs, it will be computation expensive to calculate the similarities between each pair of nodes during the learning of structure S k i . If we further take graph's localization and smoothness properties into account, it is reasonable to constrain the calculation process within the node's h-hop neighborhood (h = 2 or 3). Therefore, the computation cost of S k i can be greatly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN and Graph Pooling Revisiting</head><p>After having obtained the refined graph structure S k i , we conduct graph convolution and pooling operations in the following layers based onH k i and S k i (instead of A k i ). Thus, Equation <ref type="formula" target="#formula_1">(1)</ref> can be simplified as follows:</p><formula xml:id="formula_9">H k i = σ(S k iH k i W k ).<label>(7)</label></formula><p>Since the learned S k i satisfies n k i q=1 S k i (p, q) = 1, therefore we have the diagonal matrix D k i = Diag(d 1 , d 2 , · · · , d n k i ) with d p = n k i q S k i (p, q), which degenerates to identity matrix I k i . Similarly, the calculation of node information score in Equation <ref type="formula" target="#formula_2">(2)</ref> can also be simplified as below:</p><formula xml:id="formula_10">p = γ(G i ) = (I k i − S k i )H k i 1 ,<label>(8)</label></formula><p>which makes our model very easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Readout Function and Output Layer</head><p>As we have demonstrated in <ref type="figure">Figure 1</ref>, the neural network architecture repeats the graph convolution and pooling operations for several times, thus we would observe multiple subgraphs with different size in each level:</p><formula xml:id="formula_11">H 1 i , H 2 i , · · · , H K i .</formula><p>To generate a fixed size graph level representation, we devise a readout function that aggregates all the node representations in the subgraph. Here, we simply use the concatenation of mean-pooling and max-pooling in each subgraph as follows:</p><formula xml:id="formula_12">r k i = R(H k i ) = σ( 1 n k i n k i p=1 H k i (p, :)|| d max q=1 H k i (:, q)),<label>(9)</label></formula><p>where σ(·) is a nonlinear activation function and r k i ∈ R 2d . We then add 2 the readout outputs of different levels to form 2 In our experiment, we use fixed size node representation across all layers, i.e., d k = · · · = d1 = d = 128. </p><formula xml:id="formula_13">z i = r 1 i + r 2 i + · · · + r K i ,<label>(10)</label></formula><p>which summarizes different levels' graph representations. Finally, we feed the graph level representation into MLP layer with softmax classifier, and the loss function is defined as the cross-entropy of predictions over the labels:</p><formula xml:id="formula_14">Y = softmax(MLP(Z)) L = − i∈L c j=1 Y ij logŶ ij ,<label>(11)</label></formula><p>whereŶ ij represents the predicted probability that graph G i belongs to class j, and Y ij is the ground truth. L denotes the training set of graphs that have labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Analysis Datasets</head><p>We adopt six commonly used public benchmarks 3 for empirical studies. Statistics of the six datasets are summarized in <ref type="table">Table 1</ref>   <ref type="table">Table 2</ref>: Graph classification in terms of accuracy with standard deviation (in percentage). We use bold to highlight wins.</p><p>Graph Neural Networks. Approaches in this group include representative graph neural networks: GCN (Kipf and Welling 2017), GraphSAGE (Hamilton, Ying, and Leskovec 2017) and <ref type="bibr">GAT (Veličković et al. 2018)</ref>, which are designed to learn meaningful node level representations. Therefore, we employ our proposed readout function to summarize the node representations for graph classification.</p><p>Graph Pooling Models. In this group, we further consider numerous models that combine GNNs with pooling operator for graph level representation learning. Set2Set <ref type="bibr" target="#b17">(Vinyals, Bengio, and Kudlur 2015)</ref> and DGCNN  are two novel global graph pooling algorithms. Another five hierarchical graph pooling models including DiffPool <ref type="bibr" target="#b17">(Ying et al. 2018b</ref>), gPool , SAGPool <ref type="bibr" target="#b12">(Lee, Lee, and Kang 2019)</ref>, EdgePool <ref type="bibr" target="#b4">(Diehl 2019)</ref> and <ref type="bibr">EigenPool (Ma et al. 2019)</ref> are also compared as baselines.</p><p>HGP-SL Variants. To further analyze the effectiveness of our proposed HGP-SL operator, we consider four variants here: HGP-SL NSL (No Structure Learning) which discards the structure learning layer to verify the effectiveness of our proposed structure learning module, HGP-SL HOP which removes the structure learning layer and connects the nodes within its h-hops, HGP-SL DEN (DENse) which employs the structure learning layer to learn a dense graph structure with softmax function defined in Equation <ref type="formula" target="#formula_6">(5)</ref> and HGP-SL which utilizes sparsemax function define in Equation <ref type="formula">(6)</ref> to learn a sparse graph structure. Both HGP-SL DEN and HGP-SL use efficiency improved structure learning strategy.</p><p>Experiment and Parameter Settings. Following many previous work <ref type="bibr" target="#b17">(Ying et al. 2018b;</ref><ref type="bibr" target="#b12">Ma et al. 2019)</ref>, we randomly split each dataset into three parts: 80% as training set, 10% as validation set and the remaining 10% as test set. We repeat this randomly splitting process 10 times, and the average performance with standard derivation is reported. For baseline algorithms, we use the source code released by the authors, and their hyper-parameters are tuned to be optimal based on the validation set. In order to ensure a fair comparison, the same neural network architectures are used for the existing pooling baselines and our proposed model. The dimension of node representations is set as 128 for all methods and datasets. We implement our proposed HGP-SL with PyTorch, and the Adam optimizer is utilized to optimize the model. The learning rate and weight decays are searched in {0.1, 0.01, 0.001, 1e −4 , 1e −5 }, pooling ratio r ∈ [0.1, 0.9] and layers K ∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>. The MLP consists of three fully connected layers with number of neurons in each layer setting as 256, 128, 64, followed by a softmax classifier. Early stopping criterion is employed in the training process, i.e., we stop training if the validation loss dose not decrease for 100 consecutive epochs. The source code is publicly available 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on Graph Classification</head><p>The classification performance is reported in <ref type="table">Table 2</ref>. To summarize, we have the following observations:</p><p>• First of all, a general observation we can draw from the results is that our proposed HGP-SL consistently outperforms other state-of-the-art baselines among all datasets. For instance, our method achieves about 3.08% improvement over the best baseline in PROTEINS dataset, which is 12.97% improvement over GCN with no hierarchical pooling mechanism. This verifies the necessity of adding graph pooling module. • It is worth noting that the traditional graph kernel based methods demonstrate competitive performance. However, the carefully designed graph kernels typically involve massive human domain knowledge, which has difficulty in generalizing to graphs with arbitrary structures. Furthermore, the two-stage procedure of extracting graph features and performing graph classification might result in sub-optimal performance.</p><p>• Being consistent with previous work's findings <ref type="bibr" target="#b12">(Ma et al. 2019)</ref>, we also observe that the GNNs group can not achieve satisfied results. We argue that the major reason is because they ignore the graph structure information when globally summarizing the node representations, which further verifies the necessity of adding graph pooling module.</p><p>• In particular, the global pooling approaches Set2Set and DGCNN are surpassed by most of the hierarchical pooling methods with a few exceptions. This is because their learned graph representations are still "flat", and the hierarchical structure information or functional units in the graph are ignored, which play an important role in predicting the entire graph labels.</p><p>• We note that the hierarchical pooling models can achieve relative better performance among most baselines, which further shows the effectiveness of the hierarchical pooling mechanism. Among them, gPool and SAGPool perform poorly in ENZYMES dataset. This may be due to the limited training samples per class resulting in the neural network overfitting. EdgePool gains superior performance in this group of competitors, which scales down the size of graphs by contracting each pair of nodes in the graph. Obviously, our proposed HGP-SL outperforms EdgePool with different gains for all settings.</p><p>• Finally, HGP-SL and HGP-SL DEN obtain better performance than HGP-SL NSL and HGP-SL HOP , which justifies the effectiveness of our proposed structure learning layer. Moreover, HGP-SL HOP performs worse than HGP-SL. This is because the disconnected nodes are still unreachable in its h-hops. HGP-SL further outperforms HGP-SL DEN , which indicates the learned dense graph structure might introduce additional noisy information and degenerate the performance. Furthermore, in the realworld scenario, graphs usually have sparse topologies, thus our proposed HGP-SL could learn more reasonable graph structures compared with HGP-SL DEN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study and Visualization</head><p>HGP-SL Convolutional Neural Network Architectures.</p><p>As mentioned in previous sections, our proposed HGP-SL can be integrated into various graph neural network architectures. We consider three most widely used graph convolutional architectures as our model's building block to investigate the affect of different convolution operations: GCN (Kipf and Welling 2017), GraphSAGE (Hamilton, Ying, and Leskovec 2017) and <ref type="bibr">GAT (Veličković et al. 2018)</ref>. We evaluate them on three datasets, which cover both small and large datasets. Their results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Similar results can also be found in the remaining datasets, thus we omit them due to the limited space. As demonstrated in <ref type="table" target="#tab_3">Table 3</ref>, the performance on graph classification varies depending on which dataset and the type of GNN in HGP-SL are chosen.</p><p>In addition, we also combine the top-K selection procedure proposed in gPool and SAGPool with our proposed structure learning. We name them as gPool-SL and SAGPool-SL   for short. From the results, we observe that gPool-SL and SAGPool-SL outperform gPool and SAGPool by incorporating the structure learning mechanism, which verifies the effectiveness of our proposed structure learning.</p><p>Hyper-parameter Analysis. We further study the sensitivities of several key hyper-parameters by varying them in different scales. Specifically, we investigate how the number of neural network layers K, graph representation dimension d and pooling ratio r will affect the graph classification performance. As we can see in <ref type="figure" target="#fig_1">Figure 2</ref>, HGP-SL almost achieves the best performance across different datasets when setting K = 3, d = 128 and r = 0.8, respectively. The pooling ratio r cannot be too small, otherwise most of the graph structure information will be lost during the pooling process.</p><p>Visualization. We utilize networkx 5 to visualize the pooling results of HGP-SL and its variants. In detail, we randomly sample a graph from PROTEINS dataset, which contains 154 nodes. We build a three layer graph neural network with pooling ratio setting as 0.5, which then generates three pooled graphs with nodes as 77, 39 and 20 respectively. We plot the 3rd pooled graph in <ref type="figure" target="#fig_2">Figure 3</ref>. It shows HGP-SL NSL and HGP-SL DEN fail to preserve meaningful graph topologies, while HGP-SL is able to preserve relatively reasonable topology of the original protein graph after pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we investigate graph level representation learning for the task of graph classification. We propose a novel graph pooling operator HGP-SL, which empowers GNNs to learn hierarchical graph representations. It can also be conveniently integrated into various GNN architectures. Specifically, the graph pooling operation is a non-parametric step, which utilizes node features and graph structure information to perform down-sampling on graphs. Then, a structure learning layer is stacked on the pooling operation, which aims to learn a refined graph structure that can best preserve the essential topological information. We combine the proposed HGP-SL operator with graph convolutional neural networks to conduct graph classification task. Comprehensive experiments on six widely used benchmarks demonstrate its superiority to a range of state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Hyper-parameter sensitivity analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>HGP-SLNSL Pool3 (e) HGP-SLDEN Pool3 (f) HGP-SL Pool3 Visualization of different pooling methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>± 5.63 72.23 ± 4.49 72.54 ± 3.83 62.48 ± 2.11 60.96 ± 2.37 56.65 ± 1.74 SP 42.66 ± 5.38 75.71 ± 2.73 78.72 ± 3.89 67.44 ± 2.76 67.72 ± 2.28 71.63 ± 2.19 WL 51.16 ± 6.19 76.16 ± 3.99 76.44 ± 2.35 76.65 ± 1.99 76.19 ± 2.45 80.32 ± 1.71 GNNs GCN 43.66 ± 3.39 75.17 ± 3.63 73.26 ± 4.46 76.29 ± 1.79 75.91 ± 1.84 79.81 ± 1.58 GraphSAGE 37.99 ± 3.71 74.01 ± 4.27 75.78 ± 3.91 74.73 ± 1.34 74.17 ± 2.89 78.75 ± 1.18 GAT 39.83 ± 3.68 74.72 ± 4.01 77.30 ± 3.68 74.90 ± 1.72 75.81 ± 2.68 78.89 ± 2.05 Pooling Set2Set 33.16 ± 3.21 79.33 ± 0.84 70.83 ± 0.84 69.62 ± 1.32 73.66 ± 1.69 80.84 ± 0.67 DGCNN 32.16 ± 3.87 79.99 ± 0.44 70.06 ± 1.21 74.08 ± 2.19 78.23 ± 1.31 80.41 ± 1.02 DiffPool 60.61 ± 3.94 79.90 ± 2.95 78.61 ± 1.32 77.73 ± 0.83 77.13 ± 1.49 80.78 ± 1.12 EigenPool 63.97 ± 2.51 78.84 ± 1.06 78.63 ± 1.36 77.24 ± 0.96 75.99 ± 1.42 80.11 ± 0.73 gPool 43.33 ± 2.88 80.71 ± 1.75 77.02 ± 1.32 76.25 ± 1.39 76.61 ± 1.39 80.30 ± 1.54 SAGPool 43.99 ± 4.23 81.72 ± 2.19 78.70 ± 2.29 77.88 ± 1.59 75.74 ± 1.47 79.72 ± 0.79 EdgePool 65.33 ± 4.36 82.38 ± 0.82 79.20 ± 2.61 76.56 ± 1.01 79.02 ± 1.89 81.41 ± 0.88 Proposed HGP-SLNSL 60.18 ± 2.43 81.51 ± 1.69 77.24 ± 1.09 76.33 ± 1.43 76.32 ± 1.22 79.42 ± 0.58 HGP-SLHOP 62.16 ± 2.11 83.03 ± 1.74 78.42 ± 1.37 77.72 ± 1.54 78.78 ± 1.09 79.88 ± 1.09 HGP-SLDEN 63.51 ± 2.64 83.12 ± 0.84 78.11 ± 1.35 77.42 ± 1.23 78.76 ± 0.61 81.07 ± 1.02 HGP-SL 68.79 ± 2.11 84.91 ± 1.62 80.96 ± 1.26 78.45 ± 0.77 80.67 ± 1.16 82.15 ± 0.58</figDesc><table><row><cell>Categories Baselines</cell><cell>ENZYMES</cell><cell>PROTEINS</cell><cell>D&amp;D</cell><cell>NCI1</cell><cell>NCI109</cell><cell>Mutagenicity</cell></row><row><cell cols="2">GRAPHLET 29.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kernels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">with more descriptions as follows: ENZYMES</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(Borgwardt et al. 2005) is a dataset of protein tertiary struc-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">tures, and each enzyme belongs to one of the 6 EC top-level</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">classes. PROTEINS and D&amp;D (Dobson and Doig 2003) are</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">two protein graph datasets, where nodes represent the amino</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">acids and two nodes are connected by an edge if they are less</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">than 6 Angstroms apart. The label indicates whether or not a</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">protein is a non-enzyme. NCI1 and NCI109 (Shervashidze</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">et al. 2011) are two biological datasets screened for activity</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">against non-small cell lung cancer and ovarian cancer cell</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">lines, where each graph is a chemical compound with nodes</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">and edges representing atoms and chemical bonds, respec-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">tively. Mutagenicity (Kazius, McGuire, and Bursi 2005) is</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">a chemical compound dataset of drugs, which can be cate-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">gorized into two classes: mutagen and non-mutagen.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Baselines</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Graph Kernel Methods. This group of methods perform</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">graph classification by utilizing carefully designed kernels.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">We choose three classical algorithms: GRAPHLET (Sher-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">vashidze et al. 2009), Shortest-Path Kernel (SP) (Borgwardt</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">and Kriegel 2005) and Weisfeiler-Lehman Kernel (WL)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(Shervashidze et al. 2011) as baselines.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>04±1.01 79.82±1.06 82.02±0.81 HGP-SLSAGE 84.99±0.82 80.11±0.96 81.96±0.97 gPool-SL 81.25±1.27 77.71±1.22 80.42±1.08 SAGPool-SL 82.67±1.42 78.01±1.50 80.00±1.22</figDesc><table><row><cell>Architectures</cell><cell>PROTEINS</cell><cell>NCI109</cell><cell>Mutagenicity</cell></row><row><cell>HGP-SLGCN</cell><cell cols="2">84.91±1.62 80.67±1.16</cell><cell>82.15±0.58</cell></row><row><cell>HGP-SLGAT</cell><cell>85.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>HGP-SL performance with different architectures.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the pooling process itself is non-parametric, however the structure learning mechanism indeed has an attention parameter. Thus, the overall HGP-SL operator is not non-parametric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Benchmarks are publicly available at https://ls11-www.cs.tudortmund.de/staff/morris/graphkerneldatasets</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code is available at https://github.com/cszhangzhen/HGP-SL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://networkx.github.io/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Proof for Algorithm 1</head><p>To summarize, sparsemax(·) considers the Euclidean projection of the input vector z onto the probability simplex, which can be defined as the following optimization problem:</p><p>Then, the Lagrangian of the optimization problem in Equation (12) is:</p><p>The optimal (p * , α * , β * ) must satisfy the following Karush-Kuhn-Tucker conditions:</p><p>If for ∀i ∈ {1, · · · , n} we have p * i &gt; 0, then from Equation <ref type="formula">(16)</ref> we must satisfy α * i = 0. Thus, from Equation <ref type="formula">(14)</ref> we can get p * i = z i −β * . Let S(z) = {j ∈ {1, · · · , n}|p * j &gt; 0}. From Equation <ref type="formula">(15)</ref> we obtain j∈S(z) (z j −β * ) = 1, which yields the Line 3 in Algorithm 1, i.e., β * = τ (z). Again from Equation <ref type="formula">(16)</ref>, we have that α * i &gt; 0 implies p * i = 0, which from Equation <ref type="formula">(14)</ref> implies α * i = β * − z i ≥ 0, i.e., z i ≤ β * for i / ∈ S(z). Thus, we have the procedure in Algorithm 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
	<note>ICDM. IEEE. Borgwardt et al. 2005. Protein function prediction via graph kernels</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
	</analytic>
	<monogr>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signed graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Tang ; Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kulis ; Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from nonenzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10990</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Edge contraction pooling for graph neural networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
	<note>and Ji</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning graph pooling and hybrid convolutional operations for text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">;</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2743" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00832</idno>
	</analytic>
	<monogr>
		<title level="m">ipool-information-based pooling in hierarchical graph neural networks</title>
		<meeting><address><addrLine>Hamilton, Ying, and Leskovec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Derivation and validation of toxicophores for mutagenicity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcguire</forename><surname>Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bursi ; Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bursi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Kang ; Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
		<title level="m">Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim ; Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05859</idno>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AIS-TATS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shervashidze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
	<note>Veličković et al. 2018. Graph attention networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<idno>Xu et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Representation learning on graphs with jumping knowledge networks. ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anrl: Attributed network representation learning via deep neural networks</title>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3155" to="3161" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
