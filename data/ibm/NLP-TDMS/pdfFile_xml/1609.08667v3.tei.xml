<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
							<email>kevclark@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Mention-Ranking Coreference Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a rewardrescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-ofthe-art on the English and Chinese portions of the CoNLL 2012 Shared Task.</p><p>1 Code and trained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution systems typically operate by making sequences of local decisions (e.g., adding a coreference link between two mentions). However, most measures of coreference resolution performance do not decompose over local decisions, which means the utility of a particular decision is not known until all other decisions have been made.</p><p>Due to this difficulty, coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision. These losses contain hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics. This complicates training, especially across different languages and datasets where systems may work best with different settings of the hyperparameters.</p><p>To address this, we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics. In particular, we modify the max-margin coreference objective proposed by <ref type="bibr" target="#b18">Wiseman et al. (2015)</ref> by incorporating the reward associated with each coreference decision into the loss's slack rescaling. We also test the REINFORCE policy gradient algorithm <ref type="bibr" target="#b17">(Williams, 1992)</ref>.</p><p>Our model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expressive than entity-centric approaches to coreference (e.g., Haghighi and Klein, 2010), mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years <ref type="bibr" target="#b18">Wiseman et al., 2015)</ref>. Having independent actions is particularly useful when applying reinforcement learning because it means a particular action's effect on the final reward can be computed efficiently.</p><p>We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task. The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both 1 . We attribute this to reward rescaling being well suited for a ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreference metrics. Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuristic loss, but the mistakes tend to be less severe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Mention-Ranking Model</head><p>We use the neural mention-ranking model described in <ref type="bibr" target="#b3">Clark and Manning (2016)</ref>, which we briefly go over in this section. Given a mention m and candidate antecedent c, the mention-ranking model produces a score for the pair s(c, m) indicating their compatibility for coreference with a feedforward neural network. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent.</p><p>Input Layer. For each mention, the model extracts various words (e.g., the mention's head word) and groups of words (e.g., all words in the mention's sentence) that are fed into the neural network. Each word is represented by a vector w i ∈ R dw . Each group of words is represented by the average of the vectors of each word in the group. In addition to the embeddings, a small number of additional features are used, including distance, string matching, and speaker identification features. See <ref type="bibr" target="#b3">Clark and Manning (2016)</ref> for the full set of features and an ablation study.</p><p>These features are concatenated to produce an Idimensional vector h 0 , the input to the neural network. If c = NA, features defined over pairs of mentions are not included. For this case, we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores.</p><p>Hidden Layers. The input gets passed through three hidden layers of rectified linear (ReLU) units <ref type="bibr">(Nair and Hinton, 2010)</ref>. Each unit in a hidden layer is fully connected to the previous layer:</p><formula xml:id="formula_0">h i (c, m) = max(0, W i h i−1 (c, m) + b i ) where W 1 is a M 1 × I weight matrix, W 2 is a M 2 × M 1 matrix, and W 3 is a M 3 × M 2 matrix.</formula><p>Scoring Layer. The final layer is a fully connected layer of size 1:</p><formula xml:id="formula_1">s(c, m) = W 4 h 3 (c, m) + b 4</formula><p>where W 4 is a 1 × M 3 weight matrix. At test time, the mention-ranking model links each mention with its highest scoring candidate antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Algorithms</head><p>Mention-ranking models are typically trained with heuristic loss functions that are tuned via hyperparameters. These hyperparameters are usually given as costs for different error types, which are used to bias the coreference system towards making more or fewer coreference links.</p><p>In this section we first describe a heuristic loss function incorporating this idea from <ref type="bibr" target="#b18">Wiseman et al. (2015)</ref>. We then propose new training procedures based on reinforcement learning that instead directly optimize for coreference evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Heuristic Max-Margin Objective</head><p>The heuristic loss from Wiseman et al. is governed by the following error types, which were first proposed by .</p><p>Suppose the training set consists of N mentions m 1 , m 2 , ..., m N . Let C(m i ) denote the set of candidate antecedents of a mention m i (i.e., mentions preceding m i and NA) and T (m i ) denote the set of true antecedents of m i (i.e., mentions preceding m i that are coreferent with it or {NA} if m i has no antecedent). Then we define the following costs for linking m i to a candidate antecedent c ∈ C(m i ):</p><formula xml:id="formula_2">∆ h (c, m i ) =            α FN if c = NA ∧ T (m i ) = {NA} α FA if c = NA ∧ T (m i ) = {NA} α WL if c = NA ∧ c / ∈ T (m i ) 0 if c ∈ T (m i )</formula><p>for "false new," "false anaphor," "wrong link", and correct coreference decisions. The heuristic loss is a slack-rescaled max-margin objective parameterized by these error costs. Lett i be the highest scoring true antecedent of m i :</p><formula xml:id="formula_3">t i = argmax c∈C(m i )∧∆ h (c,m i )=0 s(c, m i )</formula><p>Then the heuristic loss is given as</p><formula xml:id="formula_4">L(θ) = N i=1 max c∈C(m i ) ∆ h (c, m i )(1 + s(c, m i ) − s(t i , m i ))</formula><p>Finding Effective Error Penalties. We fix α WL = 1.0 and search for α FA and α FN out of {0.1, 0.2, ..., 1.5} with a variant of grid search. Each new trial uses the unexplored set of hyperparame-ters that has the closest Manhattan distance to the best setting found so far on the dev set. The search is halted when all immediate neighbors (within 0.1 distance) of the best setting have been explored. We found (α FN , α FA , α WL ) = (0.8, 0.4, 1.0) to be best for English and (α FN , α FA , α WL ) = (0.8, 0.5, 1.0) to be best for Chinese on the CoNLL 2012 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning</head><p>Finding the best hyperparameter settings for the heuristic loss requires training many variants of the model, and at best results in an objective that is correlated with coreference evaluation metrics. To address this, we pose mention ranking in the reinforcement learning framework <ref type="bibr" target="#b16">(Sutton and Barto, 1998)</ref> and propose methods that directly optimize the model for coreference metrics.</p><p>We can view the mention-ranking model as an agent taking a series of actions a 1:T = a 1 , a 2 , ..., a T , where T is the number of mentions in the current document. Each action a i links the ith mention in the document m i to a candidate antecedent. Formally, we denote the set of actions available for the ith mention as A i = {(c, m i ) : c ∈ C(m i )}, where an action (c, m) adds a coreference link between mentions c and m. The mentionranking model assigns each action the score s(c, m) and takes the highest-scoring action at each step.</p><p>Once the agent has executed a sequence of actions, it observes a reward R(a 1:T ), which can be any function. We use the B 3 coreference metric for this reward (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC <ref type="bibr">(Vilain et al., 1995)</ref> and CEAF φ 4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAF φ 4 is slow to compute.</p><p>Reward Rescaling. Crucially, the actions taken by a mention-ranking model are independent. This means it is possible to change any action a i to a different one a i ∈ A i and see what reward the model would have gotten by taking that action instead: R(a 1 , ..., a i−1 , a i , a i+1 , ..., a T ). We use this idea to improve the slack-rescaling parameter ∆ in the maxmargin loss L(θ). Instead of setting its value based on the error type, we compute exactly how much each action hurts the final reward:</p><formula xml:id="formula_5">∆ r (c, m i ) = −R(a 1 , ..., (c, m i ), ..., a T ) + max a i ∈A i R(a 1 , ..., a i , ..., a T )</formula><p>where a 1:T is the highest scoring sequence of actions according to the model's current parameters. Otherwise the model is trained in the same way as with the heuristic loss.</p><p>The REINFORCE Algorithm. We also explore using the REINFORCE policy gradient algorithm <ref type="bibr" target="#b17">(Williams, 1992)</ref>. We can define a probability distribution over actions using the mention-ranking model's scoring function as follows:</p><formula xml:id="formula_6">p θ (a) ∝ e s(c,m)</formula><p>for any action a = (c, m). The REINFORCE algorithm seeks to maximize the expected reward</p><formula xml:id="formula_7">J(θ) = E [a 1:T ∼p θ ] R(a 1:T )</formula><p>It does this through gradient ascent. Computing the full gradient is prohibitive because of the expectation over all possible action sequences, which is exponential in the length of the sequence. Instead, it gets an unbiased estimate of the gradient by sampling a sequence of actions a 1:T according to p θ and computing the gradient only over the sample.</p><p>We take advantage of the independence of actions by using the following gradient estimate, which has lower variance than the standard REINFORCE gradient estimate:</p><formula xml:id="formula_8">∇ θ J(θ) ≈ T i=1 a i ∈A i [∇ θ p θ (a i )](R(a 1 , ..., a i , ..., a T ) − b i )</formula><p>where b i is a baseline used to reduce the variance, which we set to E a i ∈A i ∼p θ R(a 1 , ..., a i , ..., a T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data <ref type="bibr" target="#b14">(Pradhan et al., 2012)</ref> and evaluate with the MUC, B 3 , and CEAF φ 4 metrics. Our experiments were run using predicted mentions from Stanford's rule-based coreference system <ref type="bibr">(Raghunathan et al., 2010)</ref>.</p><p>We follow the training methodology from Clark and Manning <ref type="formula">(2016)</ref>  <ref type="bibr" target="#b9">(Hinton and Tieleman, 2012)</ref>, dropout <ref type="bibr" target="#b9">(Hinton et al., 2012)</ref> with a rate of 0.5, and pretraining with the all pairs classification and top pairs classification tasks. However, we improve on the previous system by using using better mention detection, more effective hyperparameters, and more epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>We compare the heuristic loss, REINFORCE, and reward rescaling approaches on both datasets. We find that REINFORCE does slightly better than the heuristic loss, but reward rescaling performs significantly better than both on both languages. We attribute the modest improvement of REIN-FORCE to it being poorly suited for a ranking task. During training it optimizes the model's performance in expectation, but at test-time it takes the most probable sequence of actions. This mismatch occurs even at the level of an individual decision: the model only links the current mention to a single antecedent, but is trained to assign high probability to all correct antecedents. We believe the benefit of REINFORCE being guided by coreference evaluation metrics is offset by this disadvantage, which does not occur in the max-margin approaches. The reward-rescaled max-margin loss combines the best of both worlds, resulting in superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Benefits of Reinforcement Learning</head><p>In this section we examine the reward-based cost function ∆ r and perform error analysis to determine how reward rescaling improves the mention-ranking model's accuracy.</p><p>Comparison with Heuristic Costs. We compare the reward-based cost function ∆ r with the error types used in the heuristic loss. For English, the average value of ∆ r is 0.79 for FN errors and 0.38 for FA errors when the costs are scaled so the average value of a WL error is 1.0. These are very close to the hyperparameter values (α FN , α FA , α WL ) = (0.8, 0.4, 1.0) found by grid search. However, there is a high variance in costs for each error type, suggesting that using a fixed penalty for each type as in the heuristic loss is insufficient (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Avoiding Costly Mistakes. Embedding the costs of actions into the loss function causes the rewardrescaling model to prioritize getting the more important coreference decisions (i.e., the ones with the biggest impact on the final score) correct. As a   result, it makes fewer costly mistakes at test time. Costly mistakes often involve large clusters of mentions: incorrectly combining two coreference clusters of size ten is much worse than incorrectly combining two clusters of size one. However, the cost of an action also depends on other factors such as the number of errors already present in the clusters and the utilities of the other available actions. <ref type="table" target="#tab_3">Table 2</ref> shows the breakdown of errors made by the heuristic and reward-rescaling models on the test set. The reward-rescaling model makes slightly more errors, meaning its improvement in performance must come from its errors being less severe.</p><p>Example Improvements. <ref type="table" target="#tab_2">Table 3</ref> shows two classes of mentions where the reward-rescaling loss particularly improves over the heuristic loss.</p><p>Proper nouns have a higher average cost for "false new" errors (0.90) than other mentions types (0.77). This is perhaps because proper nouns are important for connecting clusters of mentions far apart in a document, so incorrectly linking a proper noun to NA could result in a large decrease in recall. Because it more heavily weights these high-cost errors during training, the reward-rescaling model makes fewer "false new" errors for proper nouns than the heuristic loss. Although there is an increase in other kinds of errors as a result, most of these are low-cost "false anaphoric" errors.</p><p>The pronouns in the "telephone conversation" genre often group into extremely large coreference clusters, which means a "wrong link" error can have a large negative effect on the score. This is reflected in its high average cost of 1.21. After prioritizing these examples during training, the reward-rescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Mention-ranking models have been widely used for coreference resolution <ref type="bibr" target="#b5">(Denis and Baldridge, 2007;</ref><ref type="bibr" target="#b15">Rahman and Ng, 2009;</ref>. These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 <ref type="bibr" target="#b8">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b1">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b18">Wiseman et al., 2015;</ref><ref type="bibr" target="#b12">Martschat and Strube, 2015;</ref><ref type="bibr" target="#b19">Wiseman et al., 2016)</ref>.</p><p>To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN <ref type="bibr" target="#b4">(Daumé III et al., 2009</ref>) have been used to train coreference resolvers <ref type="bibr" target="#b5">(Daumé III, 2006;</ref><ref type="bibr" target="#b11">Ma et al., 2014;</ref><ref type="bibr" target="#b2">Clark and Manning, 2015)</ref>. These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperparameters that must be carefully selected for each particular language, dataset, and evaluation metric. Our reward-rescaling approach also increases the model's accuracy, resulting in significant gains over the current state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Density plot of the costs ∆r associated with different error types on the English CoNLL 2012 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>: hidden layers of sizes M 1 = 1000, M 2 = M 3 = 500, the RMSprop optimizer</figDesc><table><row><cell></cell><cell>MUC</cell><cell></cell><cell>B 3</cell><cell></cell><cell>CEAF φ4</cell><cell></cell></row><row><cell></cell><cell>Prec. Rec.</cell><cell>F 1</cell><cell>Prec. Rec.</cell><cell>F 1</cell><cell>Prec. Rec.</cell><cell>F 1</cell><cell>Avg. F 1</cell></row><row><cell></cell><cell cols="4">CoNLL 2012 English Test Data</cell><cell></cell><cell></cell></row><row><cell>Wiseman et al. (2016)</cell><cell cols="2">77.49 69.75 73.42</cell><cell cols="2">66.83 56.95 61.50</cell><cell cols="2">62.14 53.85 57.70</cell><cell>64.21</cell></row><row><cell>Clark &amp; Manning (2016)</cell><cell cols="2">79.91 69.30 74.23</cell><cell cols="2">71.01 56.53 62.95</cell><cell cols="2">63.84 54.33 58.70</cell><cell>65.29</cell></row><row><cell>Heuristic Loss</cell><cell cols="2">79.63 70.25 74.65</cell><cell cols="2">69.21 57.87 63.03</cell><cell cols="2">63.62 53.97 58.40</cell><cell>65.36</cell></row><row><cell>REINFORCE</cell><cell cols="2">80.08 69.61 74.48</cell><cell cols="2">70.70 56.96 63.09</cell><cell cols="2">63.59 54.46 58.67</cell><cell>65.41</cell></row><row><cell>Reward Rescaling</cell><cell cols="2">79.19 70.44 74.56</cell><cell cols="2">69.93 57.99 63.40</cell><cell cols="2">63.46 55.52 59.23</cell><cell>65.73</cell></row><row><cell></cell><cell cols="4">CoNLL 2012 Chinese Test Data</cell><cell></cell><cell></cell></row><row><cell cols="3">Björkelund &amp; Kuhn (2014) 69.39 62.57 65.80</cell><cell cols="2">61.64 53.87 57.49</cell><cell cols="2">59.33 54.65 56.89</cell><cell>60.06</cell></row><row><cell>Clark &amp; Manning (2016)</cell><cell cols="2">73.85 65.42 69.38</cell><cell cols="2">67.53 56.41 61.47</cell><cell cols="2">62.84 57.62 60.12</cell><cell>63.66</cell></row><row><cell>Heuristic Loss</cell><cell cols="2">72.20 66.51 69.24</cell><cell cols="2">64.71 58.16 61.26</cell><cell cols="2">61.98 58.41 60.14</cell><cell>63.54</cell></row><row><cell>REINFORCE</cell><cell cols="2">74.05 65.38 69.44</cell><cell cols="2">67.52 56.43 61.48</cell><cell cols="2">62.38 57.77 59.98</cell><cell>63.64</cell></row><row><cell>Reward Rescaling</cell><cell cols="2">73.64 65.62 69.40</cell><cell cols="2">67.48 56.94 61.76</cell><cell cols="2">62.46 58.60 60.47</cell><cell>63.88</cell></row></table><note>Table 1: Comparison of the methods together with other state-of-the-art approaches on the test sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Examples of classes of mention on which the reward-rescaling loss improves upon the heuristic loss due to its rewardbased cost function. Reported numbers are from the English CoNLL 2012 test set.</figDesc><table><row><cell>Model</cell><cell>FN</cell><cell>FA</cell><cell>WL</cell></row><row><cell>Heuristic Loss</cell><cell>1719</cell><cell>1956</cell><cell>1258</cell></row><row><cell>Reward Rescaling</cell><cell>1725</cell><cell>1994</cell><cell>1247</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of "false new," "false anaphoric," and "wrong link" errors produced by the models on the English CoNLL 2012 test set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kelvin Guu, William Hamilton, Will Monroe, and the anonymous reviewers for their thoughtful comments and suggestions. This work was supported by NSF Award IIS-1514268.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
	<note>Amit Bagga and Breck Baldwin</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2015] Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entity-level distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2016] Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical structured learning techniques for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii2006] Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>A ranking approach to pronoun resolution</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein2013] Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decentralized entity-level modeling for coreference resolution</title>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</title>
		<editor>Aria Haghighi and Dan Klein</editor>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
	</analytic>
	<monogr>
		<title level="m">Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>COURSERA: Neural Networks for Machine Learning, 4</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prune-andscore: Learning for greedy coreference resolution</title>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Strube2015] Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</title>
		<editor>Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning</editor>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning -Shared Task</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
	<note>Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng2009] Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model-theoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on message understanding</title>
		<editor>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman</editor>
		<meeting>the 6th conference on message understanding</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wiseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
