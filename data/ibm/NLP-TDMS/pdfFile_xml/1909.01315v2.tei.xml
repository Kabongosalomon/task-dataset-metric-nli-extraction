<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP GRAPH LIBRARY: A GRAPH-CENTRIC, HIGHLY- PERFORMANT PACKAGE FOR GRAPH NEURAL NET- WORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NYU Shanghai</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Web</forename><surname>Services</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Shanghai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">DEEP GRAPH LIBRARY: A GRAPH-CENTRIC, HIGHLY- PERFORMANT PACKAGE FOR GRAPH NEURAL NET- WORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advancing research in the emerging field of deep graph learning requires new tools to support tensor computation over graphs. In this paper, we present the design principles and implementation of Deep Graph Library (DGL) 1 . DGL distills the computational patterns of GNNs into a few generalized sparse tensor operations suitable for extensive parallelization. By advocating graph as the central programming abstraction, DGL can perform optimizations transparently. By cautiously adopting a framework-neutral design, DGL allows users to easily port and leverage the existing components across multiple deep learning frameworks. Our evaluation shows that DGL significantly outperforms other popular GNN-oriented frameworks in both speed and memory consumption over a variety of benchmarks and has little overhead for small scale workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural network (GNN) generalizes traditional deep learning to capture structural information in the data by modeling a set of node entities together with their relationships (edges). Its application range is broad, including molecules, social networks, knowledge graphs and recommender systems <ref type="bibr" target="#b24">(Zitnik et al., 2018;</ref><ref type="bibr" target="#b14">Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b22">Hamilton et al., 2018;</ref><ref type="bibr" target="#b22">Ying et al., 2018)</ref>, or in general any datasets that have structural information. As a vibrant and young field, accelerating research on GNN calls for developing domain packages that are simultaneously flexible and powerful for researchers, and efficient and performant for real-world applications.</p><p>Meeting both requirements are challenging: there are significant semantic gaps between the tensorcentric perspective of today's popular deep-learning (DL) frameworks and that of a graph, and performance gaps between the computation/memory-access patterns induced by the sparse nature of graphs and the underlying parallel hardware that are highly optimized for dense tensor operations. This paper gives an overview of the design principles and implementation of Deep Graph Library <ref type="bibr">(DGL)</ref>, an open-source domain package specifically designed for researchers and application developers of GNN. Specifically, we make the following contributions:</p><p>• DGL distills the computational patterns of GNNs into a few user-configurable message-passing primitives; these primitives generalize sparse tensor operations and cover both the forward inference path and the backward gradient computing path. As such, they not only serve as the building blocks optimized for today's hardware, but also lay the foundation for future optimizations as well. In addition, DGL identifies and explores a wide range of parallelization strategies, leading to speed and memory efficiency.</p><p>• DGL makes graph the central programming abstraction. The graph abstraction allows DGL to simplify user programming by taking full control of the messy details of manipulating graph data.</p><p>• A full GNN application takes more than a GNN model; the other components (e.g. data preprocessing and feature extraction) are outside the scope of DGL. As such, DGL strives to be as framework neutral as possible. DGL runs on top of PyTorch <ref type="bibr" target="#b10">(Paszke et al., 2019)</ref>, Tensor-Flow <ref type="bibr" target="#b1">(Abadi et al., 2016)</ref>, MXNet <ref type="bibr" target="#b9">(Chen et al., 2015)</ref> and leverages their capability as much it can, while minimizing the effort it takes to port a model across frameworks. Many choices we made are applicable to other domain packages that share the same aspiration.</p><p>The rest of the paper is organized as follows. We first introduce the backgrounds about GNN message passing in Sec. 2. We formulate these computations as two computational patterns -g-SpMM and g-SDMM, and discuss the parallelization strategies in Sec. 3. Sec. 4 describes the design and implementation of the DGL framework. We discuss some related works in Sec. 5 and evaluate DGL in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GRAPH NEURAL NETWORKS AND MESSAGE PASSING</head><p>There have been a significant development in extending deep neural networks to non-euclidean data such as graphs and manifolds. Many efforts <ref type="bibr" target="#b12">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b8">Bruna et al., 2013;</ref><ref type="bibr">Defferrard et al., 2016;</ref><ref type="bibr">Hamilton et al., 2017;</ref><ref type="bibr" target="#b16">Veličković et al., 2018)</ref> are made to formulate appropriate model architectures for learning on graphs, which gave birth to the Graph Neural Networks (GNNs) family. Recent studies <ref type="bibr">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b5">Battaglia et al., 2018)</ref> manage to unify different GNN variants into the message passing paradigm. Let G(V, E) be a graph with nodes V and edges E; Let x v ∈ R d1 be the feature for node v, and w e ∈ R d2 be the feature for edge (u, e, v) 2 . The message passing paradigm defines the following node-wise and edge-wise computation at step t + 1:</p><formula xml:id="formula_0">Edge-wise: m (t+1) e = φ x (t) v , x (t) u , w (t) e , (u, e, v) ∈ E.<label>(1)</label></formula><p>Node-wise:</p><formula xml:id="formula_1">x (t+1) v = ψ x (t) v , ρ m (t+1) e : (u, e, v) ∈ E .<label>(2)</label></formula><p>In the above equations, φ is a message function defined on each edge to generate a message by combining the edge feature with the features of its incident nodes; ψ is an update function defined on each node to update the node feature by aggregating its incoming messages using the reduce function ρ. In GNNs, the message and update functions are parameterized by neural network modules, and ρ can be any set function such as sum, mean, max/min, or even an LSTM network <ref type="bibr">(Hamilton et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GNN MESSAGE PASSING AS GENERALIZED SPMM AND SDDMM.</head><p>There is a strong connection between the message passing paradigm to sparse matrix operations. For example, given the node feature matrix X ∈ R |V|×d and the adjacency matrix A of a graph, the node-wise computation in the graph convolutional network (GCN) (Kipf &amp; Welling, 2017) is a sparse-dense matrix multiplication (SpMM) Y = AX. For the edge-wise computation, many GNN models <ref type="bibr" target="#b16">(Veličković et al., 2018;</ref><ref type="bibr">Kepner et al., 2016)</ref> calculate an attention weight on each edge. One popular formulation of calculating attention weight is by a dot product between the source and destination node features <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>. This corresponds to a sampled dense-dense matrix multiplication (SDDMM) operation W = A (XX T ): semantically, it multiplies two dense matrices, followed by an element-wise multiplication with a sparse mask matrix, and output a sparse matrix.</p><p>An important characteristic of SDDMM is that it maps the representation of an edge's incident nodes to the representation on the edge. Similarly, SpMM aggregates the representation of a node's inbound edges into a node representation. Both of them can be extended. Given a graph G(V, E),</p><formula xml:id="formula_2">• A generalized SDDMM (g-SDDMM) defined on graph G with message function φ m is a function g-SDDMM G,φm : R |V|×d1 , R |V|×d2 , R |E|×d3 → R |E|×d4</formula><p>where the output edge representations M = g-SDDMM G,φm (X, Y, W) are computed from the edges' own features, as well as features of their incident nodes:</p><formula xml:id="formula_3">m e = φ m (x u , y v , w e ) , ∀(u, e, v) ∈ E. • A generalized SpMM (g-SpMM) defined on graph G with message function φ z and reduce function ρ is a function g-SpMM G,φz,ρ : R |V|×d1 , R |V|×d2 , R |E|×d3 → R |V|×d4</formula><p>where the output node representations Z = g-SpMM G,φz,ρ (X, Y, W) are computed from the nodes' inbound edge features, the node features themselves, and the neighbor features:</p><formula xml:id="formula_4">z v = ρ ({φ z (x u , y v , w e ) : (u, e, v) ∈ E}) , ∀v ∈ V.</formula><p>These two primitives play an essential role in GNN computations; the forward path essentially applies a series of g-SpMM (and g-SDMM if attention is involved, as in GAT) to derive a stack of node representations. One can prove that the gradient of the objective function w.r.t. g-SDDMM and g-SpMM inputs can be expressed as another g-SDDMM and g-SpMM (see the supplementary materials for the full proof): Theorem 1. Given M = g-SDDMM G,φm (X, Y, W) and Z = g-SpMM G,φz,ρ (X, Y, W) and an objective function L = (M, Z). Then</p><p>• The partial derivative ∂L ∂W can be computed by a g-SDDMM on graph G.</p><p>• The partial derivative ∂L ∂X can be computed by a g-SpMM on the reverse graphG(V,Ẽ),Ẽ = {(u, e, v) : (v, e, u) ∈ E}.</p><p>• The partial derivative ∂L ∂Y can be computed by a g-SpMM on graph G. The benefits of such formulation are two. First, consolidating all the GNN computations into two patterns lays a foundation for system optimizations such as parallelization and auto-tuning. Second, g-SpMM naturally avoids generating intermediate storage for messages, and g-SDDMM avoids copying node representations to edges. Later, we will show that such fused computation is the key reason for a superior training speed and memory efficiency (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PARALLELIZING G-SPMM AND G-SDDMM ON TENSORIZED HARDWARE</head><p>Modern hardware like GPUs and TPUs utilizes large-scale multi-threading to achieve high throughput while hiding memory access latency. This requires the workload to have two characteristics. First, the computation-to-memory-access ratio must be high so that the cost of one memory operation is amortized over many floating point operations. Second, the workload should have sufficient parallelism to take advantage of the massive parallelization power in the hardware.. By these criteria, g-SpMM and g-SDDMM are inherently challenging workloads. First, each node's data is only used by its neighbors. With little data reuse, the resulting computation-to-memory-access ratio is low. Second, although there exist multiple ways to parallelize the g-SpMM and g-SDDMM operations -by node, edge or feature, different strategies have pros and cons and there is no jack of all trades. Feature parallelization lets each thread compute on one feature and different ones can run in parallel. Although it is free of synchronization, the parallelism is limited by hidden size. For parallelization on nodes and edges, the optimal performance depends on numerous factors (see <ref type="table" target="#tab_0">Table 1</ref>), including the preferred storage format, the specific computation pattern (i.e., g-SpMM or g-SDDMM), fine-grained synchronization to ensure atomicity (if required), degree of parallelism, and thus is ultimately both data and model dependent. DGL's current strategy is based on heuristicsusing node parallel for g-SpMM but edge parallel for g-SDDMM, and there is ample room to apply machine learning for performance optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DGL SYSTEM DESIGN</head><p>We now describe two important strategies we adopted in the development of DGL: 1) using graph as the central, user-friendly abstraction to allow deep optimization and 2) achieving maximum framework neutrality to enable seamless application integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Parallel Edge Parallel</head><p>Schedule Each thread is in charge of the entire adjacency list of a node.</p><p>Each thread is in charge of one edge.</p><p>Viability Any g-SpMM or g-SDDMM.</p><p>Any g-SDDMM and any g-SpMM with commutative and associative ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preferred Format</head><p>Compressed sparse row (CSR) due to fast lookup of adjacency list.</p><p>Coordinate list (COO) due to fast lookup of incident nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Need for synchronization No</head><p>No for g-SDDMM; g-SpMM requires atomic instructions for aggregating results to destination node memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workload Distribution Depend on node degrees Balanced</head><p>Parallelism Depend on number of nodes Depend on number of edges  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH AS A FIRST-CLASS CITIZEN</head><p>For a domain package designed for GNNs, it is natural to define graph as the central representation. While this is the consensus among different packages <ref type="bibr">(Fey &amp; Lenssen, 2019;</ref><ref type="bibr" target="#b5">Battaglia et al., 2018;</ref><ref type="bibr" target="#b23">Zhu et al., 2019;</ref><ref type="bibr" target="#b2">Alibaba, 2019)</ref>, DGL differs in a number of places. First, it fully embraces an object-oriented programming style at the graph level. Second, recognizing the fact that research work on graphs are diverse and have developed a rich set of tools, it adopts an interface familiar to graph analytic experts. Third, the package exposes necessary low-level structures to advanced users when necessary. The fourth point, which we will discuss in Section 4.2, is its extensive use of tensor data structure to allow seamless integration with base DL frameworks. <ref type="figure" target="#fig_0">Figure 1</ref> compares the programming model of DGL and PyTorch Geometric (PyG) <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref>. In DGL, DGLGraph is the key data structure created by the dgl.graph API (line 3). Different models (e.g., GraphConv for graph convolution , GATConv for graph attention model <ref type="bibr" target="#b16">(Veličković et al., 2018)</ref>) operate on a DGLGraph directly (line 15); sampling, too, returns a DGLGraph object (line 10). The returned subgraph object automatically extracts the features needed, saving the effort to manually slice from tensors (line 12-13). Consolidating graph operations in an object-oriented manner not only improves software consistency, but also enables performance enhancement transparent to users. For example, DGL automatically switches to use CSR or CSC formats for g-SpMM depending on whether it is forward or backward propagation, and uses COO for g-SDDMM.  Integrating graph with deep learning is a relatively young concept, but graph analytics has been a long-standing research field. There exist many sophisticated tools and packages. Many DGL APIs took inspiration from NetworkX (Hagberg et al., 2008) -the NumPy-equivalent python package in graph analytics. Examples are topological query APIs implanted as class member functions such as g.in _ degree for getting node indegrees of a graph g . The two methods g.edata and g.ndata for accessing edge and node features are similarly inspired, with a dictionary-like interface that allows named tensors. Importantly, those APIs, while sharing naming convention with their NetworkX counterparts, have batched versions using tensor data structure. For example, NetworkX's g.in _ degree only supports querying the degree of one node at a time, while DGL supports querying multiple nodes by providing a tensor of node IDs. Finally, we note that these APIs are more than a matter of convenience. For instance, a heterogeneous graph can have nodes or edges of different types, which can further have unaligned feature dimensions; it will be cumbersome to store them compactly in one tensor.</p><p>Finally, to maintain expressiveness and flexibility, it is important to expose internal structures so users can innovate beyond the APIs that DGLGraph offers. For instance, there have been a diverse number of studies (Kotnis &amp; Nastase, 2017; Lukovnikov et al., 2017) on negative sampling strategies for the link prediction task. Users can craft these negative edges using the internal adjacency matrix of a DGLGraph object via the g.adj API (line 17-19 in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>To define new GNN models, users can invoke the g-SpMM and g-SDDMM kernels via the g.update _ all( φ, ρ) and g.apply _ edges( φ) calls on a DGLGraph , with user-defined φ and ρ. In principle, a powerful compiler can parse any given functions and generate a fused kernel for execution. As the technique is yet to be developed (and thus is an active research), DGL provides a set of most common φ and ρ as built-ins and generates kernels for each of the combination. Note all these kernels avoid materializing edge data to save memory, which is important for all GNN models where edge features are needed (Sec. 6). For more complex user-defined functions (e.g., LSTM as a reduction function), DGL gathers node features to edges so users can compute messages in a batch. For the reduce phase, DGL groups nodes of the same degree into one bucket so that the received messages in each bucket can be stored in a dense tensor. DGL then invokes the user-defined reduce function repetitively on each bucket. This catch-all strategy makes it easy for quickly prototyping model ideas on graphs of small sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FRAMEWORK-NEUTRAL DESIGN</head><p>A natural way to build a domain package is to build it on top of one of the DL frameworks (e.g., PyTorch, TensorFlow, and MXNet). These mature frameworks already provide high-performance differentiable dense tensor operators, rich neural network modules and optimizers; there is scarcely any reason to reinvent the wheel. DGL makes a conscious decision to extend multiple frameworks and, consequently, to be as framework-neutral as possible. Our belief is that a real-world, end-to-end GNN application will require other modules outside of GNN, and that they can be, or have already been, implemented in any framework of users' choice. In addition, users may favor a particular framework simply because of its unique features.</p><p>Note that being framework-neutral is different from framework-agnostic. That is, while DGL has both PyTorch and TensorFlow backends, a PyTorch DGL model still needs to be modified if it is to be run in TensorFlow. Being completely framework-agnostic requires putting a shim over all conceivable operators across different frameworks, the cost of which is prohibitive. Instead, we adopt a practical approach and reduce framework dependencies as much as possible, while providing clear guidelines as where the changes are to be made. Importantly, DGL can achieve a high degree of framework neutrality in part due to the abstraction and implementation of DGLGraph . As we shall describe below and quantify through the models that we have implemented, the changes are often local and trivial.</p><p>A complete GNN application includes data loading and preprocessing, GNN model setup, training loop and evaluation. In theory, they are all framework dependent. The goal of our design is to make model specification as portable as possible. Versions of the same model for different frameworks differ in three categories: (I) model class inheritance (e.g., using tensorflow.keras.layer.Layer instead of torch.nn.Module ); (II) sub-modules used inside the model and parameter initialization;</p><p>(III) framework-specific operators (e.g., using tensorflow.matmul instead of torch.matmul ). A mini-porting guide and example codes are included in the supplementary materials. <ref type="table" target="#tab_2">Table 2</ref> shows the number of lines of code to change when we port several GNN layers from PyTorch to TensorFlow in DGL. They account for roughly 20%-40% of the entire model implementations.</p><p>Most of them are trivial modifications and are easy for developers versed in both frameworks. Importantly, all graph-related operations are unified and stay identical in different versions.</p><p>To achieve this level of framework neutrality with a minimum performance impact, DGL must decide what functionalities to delegate and re-direct to base frameworks, and otherwise judiciously take over the control. We summarize the main principles below; most of them shall be applicable to other domain packages that share the same aspiration.</p><p>Owning the minimum &amp; the critical. From our experience, a domain package must maintain control at places where performance or usability matters the most. For DGL, it means sparse tensor storage management and operations. This leads to the decision of defining DGLGraph (see Section 4.1). The first release of DGL used dense operations from frameworks to express sparse operations in GNNs and performed poorly. We decided to implemented sparse operations ourselves.</p><p>Leverage and delegate otherwise. Most of DGL's APIs take framework tensors as input and perform dense operations on them. DGL defines a shim to map dense tensor operations to their framework-specific implementations. For instance, summing up the hidden state of all nodes is a common readout function for graph-level classification. To batch this readout operation we define a shim function unsorted _ 1d _ segment _ sum , which translates to unsorted _ segment _ sum in Tensorflow and scatter _ add in PyTorch. Such remapping is in spirit similar to ONNX (onn, 2018), but is designed specifically for DGL.</p><p>To enable auto-differentiation, all computation involving node/edge features must be expressed with differentiable functions and operators. DGL defines custom functions that directly take the DGLGraph as well as node/edge features as inputs, and return node/edge features as outputs. These operators are then registered as PyTorch/Tensorflow/MXNet auto-differentiable functions.</p><p>DGL also takes advantage of DLPack <ref type="bibr">(dlp, 2017)</ref>, an open-source in-memory tensor structure specification for sharing tensors among deep learning frameworks, to directly process and return DL framework tensors without data copying. Many frameworks, including Pytorch, MXNet, and TensorFlow, natively support DLPack.</p><p>The above functionality calls for memory allocation and management. DGL delegates memory management to the base frameworks. A base framework usually implements sophisticated memory management (e.g., to reduce memory allocation overhead and memory consumption), which is especially important for GPU memory. Because the output shape of a graph kernel is well determined before execution, DGL calculates the output memory size, allocates memory from the frameworks for the output tensors and pass the memory to the kernel for execution. There is a long line of work for optimizing sparse matrix operators such as sparse matrix-vector multiplication (SpMV) or sparse matrix-matrix multiplication (SpMM) on both CPU and GPU. These techniques range from studying and innovating new sparse matrix formats <ref type="bibr" target="#b6">(Bell &amp; Garland, 2008;</ref><ref type="bibr">Filippone et al., 2017)</ref>, advanced parallel pattern <ref type="bibr" target="#b20">(Yang et al., 2018)</ref> to tiling and reordering <ref type="bibr" target="#b21">(Yang et al., 2011;</ref><ref type="bibr" target="#b4">Baskaran &amp; Bordawekar, 2009</ref>) in the context of graph analytics <ref type="bibr" target="#b3">(Ashari et al., 2014;</ref><ref type="bibr" target="#b17">Wang et al., 2016)</ref>        <ref type="table" target="#tab_6">Table 4</ref> evaluates the performance of mini-batch training. Compared with full graph training, the total training time also depends on the cost in sample preparation including the sampling operations and data movement from CPU to GPU. For neighbor sampling (NS), sample generation and data movement can occupy up to 85% of the total training time, which explains why DGL and PyG have similar performance. By contrast, cluster sampling (CS) is much faster and the benefit from DGL's optimized kernels gives an 1.56× speedup for training GAT. DGL beats PyG in all link prediction benchmarks due to the use of g-SDDMM kernel in computing predictions on edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MEMORY CONSUMPTION</head><p>To illustrate the advantage of DGL's g-SpMM and g-SDDMM kernels in reducing memory traffic, we further studied the memory usage of DGL and PyG. We trained a 3-layer GAT model with one attention head on a set of synthetic graphs of different scales. The average degree is fixed at 20 so the number of edges grows linearly with the number of nodes. <ref type="figure" target="#fig_2">Figure 2a</ref> shows that PyG consumes 6.3× more memory than DGL and runs out of memory on graphs of more than 60K nodes. DGL manages to keep a low memory footprint due to its g-SpMM kernel fusing the message computation with aggregation. We further investigate the case of link prediction using a 3-layer GCN model with cluster sampling on the OGB-CITATION dataset. The model computes a prediction on each edge by performing a dot-product of its source and destination node representations, which is a typical SDDMM operation. <ref type="figure" target="#fig_2">Figure 2b</ref> shows the memory usage by increasing the number of negative edges per positive ones at each mini-batch. DGL's memory consumption stays the same regardless of the number of negative samples while PyG quickly runs out of memory. Since negative sampling is universal in link prediction tasks, we expect the phenomenon to appear in other benchmarks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">FRAMEWORK OVERHEAD</head><p>We compared DGL's framework overhead with both PyG and GraphNets using PyTorch and Ten-sorFlow as backends, respectively. In order to eliminate the impact of message passing kernels, we trained an one-layer GCN over a synthetically generated chain graph and measured the epoch time.</p><p>We varied the number of nodes and plotted the speedup of PyG and GraphNets over DGL in <ref type="figure" target="#fig_3">Figure 3</ref>. Ideally, the speedup should be one. We observed a 17% overhead compared with PyG when the graph is very small, but as the graph size increases, the overhead becomes negligible. The overhead is due to DGL registering message passing kernels via Python to keep implementation independent of frameworks while PyG can register them in C++. Interestingly, DGL-TF is faster than GraphNets, which uses native TensorFlow operators, demonstrating the viability of a framework-neutral package with low overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present Deep Graph Library (DGL), a system specialized for deep learning models on graphs. DGL identifies the connection between sparse matrix computation and the message passing paradigm in graph neural networks, and consolidates these operations into generalized sparse-dense matrix multiplication (g-SpMM) and sampled dense-dense matrix multiplication (g-SDDMM). DGL explores a wide range of parallelization strategies, leading to its superior speed and memory efficiency. DGL presents two design principles. By having graph as the core programming abstraction, DGL can hide cumbersome details from users and perform optimization transparently. DGL's lessons in designing a framework-neutral domain package with low overhead shall also be applicable to other packages of the same kind. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A NOMENCLATURES</head><p>In the Appendix we adopt the following nomenclatures for representing different mathematical objects:</p><p>• a: a scalar • a: a (column) vector • A: a matrix • a i : the i-th row of matrix A • R m×n : the set of real matrices with m rows and n columns.</p><p>• {x : y} the set with all mathematical objects x that satisfies condition y.</p><p>• f (·, ·, . . . ): a function.</p><p>• f : X → Y: a function that maps from set X to Y</p><p>• ∂y ∂x or ∇ x y: the Jacobian of y with respect to x, in numerator layout. • ∂f ∂x : the partial derivative of scalar function f with respect to vector x, in numerator layout. Note that in numerator layout ∂f ∂x = ∂f ∂x1 ∂f ∂x2 · · · is a row vector.</p><p>• ∂f ∂A : the partial derivative of scalar function f with respect to matrix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B GRADIENT OF G-SPMM AND G-SDDMM</head><p>We go by reviewing the definition of g-SpMM and g-SDDMM: Definition 1. A generalized SDDMM (g-SDDMM) defined on graph G with message function φ m is a function g-SDDMM G,φm : R |V|×d1 , R |V|×d2 , R |E|×d3 → R |E|×d4 where the output edge representations M = g-SDDMM G,φm (X, Y, W) are computed from the edges' own features, as well as features of their incident nodes:</p><formula xml:id="formula_5">m e = φ m (x u , y v , w e ) , ∀(u, e, v) ∈ E.</formula><p>Definition 2. A generalized SpMM (g-SpMM) defined on graph G with message function φ z and reduce function ρ is a function</p><formula xml:id="formula_6">g-SpMM G,φz,ρ : R |V|×d1 , R |V|×d2 , R |E|×d3 → R |V|×d4</formula><p>where the output node representations Z = g-SpMM G,φz,ρ (X, Y, W) are computed from the nodes' inbound edge features, the node features themselves, and the neighbor features:</p><formula xml:id="formula_7">z v = ρ ({φ z (x u , y v , w e ) : (u, e, v) ∈ E}) , ∀v ∈ V.</formula><p>We also review the formal definition of the reverse graph. Definition 3. Given the graph G = (V, E), the reverse graphG = (V,Ẽ), whereẼ = {(u, e, v) : (v, e, u) ∈ E} contains the corresponding edges reversing directions.</p><p>We then show that the gradient of g-SpMM and g-SDDMM functions can also be expressed as g-SpMM and g-SDDMM functions. Lemma 1. Assume we are given the g-SDDMM function</p><formula xml:id="formula_8">M = g-SDDMM G,φm (X, Y, W)</formula><p>defined on graph G with message function φ m and the objective function L = (M). There exists a function Lemma 2. Assume we are given the g-SDDMM function</p><formula xml:id="formula_9">φ w : R |V|×d1 , R |V|×d2 , R |E|×(d3+d4) → R |E|×d3 such that ∂L ∂W = g-SDDMM G,φ w X,</formula><formula xml:id="formula_10">M = g-SDDMM G,φm (X, Y, W)</formula><p>defined on graph G with message function φ m and the objective function L = (M). There exists functions φ x and φ y</p><formula xml:id="formula_11">φ x : R |V|×d1 , R |V|×d2 , R |E|×(d3+d4) → R |V|×d1 φ y : R |V|×d1 , R |V|×d2 , R |E|×(d3+d4) → R |V|×d2 such that ∂L ∂X = g-SpMMG ,φ x , X, Y, W; ∂L ∂M ∂L ∂Y = g-SpMM G,φ y , X, Y, W; ∂L ∂M</formula><p>whereG represents the reverse graph of G, and denotes the summation as a reduce function of the g-SpMMs.</p><p>Proof. By chain rule, for each u, v ∈ V we have: Lemma 4. Assume we are given the g-SpMM function</p><formula xml:id="formula_12">∂L ∂x u = e ,v</formula><formula xml:id="formula_13">Z = g-SpMM G,φz,ρ (X, Y, W)</formula><p>defined on graph G with message function φ z and reduce function ρ and the objective function L = (Z). There exists functions φ x and φ y</p><formula xml:id="formula_14">φ x : R |V|×d1 , R |V|×(d2+d4) , R |E|×d3 → R |V|×d1 φ y : R |V|×d1 , R |V|×(d2+d4) , R |E|×d3 → R |V|×d2</formula><p>and set functions ρ x , ρ y , such that</p><formula xml:id="formula_15">∂L ∂X = g-SpMMG ,φ x ,ρ x X, Y; ∂L ∂Z , W ∂L ∂Y = g-SpMM G,φ y ,ρ y X, Y; ∂L ∂Z , W</formula><p>whereG represents the reverse graph of G.</p><p>Proof. We first show the correctness for ∂L ∂Y . To derive ∂L ∂X , we need to sum over all successors of u:</p><formula xml:id="formula_16">Let m e = φ z (x u , y v , w e ) for all (u, e, v) ∈ G. Since y v only takes part in the computation of z v , we have ∂L ∂y v = ∂L ∂z v ∂z v ∂y v = ∂L ∂z v</formula><formula xml:id="formula_17">∂L ∂x u = e ,v :(u,e ,v )∈E ∂L ∂z v ∂z v ∂x u = e ,v :(u,e ,v )∈E ∂L ∂z v ∂z v ∂m e ∂m e ∂x u = e ,v :(u,e ,v )∈E ∂L ∂z v ∇ m e ρ ({m e : (u , e , v ) ∈ E}) ∇ xu φ z (x u , y v , w e ) = e ,v :(v ,e ,u)∈Ẽ ∂L ∂z v ∇ m e ρ m e : (v , e , u ) ∈Ẽ ∇ xu φ z (x u , y v , w e )</formula><p>We can see that for each node v, ∂L ∂yv is only a function of its own features, the predecessors' features, and the features of inbound edges. Therefore, ∂L ∂Y can be computed via a g-SpMM defined on graph G.</p><p>Similarly, for each node u, ∂L ∂xu is only a function of its own features, the successors' features, and the features of outbound edges. It can be computed via a g-SpMM defined on the reverse graphG Link prediction. The GCMC models on ML-100K, ML-1M and ML-10M all adopt an encoderdecoder architecture as proposed in the original paper. The input is a one-hot encoding of the movie and item nodes. The encoder has one graph convolution layer which projects the input encoding to a layer of 500 units with summation as the message aggregator, and one fully-connected layer which outputs a layer of 75 units. All the models use a bi-linear model as the decoder and the number of basis is set to two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 MINI-BATCH TRAINING</head><p>Node classification.</p><p>• For training with neighbor sampling on the REDDIT graph, we use a batch size of 1024 and the sampling fanouts are 25 and 10 from the first to the last layer. Both the GraphSAGE and the GAT models used have three layers and a hidden size of 16. The GAT model has 8 attention heads. • For training with neighbor sampling on the OGBN-PRODUCT graph, we use a batch size of 1024 and sampling fanouts of 15, 10, 5 for GraphSAGE, and a batch size of 128 and sampling fanouts of 10, 10, 10 for GAT. Both the GraphSAGE and the GAT models have three layers with a hidden size of 256. The GAT model has 8 attention heads. • For training with cluster sampling on the OGBN-PRODUCT graph, we first partition the graph into 15000 clusters. The training batch size is 32, meaning each mini-batch contains the induced subgraph of nodes from 32 clusters. Both the GraphSAGE and the GAT models have three layers with a hidden size of 256. The GAT model has 8 attention heads.</p><p>Link prediction. For all the experiments, we first partition the input graph into 15000 clusters. The training batch size is 256, meaning each mini-batch contains the induced subgraph of nodes from 256 clusters. All the models adopt an encoder-decoder architecture. The encoder models (i.e., GCN or GAT) all have three layers with a hidden size of 256. The GAT models all have one attention head. The decoder model predicts edges by a dot product between the representations of the incident nodes.</p><p>By default, only one negative sample is generated per positive sample by corrupting one end-point of the edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E KERNEL SENSITIVITY TO GRAPH AND MODEL CONFIGURATION</head><p>We studied how graph structures and model configurations influence the speed of g-SpMM and g-SDDMM kernels and thus the choice of parallel strategies (i.e., node or edge parallel). We benchmarked a g-SpMM and a g-SDDMM kernel on two input graphs, the REDDIT graph from <ref type="bibr">(Hamilton et al., 2017)</ref> and a nearest neighbor graph generated by <ref type="bibr" target="#b11">(Qi et al., 2017)</ref>, with varying feature sizes.  The kernel throughput (in GFLOPS) measured on a NVIDIA V100 GPU is shown in <ref type="figure" target="#fig_6">Figure 4</ref>. It shows that the optimal choice of edge parallel or node parallel relies on graph structure, feature size and the sparse format in use. For SpMM, edge parallel is slightly better than node parallel for small feature size but eventually becomes worse when the feature size scales up due to the overhead from atomic aggregation. For SDDMM, edge parallel on COO outperforms CSR by a large margin up to feature size equal to 16 but again becomes worse afterwards because of the better memory locality of CSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F GUIDE FOR PORTING GNN MODELS ACROSS DEEP LEARNING</head><p>FRAMEWORKS DGL provides a framework-neutral design and allows users to develop GNN models on different deep learning frameworks, such as PyTorch, TensorFlow and MXNet. Assume a user finds an existing GNN model in one framework and wishes to port it to another one that s/he is familiar with. Such porting needs to address three categories of differences in the deep learning frameworks.</p><p>Porting models usually involves in three steps.  <ref type="figure">Figure 5</ref> shows such an example. • Step 2 is to change the sub-modules used inside the model. These sub-modules are usually defined in the initialization method of the model class. Different frameworks usually define similar sub-modules but with different sub-module names and different arguments. They also initialize the parameters in the sub-modules differently. For example, the fully connected layer in Pytorch is defined in nn.Linear but it is defined in layers.Dense in TensorFlow. The arguments of the sub-modules are also different. We always need to define the input and output dimensions for the Pytorch sub-modules but only need to define the output dimensions for the TensorFlow sub-modules. In addition, PyTorch initializes parameters after the definition of nn modules, while TensorFlow specifies initialization method together with layer definition. <ref type="figure" target="#fig_7">Figure 6</ref> shows an example of such differences. • Step 3 is to replace the framework-specific operators. Similar to sub-modules, different frameworks define similar tensor operators but with different names and different input arguments. For example, matrix multiplication is defined in tensorflow.matmul in TensorFlow and is defined in torch.matmul in Pytorch. <ref type="figure" target="#fig_8">Figure 7</ref> shows a complete example of porting GraphSAGE from PyTorch to TensorFlow. We place the code side by side to contrast the key differences. As shown above, the PyTorch version of SAGEConv needs to inherit from torch.nn.Module while the TensorFlow version inherits from tensorflow.keras.layers.Layers . Most of the modifications are in the __ init __ function of the class, where we change the members defined as nn modules to TensorFlow's counterparts. In this example, the forward function (cf. call function in TensorFlow) only invokes DGL's message passing computation via update _ all . Because no tensor operators are explicitly invoked, there are no modifications for tensor operators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Computing graph convolution on a subgraph in DGL and PyG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>or scientific computing applications(LeVeque, 2007), just to name a few. Our work formally connects the area to GNN applications through the notions of generalized SpMM and SDDMM. We present the emerging challenges and hope to open up new innovations in this domain.6 EVALUATIONIn this section, we compare DGL with other popular GNN frameworks: PyTorch-Geometric v1.5.0 (PyG) with PyTorch v1.5.0 as backend and GraphNet v1.1.0 with TensorFlow v2.2.0 as backend. 36.1 TRAINING SPEEDWe consider two benchmark tasks: node classification and link prediction, and two training methods: full graph training and mini-batch training. Node classification datasets include the REDDIT graph from(Hamilton et al., 2017), the OGBN-ARXIV, OGBN-PROTEIN, and OGBN-PRODUCT graphs from the Open Graph Benchmarks (OGB)(Hu et al., 2020). For link prediction, we use benchmarks from MOVIELENS(ML)(Harper &amp; Konstan, 2015), the OGBL-CITATION and OGBL-PPA graphs from OGB.To demonstrate the generality of DGL's optimizations, we benchmark a variety of state-of-theart GNN models, includingGCN (Kipf &amp; Welling, 2017),GraphSAGE (Hamilton et al., 2017), GAT<ref type="bibr" target="#b16">(Veličković et al., 2018)</ref>, R-GCN<ref type="bibr" target="#b13">(Schlichtkrull et al., 2017)</ref> andGCMC (Berg et al., 2017). All the node classification tasks use cross entropy loss on the node representations learned by the GNN models while the link prediction tasks perform edge predictions by computing the dot-product of the source and destination node representations. For mini-batch training, we experiment with two sampling methods: neighbor sampling (NS)(Hamilton et al., 2017)  and cluster sampling (CS)(Chiang  et al., 2019). The supplementary material includes additional details about the datasets and model configurations.All experiments record the training time of one epoch averaged over 10 runs. For full graph training, we measure the training time on both CPU and GPU. The testbeds are one AWS EC2 p3.2xlarge instance (one NVidia V100 GPU with 16GB GPU RAM and 8 VCPUs) and one m5n.16xlarge instance (64 VCPUs and 256GB RAM) for experiments on GPU and CPU respectively. For minibatch training, we perform sampling on CPU and copy the sampled subgraphs and features to GPU for training. The testbed is a p3.2xlarge instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Memory usage of PyG and DGL. (a) GAT on synthetic graphs; (b) GCN w/ CS on OGBL-CITATION.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Raw framework speedup of PyG and GraphNets over DGL. (50%) of DGL's g-SpMM and g-SDDMM kernels using multi-threading compared with PyG's (only 10%). The large gap of ML-1M is further caused by the huge intermediate message tensor, resulting in a lot of time spent in memory traffic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>• [A; B; · · · ] = [A B . . .]: horizontal concatenation of matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>:(u ,e ,v)∈E ∂L ∂z v ∇ m e ρ ({m e : (u , e , v) ∈ E}) ∇ yv φ z (x u , y v , w e )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Throughput of SpMM and SDDMM with different parallel strategies, sparse formats on two input graphs. We fix the number of heads to 8 and vary the feature size of each head. The figures in the upper row are for the REDDIT dataset while the ones in the lower row are for the k-NN graph. NP stands for node parallel while EP stands for edge parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Create a fully connected sub-module in PyTorch and TensorFlow. The two graphs have very different degree frequencies, with REDDIT (Figure 4 upper row) having power-law degree distribution while the k-NN graph (Figure 4 lower row) having a constant indegree equal to 32. The g-SpMM kernel is extracted from the Graph Attention Network (GAT) (Veličković et al., 2018) model; it multiplies a neighbor node's representation x u with the attention weight α e on the edge during message aggregation. With multiple attention heads (8 in our experiment), the formulation of each head h is as follows: z v,h = (u,e,v)∈E α e,h x u,h The g-SDDMM kernel computes the attention weight by a dot-product of the source and destination nodes: α e,h = x u,h , x v,h , ∀(u, e, v) ∈ E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The implementation of GraphSAGE in PyTorch and TensorFlow. • Step 1 is to change model class inheritance. For example, when porting from Pytorch to TensorFlow, the model class should inherit from tensorflow.keras.layer.Layer instead of torch.nn.Module .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the node and edge parallel strategies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The categorized number of lines of codes (LoC) need to change when porting a GNN layer in DGL from PyTorch to TensorFlow. Code comments are excluded.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>defined message function and then scatters them to the target nodes for aggregation. This scatter-gather pattern is inefficient due to generating large intermediate message tensors. GraphNet<ref type="bibr" target="#b5">(Battaglia et al., 2018)</ref> and AliGraph<ref type="bibr" target="#b23">(Zhu et al., 2019)</ref> are two TensorFlow packages for building GNN models. Both frameworks allow customizable message functions but the reducers are limited to TensorFlow's operators for segment reduction. Euler<ref type="bibr" target="#b2">(Alibaba, 2019)</ref> focuses on sampling-based mini-batching training on large graphs but lacks GPU support. NeuGraph(Ma et al., 2019)  accelerates GNN training by partitioning graphs to multiple GPUs. All of these systems are tied to one specific base DL framework.</figDesc><table /><note>Due to the rising interests in GNNs, frameworks designed specifically for expressing and accelerating GNNs are developed. PyTorch-Geometric (PyG) (Fey &amp; Lenssen, 2019) is an extension for geometric deep learning to the PyTorch framework. PyG's programming model is centered around sparse tensor abstraction. During message passing, it first gathers node features to edges, applies user-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows the results of full graph training. For GraphSAGE on GPU, both DGL and PyG use the vendor-provided cuSPARSE (Naumov et al., 2010) library for computing SpMM so the performance is similar. DGL is slower by a small margin (2-11%) due to framework overhead. For GAT, DGL is 1.68× faster than PyG on OGBN-ARXIV because DGL's g-SpMM kernel avoids generating message tensors while PyG's scatter-gather kernel does. This also explains PyG running out-of-memory on</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>DGL</cell><cell cols="2">CPU</cell><cell>PyG</cell><cell cols="2">GPU DGL</cell><cell>PyG</cell></row><row><cell></cell><cell cols="5">Node Classification</cell><cell></cell></row><row><cell>REDDIT</cell><cell>SAGE</cell><cell cols="2">13.80</cell><cell cols="2">99.47</cell><cell>0.432</cell><cell>0.403</cell></row><row><cell>REDDIT</cell><cell>GAT</cell><cell>9.15</cell><cell></cell><cell cols="2">OOM</cell><cell>0.718</cell><cell>OOM</cell></row><row><cell>OGBN-ARXIV</cell><cell>SAGE</cell><cell>3.31</cell><cell></cell><cell cols="2">8.389</cell><cell>0.104</cell><cell>0.098</cell></row><row><cell>OGBN-ARXIV</cell><cell>GAT</cell><cell cols="2">1.237</cell><cell cols="2">43.21</cell><cell>0.086</cell><cell>0.234</cell></row><row><cell>OGBN-PROTEIN</cell><cell>R-GCN</cell><cell cols="2">26.31</cell><cell cols="2">373.8</cell><cell>0.706</cell><cell>OOM</cell></row><row><cell></cell><cell cols="3">Link Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ML-100K</cell><cell>GCMC</cell><cell cols="2">0.064</cell><cell cols="2">1.569</cell><cell>0.021</cell><cell>0.012</cell></row><row><cell>ML-1M</cell><cell>GCMC</cell><cell cols="2">0.351</cell><cell cols="2">40.47</cell><cell>0.045</cell><cell>0.103</cell></row><row><cell>ML-10M</cell><cell>GCMC</cell><cell>5.08</cell><cell></cell><cell cols="2">OOM</cell><cell>0.412</cell><cell>OOM</cell></row></table><note>OGBN-PROTEIN due to the graph being the densest one among all and having edge features. Link prediction benchmarks show similar results. DGL is slower on small graphs (e.g., ML-100K) but is 1.83× faster on ML-1M. DGL can train on ML-10M while PyG runs out of memory. On CPU, DGL outperforms PyG on all benchmarks by 1.9×-64×. This is attributing to the high CPU utilization</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Epoch running time in seconds (full graph training). OOM means out-of-memory.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>DGL</cell><cell>PyG</cell></row><row><cell></cell><cell>Node Classification</cell><cell></cell><cell></cell></row><row><cell>REDDIT</cell><cell>SAGE w/ NS</cell><cell>19.90</cell><cell>20.45</cell></row><row><cell>REDDIT</cell><cell>GAT w/ NS</cell><cell>21.07</cell><cell>21.89</cell></row><row><cell>OGBN-PRODUCT</cell><cell>SAGE w/ NS</cell><cell>33.34</cell><cell>35.00</cell></row><row><cell>OGBN-PRODUCT</cell><cell>GAT w/ NS</cell><cell>67.0</cell><cell>187.0</cell></row><row><cell>OGBN-PRODUCT</cell><cell>SAGE w/ CS</cell><cell>8.887</cell><cell>8.614</cell></row><row><cell>OGBN-PRODUCT</cell><cell>GAT w/ CS</cell><cell>14.50</cell><cell>58.36</cell></row><row><cell></cell><cell>Link Prediction</cell><cell></cell><cell></cell></row><row><cell>OGBL-CITATION</cell><cell>GCN w/ CS</cell><cell>5.772</cell><cell>6.287</cell></row><row><cell>OGBL-CITATION</cell><cell>GAT w/ CS</cell><cell>6.081</cell><cell>8.290</cell></row><row><cell>OGBL-PPA</cell><cell>GCN w/ CS</cell><cell>5.782</cell><cell>6.421</cell></row><row><cell>OGBL-PPA</cell><cell>GAT w/ CS</cell><cell>6.224</cell><cell>8.198</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Epoch running time in seconds for mini-</cell></row><row><cell>batch training using neighbor sampling (NS) and</cell></row><row><cell>cluster sampling (CS).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 257-266, 2019. Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pp. 3844-3852, 2016. Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. CoRR, abs/1903.02428, 2019. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017. Bhushan Kotnis and Vivi Nastase. Analysis of the impact of negative sampling on link prediction in knowledge graphs. arXiv preprint arXiv:1708.06816, 2017. Randall J LeVeque. Finite difference methods for ordinary and partial differential equations: steadystate and time-dependent problems, volume 98. Siam, 2007. Denis Lukovnikov, Asja Fischer, Jens Lehmann, and Sören Auer. Neural network-based question answering over knowledge graphs on word and character level. In Proceedings of the 26th international conference on World Wide Web, pp. 1211-1220, 2017.</figDesc><table><row><cell>Salvatore Filippone, Valeria Cardellini, Davide Barbieri, and Alessandro Fanfarillo. Sparse matrix-</cell></row><row><cell>vector multiplication on gpgpus. ACM Transactions on Mathematical Software (TOMS), 43(4):</cell></row><row><cell>1-49, 2017.</cell></row><row><cell>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural</cell></row><row><cell>message passing for quantum chemistry. In International Conference on Machine Learning, 2017.</cell></row><row><cell>Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function</cell></row><row><cell>using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United</cell></row><row><cell>States), 2008.</cell></row><row><cell>2030-</cell></row><row><cell>2041, 2018.</cell></row><row><cell>F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm</cell></row><row><cell>transactions on interactive intelligent systems (tiis), 5(4):1-19, 2015.</cell></row><row><cell>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,</cell></row><row><cell>and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv</cell></row><row><cell>preprint arXiv:2005.00687, 2020.</cell></row><row><cell>Jeremy Kepner, Peter Aaltonen, David Bader, Aydin Buluç, Franz Franchetti, John Gilbert, Dylan</cell></row><row><cell>Hutchison, Manoj Kumar, Andrew Lumsdaine, Henning Meyerhenke, et al. Mathematical founda-</cell></row><row><cell>tions of the graphblas. In 2016 IEEE High Performance Extreme Computing Conference (HPEC),</cell></row><row><cell>pp. 1-9. IEEE, 2016.</cell></row><row><cell>Thomas N.</cell></row></table><note>Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017. Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical queries on knowledge graphs. In Advances in Neural Information Processing Systems, pp.Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou, and Yafei Dai. Neugraph: parallel deep neural network computation on large graphs. In 2019 {USENIX} Annual Technical Conference ({USENIX}{ATC} 19), pp. 443-458, 2019.M Naumov, LS Chien, P Vandermersch, and U Kapasi. Cusparse library. In GPU Technology Conference, 2010.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>By chain rule, for each (u, e, v) ∈ E we have: ∇ we φ m (x u , y v , w e )</figDesc><table><row><cell>Proof. ∂L ∂w e</cell><cell>=</cell><cell>∂L ∂m e</cell><cell>∂m e ∂w e</cell><cell>=</cell><cell>∂L ∂m e</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Y, W;</cell><cell>∂L ∂M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Proof. Let m e = φ z (x u , y v , w e ) for all (u, e, v) ∈ E. Since m e only participates in computation of z v , we have ∇ me ρ ({m e : (u , e , v) ∈ E}) ∇ we φ (x u , y v , w e )</figDesc><table><row><cell>∂L ∂w e</cell><cell>=</cell><cell>∂L ∂z v</cell><cell>∂z v ∂m e</cell><cell cols="2">∂m e ∂w e</cell><cell>=</cell><cell>∂L ∂z v</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>∂L</cell><cell>∂m e</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:(u,e ,v )∈E</cell><cell>∂m e</cell><cell>∂x u</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell cols="2">e ,v :(u,e ,v )∈E</cell><cell>∂L ∂m e</cell><cell>∇ xu φ m (x u , y v , w e )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell cols="2">e ,v :(v ,e ,u)∈Ẽ</cell><cell>∂L ∂m e</cell><cell>∇ xu φ m (x u , y v , w e )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>message function on the reverse graph</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">reduce function</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">∂L ∂y v</cell><cell>=</cell><cell cols="2">u ,e :(u ,e ,v)∈E</cell><cell>∂L ∂m e</cell><cell>∂m e ∂y v</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell cols="2">u ,e :(u ,e ,v)∈E</cell><cell>∂L ∂m e</cell><cell>∇ yv φ m (x u , y v , w e )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>message function</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">reduce function</cell></row><row><cell cols="8">Lemma 3. Assume we are given the g-SpMM function</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Z = g-SpMM G,φz,ρ (X, Y, W)</cell></row><row><cell cols="8">defined on graph G with message function φ z and reduce function ρ and the objective function</cell></row><row><cell cols="8">L = (Z). There exists a function φ w</cell></row><row><cell cols="5">φ such that</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">∂L ∂W</cell><cell cols="2">= g-SDDMM G,φ w X, Y;</cell><cell>∂L ∂Z</cell><cell>, W</cell></row></table><note>w : R |V|×d1 , R |V|×(d2+d4) , R |E|×d3 → R |E|×d3</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use a slightly different notation than a traditional (u, v) pair, where e is the ID associated with the edge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Benchmark scripts are available at https://github.com/dglai/dgl-0.5-benchmark/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">are learnable weights. H (0) is the initial node features. Since the graph does not have raw node features, we use a scalar 1 for each node. The original A r 's are binary adjacency matrices and they become weighted in the case ofOGBN-PROTEIN.   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof of Theorem 1. The theorem can be proved trivially from the lemmas above, as the addition of two g-SDDMM functions on the same graph is still a g-SDDMM function on the same graph. The same holds for g-SpMM functions as well.  For ML-100K, ML-1M and ML-10M, we use separate one-hot encoding for user and movie nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C DATASET STATISTICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D EXPERIMENT CONFIGURATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 FULL GRAPH TRAINING</head><p>Here we list the hyper-parameter configurations used in comparing training speed between DGL and PyG (Sec. 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node classification.</head><p>• The GraphSAGE model on REDDIT has two layers, each with 16 hidden size and the aggregator is summation. • The GAT model on REDDIT has three layers, each with 16 hidden size and one attention head. • The GraphSAGE model on OGBN-ARXIV has three layers, each with 256 hidden size and the aggregator is summation. • The GAT model on OGBN-ARXIV has three layers, each with 16 hidden size and four attention heads. • The R-GCN model on OGBN-PROTEIN has three layers with 32 hidden size. The graph has 8 edge features in the range of [0, 1], which can be viewed as connectivity strength for 8 relations.</p><p>The RGCN model takes the following formulation:</p><p>where H (l) is the node representations after the l-th RGCN layer, σ is the ReLU activation function, D r is the degree matrix for relation r, A r is the adjacency matrix for relation r, and W (l) r , W l</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Open neural network exchange format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dlpack</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alibaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Euler</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba/euler" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast sparse matrix-vector multiplication on gpus for graph applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Ashari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Sedaghati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Eisenlohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Parthasarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="781" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimizing sparse matrix-vector multiplication on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Muthu Manikandan Baskaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordawekar</surname></persName>
		</author>
		<idno>RC24704</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="812" to="859" />
		</imprint>
	</monogr>
	<note type="report_type">IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient sparse matrix-vector multiplication on cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<idno>NVR-2008-004</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Nvidia Corporation</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Nvidia Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gunrock: A high-performance graph processing library on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Riffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design principles for sparse matrix multiplication on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aydın</forename><surname>Buluç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Parallel Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="672" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast sparse matrix-vector multiplication on gpus: implications for graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1103.2405</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aligraph: a comprehensive graph neural network platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2094" to="2105" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SAGEConv(nn.Module): 5 def __ init __ (self, in _ feat, out _ feat, 6 feat _ drop=0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Modeling polypharmacy side effects with graph convolutional networks. init __ (</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout(feat _ drop) 10 self.activation = activation 11 self.fc _ self = nn.Linear(dst _ feat, out _ feat) 12 self.fc _ neigh = nn.Linear(src _ feat, out _ feat) 13 gain = nn.init.calculate _ gain(&apos;relu&apos;) 14 nn.init.xavier _ uniform _ ( 15 self.fc _ self.weight, gain=gain) 16 nn.init.xavier _ uniform _ ( 17 self.fc _ neigh.weight, gain=gain) 18 19 def forward(self, graph, feat): 20 feat _ src = feat _ dst = self.feat _ drop(feat) 21 graph.srcdata[&apos;h&apos;] = feat _ src 22 graph.update _ all</title>
	</analytic>
	<monogr>
		<title level="m">src _ feat, dst _ feat = expand _ as _ pair(in _ feat) 9 self.feat _ drop = nn</title>
		<imprint/>
	</monogr>
	<note>fn.copy _ u. 23 fn.mean</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">25 rst = self.fc _ self(feat _ dst) 26 + self.fc _ neigh(h _ neigh) 27 return self.activation(rst) SAGEConv(layers.Layer): 5 def __ init __ (self, in _ feats, out _ feats, 6 feat _ drop=0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dstdata</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<pubPlace>SAGEConv, self</pubPlace>
		</imprint>
	</monogr>
	<note>init __ (</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VarianceScaling( 12 scale=np.sqrt(2), mode=&quot;fan _ avg&quot;, 13 distribution=&quot;untruncated _ normal&quot;) 14 self.fc _ self = layers.Dense( 15 out _ feats, kernel _ initializer=xinit) 16 self.fc _ neigh = layers.Dense( 17 out _ feats, kernel _ initializer=xinit) 18 19 def call(self, graph, feat): 20 feat _ src = feat _ dst = self.feat _ drop(feat) 21 graph.srcdata[&apos;h&apos;] = feat _ src 22 graph.update _ all</title>
	</analytic>
	<monogr>
		<title level="m">drop = layers.Dropout(feat _ drop) 10 self.activation = activation 11 xinit = tf.keras.initializers</title>
		<imprint/>
	</monogr>
	<note>fn.copy _ u. 23 fn.mean</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">25 rst = self.fc _ self(feat _ dst) 26 + self.fc _ neigh(h _ neigh) 27 return self</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dstdata</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>activation(rst</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
