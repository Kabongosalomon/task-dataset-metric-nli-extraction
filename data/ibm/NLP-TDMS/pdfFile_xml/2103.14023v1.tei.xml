<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanglan</forename><surname>Ou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncertainty in each agent's future behavior. Forecasting multiagent trajectories requires modeling two key dimensions: (1) time dimension, where we model the influence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others. Most prior methods model these two dimensions separately, e.g., first using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model. This approach is suboptimal since independent feature encoding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent's state at one time to directly affect another agent's state at a future time. To this end, we propose a new Transformer, termed AgentFormer, that simultaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents. Based on AgentFormer, we propose a stochastic multi-agent trajectory prediction model that can attend to features of any agent at any previous timestep when inferring an agent's future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent's behavior to affect other agents. Extensive experiments show that our method significantly improves the state of the art on wellestablished pedestrian and autonomous driving datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The safe planning of autonomous systems such as selfdriving vehicles requires forecasting accurate future trajec- tories of surrounding agents (e.g., pedestrians, vehicles). However, multi-agent trajectory forecasting is challenging since the social interaction between agents, i.e., behavioral influence of an agent on others, is a complex process. The problem is further complicated by the uncertainty of each agent's future behavior, i.e., each agent has its latent intent unobserved by the system (e.g., turning left or right) that governs its future trajectory and in turn affects other agents. Therefore, a good multi-agent trajectory forecasting method should effectively model <ref type="bibr" target="#b0">(1)</ref> the complex social interaction between agents and (2) the latent intent of each agent's future behavior and its social influence on other agents. Multi-agent social interaction modeling involves two key dimensions as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (Top)</ref>: (1) time dimension, where we model how past agent states (positions and velocities) influence future agent states; (2) social dimension, where we model how each agent's state affects the state of other agents. Most prior multi-agent trajectory forecasting methods model these two dimensions separately (see <ref type="figure" target="#fig_0">Fig. 1 (Middle)</ref>). Approaches like <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref> first use temporal models (e.g., LSTMs <ref type="bibr" target="#b16">[17]</ref> or Transformers <ref type="bibr" target="#b46">[47]</ref>) to summarize trajectory features over time for each agent independently and then input the summarized temporal features to social models (e.g., graph neural networks <ref type="bibr" target="#b22">[23]</ref>) to capture social interaction between agents. Alternatively, methods like <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18]</ref> first use social models to produce social features for each agent at each independent timestep and then apply temporal models over the social features. In this work, we argue that modeling the time and social dimensions separately can be suboptimal since the independent feature encoding over either the time or social dimension is not informed by features across the other dimension, and the encoded features may not contain the necessary information for modeling the other dimension.</p><p>To tackle this problem, we propose a new Transformer model, termed AgentFormer, that simultaneously learns representations from both the time and social dimensions. AgentFormer allows an agent's state at one time to affect another agent's state at a future time directly instead of through intermediate features encoded over one dimension. As Transformers require sequences as input, we leverage a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents (see <ref type="figure" target="#fig_0">Fig. 1 (Bottom)</ref>). However, directly applying standard Transformers to these multi-agent sequences will result in a loss of time and agent information since standard attention operations discard the timestep and agent identity associated with each element in the sequence. We solve the loss of time information using a time encoder that appends a timestamp feature to each element. However, the loss of agent identity is a more complicated problem: unlike time, there is no innate ordering between agents, and assigning an agent index-based encoding will break the required permutation invariance of agents and create artificial dependencies on agent indices in the model. Instead, we propose a novel agent-aware attention mechanism to preserve agent information. Specifically, agent-aware attention generates two sets of keys and queries via different linear transformations; one set of keys and queries is used to compute inter-agent attention (agent to agent) while the other set is designated for intra-agent attention (agent to itself). This design allows agent-aware attention to attend to elements of the same agent differently than elements of other agents, thus keeping the notion of agent identity. Agent-aware attention can be implemented efficiently via masked operations. Furthermore, AgentFormer can also encode rule-based connectivity between agents (e.g., based on distance) by masking out the attention weights between unconnected agents.</p><p>Based on AgentFormer, which allows us to model social interaction effectively, we propose a multi-agent trajectory prediction framework that also models the social influence of each agent's future trajectory on other agents. The probabilistic formulation of the model follows the conditional variational autoencoder (CVAE <ref type="bibr" target="#b20">[21]</ref>) where we model the generative future trajectory distribution conditioned on context (e.g., past trajectories, semantic maps). We introduce a latent code for each agent to represent its latent intent. To model the social influence of each agent's future behavior (governed by latent intent) on other agents, the latent codes of all agents are jointly inferred from the future trajectories of all agents during training, and they are also jointly used by a trajectory decoder to output socially-aware multi-agent future trajectories. Thanks to AgentFormer, the trajectory decoder can attend to features of any agent at any previous timestep when inferring an agent's future position. To improve the diversity of sampled trajectories and avoid similar samples caused by random sampling, we further adopt a multi-agent trajectory sampler that can generate diverse and plausible multi-agent trajectories by mapping context to various configurations of all agents' latent codes.</p><p>We evaluate our method on well-established pedestrian datasets, ETH <ref type="bibr" target="#b37">[38]</ref> and UCY <ref type="bibr" target="#b27">[28]</ref>, and an autonomous driving dataset, nuScenes <ref type="bibr" target="#b2">[3]</ref>. On ETH/UCY and nuScenes, we outperform state-of-the-art multi-agent prediction methods with substantial performance improvement (41% and 18%). We further conduct extensive ablation studies to show the superiority of AgentFormer over various combinations of social and temporal models. We also demonstrate the efficacy of agent-aware attention against agent encoding.</p><p>To summarize, the main contributions of this paper are: (1) We propose a new Transformer that simultaneously models the time and social dimensions of multi-agent trajectories with a sequence representation. <ref type="bibr" target="#b1">(2)</ref> We propose a novel agent-aware attention mechanism that preserves the agent identity of each element in the multi-agent trajectory sequence. (3) We present a multi-agent forecasting framework that models the latent intent of all agents jointly to produce socially-plausible future trajectories. (4) Our approach significantly improves the state of the art on wellestablished pedestrian and autonomous driving datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sequence Modeling. Sequences are an important representation of data such as video, audio, price, etc. Historically, RNNs (e.g., LSTMs <ref type="bibr" target="#b16">[17]</ref>, GRUs <ref type="bibr" target="#b6">[7]</ref>) have achieved remarkable success in sequence modeling, with applications to speech recognition <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b34">35]</ref>, image captioning <ref type="bibr" target="#b52">[53]</ref>, machine translation <ref type="bibr" target="#b31">[32]</ref>, human pose estimation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b23">24]</ref>, etc. In particular, RNNs have been the preferred temporal models for trajectory and motion forecasting. Many RNN-based methods model the trajectory pattern of pedestrians to predict their 2D future locations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b60">61]</ref>. Prior work has also used RNNs to model the temporal dynamics of 3D human pose <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60]</ref>. With the invention of Transformers and positional encoding <ref type="bibr" target="#b46">[47]</ref>, many works start to adopt Transformers for sequence modeling due to their strong ability to capture long-range dependencies. Transformers have first dominated the natural language processing (NLP) domain across various tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54]</ref>. Beyond NLP, numerous visual Transformers have been proposed to tackle vision tasks, such as image classification <ref type="bibr" target="#b9">[10]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, and instance segmentation <ref type="bibr" target="#b49">[50]</ref>. Recently, Transformers have also been used for trajectory forecasting. Transformer-TF <ref type="bibr" target="#b11">[12]</ref> applies the standard Transformer to predict the future trajectories of each agent independently. STAR <ref type="bibr" target="#b54">[55]</ref> uses separate temporal and spatial Transformers to forecast multi-agent trajectories. Interaction Transformer <ref type="bibr" target="#b29">[30]</ref> combines RNNs and Transformers for multi-agent trajectory modeling. Different from prior work, Our AgentFormer leverages a sequence representation of multi-agent trajectories and a novel agent-aware attention mechanism to preserve time and agent information in the sequence.</p><p>Trajectory Prediction. Early work on trajectory prediction adopts a deterministic approach using models such as social forces <ref type="bibr" target="#b15">[16]</ref>, Gaussian process (GP) <ref type="bibr" target="#b48">[49]</ref>, and RNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref>. A thorough review of these deterministic methods is provided in <ref type="bibr" target="#b42">[43]</ref>. As the future trajectory of an agent is uncertain and often multi-modal, recent trajectory prediction methods start to model the trajectory distribution with deep generative models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref> such as conditional variational autoencoders (CVAEs) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b44">45]</ref>, generative adversarial networks (GANs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">62]</ref>, and normalizing flows (NFs) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b13">14]</ref>. Most of these methods follow a seq2seq structure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> and predict future trajectories using intermediate features of past trajectories. In contrast, our AgentFormer-based trajectory prediction framework can directly attend to features of any agent at any previous timestep when inferring an agent's future position. Moreover, our approach models the future trajectories of all agents jointly to predict socially-aware trajectories.</p><p>Social Interaction Modeling. Methods for social interaction modeling can be categorized based on how they model the time and social dimensions. While RNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref> and Transformers <ref type="bibr" target="#b46">[47]</ref> are the prefered temporal models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55]</ref>, graph neural networks (GNNs) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> are often employed as the social models for interaction modeling <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>. One popular type of methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref> first uses temporal models to summarize trajectory features over time for each agent independently and then feeds the temporal features to social models to obtain socially-aware agent features. Alternatively, approaches like <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18]</ref> first use social models to produce social features of each agent at each independent timestep and then apply temporal models to summarize the social features over time for each agent. One common characteristic of these prior works is that they model the time and social dimensions on separate levels. This can be suboptimal since it prevents an agent's feature at one time from directly interacting with another agent's feature at a different time, thus limiting the model's ability to capture long-range dependencies. Instead, our method models both the time and social dimensions simultaneously, allowing direct feature interaction across time and agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We formulate multi-agent trajectory prediction as modeling the generative future trajectory distribution of N (variable) agents conditioned on their past trajectories. For observed timesteps t ≤ 0, we represent the joint state of all N agents at time t as X t = (x t 1 , x t 2 , . . . , x t N ), where x t n ∈ R ds is the state of agent n at time t, which includes the position, velocity and (optional) heading angle of the agent. We denote the history of all agents as X = X −H , X −H+1 , . . . , X 0 which includes the joint agent state at all H + 1 observed timesteps. Similarly, the joint state of all N agents at future time</p><formula xml:id="formula_0">t (t &gt; 0) is de- noted as Y t = (y t 1 , y t 2 , . . . , y t N )</formula><p>, where y t n ∈ R dp is the future position of agent n at time t. We denote the future trajectories of all N agents over T future timesteps as</p><formula xml:id="formula_1">Y = Y 1 , Y 2 , . . . , Y T .</formula><p>Depending on the data, optional contextual information I may also be given, such as a semantic map around the agents (annotations of sidewalks, road boundaries, etc.). Our goal is to learn a generative model p θ (Y|X, I) where θ are the model parameters.</p><p>In the following, we first introduce the proposed agentaware Transformer, AgentFormer, for joint modeling of socio-temporal relations. We then present a stochastic multi-agent trajectory prediction framework that jointly models the latent intent of all agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AgentFormer: Agent-Aware Transformers</head><p>Our agent-aware Transformer, AgentFormer, is a model that learns representations from multi-agent trajectories over both time and social dimensions simultaneously, in contrast to standard approaches that model the two dimensions in separate stages. AgentFormer has two types of modules -encoders and decoders, which follow the encoder and decoder design of the original Transformer <ref type="bibr" target="#b46">[47]</ref> but with two major differences: (1) it replaces positional encoding with a time encoder; (2) it uses a novel agent-aware attention mechanism instead of the scaled dot-product attention. As we will discuss below, these two modifications are motivated by a sequence representation of multi-agent trajectories that is suitable for Transformers.</p><p>Multi-Agent Trajectories as a Sequence. The past multiagent trajectories X can be denoted as a sequence X =</p><formula xml:id="formula_2">x −H 1 , . . . , x −H N , x −H+1 1 , . . . , x −H+1 N , . . . , x 0 1 , . . . , x 0 N of length L p = N × (H + 1)</formula><p>. Similarly, the future multiagent trajectories can also be represented as a sequence</p><formula xml:id="formula_3">Y = y 1 1 , . . . , y 1 N , y 2 1 , . . . , y 2 N , . . . , y T 1 , . . . , y T N of length L f = N × T .</formula><p>We adopt this sequence representation to be compatible with Transformers. At first glance, it may seem that we can directly apply standard Transformers to these sequences to model temporal and social relations. However, there are two problems with this approach: (1) loss of time information, as Transformers have no notion of time when computing attention for each element (e.g., x t n ) w.r.t. other elements in the sequence; for instance, x t n does not know x t m is a feature of the same timestep while x t+1 n is a feature of the next timestep; (2) loss of agent information, since Transformers do not consider agent identities when applying attention to each element, and elements of the same agent are not distinguished from elements of other agents; for example, when computing attention for x t n , both x t+1 n and x t+1 m are treated the same, disregarding the fact that x t+1 n is from the same agent while x t+1 m is from a different agent. Below, we present the solutions to these two problems -(1) time encoder and (2) agent-aware attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Encoder.</head><p>To inform AgentFormer about the timestep associated with each element in the trajectory sequence, we employ a time encoder similar to the positional encoding in the original Transformer. Instead of encoding the position of each element based on its index in the sequence, we compute a timestamp feature based on the timestep t of the element. The timestamp uses the same sinusoidal design as the positional encoding. Let us take the past trajectory sequence X as an example. For each element x t n , the timestamp feature τ t n ∈ R dτ is defined as</p><formula xml:id="formula_4">τ t n (k) = sin((t + H)/10000 i/dτ ), i is even cos((t + H)/10000 (i−1)/dτ ), i is odd</formula><p>where τ t n (i) denotes the i-th feature of τ t n and d τ is the feature dimension of the timestamp. The time encoder outputs a timestamped sequenceX and each elementx t n ∈ R dτ inX is computed asx t n = W 2 (W 1 x t n ⊕ τ t n ) where W 1 ∈ R dτ ×ds and W 2 ∈ R dτ ×2dτ are weight matrices and ⊕ denotes concatenation.</p><p>Agent-Aware Attention. To preserve agent information in the trajectory sequence, it may be tempting to employ a similar strategy to the time encoder, such as an agent encoder that assigns an agent index-based encoding to each element in the sequence. However, using such agent encoding is not effective as we will show in the experiments. The reason is that, different from time which is naturally ordered, there is no innate ordering between agents, and assigning encodings based on agent indices will break the required permutation invariance of agents and create artificial dependencies on agent indices in the model.</p><p>We tackle the loss of agent information from a different angle by proposing a novel agent-aware attention mechanism. The agent-aware attention takes as input keys K,  queries Q and values V, each of which uses the sequence representation of multi-agent trajectories. As an example, let the keys K and values V be the past trajectory sequence X ∈ R Lp×ds , and let the queries Q be the future trajectory sequence Y ∈ R L f ×dp . Recall that X is of length L p = N ×(H+1) as X contains the trajectory features of N agents of H + 1 past timesteps; Y is of length L f = N × T containing trajectory features of T future timesteps. The output of agent-aware attention is computed as</p><formula xml:id="formula_5">AgentAwareAttention(Q, K, V) = softmax A √ d k V (1) A = M (Q self K T self ) + (1 − M) (Q other K T other ) (2) Q self = QW Q self , K self = KW K self (3) Q other = QW Q other , K other = KW K other<label>(4)</label></formula><p>where denotes element-wise product and we use two sets of projections {W Q self , W K self } and {W Q other , W K other } to generate projected keys K self , K other ∈ R Lp×d k and queries Q self , Q other ∈ R L f ×d k with key (query) dimension d k . Each element A ij in the attention weight matrix A represents the attention weight between the i-th query q i and j-th key k j . As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, when computing the attention weight matrix A ∈ R L f ×Lp , we also use a mask M ∈ R L f ×Lp which is defined as</p><formula xml:id="formula_6">M ij = 1(i mod N = j mod N )<label>(5)</label></formula><p>where M ij denotes each element inside the mask M and 1(·) denotes the indicator function. As · mod N computes the agent index of a query/key, M ij equals to one if the i-th query q i and j-th key k j belongs to the same agent, and M ij equals to zero otherwise, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Using the mask M, Eq. (2) computes each element A ij of the attention weight matrix A differently based on the agreement of agent identity: If q i and k j have the same agent identity, A ij is computed using the projected queries Q self and keys K self designated for intra-agent attention (agent to itself); If q i and k j have different agent identities, A ij is computed using the projected queries Q other and keys K other designated for inter-agent attention (agent to other agents). In this AgentFormer Encoder  way, the agent-aware attention learns to attend to elements of the same agent in the sequence differently than elements of other agents, thus preserving the notion of agent identity. Note that AgentFormer only uses agent-aware attention to replace the scaled dot-product attention in the original Transformer and still allows multi-head attention to learn distributed representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVAE Past Encoder</head><p>Encoding Agent Connectivity. AgentFormer can also encode rule-based agent connectivity information by masking out the attention weights between unconnected agents. Specifically, we define that two agents n and m are connected if their distance D nm at the current time (t = 0) is smaller than a threshold η. If agents n and m are not connected, we set the attention weight A ij = −∞ between any query q i of agent n and any key k j of agent m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Agent Prediction with AgentFormer</head><p>Having introduced AgentFormer for modeling temporal and social relations, we are now ready to apply it in our multi-agent trajectory prediction framework based on CVAEs. As discussed at the start of Sec. 3, the goal of multi-agent trajectory prediction is to model the future trajectory distribution p θ (Y|X, I) conditioned on past trajectories X and contextual information I. To account for stochasticity and multi-modality in each agent's future behavior, we introduce latent variables Z = {z 1 , . . . , z N } where z n ∈ R dz represents the latent intent of agent n. We can then rewrite the future trajectory distribution as</p><formula xml:id="formula_7">p θ (Y|X, I) = p θ (Y|Z, X, I)p θ (Z|X, I)dZ ,<label>(6)</label></formula><p>where p θ (Z|X, I) = N n=1 p θ (z n |X, I) is a conditional Gaussian prior factorized over agents and p θ (Y|Z, X, I) is a conditional likelihood model. To tackle the intractable in-tegral in Eq. (6), we use the negative evidence lower bound (ELBO) L elbo in the CVAE as our loss function:</p><formula xml:id="formula_8">L elbo = − E q φ (Z|Y,X,I) [log p θ (Y|Z, X, I)] + KL(q φ (Z|Y, X, I) p θ (Z|X, I)) ,<label>(7)</label></formula><p>where q φ (Z|Y, X, I) = N n=1 q φ (z n |Y, X, I) is an approximate posterior distribution factorized over agents and parametrized by φ. In our probabilistic formulation, the latent codes Z of all agents in the posterior q φ (Z|Y, X, I) are jointly inferred from the future trajectories Y of all agents; similarly, the future trajectories Y in the conditional likelihood p θ (Y|Z, X, I) are modeled using the latent codes Z of all agents. This design allows each agent's latent intent represented by z n to affect not just its own future trajectory but also the future trajectories of other agents, which enables us to generate socially-aware multi-agent trajectories. Having described the probabilistic formulation, we now introduce the detailed model architecture as outlined in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Encoding Context (Semantic Map). As aforementioned, our model can optionally take as input contextual information I if provided by the data. Here, we assume I ∈ R H0×W0×C is a semantic map around the agents at the current timestep (t = 0) with annotated semantic information (e.g., sidewalks, crosswalks, and road boundaries). For each agent n, we rotate I to align with the agent's heading angle and crop an image patch I n ∈ R H×W ×C around the agent. We use a hand-designed convolutional neural network (CNN) to extract visual features v n from I n , which will later be used by other modules in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVAE Past</head><p>Encoder. The past encoder starts with the multi-agent past trajectory sequence X. If the semantic map I is provided, the past encoder concatenates each element x t n ∈ X with the corresponding visual feature v n of agent n. The new sequence is then fed into the time encoder to obtain a timestamped sequence, which is then input to the AgentFormer encoder as keys, queries, and values. The output of the encoder is a past feature sequence</p><formula xml:id="formula_9">C = c −H 1 , . . . c −H N , c −H+1 1 , . . . c −H+1 N , .</formula><p>. . , c 0 1 , . . . , c 0 N that summarizes the past agent trajectories X and context I.</p><p>CVAE Prior. The prior module first performs an agent-wise pooling that computes a mean agent feature C n from the past features across timesteps: C n = mean(c −H n , . . . , c 0 n ). We then use a multilayer perceptron (MLP) to map C n to the Gaussian parameters (µ p n , σ p n ) of the prior distribution p θ (z n |X, I) = N (µ p n , Diag(σ p n ) 2 ).</p><p>CVAE Future Encoder. Given the multi-agent future trajectory sequence Y, similar to the past encoder, the future encoder appends visual features from the semantic map I to Y and feeds the resulting sequence to the time encoder to produce a timestamped sequence. The timestamped sequence is then input as queries to the AgentFormer decoder along with the past feature sequence C which serves as both keys and values. We use the AgentFormer decoder here because it allows the feature extraction of Y to condition on X through C, thus effectively modeling the X-conditioning in the posterior q φ (Z|Y, X, I). We then perform an agentwise mean pooling across timesteps on the output sequence of the AgentFormer decoder to extract a feature for each agent. Each agent feature is then input to an MLP to obtain the Gaussian parameters (µ q n , σ q n ) of the approximate posterior distribution q φ (z n |Y, X, I) = N (µ q n , Diag(σ q n ) 2 ).</p><p>CVAE Future Decoder. Unlike the original Transformer decoder, our future trajectory decoder is autoregressive, which means it outputs trajectories one step at a time and feeds the currently generated trajectories back into the model to produce the trajectories of the next timestep. This design mitigates compounding errors during test time at the expense of training speed. Starting from an initial sequence (ŷ 0 1 , . . . ,ŷ 0 N ) whereŷ 0 n =x 0 n (x 0 n is the position feature inside x 0 n ), the future decoder module maps an input sequence (ŷ 0 1 , . . . ,ŷ 0 N , . . . ,ŷ t 1 , . . . ,ŷ t N ) to an output sequence (ŷ 1 1 , . . . ,ŷ 1 N , . . . ,ŷ t +1 1 , . . . ,ŷ t +1 N ) and grows the input sequence into (ŷ 0 1 , . . . ,ŷ 0 N , . . . ,ŷ t +1 1 , . . . ,ŷ t +1 N ). By autoregressively applying the decoder T times, we obtain the output sequenceŶ = (ŷ 1 1 , . . . ,ŷ 1 N , . . . ,ŷ T 1 , . . . ,ŷ T N ). Inside the future decoder module ( <ref type="figure" target="#fig_3">Fig. 3 (Right)</ref>), we first form a feature sequence F = (f 0 1 , . . . , f 0 N , . . . , f t 1 , . . . , f t N ) where f t n =ŷ t n ⊕ z n , thus concatenating the currently generated trajectories with the corresponding latent codes. The latent codes are sampled from the approximate posterior during training but from the trajectory sampler (as discussed below) at test time. The feature sequence F is then concatenated with the semantic map features and timestamped before being input as queries to the AgentFormer decoder alongside the past feature sequence C which serves as keys and values. The AgentFormer decoder enables the future trajectories to directly attend to features of any agent at any previous timestep (e.g., c −H 3 orŷ 1 2 ), allowing the model to effectively infer future trajectories based on the whole agent history. We use proper masking inside the AgentFormer decoder to enforce causality of the decoder output sequence. Each element of the output sequence is then passed through an MLP to generate the decoded future agent positionŷ t n . As we use a Gaussian to model the conditional likelihood p θ (Y|Z, X, I) = N (Ŷ, I/β), where I is the identity matrix and β is a weighting factor, the first term in Eq. (7) equals the mean squred error (MSE): L mse = 1 2β Y−Ŷ 2 . Trajectory Sampler. We adapt a diversity sampling technique, DLow <ref type="bibr" target="#b58">[59]</ref>, to our multi-agent trajectory prediction setting and employ a trajectory sampler to produce diverse and plausible trajectories once our CVAE model is trained. The trajectory sampler generates K sets of latent codes</p><formula xml:id="formula_10">{Z (1) , . . . , Z (K) } where each set Z (k) = {z (k) 1 , . . . , z (k)</formula><p>N } contains the latent codes of all agents and can be decoded by the CVAE decoder into a multi-agent future trajectory sampleŶ <ref type="bibr">(k)</ref> . Each latent code z (k) n ∈ Z (k) is generated by a linear transformation of a Gaussian noise n ∈ R dz :</p><formula xml:id="formula_11">z (k) n = A (k) n n + b (k) n , n ∼ N (0, I),<label>(8)</label></formula><p>where A n } are generated by the trajectory sampler module <ref type="figure" target="#fig_3">(Fig. 3</ref>) through agent-wise pooling of the past feature sequence C and an MLP. The trajectory sampler loss is defined as</p><formula xml:id="formula_12">Lsamp = min k Ŷ (k) − Y 2 + N n=1 KL(r θ (z (k)</formula><p>n |X, I) p θ (zn|X, I))</p><formula xml:id="formula_13">+ 1 K(K − 1) K k 1 =1 K k 1 =k 2 exp − Ŷ (k 1 ) −Ŷ (k 2 ) 2 σ d ,<label>(9)</label></formula><p>where σ d is a scaling factor. The first term encourages the future trajectory samplesŶ (k) to cover the ground truth Y. The second KL term encourages each latent code z (k) n to follow the prior and be plausible; the KL can be computed analytically as both distributions inside are Gaussians. The third term encourages diversity among the future trajectory samplesŶ (k) by penalizing small pairwise distance. When training the trajectory sampler with Eq. (9), we freeze the weights of the CVAE modules. At test time, we sample latent codes {Z <ref type="bibr" target="#b0">(1)</ref> , . . . , Z (K) } using the trajectory sampler instead of sampling from the CVAE prior and decode the latent codes into trajectory samples {Ŷ <ref type="bibr" target="#b0">(1)</ref> , . . . ,Ŷ (K) }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We evaluate our method on well-established public datasets: the ETH <ref type="bibr" target="#b37">[38]</ref>, UCY <ref type="bibr" target="#b27">[28]</ref>, and nuScenes <ref type="bibr" target="#b2">[3]</ref> datasets. The ETH/UCY datasets are the major benchmark for pedestrian trajectory prediction. There are five datasets in ETH/UCY, each of which contains pedestrian trajectories captured at 2.5Hz in multi-agent social scenarios with rich interaction. nuScenes is a recent large-scale autonomous driving dataset, which consists of 1000 driving scenes with each scene annotated at 2Hz. nuScenes also provides HD semantic maps with 11 semantic classes.</p><p>Metrics. We report the minimum average displacement error ADE K and final displacement error FDE K of K trajectory samples of each agent compared to the ground truth:</p><formula xml:id="formula_14">ADE K = 1 T min K k=1 T t=1 ŷ t,(k) n − y t n 2 , FDE K = min K k=1 ŷ T,(k) n −y T n 2 , whereŷ t,(k) n</formula><p>denotes the future position of agent n at time t in the k-th sample and y T n is the corresponding ground truth. ADE K and FDE K are the standard metrics for trajectory prediction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Evaluation Protocol. For the ETH/UCY datasets, we adopt a leave-one-out strategy for evaluation, following prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55]</ref>. We forecast 2D future trajectories of 12 timesteps (4.8s) based on observed trajectories of 8 timesteps (3.2s). Similar to most prior works, we do not use any semantic/visual information for ETH/UCY for fair comparisons. All metrics are computed with K = 20 samples. For the nuScenes dataset, following prior work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref>, we use the vehicle-only train-valtest split provided by the nuScenes prediction challenge and predict 2D future trajectories of 12 timesteps (6s) based on observed trajectories of 4 timesteps (2s). We report results with metrics computed using K = 5 and K = 10 samples.</p><p>Implementation Details. For all datasets, we represent trajectories in a scene-centered coordinate where the origin is the mean position of all agents at t = 0. The future decoder in <ref type="figure" target="#fig_3">Fig. 3</ref> outputs the offset to the agent's current positionx 0 n , sox 0 n is added to obtainŷ t n for each element in the output sequence. Following prior work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b54">55]</ref>, random rotation of the scene is adopted for data augment. Our multi-agent prediction model <ref type="figure" target="#fig_3">(Fig. 3</ref>) uses two stacks (defined in <ref type="bibr" target="#b46">[47]</ref>) of identical layers in each AgentFormer encoder/decoder with 0.1 dropout rate. The dimensions d k , d v , d τ of keys, queries, and timestamps in AgentFormer are all set to 256, and the hidden dimension of feedforward layers is 512. The number of heads for multi-head agent-aware attention is 8. All MLPs in the model have hidden dimensions (512, 256). For the CVAE, the latent code dimension d z is 32, the coefficient β of the MSE loss equals 1, and we clip the maximum value of the KL term in L elbo (Eq. (7)) down to 2. We also use the variety loss in SGAN <ref type="bibr" target="#b14">[15]</ref> in addition to L elbo . The agent connectivity threshold η is set to 100. We Method ADE20/FDE20 ↓ (m), K = 20 Samples  <ref type="table">Table 2</ref>. Baseline comparisons on the nuScenes dataset. Our method outperforms prior state-of-the-art methods consistently for both 5 and 10 samples. Symbol "-" means results are not available. train the CVAE model using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> for 100 epochs on ETH/UCY and nuScenes. We use an initial learning rate of 10 −4 and halve the learning rate every 10 epochs. More details including the CNN for encoding semantic maps and the training procedure of the trajectory sampler can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Baseline Comparisons. On the ETH/UCY datasets, we compare our approach with current state-of-the-art methods -Trajectron++ <ref type="bibr" target="#b44">[45]</ref>, PECNet <ref type="bibr" target="#b33">[34]</ref>, STAR <ref type="bibr" target="#b54">[55]</ref>, and Transformer-TF [12] -as well as common baselines -SGAN <ref type="bibr" target="#b14">[15]</ref> and Sophie <ref type="bibr" target="#b43">[44]</ref>. The performance of all methods is summarized in <ref type="table" target="#tab_1">Table 1</ref>, where we use officiallyreported results for the baselines. We can observe that our AgentFormer significantly outperforms the baselines in prediction accuracy as measured by ADE and FDE. Particularly, our method reduces the FDE of the current state of the art, Trajectron++, from 0.41 to 0.29, achieving a 41% increase in performance. As FDE measures the final displacement error of predicted trajectories, it places more emphasis on a method's ability to predict distant futures than ADE. We believe the strong performance of our method in FDE can be attributed to the design of AgentFormer, which can model long-range trajectory dependencies effectively by directly attending to features of any agent at any previous timestep when inferring an agent's future position. Compared to ETH/UCY, the trajectories in nuScenes are much longer as we evaluate with a longer time horizon (6s) and vehicles are much faster than pedestrians. Thus, nuScenes presents a different challenge for multi-agent prediction methods. On the nuScenes dataset, we evaluate our  <ref type="table">Table 3</ref>. Ablation studies on the ETH/UCY datasets. "TF" means Transformer and "AA Attention" denotes agent-aware attention.  <ref type="table">Table 4</ref>. Ablation studies on the nuScenes dataset. "TF" means Transformer and "AA Attention" denotes agent-aware attention.</p><p>approach against state-of-the-art vehicle prediction methods -Trajectron++ <ref type="bibr" target="#b44">[45]</ref>, MTP <ref type="bibr" target="#b7">[8]</ref>, MultiPath <ref type="bibr" target="#b4">[5]</ref>, Cover-Net <ref type="bibr" target="#b38">[39]</ref>, DSF-AF <ref type="bibr" target="#b32">[33]</ref>, and DLow-AF <ref type="bibr" target="#b58">[59]</ref>. We report the performance of all methods in <ref type="table">Table 2</ref>, where the results of Trajectron++ are taken from the nuScenes prediction challenge leaderboard, the performance of DLow-AF is from <ref type="bibr" target="#b32">[33]</ref>, and we also use the officially-reported results for the other baselines. The FDE of some baselines is not available since the number has not been reported. We can see that our approach, AgentFormer, outperforms the baselines consistently in ADE and FDE for both 5 samples and 10 samples settings. Notably, our method reduces the state-ofthe-art ADE 5 from 1.88 to 1.59, achieving an 18% performance improvement. Among methods that report FDE, our approach achieves an FDE 5 of 3.14, which is significantly lower than 4.67 from the second-best method.</p><p>Ablation Studies. We further perform extensive ablation studies on ETH/UCY and nuScenes to investigate the contribution of key technical components in our method. The first ablation study explores variants of our method that use separate social and temporal models to replace our joint socio-temporal model, AgentFormer, in our multiagent prediction framework. We choose GCN <ref type="bibr" target="#b22">[23]</ref> or Transformer (TF) as the social model, and LSTM or Transformer as the temporal model. In total, there are 4 (2 × 2) combinations of social and temporal models. The ablation results are summarized in the first group of <ref type="table">Table 3</ref>   that all combinations of separate social and temporal models lead to inferior performance compared to our method which models the social and temporal dimensions jointly.</p><p>The second ablation study investigates the role of (1) joint latent intent modeling, (2) agent-aware attention, and (3) semantic maps, and we denote the corresponding variants as "w/o joint latent", "w/o AA attention", and "w/o semantic map". We further test a variant "w/ agent encoding" where we replace agent-aware attention with agent encoding. The results are reported in the second group of <ref type="table">Table 3</ref> and 4. We can see that all variants lead to considerably worse performance compared to our full method. In particular, the variants "w/o AA attention" and "w/ agent encoding" result in pronounced performance drop, which indicates that agent-aware attention is essential in our method and alternatives like agent encoding are not effective.</p><p>Attention Visualization. To showcase our method's ability to attend to any agent at any prior timestep when forecasting future trajectories, we visualize the attention in <ref type="figure" target="#fig_5">Fig. 4</ref>. We can observe that when predicting the future position (red dot) of an agent, the model pays more attention (darker color) to the agent's own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents. More attention visualization can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new Transformer, Agent-Former, that can simultaneously model the time and social dimensions of multi-agent trajectories using a sequence representation. To preserve agent identities in the sequence, we proposed a novel agent-aware attention mechanism that can attend to features of the same agent differently than features of other agents. Based on AgentFormer, we presented a stochastic multi-agent trajectory prediction framework that jointly models the latent intent of all agents to produce diverse and socially-aware multi-agent future trajectories. Experiments demonstrated that our method signif-icantly improved state-of-the-art performance on challenging pedestrian and autonomous driving datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Implementation Details</head><p>Encoding Semantic Maps. The semantic map I n ∈ R H×W ×C for each agent n has spatial dimensions (100, 100) with 3 meters between adjacent pixels. It has C = 3 channels annotating drivable areas, road dividers, and lane dividers obtained using the official nuScenes software development kit. Since the semantic map is relatively easy to parse, we use a simple hand-designed CNN to extract visual features v n from it. In particular, the CNN has four convolutional layers with channels (32, 32, 32, 1), kernel size <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3)</ref>, and strides (2, 2, 1, 1). A final linear layer is used to obtain a 32-dimensional feature.</p><p>Training Trajectory Sampler. The scaling factor σ d in the trajectory sampler loss L samp (Eq. (9) in the main paper) is set to 5 for ETH/UCY and 20 for nuScenes. We clip the maximum value of the KL term in L samp down to 2. We train the trajectory sampler using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> for 50 epochs on ETH/UCY and nuScenes. We use an initial learning rate of 10 −4 and halve the learning rate every 5 epochs.</p><p>Ablation Study Details. We first provide details for the ablation study of separate social and temporal models (first group of <ref type="table">Table 3</ref> and 4 in the main paper). We first use a temporal model (LSTM or Transformer) to extract the temporal feature of each agent and then apply a social model (GCN <ref type="bibr" target="#b22">[23]</ref> or Transformer) over the temporal features to obtain social features for each agent; final trajectories are decoded from the social features using either an LSTM or Transformer. For the GCN, we use two graph convolutional layers with channels (256, 256) and residual connections within each layer. The hidden dimensions of the LSTMs are set to 256. The Transformers have two layers with key/query dimensions 256 and 8 heads; the feedforward layer has 512 hidden units, and the dropout ratio is 0.1. We use the positional encoding <ref type="bibr" target="#b46">[47]</ref> for the temporal Transformer but not for the social Transformer as agents are permutation-invariant.</p><p>Next, we provide details for the ablation study of each key technical component (second group of <ref type="table">Table 3</ref> and 4 in the main paper). For the variant without joint latent modeling ("w/o joint latent"), we append the latent codes to the trajectory sequence after the AgentFormer decoder instead of before the decoder. In this way, the latent code of one agent will not affect the future trajectory of another agent. For the variant without the agent-aware attention ("w/o AA attention"), we replace our agent-aware attention with standard scaled dot-product attention used in the original transformer <ref type="bibr" target="#b46">[47]</ref>. For the variant with agent encoding ("w/ agent encoding"), in addition to removing the agent aware attention, we also append an agent encoding to each element in the trajectory sequence. The agent encoding is computed similarly as the positional encoding <ref type="bibr" target="#b46">[47]</ref> but uses the agent index instead of the position index. For the variant without semantic maps ("w/o semantic map"), we simply do not append any visual features extracted from the semantic maps to the trajectory sequence.</p><p>Other Details. Our models are implemented using PyTorch <ref type="bibr" target="#b36">[37]</ref> and are trained with a single NVIDIA RTX 2080 Ti and standard CPUs. The training time is approximately one day for each dataset in ETH/UCY and three days for nuScenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Attention Visualization</head><p>As discussed in the main paper, our method can attend to any agent at any previous timestep when predicting the future position of an agent. Here, we provide more visualization of the attention in <ref type="figure">Fig. 6</ref> to understand the behavior of our model. Across all the examples, it is evident that when predicting the target future position of an agent, the model pays more attention to the agent's own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.  <ref type="figure">Figure 6</ref>. Attention Visualization on ETH/UCY. We plot the attention to past (blue) and future (green) trajectory features of all agents when inferring a target position (red). Darker color means higher attention. When predicting the target future position of an agent, the model pays more attention to the agent's own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Trajectory Sample Visualization</head><p>To demonstrate the importance of agent-aware attention, we also provide qualitative comparisons of our method against the variant without agent-aware attention (w/o AA attention) on the nuScenes dataset in <ref type="figure">Fig. 7</ref>. We can observe that the future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future trajectories significantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ours w/o AA attention w/o AA attention Past Trajectory Future Trajectory Samples GT Future Trajectory Road Walkway Undrivable Area <ref type="figure">Figure 7</ref>. Trajectory Sample Visualization on nuScenes. We compare our method against the variant without agent-aware attention (w/o AA attention). The future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future trajectories significantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Different from standard approaches that model multiagent trajectories in the time and social dimensions separately, our AgentFormer allows for joint modeling of the time and social dimensions while preserving time and agent information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of agent-aware attention. The mask M allows the attention weights in A to be computed differently based on whether the i-th query and j-th key belong to the same agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our AgentFormer-based multi-agent trajectory prediction framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>n</head><label></label><figDesc>∈ R dz×dz is a non-singular matrix and b (k) n ∈ R dz is a vector. Eq. (8) induces a Gaussian sampling distribution r θ (z (k) n |X, I) over z (k) n . The distribution is conditioned on X and I because its inner parameters {A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Attention Visualization. We plot the attention to past (blue) and future (green) trajectory features of all agents when inferring a target position (red). Darker color means higher attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Our method can naturally handle a time-varying number of agents because of the flexible sequence representation of multi-agent trajectories. We can simply remove the trajectory features of missing agents at each timestep from the sequence. The mask M of the example sequence (when applying self-attention) is computed based on the agreement of agent identity between each query and key.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>AgentFormer Decoder CVAE Future Encoder Value CVAE Future Decoder (Autoregressive)</head><label></label><figDesc></figDesc><table><row><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell></cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Element-wise)</cell><cell>(Element-wise)</cell><cell>(Element-wise)</cell></row><row><cell cols="3">Agent-wise Pooling</cell><cell cols="3">Agent-wise Pooling</cell><cell></cell><cell cols="3">Agent-wise Pooling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CVAE Prior</cell><cell></cell><cell></cell><cell cols="3">Trajectory Sampler</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Value</cell><cell>AgentFormer</cell><cell>AgentFormer</cell><cell>AgentFormer</cell></row><row><cell></cell><cell></cell><cell>Key</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Key</cell><cell>Decoder</cell><cell>Decoder</cell><cell>Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Query</cell><cell>Query</cell><cell>Query</cell></row><row><cell></cell><cell>Query</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time</cell><cell>Time</cell><cell>Time</cell></row><row><cell></cell><cell>Time Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell>Encoder</cell><cell>Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Optional)</cell><cell>(Optional)</cell><cell>(Optional)</cell></row><row><cell cols="3">Add Context (Optional)</cell><cell></cell><cell></cell><cell>Key</cell><cell>Query</cell><cell>Value</cell><cell></cell><cell>Add Context</cell><cell>Add Context</cell><cell>Add Context</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Time Encoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Add Context (Optional)</cell><cell></cell></row><row><cell cols="3">Number of Agents N = 3 (for illustration)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Past Trajectories</cell><cell>: GT Future Trajectories</cell></row><row><cell>Agent 1</cell><cell>Agent 2</cell><cell>Agent 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Latent Code (Agent n)</cell><cell>: Pred Future Trajectories</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>.52 0.72/1.61 0.60/1.26 0.34/0.69 0.42/0.84 0.58/1.18 SoPhie [44] 0.70/1.43 0.76/1.67 0.54/1.24 0.30/0.63 0.38/0.78 0.54/1.15 Transformer-TF [12] 0.61/1.12 0.18/0.30 0.35/0.65 0.22/0.38 0.17/0.32 0.31/0.55 STAR [55] 0.36/0.65 0.17/0.36 0.31/0.62 0.26/0.55 0.22/0.46 0.26/0.53 PECNet [34] 0.54/0.87 0.18/0.24 0.35/0.60 0.22/0.39 0.17/0.30 0.29/0.48 AgentFormer) 0.26/0.39 0.11/0.14 0.26/0.46 0.15/0.23 0.14/0.24 0.18/0.29 Baseline comparisons on the ETH/UCY datasets. Our method outperforms previous art with large FDE improvements. FDE 5 ↓ ADE 10 ↓ FDE 10 ↓</figDesc><table><row><cell>ETH</cell><cell>Hotel</cell><cell>Univ</cell><cell>Zara1</cell><cell>Zara2</cell><cell>Average</cell></row><row><cell cols="6">SGAN [15] 0.81/1Trajectron++ [45] 0.39/0.83 0.12/0.21 0.20/0.44 0.15/0.33 0.11/0.25 0.19/0.41</cell></row><row><cell cols="3">K = 5 Samples ADE 5 ↓ MTP [8] Ours (Method 2.93 -</cell><cell cols="3">K = 10 Samples 2.93 -</cell></row><row><cell>MultiPath [5]</cell><cell>2.32</cell><cell>-</cell><cell>1.96</cell><cell>-</cell><cell></cell></row><row><cell>CoverNet [39]</cell><cell>1.96</cell><cell>-</cell><cell>1.48</cell><cell>-</cell><cell></cell></row><row><cell>DSF-AF [33]</cell><cell>2.06</cell><cell>4.67</cell><cell>1.66</cell><cell cols="2">3.71</cell></row><row><cell>DLow-AF [59]</cell><cell>2.11</cell><cell>4.70</cell><cell>1.78</cell><cell cols="2">3.58</cell></row><row><cell>Trajectron++ [45]</cell><cell>1.88</cell><cell>-</cell><cell>1.51</cell><cell>-</cell><cell></cell></row><row><cell>Ours (AgentFormer)</cell><cell>1.59</cell><cell>3.14</cell><cell>1.31</cell><cell cols="2">2.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.43 0.17/0.27 0.28/0.50 0.19/0.33 0.19/0.34 0.23/0.37 GCN TF 0.36/0.50 0.12/0.18 0.27/0.50 0.18/0.31 0.16/0.28 0.22/0.35 TF LSTM 0.35/0.43 0.16/0.24 0.27/0.48 0.18/0.30 0.18/0.33 0.23/0.36 TF TF 0.35/0.45 0.12/0.19 0.28/0.52 0.19/0.32 0.15/0.27 0.22/0.35</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell cols="4">ADE20/FDE20 ↓ (m), K = 20 Samples</cell><cell></cell></row><row><cell>Social</cell><cell>Temporal</cell><cell>ETH</cell><cell>Hotel</cell><cell>Univ</cell><cell>Zara1</cell><cell>Zara2</cell><cell>Average</cell></row><row><cell cols="3">GCN 0.33/0Joint Socio-Temporal LSTM ETH</cell><cell>Hotel</cell><cell>Univ</cell><cell>Zara1</cell><cell>Zara2</cell><cell>Average</cell></row><row><cell cols="2">Ours w/o joint latent</cell><cell cols="6">0.31/0.39 0.11/0.15 0.28/0.51 0.18/0.30 0.16/0.29 0.21/0.33</cell></row><row><cell cols="8">Ours w/o AA attention 0.31/0.40 0.13/0.20 0.30/0.53 0.18/0.28 0.19/0.34 0.22/0.35</cell></row><row><cell cols="8">Ours w/ agent encoding 0.30/0.40 0.13/0.20 0.30/0.54 0.18/0.29 0.19/0.34 0.22/0.35</cell></row><row><cell cols="2">Ours (AgentFormer)</cell><cell cols="6">0.26/0.39 0.11/0.14 0.26/0.46 0.15/0.23 0.14/0.24 0.18/0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>↓ FDE 5 ↓ ADE 10 ↓ FDE 10 ↓ Temporal ADE 5 ↓ FDE 5 ↓ ADE 10 ↓ FDE 10 ↓</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">K = 5 Samples</cell><cell cols="2">K = 10 Samples</cell></row><row><cell cols="3">Social Temporal ADE 5 GCN LSTM 1.97</cell><cell>3.97</cell><cell>1.58</cell><cell>2.93</cell></row><row><cell>GCN</cell><cell>TF</cell><cell>1.74</cell><cell>3.52</cell><cell>1.39</cell><cell>2.59</cell></row><row><cell>TF</cell><cell>LSTM</cell><cell>1.79</cell><cell>3.65</cell><cell>1.48</cell><cell>2.76</cell></row><row><cell>TF</cell><cell>TF</cell><cell>1.98</cell><cell>4.20</cell><cell>1.54</cell><cell>3.07</cell></row><row><cell cols="2">Joint Socio-Ours w/o semantic map</cell><cell>1.73</cell><cell>3.57</cell><cell>1.46</cell><cell>2.81</cell></row><row><cell cols="2">Ours w/o joint latent</cell><cell>1.66</cell><cell>3.28</cell><cell>1.40</cell><cell>2.60</cell></row><row><cell cols="2">Ours w/o AA attention</cell><cell>1.82</cell><cell>3.70</cell><cell>1.49</cell><cell>2.83</cell></row><row><cell cols="2">Ours w/ agent encoding</cell><cell>1.83</cell><cell>3.70</cell><cell>1.50</cell><cell>2.82</cell></row><row><cell cols="2">Ours (AgentFormer)</cell><cell>1.59</cell><cell>3.14</cell><cell>1.31</cell><cell>2.48</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = 1 t = 2 t = 3 t = 4 t = 5 t = 1 t = 2 t = 3 t = 4 t = 5</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Handling a Time-Varying Number of Agents</head><p>For clarity and ease of exposition, we assume the number of agents remains the same across timesteps in the main paper. However, this assumption is not necessary, and our method can easily generalize to use cases where the number of agents changes over time due to agents going out of the scene or being missed by detection. We illustrate how to apply our method to such cases in <ref type="figure">Fig. 5</ref>. Owning to the flexible sequence representation we employ for multi-agent trajectories, we can simply remove the features of missing agents at each timestep from the sequence. The reason why we do not need to fill the missing features is that our method uses time encoding to preserve time information, unlike RNNs which have to use recurrence to encode timesteps and thus necessitate the features of all timesteps. As the number of agents is no longer N for all timesteps, the computation of the mask M in agent-aware attention needs to be changed accordingly:</p><p>where Agent(·) extracts the agent index of a query/key and 1(·) denotes the indicator function. An example of mask M is shown in <ref type="figure">Fig. 5 (Right)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Social</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent-Aware Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Agent Trajectories Trajectory Features Trajectory Features in 2D</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="86" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2090" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transformer networks for trajectory forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Giuliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Generative adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative hybrid representations for activity forecasting with no-regret learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stgat: Modeling spatial-temporal interactions for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikun</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoqi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2688" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Socialbigat: multimodal trajectory forecasting using bicycle-gan and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2019. Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evolvegraph: Multi-agent trajectory prediction with dynamic relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiho</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end contextual perception and prediction with interaction transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lingyun Luke Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05927</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yecheng Jason</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevana</forename><surname>Priya Inala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15084</idno>
		<title level="m">Diverse sampling for normalizing flow based trajectory forecasting</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">It is not the journey but the destination: Endpoint conditioned trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="759" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Analysis of recurrent neural networks for probabilistic modeling of driver behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">A</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1289" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Corina</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR, 2015. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">R2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vernaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="772" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human motion trajectory prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="895" to="935" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03093</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multiple futures prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Yichuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00997</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4601" to="4607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07847,2020.3</idno>
		<title level="m">Joint 3d tracking and forecasting with graph neural network and diversity sampling</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The microsoft 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fil</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5934" to="5938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph transformer networks for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3d ego-pose estimation via imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Diverse trajectory forecasting with determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04967</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10082" to="10092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dlow: Diversifying latent flows for diverse human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="346" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Residual force control for agile human behavior imitation and extended motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12085" to="12094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12126" to="12134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
