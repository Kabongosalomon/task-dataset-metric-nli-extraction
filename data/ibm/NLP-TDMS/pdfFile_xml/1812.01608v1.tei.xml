<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
							<email>jmenick@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
							<email>nalk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amsterdam</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets. * Equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A successful generative model has two core aspects: it produces targets that have high fidelity and it generalizes well on held-out data. Autoregressive (AR) models trained by conventional maximum likelihood estimation (MLE) have produced superior scores on held-out data across a wide range of domains such as text <ref type="bibr" target="#b16">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b18">Wu et al., 2016)</ref>, audio <ref type="bibr" target="#b13">(van den Oord et al., 2016a)</ref>, images  and videos <ref type="bibr" target="#b4">(Kalchbrenner et al., 2016)</ref>. These scores are a measure of the models' ability to generalize in that setting. From the perspective of sample fidelity, the outputs generated by AR models have also achieved state-of-the-art fidelity in many of the aforementioned domains with one notable exception. In the domain of unconditional large-scale image generation, AR samples have yet to manifest long-range structure and semantic coherence.</p><p>One source of difficulties impeding high-fidelity image generation is the multi-faceted relationship between the MLE scores achieved by a model and the model's sample fidelity. On the one hand, MLE is a well-defined measure as improvements in held-out scores generally produce improvements in the visual fidelity of the samples. On the other hand, as opposed to for example adversarial methods <ref type="bibr" target="#b0">(Arora &amp; Zhang, 2017)</ref>, MLE forces the model to support the entire empirical distribution. This guarantees the model's ability to generalize at the cost of allotting capacity to parts of the distribution that are irrelevant to fidelity. A second source of difficulties arises from the high dimensionality of large images. A 256 × 256 × 3 image has a total of 196,608 positions that need to be architecturally connected in order to learn dependencies among them; the representations at each position require sufficient capacity to express their respective surrounding contexts. These requirements translate to large amounts of memory and computation.  <ref type="figure">Figure 1</ref>: A representation of Multidimensional Upscaling. Left: depth upscaling is applied to a generated 3-bit 256 × 256 RGB subimage from CelebAHQ to map it to a full 8-bit 256 × 256 RGB image. Right: size upscaling followed by depth upscaling are applied to a generated 3-bit 32 × 32 RGB subimage from ImageNet to map it to the target resolution of the 8-bit 128 × 128 RGB image. We stress that the rightmost column of both figures are true unconditional samples from our model at full 8bit depth.</p><p>These difficulties notwithstanding, we aim to learn the full distribution over 8-bit RGB images of size up to 256 × 256 well enough so that the samples have high fidelity. We aim to guide the model to focus first on visually more salient bits of the distribution and later on the visually less salient bits. We identify two visually salient subsets of the distribution: first, the subset determined by sub-images ("slices") of smaller size (e.g. 32 × 32) sub-sampled at all positions from the original image; and secondly, the subset determined by the few (e.g. 3) most significant bits of each RGB channel in the image. We use Multidimensional Upscaling to map from one subset of the distribution to the other one by upscaling images in size or in depth. For example, the generation of a 128 × 128 8-bit RGB image proceeds by first upscaling it in size from a 32 × 32 3-bit RGB image to a 128 × 128 3-bit RGB image; we then upscale the resulting image in depth to the original resolution of the 128 × 128 8-bit RGB image. We thus train three networks: (a) a decoder on the small size, low depth image slices subsampled at every n pixels from the original image with the desired target resolution; (b) a size-upscaling decoder that generates the large size, low depth image conditioned on the small size, low depth image; and (c) a depth-upscaling decoder that generates the large size, high depth image conditioned on the large size, low depth image. <ref type="figure">Figure 1</ref> illustrates this process.</p><p>To address the latter difficulties that ensue in the training of decoders (b) and (c), we develop the Subscale Pixel Network (SPN) architecture. The SPN divides an image of size N ×N into sub-images of size N S × N S sliced out at interleaving positions (see <ref type="figure" target="#fig_1">Figure 2</ref>), which implicitly also captures a form of size upscaling. The N × N image is generated one slice at a time conditioned on previously generated slices in a way that encodes a rich spatial structure. SPN consists of two networks, a conditioning network that embeds previous slices and a decoder proper that predicts a single target slice given the context embedding. The decoding part of the SPN acts over image slices with the same spatial structure and it can share weights for all of them. The SPN is an independent image decoder with an implicit size upscaling mechanism, but it can also be used as an explicit size upscaling network by initializing the first slice of the SPN input at sampling time with one generated separately during step (a).</p><p>We extensively evaluate the performance of SPN and the size and depth upscaling methods both quantitatively and from a fidelity perspective on two unconditional image generation benchmarks, CelebAHQ-256 and ImageNet of various sizes up to 256. From a MLE scores perspective, we compare with previous work to obtain state-of-the-art results on CelebAHQ-256, both at full 8-bit resolution and at the reduced 5-bit resolution <ref type="bibr" target="#b7">(Kingma &amp; Dhariwal, 2018)</ref>, and on ImageNet-64. We also establish MLE baselines for ImageNet-128 and ImageNet-256. From a sample fidelity perspective, we show the strong benefits of multidimensional upscaling as well as the benefits of the SPN. We produce CelebAHQ-256 samples (at full 8-bit resolution) that are of similar visual fidelity to those produced with methods such as GANs that lack however an intrinsic measure of generalization <ref type="bibr" target="#b10">(Mescheder, 2018;</ref><ref type="bibr" target="#b6">Karras et al., 2017)</ref>. We also produce some of the first successful samples on unconditional ImageNet-128 (also at 8-bit) showing again the striking impact of the SPN and of multidimensional upscaling on sample quality and setting a fidelity baseline for future methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CONVENTIONAL GENERATION ORDERING</head><p>A standard AR image model such as the PixelCNN (van den Oord et al., 2016b) generates an HxW colour image starting at the top-left position and ending at the bottom-right position, fully generating the three 8-bit channels of each pixel in a given position:</p><formula xml:id="formula_0">P (x) = H h=1 W w=1 {R,G,B} c P (x h,w,c |x &lt; )<label>(1)</label></formula><p>where x &lt; corresponds to all previously generated intensity values in the ordering and h, w, and c are row, column, and colour channel indices. The raster scan ordering <ref type="figure" target="#fig_2">(Figure 3(a)</ref>) is conventionally used in AR models. Each conditional distribution P (x h,w,c |x &lt; ) is parametrized by a deep neural network (van den Oord et al., 2016b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SUBSCALE ORDERING IN IMAGES</head><p>We define an alternative ordering that divides a large image into a sequence of equally sized slices and has various core properties. First, it makes it easy to compactly encode long-range dependencies across the many pixels in the large images. It also induces a spatial structure over the original image by aligning the subsampled slices; this also has an implicit size upscaling side effect. From the perspective of the neural architecture, it makes it possible for the same decoder within the SPN to be consistently applied to all slices, since they are structurally similar; the smaller slices also allow for self-attention <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> in the SPN to be used without local contexts . We think of the ordering as the two-dimensional analogue of the one-dimensional subscale ordering introduced in .</p><p>The subscale ordering is defined as follows:</p><formula xml:id="formula_1">P (x) = S i=1 S j=1 H/S h=1 W/S w=1 {R,G,B} c P (x i+S * h,j+S * w,c |x &lt; )<label>(2)</label></formula><p>where x &lt; corresponds to all previously generated intensity values according to this ordering. <ref type="figure" target="#fig_2">Figure  3</ref>(d) illustrates the subscale ordering. A scaling factor S is selected and each slice of size H/S ×W/S is obtained by selecting a pixel every S pixels in both height and width; there are thus S 2 interleaved slices in the original image, with each specified by its row and column offset (i, j). We sometimes refer to this offset as the "meta-position" of a slice.   <ref type="bibr" target="#b12">(Reed et al., 2017)</ref>. The Subscale ordering alone, with size-only, depth-only and with multidimensional upscaling are, respectively, in blocks (d), (e), (f) and (g).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SIZE UPSCALING IN SUBSCALE ORDERING</head><p>The subscale ordering itself already captures size upscaling implicitly. Analogous to the multi-scale ordering (van den Oord et al., 2016b), and depicted in 3(b), we can perform size upscaling explicitly, by training a single slice decoder on subimages and generate the first slice of a subscale ordering from the single slice decoder itself. The rest of the image is then generated according to the subscale ordering by the main network (see 3(e)). The single-slice model can be trained on just the first slices of images, or on slices at all positions in all images given the shared spatial structure among the slices. For this reason, the same SPN that captures the subscale ordering can act simultaneously as a full-blown image model as well as a size upscaling model if initialized with the outputs of a single-slice decoder. A separate formulation of size upscaling is the Parallel Multi-Scale <ref type="bibr" target="#b12">(Reed et al., 2017)</ref> ordering where the pixels in an image are doubled at every stage by distinct neural networks and are generated in parallel without sequentiality (3(c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">DEPTH UPSCALING</head><p>Multidimensional upscaling applies upscaling not just in the height and width of the image, but also in the remaining dimension that is channel depth. This is performed in stages such that a network first generates the d 1 most significant bits of an image using a conventional or subscale ordering; then a second network generates the next d 2 most significant bits of the image conditioned on all the d 1 bits of the image; and so on to further stages. Using the conventional ordering as basis, the first stage of depth upscaling looks as follows:</p><formula xml:id="formula_2">P (x :d1 ) = H h=1 W w=1 {R,G,B} c P (x :d1 h,w,c |x :d1 &lt; )<label>(3)</label></formula><p>Next, a second stage of depth upscaling has the following form, conditioned on the first d 1 bits of each channel:</p><formula xml:id="formula_3">P (x d1:d2 ) = H h=1 W w=1 {R,G,B} c P (x d1:d2 h,w,c |x d1:d2 &lt; , x :d1 )<label>(4)</label></formula><p>We do not share weights among the networks at different stages of depth upscaling. We note that in depth upscaling bits of lower significance are only generated when the more significants bits at all positions have been generated in a previous stage. Just like for size upscaling from the previous  section, the goal of multidimensional upscaling is to let the model focus on visually salient bits of an image unaffected by less salient and less predictable bits of the image. Depth upscaling is related to the method underlying the Grayscale PixelCNN that models 4-bit greyscale images subsampled from colored images <ref type="bibr" target="#b8">Kolesnikov &amp; Lampert (2016a</ref>   construct a representation of the generated context for each dimension of each pixel. Existing AR approaches inherently require an amount of computation and memory that is superlinear in the number of pixels. In particular, the quadratic memory requirements of self-attention become severely limiting for images larger than 32 × 32 and intractable in practice for the 196,608 distinct positions we consider in a 256 × 256 colour image.</p><p>Mitigating the memory requirements and computational requirements of encoding the dependencies amongst so many variables often comes at the expense of global context. Modeling choices such as cropping images within the decoders <ref type="bibr" target="#b4">(Kalchbrenner et al., 2016)</ref> or performing self-attention over local neighborhoods  neglect global dependencies, while model parallelism, though technically feasible with the joint use of a very large number of accelerators, does not overcome the challenges in learning the global structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SUBSCALE PIXEL NETWORK</head><p>To address these challenges, we devise the Subscale Pixel Network (SPN), an architecture that embodies the subscale ordering from Section 2.2. For an image of size H × W × 3 × D, where D is the number of bits used for the current generation stage, one first chooses a scaling factor S and obtains the S 2 slices of the original image of size H/S × W/S × 3 × D. We use H = W = 256 and S = 8 as well as H = W = 128 and S = 4, for the larger images we process, so that the slices have size 32 × 32 × 3 × D. This scheme of choosing S such that slices are always 32 × 32 renders the memory and computation requirements effectively constant as the true image size H × W changes.</p><p>The SPN architecture is composed of two parts: an embedding part for slices at preceding metapositions that conditions the decoder for the current slice that is being generated <ref type="figure" target="#fig_3">(Figure 4 (a)</ref>). The embedding part is a convolutional neural network with residual blocks that takes as input preceding slices that are concatenated along the depth dimension. One detail is the way the slices are ordered along the channel dimension when concatenated. As illustrated in <ref type="figure" target="#fig_3">Figure 4 (a)</ref>, empty padding slices are used to preserve relative meta-positions of each preceding slice with respect to the current target slice. For example, the slice above any target slice in the two-dimensional meta-grid is always aligned in the same position along the depth axis in the input. This achieves equivariance in the embedding architecture with respect to the (i, j) offset of a slice. The padding slices also ensure that the depth of the input slice tensor remains the same for all target slices. In addition to the slice tensor, the embedding part also receives as input the meta-position of the target slice as an embedding of 8 units tiled spatially across the slice tensor. The pixel intensity values are also embedded as one-hot indices of size 8. The context embedding network passes its input through a series of self-attention layers and then a series of residual blocks, finally emitting a slice-sized feature map s that summarizes the context for the decoder.</p><p>The decoder takes as input the encoded slice tensor s in a position-preserving manner: each position in the target slice is given as input the encoded representations of pixels at that same position in the preceding slices. In addition it processes the target slice in the raster-scan order. The decoder that we use is a hybrid architecture combining masked convolution and self-attention <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>. We employ an initial 1D self-attention network <ref type="bibr" target="#b16">Vaswani et al. (2017)</ref> that is used to gather the entire available context in the slice (see <ref type="figure" target="#fig_3">Figure 4(b)</ref>). The slice is reshaped into a 1D tensor before it is given as input to masked 1D self-attention layers; the masking is performed over the previous pixels only (as opposed to over the current RGB channels) in the self-attention layers. Then the output of the layers is reshaped back into a 2D tensor, concatenated depth-wise with the output of the slice embedding network, and given as conditioning input to a Gated PixelCNN as in Equation 5 in van den <ref type="bibr" target="#b15">Oord et al. (2016c)</ref>. The PixelCNN network models the target slice with full masking over pixels and channel dimensions. We can see how memory requirements are significantly lower -up to S 2 = 64× lower with S = 8 -due to the smaller spatial size of the slices and their compact concatenation along the channel dimension of the input tensor. Due to this structure, the entire previously generated context is captured at each position of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LEARNING</head><p>The log-likelihood derived from equation 2 decomposes as a sum over slices. An unbiased estimator of the log-loss is obtained by uniformly sampling a choice of target slice and evaluating its logprobability conditioned upon previous slices as depicted in <ref type="figure" target="#fig_3">Figure 4 (a)</ref>. We perform maximum likelihood learning by doing stochastic gradient descent on this Monte Carlo estimate, with all gradients computed by backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MULTIDIMENSIONAL UPSCALING WITH SPN</head><p>As seen in Section 2.3, the SPN naturally serves as a size-upscaling network when the first slice of the input tensor is initialized with with an externally generated subimage. In our experiments, we ensure that the smaller subimages used for the initialization and those used in the training of the SPN decoder are identical to each other.</p><p>Analogously, the SPN can be used to upscale the depth of the channels of an image. The image to be upscaled in depth is itself divided into slices by the subscale method (secion 2) and the slices are then ImageNet 32x32 ImageNet 64x64 Gated PixelCNN (van den Oord et al., 2016c) 3.83 3.57 Parallel Multiscale <ref type="bibr" target="#b12">(Reed et al., 2017)</ref> 3.95 3.70 PixelSNAIL <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> 3.80 -Image Transformer  3.  concatenated along the channel dimension into a slice tensor for the conditioning image x :d1 . The latter is then added as a fixed additional input to the embedding part of the SPN in order to model P (x d1:d2 h,w,c |x d1:d2 &lt; , x :d1 ). The model of P (x :d1 ) is a normal SPN, but trained on data with low bit depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We demonstrate experimentally that our model is capable of high fidelity samples at high resolution, producing unconditional CelebA-HQ samples of quality better than the Glow model <ref type="bibr" target="#b7">(Kingma &amp; Dhariwal, 2018)</ref> and improving the MLE scores. Furthermore, we show that these results extend to high-resolution ImageNet images, with state-of-the-art log-likelihoods at 128x128 by a large margin and the first benchmark on 256x256 ImageNet. Unconditional samples at these resolutions are characterized by unprecedented global coherence.</p><p>Because our networks operate on small images (32 × 32 slices), we can train large networks both in terms of the number of hidden units and in terms of network depth (see Appendix C for details of sizes). The context-embedding network contains 5 convolutional layers and 6-8 self-attention layers depending on the dataset. The masked decoder consists of a PixelCNN with 15 layers in all experiments. The 1D Transformer in the decoder <ref type="figure" target="#fig_3">(Figure 4(b)</ref>) has between 8 and 10 layers depending on the dataset. See <ref type="table">Table 4</ref> for all dataset-specific hyperparameter details. 4.1 DOWNSAMPLED IMAGENET AT 32 × 32 AND 64 × 64</p><p>We first benchmark the performance of our hybrid decoder alone (i.e. no subscaling, <ref type="figure" target="#fig_3">Figure 4(b)</ref>) and show that it compares favorably to state of the art models on 32 × 32 Downsampled ImageNet (see <ref type="table">Table 1</ref>). We find that SPN hurts in this low-resolution setting with S = 2 and even further with S = 4. This is likely because the size of the resulting image slices becomes very small and the image coarse grained.</p><p>On 64 × 64 Downsampled ImageNet, we achieve a state of the art log-likelihood of 3.52 bits/dim. We hypothesize that PixelSNAIL would achieve a similar score, but results at this resolution were not reported in <ref type="bibr" target="#b1">Chen et al. (2017)</ref>. At this resolution, SPN scores similarly with 3.53 bits/dim. The improvement over Glow in the 5-bit setting is very significant (  For these experiments we use the standard ILSVRC Imagenet dataset <ref type="bibr" target="#b9">(Kolesnikov &amp; Lampert, 2016b)</ref> resized with Tensorflow's resize area function. Parallel Multiscale PixelCNN <ref type="bibr" target="#b12">(Reed et al., 2017</ref>) is the only model in the literature which reports log-likelihood on 128 × 128 ImageNet. SPN improves the log-likelihood over this model from 3.55 bits/dim to 3.08 bits/dim (see <ref type="table" target="#tab_4">Table 2</ref>). <ref type="figure" target="#fig_4">Figure 5</ref> gives 128 × 128 8-bit ImageNet samples for both the setting of depth upscaling only and of complete multidimensional upscaling. These settings do not affect the NLL, but the samples with depth upscaling show significant semantic coherence that is usually lacking in samples without upscaling.</p><p>In addition, multidimensional upscaling seems to increase the overall rate of success of the samples. Additional intermediate ImageNet samples can be seen in <ref type="figure">Figures 10, 11</ref> and 12 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CELEBAHQ</head><p>At 256 × 256 we can produce high-fidelity samples of celebrity faces from the CelebAHQ dataset. The quality compares favorably to the samples of other models such as Glow and GANs <ref type="bibr" target="#b6">(Karras et al., 2017)</ref>. We show in <ref type="table" target="#tab_5">Table 3</ref> that the achieved MLE scores are a significant improvement over previously reported scores. <ref type="figure" target="#fig_5">Figure 6</ref> showcases some samples for 8-bit CelebAHQ-256. <ref type="figure">Figure 7</ref> in the Appendix includes 5-bit samples, <ref type="figure">Figure 8</ref> includes 3-bit samples while <ref type="figure" target="#fig_6">Figure 9</ref> includes 3-bit samples with the temperature of the output distribution set to 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The problem of whether it is possible to learn the distribution of complex natural images and attain high sample fidelity has been a long-standing one in the tradition of generative models. The SPN and Multidimensional Upscaling model that we introduce accomplishes a large step towards solving this problem, by attaining both state-of-the-art MLE scores on large-scale images from complex domains such as CelebAHQ-256 and ImageNet-128 and by being able to generate high fidelity full 8-bit samples from the resulting learnt distributions without alterations to the sampling process (via e.g. heavy modifications of the temperature of the output distribution). The generated samples show an unprecedented amount of semantic coherence and exactness of details even at the large scale size of full 8-bit 128 × 128 and 256 × 256 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>In some cases for purposes of analysis the entropy of the softmax output distributions has been artificially reduced via a "temperature" divisor on the predicted logits. When we say the temperature is 0.95, we mean that the logits of a trained model have been divided by this constant at sampling time.  <ref type="table">Table 4</ref> for all the detailed hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C</head><p>Our experiments operate at a fairly large scale in terms of both the amount of compute used and the size of the networks. We proportionately increase the batch size so that the number of pixels in a batch is not affected by the subscaling. These large batch sizes (a maximum of 2048) are achieved by increasing the degree of data parallelism by running on Google Cloud TPU pods <ref type="bibr">(Jouppi et al., 2017)</ref>. For Imagenet 32 we used 64 tensorcores. For ImageNet 64, 128 and 256, we use 128 tensorcores. The fast interconnect between these devices affords much faster synchronous gradient computation than would be possible using the same number of GPUs. When overfitting is a problem, as in small datasets like CelebA-HQ, we rather decrease the batch size and use a lower number of 32 tensorcores.</p><p>Our SPN architectures have between ∼50M and ∼250M parameters depending on the dataset. See <ref type="table">Table 4</ref> for the number of parameters in the SPN architecture for each dataset. Depth-upscaling <ref type="figure">Figure 8</ref>: 256x256 CelebA-HQ 3bit samples from SPN doubles the number of parameters due to using two separate networks with untied weights. Sizeupscaling adds more parameters still for the separate decoder-only network which models the first slice as seen in <ref type="figure" target="#fig_2">Figure 3 (g)</ref>. Thus the maximal number of parameters used to generate a sample in the paper occurs in the multidimensional upscaling setting for ImageNet 128, where the total parameter count reaches ∼650M (the decoder-only network used to model the first slice has ∼150M parameters). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The receptive field in a Subscale Pixel Networks (a) and the four image slices subsampled from the original image (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Different generation ordering schemes, where the numbers indicate the step-by-step order. Distinct colors correspond to distinct neural networks. (a) and (b) are from (van den Oord et al., 2016b). (c) is from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) The architecture of a Subscale Pixel Network, with a conditional and a decoding part. (b) Scheme of the parts in the decoder itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Left: 8-bit 128x128 RGB ImageNet samples from SPN with depth upscaling only. Right: 8-bit 128x128 RGB ImageNet samples from SPN with full-blown Multidimensional Upscaling. Temperature is 0.99 for the initial 32x32 sub-image and otherwise 1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>: 8-bit 256x256 RGB CelebA-HQ samples from SPN with Depth-Upscaling. Temperature is 0.99 for the low-bit-depth image and otherwise 1.0 Figure 7: 256x256 CelebA-HQ 5bit samples from SPN APPENDIX B Please see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>256x256 CelebA-HQ 3bit samples from SPN with temperature 0.95 Figure 10: 128x128 ImageNet 3bit; upscaled 32x32 slices Figure 11: 128x128 ImageNet 3bit samples from model trained on 32x32 slices Figure 12: 128x128 ImageNet 3bit samples from SPN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>NLL scores for high-resolution Imagenet in bits/dim. The parenthesized numbers in Depth Upscaling rows indicate NLL for P (x :d1 ) and P (x d1:d2 h,w,c |x d1:d2</figDesc><table /><note>&lt; , x :d1 ) respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>).</figDesc><table><row><cell></cell><cell cols="2">ImageNet 64 x 64 (5bit) CelebA-HQ 256 x 256 (5bit)</cell></row><row><cell cols="2">Glow (Kingma &amp; Dhariwal, 2018) 1.76</cell><cell>1.03</cell></row><row><cell>SPN</cell><cell>1.41</cell><cell>0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Negative Log-likelihood scores for 5-bit datasets in bits/dim. 4.2 IMAGENET AT 128 × 128</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>We would like to thank Alex Graves, Karen Simonyan, Aaron van den Oord, Tim Harley, Sander Dieleman, Tim Salimans, Lasse Espeholt, Ali Razavi, Jeffrey De Fauw and Andy Brock for insightful discussions. In particular we wish to thank Andriy Mnih for formative ideas about autoregressivity in the bit-depth of an image.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageNet <ref type="bibr">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> CelebA HQ Optimization batch size <ref type="bibr">(1024,</ref><ref type="bibr">2048,</ref><ref type="bibr">2048,</ref><ref type="bibr">2048)</ref> 256 learning rate <ref type="bibr">(sched, sched, 1e-5, 1e-5)</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do gans actually learn the distribution? an empirical study. CoRR, abs/1706.08224</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.08224" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pixelsnail: An improved autoregressive generative model. CoRR, abs/1712.09763</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.09763" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bhatia ; Rajendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulland</surname></persName>
		</author>
		<imprint>
			<pubPlace>Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami; Robert Hagmann, Richard C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nix</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04760" />
		<editor>Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon</editor>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Andy Swing</publisher>
			<pubPlace>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg</pubPlace>
		</imprint>
	</monogr>
	<note>datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.00527</idno>
		<ptr target="http://arxiv.org/abs/1610.00527" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1802.08435</idno>
		<ptr target="http://arxiv.org/abs/1802.08435" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.10196" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno>abs/1807.03039</idno>
		<ptr target="https://arxiv.org/abs/1807.03039" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep probabilistic modeling of natural images using a pyramid decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno>abs/1612.08185</idno>
		<ptr target="http://arxiv.org/abs/1612.08185" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep probabilistic modeling of natural images using a pyramid decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno>abs/1612.08185</idno>
		<ptr target="http://arxiv.org/abs/1612.08185" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the convergence properties of GAN training. CoRR, abs/1801.04406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.04406" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Image transformer. CoRR, abs/1802.05751</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1703.03664</idno>
		<ptr target="http://arxiv.org/abs/1703.03664" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Parallel multiscale autoregressive density estimation. CoRR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<ptr target="http://arxiv.org/abs/1609.03499" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<ptr target="http://arxiv.org/abs/1601.06759" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conditional image generation with pixelcnn decoders. CoRR, abs/1606.05328</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.05328" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attention is all you need. CoRR, abs/1706.03762</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<ptr target="http://arxiv.org/abs/1803.07416" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144" />
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
