<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolu</forename><surname>Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
							<email>wuhefeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-label image classification is a fundamental yet practical task in computer vision, as real-world images gen- * Corresponding author is Hefeng Wu. This work was supported in Label distribution · · · · · · · · · · · · kite · · · Semantic-Specific Graph Representation Learning <ref type="figure">Figure 1</ref>. Illustration of our Semantic-Specific Graph Representation Learning framework. It incorporates category semantics to guide learning semantic-specific representation via a semantic decoupling module and explores their interactions via a semantic interaction module. erally contain multiple diverse semantic objects. Recently, it is receiving increasing attention <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, since it underpins plenty of critical applications in content-based image retrieval and recommendation systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>. Besides handling the challenges of complex variations in viewpoint, scale, illumination and occlusion, predicting the presence of multiple labels further requires mining semantic object regions as well as modeling the associations and interactions among these regions, rendering multi-label image classification an unsolved and challenging task.</p><p>Current methods for multi-label image classification usually employ object localization techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> or resort to visual attention networks <ref type="bibr" target="#b33">[34]</ref> to locate semantic object regions. However, object localization techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref> have to search numerous category-agnostic and redundant proposals and can hardly be integrated into deep neural networks for end-to-end training, while visual attention networks can merely locate object regions roughly due to the lack of supervision or guidance. Some other works introduce RNN/LSTM <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2]</ref> to further model contextual dependencies among semantic regions and capture label dependencies. However, the RNN/LSTM sequentially models regions/labels dependencies, which cannot fully exploit this property since direct association exists between each region or label pair. Besides, they do not explicitly model the statistical label co-occurrence, which is also key to aid multi-label image classification.</p><p>To address these issues, we propose a novel Semantic-Specific Graph Representation Learning (SSGRL) framework that incorporates category semantics to guide learning semantic-specific features and explore their interactions to facilitate multi-label image classification. More specifically, we first design a semantic decoupling module that utilizes the semantic features of the categories to guide learning category-related image features that focus more on the corresponding semantic regions (see <ref type="figure">Figure 1</ref>). Then, we construct a graph based on the statistical label co-occurrence to correlate these features and explore their interactions via a graph propagation mechanism. <ref type="figure">Figure 1</ref> illustrates a basic pipeline of the proposed SSGRL framework.</p><p>The contributions can be summarized into three folds: 1) We formulate a novel Semantic-Specific Graph Representation Learning framework that better learns semanticspecific features and explores their interactions to aid multilabel image recognition. 2) We introduce a novel semantic decoupling module that incorporates category semantics to guide learning semantic-specific features. 3) We conduct experiments on various benchmarks including PAS-CAL VOC 2007 &amp; 2012 <ref type="bibr" target="#b6">[7]</ref>, Microsoft-COCO <ref type="bibr" target="#b18">[19]</ref>, and Visual Genome with larger scale categories <ref type="bibr" target="#b15">[16]</ref> and demonstrate that our framework exhibits obvious performance improvement. Specifically, it improves the mAP from 92.5% to 95.0% and 92.2% to 94.8% on the Pascal VOC 2007 and 2012 dataset respectively, from 77.1% to 83.8% on the Microsoft-COCO dataset, from 33.5% to 36.6% on the Visual Genome 500 dataset compared with current state-ofthe-art methods. By simply pre-training on the Microsoft-COCO dataset and fusing two scale results, our framework can further boost the mAP to 95.4% on the Pascal VOC 2012 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recent progress on multi-label image classification relies on the combination of object localization and deep learning techniques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. Generally, they introduced object proposals <ref type="bibr" target="#b34">[35]</ref> that were assumed to contain all possible foreground objects in the image and aggregated features extracted from all these proposals to incorporate local information. Although these methods achieved notable performance improvement, the step of region candidate localization usually incurred redundant computation cost and prevented the model from end-to-end training with deep neural networks. Zhang et al. <ref type="bibr" target="#b32">[33]</ref> further utilized a learning based region proposal network and integrated it with deep neural networks. Although this method could be jointly optimized, it required additional annotations of bounding boxes to train the proposal generation component. To solve this issue, some other works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> resorted to attention mechanism to locate the informative regions, and these methods could be trained with image level annotations in an endto-end manner. For example, Wang et al. <ref type="bibr" target="#b25">[26]</ref> introduced spatial transformer to adaptively search semantic-aware regions and then aggregated features from these regions to identify multiple labels. However, due to the lack of supervision and guidance, these methods could merely locate the regions roughly.</p><p>Modeling label dependencies can help capture label cooccurrence, which is also key to aid multi-label recognition. To achieve this, a series of works introduced graphic models, such as Conditional Random Field <ref type="bibr" target="#b7">[8]</ref>, Dependency Network <ref type="bibr" target="#b9">[10]</ref>, or co-occurrence matrix <ref type="bibr" target="#b28">[29]</ref> to capture pairwise label correlations. Recently, Wang et al. <ref type="bibr" target="#b23">[24]</ref> formulated a CNN-RNN framework that utilized the semantic redundancy and the co-occurrence dependency implicitly to facilitate effective multi-label classification. Some works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2]</ref> further took advantage of proposal generation/visual attention mechanism to search local discriminative regions and LSTM <ref type="bibr" target="#b12">[13]</ref> to explicitly model label dependencies. For example, Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed a recurrent attention reinforcement learning framework to iteratively discover a sequence of attentional and informative regions, and modeled long-term dependencies among these attentional regions that help to capture semantic label cooccurrence. However, the RNN/LSTM <ref type="bibr" target="#b12">[13]</ref> modeled the label dependencies in a sequential manner, and they could not fully exploit the property since mutual dependency might exist between each label pair.</p><p>Different from all these methods, our framework incorporates category semantics to guide learning semanticaware feature vectors. In addition, we directly correlate all label pairs in the form of a structured graph and introduce a graph propagation mechanism to explore their mutual interactions under the explicit guidance of statistical label cooccurrence. Thus, our framework can better learn categoryrelated features and explore their interactions, leading to evident performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SSGRL Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In this section, we first give an overall description of the proposed SSGRL framework that consists of two crucial modules, i.e, semantic decoupling and semantic interaction. Given an image, we first feed it into a fully convolutional network to generate its feature maps. Then, for each category, the semantic decoupling module incorporates the category semantics to guide learning semantic-specific rep-resentations that focus on the semantic regions of this category. Finally, the semantic interaction module correlates these representations using a graph that is constructed based on the statistical label co-occurrence, and it explores the semantic interactions using a graph propagation network to further learn contextualized features, which are then used to predict the final label distribution. <ref type="figure">Figure 2</ref> illustrates a detailed pipeline of the SSGRL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic Decoupling</head><p>The semantic decoupling module aims to learn semanticspecific feature representation by taking the category semantics as guidance. Here, we adopt a semantic guided attention mechanism to implement this module.</p><p>Given an input image I, the framework first extracts its feature maps f I ∈ R W ×H×N , where W , H, and N are the width, height and channel number of the feature maps, formulated as</p><formula xml:id="formula_0">f I = f cnn (I),<label>(1)</label></formula><p>where f cnn (·) is a feature extractor, and it is implemented by a fully convolutional network. For each category c, the framework extracts a d s -dimensional semantic-embedding vector using the pre-trained GloVe <ref type="bibr" target="#b20">[21]</ref> model</p><formula xml:id="formula_1">x c = f g (w c ),<label>(2)</label></formula><p>where w c is the semantic word of category c. Then, we introduce a semantic guided attention mechanism which incorporates the semantic vector x c to guide focusing more on the semantic-aware regions and thus learning a feature vector corresponding to this category. More specifically, for each location (w, h), we first fuse the corresponding image feature f I wh and x c using a low-rank bilinear pooling method <ref type="bibr" target="#b13">[14]</ref> f</p><formula xml:id="formula_2">I c,wh = P T tanh (U T f I wh ) (V T x c ) + b,<label>(3)</label></formula><p>where tanh(·) is the hyperbolic tangent function,</p><formula xml:id="formula_3">U ∈ R N ×d1 , V ∈ R ds×d1 , P ∈ R d1×d2 , b ∈ R d2</formula><p>are the learnable parameters, and is the element-wise multiplication operation. d 1 and d 2 are the dimensions of the joint embeddings and the output features. Then, an attentional coefficient is computed under the guidance of x c bỹ</p><formula xml:id="formula_4">a c,wh = f a (f I c,wh ).<label>(4)</label></formula><p>This coefficient indicates the importance of location (w, h). f a (·) is an attentional function and it is implemented by a fully connected network. The process is repeated for all locations. To make the coefficients easily comparable across different samples, we normalize the coefficients over all locations using a softmax function</p><formula xml:id="formula_5">a c,wh = exp(ã c,wh ) w ,h exp(ã c,w h ) .<label>(5)</label></formula><p>Finally, we perform weighted average pooling over all locations to obtain a feature vector</p><formula xml:id="formula_6">f c = w,h a c,wh f c,wh (6)</formula><p>that encodes the information related to category c. We repeat the process for all categories and obtain all the category-related feature vectors {f 0 , f 1 , . . . , f C−1 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Interaction</head><p>Once obtaining the feature vectors corresponding to all categories, we correlate these vectors in the form of a graph that is constructed based on the statistical label cooccurrence and introduce a graph neural network to propagate message through the graph to explore their interactions. Graph construction. We first introduce the graph G = {V, A}, in which nodes refer to the categories and edges refer to the co-occurrence between corresponding categories. Specifically, suppose that the dataset covers C categories, V can be represented as {v 0 , v 2 , . . . , v C−1 } with element v c denoting category c and A can be represented as {a 00 , a 01 , . . . , a 0(C−1) , . . . , a (C−1)(C−1) } with element a cc denoting the probability of the existence of object belonging to category c in the presence of object belonging to category c. We compute the probabilities between all category pairs using the label annotations of samples on the training set, thus we do not introduce any additional annotation.</p><p>Inspired by the current graph propagation works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3]</ref>, we adopt a gated recurrent update mechanism to propagate message through the graph and learn contextualized node-level features. Specifically, for each node v c ∈ V, it has a hidden state h t c at timestep t. In this work, as each node corresponds to a specific category and our model aims to explore the interactions among the semanticspecific features, we initialize the hidden state at t = 0 with the feature vector that relates to the corresponding category, formulated as</p><formula xml:id="formula_7">h 0 c = f c .<label>(7)</label></formula><p>At timestep t, the framework aggregates message from its neighbor nodes, expressed as</p><formula xml:id="formula_8">a t c = c (a cc )h t−1 c , c (a c c )h t−1 c .<label>(8)</label></formula><p>In this way, the framework encourages message propagation if node c has a high correlation with node c, and it suppresses propagation otherwise. Therefore, it can propagate message through the graph and explore node interactions under the guidance of the prior knowledge of statistical label co-occurrence. Then, the framework updates the hidden state based on the aggregated feature vector a t c and its hidden state at previous timestep h t−1 c via a gated mechanism similar to the Gated Recurrent Unit, formulated as</p><formula xml:id="formula_9">z t c =σ(W z a t c + U z h t−1 c ) r t c =σ(W r a t c + U r h t−1 c ) h t c = tanh Wa t c + U(r t c h t−1 c ) h t c =(1 − z t c ) h t−1 c + z t c h t c<label>(9)</label></formula><p>where σ(·) is the logistic sigmoid function, tanh(·) is the hyperbolic tangent function, and is the element-wise multiplication operation. In this way, each node can aggregate message from other nodes and simultaneously transfer its information through the graph, enabling interactions among all feature vectors corresponding to all categories. The process is repeated T times, and the final hidden states are generated, i.e., {h T 0 , h T 1 , . . . , h T C−1 }. Here, the hidden state of each node h T c not only encodes features of category c, but also carries contextualized message from other categories. Finally, we concatenate h T c and the input feature vector h 0 c to predict the confidence score of the presence of category c, formulated as</p><formula xml:id="formula_10">o c =f o (h T c , h 0 c ) s c =f c (o c )<label>(10)</label></formula><p>where f o (·) is an output function that maps the concatenation of h T c and h 0 c into an output vector o c . We adopt C classification functions with unshared parameters {f 0 , f 1 , . . . , f C−1 }, in which f c (·) takes o c as input to predict a score to indicate the probability of category c. We perform the process for all categories and obtain a score vector s = {s 0 , s 1 , . . . , s C−1 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>Following existing multi-label image classification works <ref type="bibr" target="#b33">[34]</ref>, we implement the feature extractor f cnn (·) based on the widely used ResNet-101 <ref type="bibr" target="#b10">[11]</ref>. Specifically, we replace the last average pooling layer with another average pooling layer with a size of 2×2 and a stride of 2, with other layers unchanged for implementation. For the low rank bilinear pooling operation, N , d s , d 1 , and d 2 are set as 2,048, 300, 1,024, and 1,024, respectively. Thus, f a (·) is implemented by a 1,024-to-1 fully connected layer that maps the 1,024 feature vector to one single attentional coefficient.</p><p>For the graph neural network, we set the dimension of the hidden state as 2,048 and the iteration number T as 3. The dimension of output vector o c is also set as 2,048. Thus, the output network o(·) can be implemented by a 4,096-to-2,048 fully connected layer followed by the hyperbolic tangent function, and each classification network f c (·) can be implemented by a 2,048-to-1 fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization</head><p>Given a dataset that contains M training samples</p><formula xml:id="formula_11">{I i , y i } M −1</formula><p>i=0 , in which I i is the i-th image and y i = {y i0 , y i1 , . . . , y i(C−1) } is the corresponding annotation. y ic is assigned as 1 if the sample is annotated with category c and 0 otherwise. Given an image I i , we can obtain a predicted score vector s i = {s i0 , s i1 , . . . , s i(C−1) } and compute the corresponding probability vector p i = {p i0 , p i1 , . . . , p i(C−1) } via a sigmoid function</p><formula xml:id="formula_12">p ic = σ(s ic ).<label>(11)</label></formula><p>We adopt the cross entropy as the objective loss function The proposed framework is trained with the loss L in an end-to-end fashion. Specifically, we first utilize the ResNet-101 parameters pre-trained on the ImageNet dataset <ref type="bibr" target="#b4">[5]</ref> to initialize the parameters of the corresponding layers in f cnn and initialize the parameters of other layers randomly. As the lower layers' parameters pre-trained on the ImageNet dataset generalize well across different datasets, we fix the parameters of the previous 92 convolutional layers in f cnn (·) and jointly optimize all the other layers. The framework is trained with ADAM algorithm <ref type="bibr" target="#b14">[15]</ref> with a batch size of 4, momentums of 0.999 and 0.9. The learning rate is initialized as 10 −5 and it is divided by 10 when the error plateaus. During training, the input image is resized to 640 × 640, and we randomly choose a number from {640, 576, 512, 384, 320} as the width and height to randomly crop patches. Finally, the cropped patches are further resized to 576 × 576. During testing, we simply resize the input image to 640 × 640 and perform center crop with a size of 576 × 576 for evaluation.</p><formula xml:id="formula_13">L = N −1 i=0 C−1 c=0 (y ic log p ic + (1 − y ic ) log(1 − p ic )) .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics</head><p>To fairly compare with existing methods, we follow them to adopt the average precision (AP) on each category and mean average precision (mAP) over all categories for evaluation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. We also follow previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17]</ref> to present the precision, recall, and F1-measure for further comparison. Here, we assign the labels with top-3 highest scores for each image and compare them with the ground truth labels. Concretely, we adopt the overall precision, recall, F1-measure (OP, OR, OF1) and per-class precision, recall, F1-measure (CP, CR, CF1), which are defined as below</p><formula xml:id="formula_14">OP = i N c i i N p i , OR = i N c i i N g i , OF1 = 2 × OP × OR OP + OR , CP = 1 C i N c i N p i CR = 1 C i N c i N g i CF1 = 2 × CP × CR CP + CR<label>(13)</label></formula><p>where C is the number of labels, N c i is the number of images that are correctly predicted for the i-th label, N p i is the number of predicted images for the i-th label, N g i is the number of ground truth images for the i-th label. The above metrics require a fixed number of labels, but the label numbers of different images are generally various. Thus, we further present the OP, OR, OF1 and CP, CR, CF1 metrics under the setting that a label is predicted as positive if its estimated probability is greater than 0.5 <ref type="bibr" target="#b33">[34]</ref>. Among these metrics, mAP, OF1, and CF1 are the most important metrics that can provide a more comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art</head><p>To prove the effectiveness of the proposed framework, we conduct extensive experiments on various widely used benchmarks, i.e., Microsoft COCO <ref type="bibr" target="#b18">[19]</ref>, Pascal VOC 2007 &amp; 2012 <ref type="bibr" target="#b6">[7]</ref>, and Visual Genome <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison on Microsoft COCO</head><p>Microsoft COCO <ref type="bibr" target="#b18">[19]</ref> is originally constructed for object detection and segmentation, and it has been adopted to evaluate multi-label image classification recently. The dataset contains 122,218 images and covers 80 common categories, which is further divided into a training set of 82,081 images and a validation set of 40,137 images. Since the ground truth annotations of test set are unavailable, our method and all existing competitors are trained on the training set and evaluated on the validation set. For the OP, OR, OF1 and CP, CR, CF1 metrics with top-3 constraint, we follow existing methods <ref type="bibr" target="#b23">[24]</ref> to exclude the labels with probabilities lower than a threshold (0.5 in our experiments).</p><p>The comparison results are presented in <ref type="table" target="#tab_0">Table 1</ref>. As shown, existing best-performing methods are RDAR and ResNet-SRN, in which RDAR adopts a spatial transformer to locate semantic-aware regions and an LSTM network to implicitly capture label dependencies, while ResNet-SRN builds on ResNet-101 and applies attention mechanism to model label relation. -  <ref type="table">Table 3</ref>. Comparison of AP and mAP in % of our model and state-of-the-art methods on the PASCAL VOC 2012 dataset. Upper part presents the results of single model and lower part presents those that aggregate multiple models. "Ours" and "Ours (pre)" denote our framework without and with pre-training on the COCO dataset. "Ours (pre &amp; fusion)" denotes fusing our two scale results. The best and second best results are highlighted in red and blue, respectively. Best viewed in color.</p><formula xml:id="formula_15">- - - - - - - - - - - - - - - - - - - 89.3 VGG19+SVM [22] - - - - - - - - - - - - - - - - - - - -</formula><p>ResNet-SRN. Different from these methods, our framework incorporates category semantics to better learn semanticspecific feature representations and explores their interactions under the explicit guidance of statistical label cooccurrence, leading to a notable performance improvement on all metrics. Specifically, it achieves the mAP, CF1, and OF1 of 83.8%, 72.7%, and 76.2%, improving those of the previous best methods by 6.7%, 5.3%, and 3.3%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison on Pascal VOC 2007 and 2012</head><p>Pascal <ref type="bibr">VOC 2007</ref><ref type="bibr">VOC &amp; 2012</ref> are the most widely used datasets to evaluate the multi-label image classification task, and most of the existing works report their results on these datasets. Therefore, we conduct experiments on these datasets for more comprehensive comparison. Both datasets cover 20 common categories. Thereinto, Pascal VOC 2007 contains a trainval set of 5,011 images and a test set of 4,952 images, while VOC 2012 consists of 11,540 images as trainval set and 10,991 as test set. For fair comparisons, the proposed framework and existing competitors are all trained on the trainval set and evaluated on the test set.</p><p>We first present the AP of each category and mAP over all categories on the Pascal VOC 2007 dataset in <ref type="table" target="#tab_2">Table 2</ref>. Most of existing state-of-the-art methods focus on locating informative regions (e.g., proposal candidates <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, attentive regions <ref type="bibr" target="#b25">[26]</ref>, random regions <ref type="bibr" target="#b24">[25]</ref>) to aggregate local discriminative features to facilitate recognizing multiple labels of the given image. For example, RCP achieves a mAP of 92.5%, which is the best result to date. Differently, our framework incorporates category semantics to better learn semantic-specific features and explores their interactions under the explicit guidance of statistical label dependencies, further improving the mAP to 93.4%. In addition, by pre-training the framework on the COCO dataset, our framework can obtain an even better performance, i.e., 95.0% as shown in <ref type="table" target="#tab_2">Table 2</ref>. Note that existing methods aggregate multiple models <ref type="bibr" target="#b21">[22]</ref> or fuse the result with other methods <ref type="bibr" target="#b29">[30]</ref> to improve the overall performance. For example, FeV+LV (fusion) aggregates its results with those of VGG16&amp;19+SVM, improving the mAP from 90.6% to 92.0%. Although our results are generated by a single model, it still outperforms all these aggregated results.  We also compare performance on the Pascal VOC 2012 dataset, as depicted in <ref type="table">Table 3</ref>. Although VOC 2012 is more challenging and larger in size, our framework still achieves the best performance compared with state-of-theart competitors. Specifically, it obtains the mAP of 93.9% and 94.8% without and with pre-training on the COCO dataset, improving over the previous best method by 1.7% and 2.6%, respectively. Similarly, existing methods also aggregate results of multiple models to boost the performance. To ensure fair comparison, we train another model with an input of 448 × 448. Specifically, during training, we resize the input image to 512 × 512, and we randomly choose a number from 512, 448, 384, 320, 256 as the width and height to randomly crop patches, and further resize the cropped patches to 448 × 448. We denote the previous model as scale-640 and this model as scale-512. The two models are all pre-trained on the COCO dataset and retrained on the VOC 2012 dataset. Then, We perform ten crop evaluation (the four corner crops and the center crop as well as their horizontally flipped versions) for each scale and aggregate results from the two scales. As shown in the lower part of <ref type="table">Table 3</ref>, our framework boosts the mAP to 95.4%, suppressing all existing methods with single and multiple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Comparison on Visual Genome 500</head><p>Visual Genome <ref type="bibr" target="#b15">[16]</ref> is a dataset that contains 108,249 images and covers 80,138 categories. Since most categories have very few samples, we merely consider the 500 most frequent categories, resulting in a VG-500 subset. We randomly select 10,000 images as the test set and the rest 98,249 images as the training set. Compared with existing benchmarks, it covers much more categories, i.e., 500 v.s. 20 on Pascal VOC <ref type="bibr" target="#b6">[7]</ref> and 80 categories on Microsoft- COCO <ref type="bibr" target="#b18">[19]</ref>. To demonstrate the effectiveness of our proposed framework on this dataset, we implement a ResNet-101 baseline network and train it using the same process as ours. As ResNet-SNR <ref type="bibr" target="#b33">[34]</ref> is the best-performing method on Microsoft-COCO dataset, we further follow its released code to train ResNet-SNR on this dataset for comparison. All the methods are trained on the training set and evaluated on the test set. The comparison results are presented in <ref type="table">Table 4</ref>. Our framework also performs much better than existing stateof-the-art and ResNet-101 baseline methods. Specifically, it achieves the mAP of 36.6%, improving that of the existing best method by 3.1%. This comparison clearly suggests that our framework can also work better on recognizing largescale categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablative study</head><p>The proposed framework builds on the ResNet-101 <ref type="bibr" target="#b10">[11]</ref>, thus we compare with this baseline to analyze the contributions of semantic-specific graph representation learning (SSGRL). Specifically, we simply replace the last fully connected layer of the ResNet-101 with a 2,048-to-C fully connected layer and use C sigmoid functions to predict the probability of each category. The training and test settings are exactly the same as those described in Section 3.5. We conduct experiments on the Microsoft-COCO dataset and present the results in formance comparisons, we further present the AP of each category in <ref type="figure" target="#fig_2">Figure 3</ref>. It shows that the AP improvement is more evident for the categories that are more difficult to recognize (i.e., the categories that the baseline obtains lower AP). For example, for the categories like giraffe and zebra, the baseline obtains very high AP, and our framework just achieves slight improvement. In contrast, for more difficult categories such as toaster and hair drier, our framework improves the AP by a sizeable margin, 24.7% and 32.5% improvement for toaster and hair drier, respectively. The foregoing comparisons verify the contribution of the proposed SSGRL as a whole. Actually, the SSGRL contains two critical modules that work cooperatively, i.e., semantic decoupling (SD) and semantic interaction (SI). In the following, we further conduct ablative experiments to analyze the actual contribution of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Contribution of semantic decoupling</head><p>We evaluate the contribution of SD module by comparing the performance with and without this module. To this end, we perform average pooling on f I to get image feature vector f , and use the following two settings to initialize the graph nodes: 1) directly use f (namely Ours w/o SD); 2) concatenate f and corresponding semantic vector (i.e., x c for the node corresponding to category c), which is mapped to a 2,048 feature vector for initialization (namely Ours w/o SD-concat). As shown in <ref type="table" target="#tab_5">Table 5</ref>, "Ours w/o SD" performs slightly better than the baseline method, since it does not incur any additional information but increases the model complexity. "Ours w/o SD-concat" performs slightly worse than the baseline and "Ours w/o SD". This suggests directly concatenating the semantic vector provide no additional or even interferential information.</p><p>As discussed above, our framework can learn semanticspecific feature maps that focus on corresponding semantic regions via the semantic decoupling. Here, we further visualize some examples in <ref type="figure" target="#fig_3">Figure 4</ref>. In each row, we present the input image, the semantic maps corresponding to categories with the top 3 highest confidences, and the predicted label distribution. It shows that our semantic decoupling module can well highlight the semantic regions if the ob-  jects of the corresponding categories exist. For example, the second example has objects of skis, snowboard, and person, our semantic decoupling module highlights the corresponding regions of two skis, snowboard and person leg. Similar phenomena are observed for other examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Contribution of semantic interaction</head><p>To validate the contribution of SI module, we remove the graph propagation network, and thus the classifier f c (·) directly takes the corresponding decoupled feature vector f c as input to predict the probability of category c (namely Ours w/o SI). As shown in <ref type="table" target="#tab_5">Table 5</ref>, we find that its mAP is 82.2%, decreasing the mAP by 1.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel Semantic-Specific Graph Representation Learning framework, in which a semantic guided attentional mechanism is designed to learn semantic-related feature vectors and a graph propagation network is introduced to simultaneously explore interactions among these feature vectors under the guidance of statistical label co-occurrence. Extensive experiments on various benchmarks including Microsoft-COCO, Pascal VOC 2007 &amp; 2012, and Visual Genome demonstrate the effectiveness of the proposed framework over all existing leading methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, 61876045, and U1811463, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), in part by the Natural Science Foundation of Guangdong Province under Grant No. 2017A030312006, and in part by Zhujiang Science and Technology New Star Project of Guangzhou under Grant No. 201906010057.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>… t = 1 GNNFigure 2 .</head><label>12</label><figDesc>Illustration of our Semantic-Specific Graph Representation Learning framework. Given an input image, we first feed it into a CNN to extract image representation. Then, a semantic decoupling (SD) module incorporates category semantics to guide learning semantic-specific representations, and a semantic interaction module correlates these representations using a graph and adopts a graph neural network (GNN) to explore their interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The AP (in %) of each category of our proposed framework and the ResNet-101 baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Several examples of input images (left), semantic feature maps corresponding to categories with top 3 highest confidences (middle), and predicted label distribution (right). The ground truth labels are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>SRN-att [34] 76.1 85.8 57.5 66.3 88.1 61.1 72.1 81.2 63.3 70.0 84.1 67.7 75.0 ResNet-SRN [34] 77.1 85.2 58.8 67.4 87.4 62.5 72.9 81.6 65.4 71.2 82.7 69.9 75.8 Ours 83.8 91.9 62.5 72.7 93.8 64.1 76.2 89.9 68.5 76.8 91.3 70.8 79.7 Comparison of mAP, CP, CR, CF1 and OP, OR, OF1 (in %) of our framework and state-of-the-art methods under the settings of all and top-3 labels on the Microsoft COCO dataset. "-" denotes the corresponding result is not provided.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">mAP CP</cell><cell cols="3">CR CF1 OP</cell><cell cols="3">OR OF1 CP</cell><cell cols="3">CR CF1 OP</cell><cell cols="2">OR OF1</cell></row><row><cell>WARP [9]</cell><cell>-</cell><cell cols="6">59.3 52.5 55.7 59.8 61.4 60.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN-RNN [24]</cell><cell>-</cell><cell cols="6">66.0 55.6 60.4 69.2 66.4 67.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RLSD [33]</cell><cell>-</cell><cell cols="6">67.6 57.2 62.0 70.1 63.4 66.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RARL [2]</cell><cell>-</cell><cell cols="6">78.8 57.2 66.2 84.0 61.6 71.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RDAR [26]</cell><cell cols="7">73.4 79.1 58.7 67.4 84.0 63.0 72.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KD-WSD [20]</cell><cell>74.6</cell><cell>-</cell><cell>-</cell><cell>66.8</cell><cell>-</cell><cell>-</cell><cell>72.7</cell><cell>-</cell><cell>-</cell><cell>69.2</cell><cell>-</cell><cell>-</cell><cell>74.0</cell></row><row><cell>ResNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The mAP, CF1, and OF1 are 73.4%, 67.4%, 72.0% by RDAR and 77.1%, 67.4%, 72.9% by Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP CNN-RNN [24] 96.7 83.1 94.2 92.8 61.2 82.1 89.1 94.2 64.2 83.6 70.0 92.4 91.7 84.2 93.7 59.8 93.2 75.3 99.7 78.6 84.0 RMIC [12] 97.1 91.3 94.2 57.1 86.7 90.7 93.1 63.3 83.3 76.4 92.8 94.4 91.6 95.1 92.3 59.7 86.0 69.5 96.4 79.0 84.5 VGG16+SVM [22]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>89.3 RLSD [32] 96.4 92.7 93.8 94.1 71.2 92.5 94.2 95.7 74.3 90.0 74.2 95.4 96.2 92.1 97.9 66.9 93.5 73.7 97.5 87.6 88.5 HCP [28] 98.6 97.1 98.0 95.6 75.3 94.7 95.8 97.3 73.1 90.2 80.0 97.3 96.1 94.9 96.3 78.3 94.7 76.2 97.9 91.5 90.9 FeV+LV [30] 97.9 97.0 96.6 94.6 73.6 93.9 96.5 95.5 73.7 90.3 82.8 95.4 97.7 95.9 98.6 77.6 88.7 78.0 98.3 89.0 90.<ref type="bibr" target="#b5">6</ref> Comparison of AP and mAP in % of our framework and state-of-the-art methods on the PASCAL VOC 2007 dataset. Upper part presents the results of single model and lower part presents those that aggregate multiple models. "Ours" and "Ours (pre)" denote our framework without and with pre-training on the COCO dataset. The best and second best results are highlighted in red and blue, respectively. "-" denotes the corresponding result is not provided. Best viewed in color.</figDesc><table><row><cell>RDAR [26]</cell><cell cols="2">98.6 97.4 96.3 96.2 75.2 92.4 96.5 97.1 76.5 92.0 87.7 96.8 97.5 93.8</cell><cell cols="2">98.5 81.6 93.7 82.8 98.6 89.3 91.9</cell></row><row><cell>RARL [2]</cell><cell cols="2">98.6 97.1 97.1 95.5 75.6 92.8 96.8 97.3 78.3 92.2 87.6 96.9 96.5 93.6</cell><cell cols="2">98.5 81.6 93.1 83.2 98.5 89.3 92.0</cell></row><row><cell>RCP [25]</cell><cell cols="2">99.3 97.6 98.0 96.4 79.3 93.8 96.6 97.1 78.0 88.7 87.1 97.1 96.3 95.4</cell><cell cols="2">99.1 82.1 93.6 82.2 98.4 92.8 92.5</cell></row><row><cell>Ours</cell><cell cols="2">99.5 97.1 97.6 97.8 82.6 94.8 96.7 98.1 78.0 97.0 85.6 97.8 98.3 96.4</cell><cell cols="2">98.8 84.9 96.5 79.8 98.4 92.8 93.4</cell></row><row><cell>Ours (pre)</cell><cell cols="2">99.7 98.4 98.0 97.6 85.7 96.2 98.2 98.8 82.0 98.1 89.7 98.8 98.7 97.0</cell><cell cols="2">99.0 86.9 98.1 85.8 99.0 93.7 95.0</cell></row><row><cell cols="3">VGG16&amp;19+SVM [22] 98.9 95.0 96.8 95.4 69.7 90.4 93.5 96.0 74.2 86.6 87.8 96.0 96.3 93.1</cell><cell cols="2">97.2 70.0 92.1 80.3 98.1 87.0 89.7</cell></row><row><cell>FeV+LV (fusion) [30]</cell><cell cols="2">98.2 96.9 97.1 95.8 74.3 94.2 96.7 96.7 76.7 90.5 88.0 96.9 97.7 95.9</cell><cell cols="2">98.6 78.5 93.6 82.4 98.4 90.4 92.0</cell></row><row><cell>Methods</cell><cell>aero bike bird boat bottle bus</cell><cell cols="2">car cat chair cow table dog horse mbike person plant sheep sofa train tv</cell><cell>mAP</cell></row><row><cell>RMIC [12]</cell><cell cols="2">98.0 85.5 92.6 88.7 64.0 86.8 82.0 94.9 72.7 83.1 73.4 95.2 91.7 90.8</cell><cell cols="2">95.5 58.3 87.6 70.6 93.8 83.0 84.4</cell></row><row><cell>VGG16+SVM [22]</cell><cell cols="2">99.0 88.8 95.9 93.8 73.1 92.1 85.1 97.8 79.5 91.1 83.3 97.2 96.3 94.5</cell><cell cols="2">96.9 63.1 93.4 75.0 97.1 87.1 89.0</cell></row><row><cell>VGG19+SVM [22]</cell><cell cols="2">99.1 88.7 95.7 93.9 73.1 92.1 84.8 97.7 79.1 90.7 83.2 97.3 96.2 94.3</cell><cell cols="2">96.9 63.4 93.2 74.6 97.3 87.9 89.0</cell></row><row><cell>HCP [28]</cell><cell cols="2">99.1 92.8 97.4 94.4 79.9 93.6 89.8 98.2 78.2 94.9 79.8 97.8 97.0 93.8</cell><cell cols="2">96.4 74.3 94.7 71.9 96.7 88.6 90.5</cell></row><row><cell>FeV+LV [30]</cell><cell cols="2">98.4 92.8 93.4 90.7 74.9 93.2 90.2 96.1 78.2 89.8 80.6 95.7 96.1 95.3</cell><cell cols="2">97.5 73.1 91.2 75.4 97.0 88.2 89.4</cell></row><row><cell>RCP [25]</cell><cell cols="2">99.3 92.2 97.5 94.9 82.3 94.1 92.4 98.5 83.8 93.5 83.1 98.1 97.3 96.0</cell><cell cols="2">98.8 77.7 95.1 79.4 97.7 92.4 92.2</cell></row><row><cell>Ours</cell><cell cols="2">99.5 95.1 97.4 96.4 85.8 94.5 93.7 98.9 86.7 96.3 84.6 98.9 98.6 96.2</cell><cell cols="2">98.7 82.2 98.2 84.2 98.1 93.5 93.9</cell></row><row><cell>Ours (pre)</cell><cell cols="2">99.7 96.1 97.7 96.5 86.9 95.8 95.0 98.9 88.3 97.6 87.4 99.1 99.2 97.3</cell><cell cols="2">99.0 84.8 98.3 85.8 99.2 94.1 94.8</cell></row><row><cell cols="3">VGG16&amp;19+SVM [22] 99.1 89.1 96.0 94.1 74.1 92.2 85.3 97.9 79.9 92.0 83.7 97.5 96.5 94.7</cell><cell cols="2">97.1 63.7 93.6 75.2 97.4 87.8 89.3</cell></row><row><cell>FeV+LV (fusion) [30]</cell><cell cols="2">98.9 93.1 96.0 94.1 76.4 93.5 90.8 97.9 80.2 92.1 82.4 97.2 96.8 95.7</cell><cell cols="2">98.1 73.9 93.6 76.8 97.5 89.0 90.7</cell></row><row><cell>HCP+AGS [28, 6]</cell><cell cols="2">99.8 94.8 97.7 95.4 81.3 96.0 94.5 98.9 88.5 94.1 86.0 98.1 98.3 97.3</cell><cell cols="2">97.3 76.1 93.9 84.2 98.2 92.7 93.2</cell></row><row><cell>RCP+AGS [25, 6]</cell><cell cols="2">99.8 94.5 98.1 96.1 85.5 96.1 95.5 99.0 90.2 95.0 87.8 98.7 98.4 97.5</cell><cell cols="2">99.0 80.1 95.9 86.5 98.8 94.6 94.3</cell></row><row><cell>Ours (pre &amp; fusion)</cell><cell cols="2">99.9 96.6 98.4 97.0 88.6 96.4 95.9 99.2 89.0 97.9 88.6 99.4 99.3 97.9</cell><cell cols="2">99.2 85.8 98.6 86.7 99.4 95.1 95.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>As can be observed, the mAP drops from 83.8% to 80.3%. To deeply analyze their per-</figDesc><table><row><cell>Methods</cell><cell>mAP</cell></row><row><cell>ResNet-101 [11]</cell><cell>80.3</cell></row><row><cell>Ours w/o SD</cell><cell>80.9</cell></row><row><cell cols="2">Ours w/o SD-concat 79.6</cell></row><row><cell>Ours w/o SI</cell><cell>82.2</cell></row><row><cell>Ours</cell><cell>83.8</cell></row><row><cell cols="2">Table 5. Comparison of mAP (in %) of our framework (Ours), our</cell></row><row><cell cols="2">framework without SD module (Ours w/o SD and Ours w/o SD-</cell></row><row><cell cols="2">concat) and our framework without SI module (Ours w/o SI) on</cell></row><row><cell>the Microsoft-COCO dataset.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-embedded representation learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent attentional reinforcement learning for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A concept-based image retrieval system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Keng</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Pung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee-Sen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on System Sciences</title>
		<meeting>the International Conference on System Sciences</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Subcategory-aware object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collective multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Ghamrawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 14th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Alexander Toshev, and Sergey Ioffe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
	</analytic>
	<monogr>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-label classification using conditional dependency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suicheng</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJ-CAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJ-CAI)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1300</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforced multi-label image classification by exploring curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving pairwise ranking for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1837" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly-supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04573</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond object proposals: Random crop pooling for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5678" to="5688" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep reasoning with knowledge graph for social relationship understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2021" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hcp: A flexible cnn framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1901" to="1907" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Correlative multi-label multi-instance image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pinterest board recommendation for twitter users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="963" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-label image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01082</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-label image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5513" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
