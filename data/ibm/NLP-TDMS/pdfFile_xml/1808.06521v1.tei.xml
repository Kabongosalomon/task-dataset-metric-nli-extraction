<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CU-Net: Coupled U-Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
							<email>xipeng@binghamton.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Binghamton University New York</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
							<email>yizhe.zhu@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University New Jersey</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CU-Net: Coupled U-Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ZHIQIANG TANG: COUPLED U-NETS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We design a new connectivity pattern for the U-Net architecture. Given several stacked U-Nets, we couple each U-Net pair through the connections of their semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling connections could make the information flow more efficiently across U-Nets. The feature reuse across U-Nets makes each U-Net very parameter efficient. We evaluate the coupled U-Nets on two benchmark datasets of human pose estimation. Both the accuracy and model parameter number are compared. The CU-Net obtains comparable accuracy as state-of-the-art methods. However, it only has at least 60% fewer parameters than other approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The U-Net <ref type="bibr" target="#b24">[24]</ref> architecture has been widely used in the location-sensitive tasks such as human pose estimation <ref type="bibr" target="#b18">[18]</ref>, semantic segmentation <ref type="bibr" target="#b17">[17]</ref>, etc. The top-down and bottom-up processing facilitates the inference at multiple scales. The shortcut connections between the corresponding top-down and bottom-up blocks help keep the spatial information.</p><p>More recently, the DenseNet <ref type="bibr" target="#b10">[10]</ref> has shown superior performance in both image classification accuracy and parameter efficiency than the ResNet <ref type="bibr" target="#b8">[8]</ref>. The dense connectivity improves the feature reuse in the network forward process and the gradient propagation during the backward process. Thus, it could use less parameters to achieve comparable or even better accuracy. A natural question arises: how could we use the dense connectivity to improve the performance of the U-Net?</p><p>Some works <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15]</ref> tried to combine the dense connectivity and the U-Net. They follow the DenseNet design. In particular, each top-down or bottom-up resolution has a dense block containing several densely connected convolutional layers. This straightforward application of dense connectivity is restricted within local blocks of a single U-Net. Another question arises: could we integrate the dense connectivity into several stacked U-Nets? c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. The dense U-Net and stacked U-Nets have shortcut connections only inside each U-Net. In contrast, coupled U-Nets also have connections for semantic blocks across U-Nets. The CU-Net is a hybrid of dense U-Net and stacked U-Net, integrating the merits of both dense connectivity and multi-stage top-down and bottom-up refinement.</p><p>In this paper, we propose a global connection pattern. Given several stacked U-Nets, we add shortcut connections for each U-Net pair, generating the coupled U-Nets (CU-Net). The key idea is we connect blocks of the same semantic meanings, i.e. having the same resolution in either top-down or bottom-up context. Please refer to <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration. Basically, a pair of U-Nets are connected at both top-down and bottom-up context.</p><p>The proposed coupled U-Nets have three merits. First, the coupling connections are global, extending from the first U-Net to the last one. It encourages the feature reuse as well as gradient propagation globally across different U-Nets. In contrast, the straightforward application of dense connectivity only helps the information flow inside a single U-Net. Second, we could easily add a supervision at the end of each U-Net if several U-Nets are coupled together. In other words, the coupled U-Nets could naturally take advantage of multiple supervisions. However, a single dense U-Net generally only has one supervision at the end. Third, the coupled U-Nets also preserve the advantage of stacked U-Nets. Generally, several stacked U-Nets could achieve higher accuracy than a large U-Net of the equivalent model size. This benefits from the multi-stage top-down and bottom-up inference along the U-Net cascade. The proposed coupled U-Nets still inherit this nice property. Furthermore, the U-Nets coupling could largely improve the information flow based on the traditional stacked U-Nets. This could significantly reduce the model parameter number, yielding very compact models. In summary, our key contributions are:</p><p>• To the best of our knowledge, we are the first to propose coupled U-Nets (CU-Net) by connecting semantic blocks of pairwise U-Nets. The information can flow more efficiently and the feature reuse across U-Net pairs makes each U-Net light-weighted.</p><p>• We investigate to use intermediate supervisions with coupled U-Nets. With a moderate amount of intermediate supervisions, the coupled U-Nets could get the highest accuracy. We also observe that full intermediate supervisions are not the optimal choice.</p><p>• Exhaustive experiments are conducted on the human pose estimation. CU-Net demonstrates superior localization accuracy and use at least 60% less parameters compared with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review recent work on designing convolutional network architectures and recent developments in human pose estimation. Network Architecture. The research on network architectures have been active since AlexNet <ref type="bibr" target="#b14">[14]</ref> appeared. First, by using smaller filters, the VGG <ref type="bibr" target="#b25">[25]</ref> network become several times deeper than the AlexNet and obtain much better performance. Then the Highway Networks <ref type="bibr" target="#b26">[26]</ref> could extend its depths to more than 100 layers with the shortcut connections. Furthermore, the identity mappings make it possible to train ResNet <ref type="bibr" target="#b8">[8]</ref> with more than one thousand layers. More recently, the DenseNet <ref type="bibr" target="#b10">[10]</ref> outperforms the ResNet benefitting from its dense connections.</p><p>The U-Net <ref type="bibr" target="#b24">[24]</ref> architecture was proposed for the biomedical image segmentation. It has been used in semantic segmentation <ref type="bibr" target="#b17">[17]</ref>, face alignment <ref type="bibr" target="#b19">[19]</ref>, etc. Newell et al. <ref type="bibr" target="#b18">[18]</ref> use the stacked U-Nets in human pose estimation. The also apply the residual module <ref type="bibr" target="#b8">[8]</ref> in the stacked U-Nets. Recently, some efforts <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15]</ref> try to bring the dense connectivity <ref type="bibr" target="#b10">[10]</ref> into the U-Net. However, their shortcut connections are only within a single U-Net.</p><p>Human Pose Estimation. CNNs based approaches <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref> dominate the human pose estimation and prediction. Newell et al. <ref type="bibr" target="#b18">[18]</ref> apply the stacked U-Nets and get high estimation accuracy. Nearly all recent state-of-the-art methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b31">31]</ref> build on it. They use more sophisticated modules, graphical models, or additional adversarial networks <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33]</ref>. We focus on largely reducing the model parameters but still obtaining comparable accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network Architecture</head><p>In this section, we first introduce a naive dense U-Net and recap the stacked U-Nets. After analyzing their strengths and weaknesses, we propose a new architecture coupled U-Nets. We also discuss using coupled U-Nets with intermediate supervisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Naive Dense U-Net</head><p>A U-Net <ref type="bibr" target="#b24">[24]</ref> contains the same number of top-down and bottom-up blocks. There are usually skip connections between them. An illustration is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The main difference the naive dense U-Net from the traditional U-Net is the previous convolution layers become dense blocks. More specifically, the successive convolution layers at the same spatial resolution are densely connected, forming a dense block.</p><p>Besides, the dense connections result in increasing feature channels in the dense block. To adapt the feature channel number, the 1×1 convolution is used to after each dense block to compress the features.</p><p>The dense connections could increase the information flow in the U-Net to some extent. However, they are only within the local blocks. Besides, The naive dense U-Net has only one single U-Net. If we have several U-Nets, is there any more specific design?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stacked U-Nets</head><p>Recently, some works <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b30">30]</ref> stack multiple U-Nets together. <ref type="figure" target="#fig_0">Figure 1</ref> gives an illustration of stacked U-Nets. Basically, the features would go sequentially from the first U-Net to the last one. The last U-Net makes the final prediction of the model. Since stacked U-Nets have more advantages than a single U-Net, could we incorporate the dense connectivity into them? The hybrid should keep the merits of both stacked U-Nets and dense connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coupled U-Nets</head><p>Although U-Nets stacked together could refine the prediction stage-by-stage, there is no communication among them except for their inputs and outputs. To make information flow more efficiently across different U-Nets, we propose to couple U-Net pairs. Blocks at the same locations of two U-Nets have shortcut connections. <ref type="figure">Figure 2</ref> gives an illustration.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the experiments, we apply the CU-Net on the human pose estimation. First, we compare different hyper-parameter configurations of the CU-Net and choose one setting with the trade-off of accuracy and parameter efficiency. Then we investigate how the CU-Net performs with intermediate supervisions. After that, we compare the CU-Net with the naive dense U-Net. At last, we compare the CU-Net with state-of-the-art human pose estimators in terms of both accuracy and the parameter number.</p><p>Training. We implement the CU-Net based on the Pytorch toolbox. The optimizer RMSprop is used to train the networks. The initial learning rate starts from 2.5 × 10 −4 which is decayed to 5 × 10 −5 after the validation accuracy becomes stable.</p><p>Datasets. For human pose estimation, we use benchmark datasets: MPII Human Pose <ref type="bibr" target="#b1">[1]</ref> and Leeds Sports Pose (LSP) <ref type="bibr" target="#b13">[13]</ref>. We also use random scaling (0.75-1.25), rotation (-/+30) and left-right flip to augment the data. We measure the human pose estimation accuracy by the Percentage of Correct Keypoints (PCK). More specifically, we PCKh@0.5 and PCK@0.2 are used to measure the accuracy on MPII and LSP respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyper-Parameter Selection</head><p>There are two important hyper-parameters in designing the CU-Net. One is the feature number m in the main feature stream. In the experiments, m remains the same when the feature map resolution changes. The other hyper-parameter is the generated feature number n in a block of U-Net. We have tried 6 combinations of m and n. <ref type="table" target="#tab_0">Table 1</ref> gives the PCKhs on the MPII validation set. Besides, we choose 4 from the 6 settings and show how their validation PCKhs change during the training process in <ref type="figure">Figure 3</ref>.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, the smallest m and n are 64 and 16. We set the increments 64 and 8 for m and n. We could observe how the accuracy (PCKh) and the parameter number change along with the two hyper-parameters. First, the accuracy increases when m and n grow. Furthermore, the increase is 2.6%, 1.4%, 0.4%, 0.3% and 0.3% from the left to the right. The increase slows down. Similar phenomena could be observed in <ref type="figure">Figure 3</ref>. The training is more stable when m and n become larger according to the curves in <ref type="figure">Figure 3</ref>.</p><p>Besides, the parameter number also grows as m and n become larger. Moreover, the growths are 0.5M, 0.4M, 0.5M, 0.5M and 0.5M. The parameter growth remains consistent. We would like to select a model with high accuracy and low model complexity. Through  balancing the accuracy and parameter number, we choose m=128 and n=32. We fix this setting in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Investigation of CU-Net with Intermediate Supervisions</head><p>Generally, the supervision of a CU-Net is the supervision of its last U-Net. Since a CU-Net contains several U-Nets, we consider to add supervisions for preceding U-Nets. More specifically, we only add the supervision at the end of a U-Net. Fortunately, the coupling connections do not prevent us from doing this. Note that if the supervision number is smaller than the U-Net number, we distribute the supervisions as uniformly as possible. For example, if 2 supervisions exist in 4 coupled U-Nets, they are at the end of the second and fourth U-Nets. <ref type="table" target="#tab_1">Table 2</ref> gives the PCKh comparison of CU-Net with different number of supervisions. For 2 coupled U-Nets, adding a supervision for the first U-Net makes the validation PCKh drop by 0.2%. The coupling connections already strengthen the gradient propagation. The additional supervision makes the gradient too strong so that the model overfits the training set a little bit.</p><p>However, observations are different for more coupled U-Nets. According to <ref type="table" target="#tab_1">Table 2</ref>, additional supervisions could improve the PCKh of 4 coupled U-Nets (CU-Net-4). However, the CU-Net-4 obtains the highest PCKh with 1 additional supervision. Similar results appear for the CU-Net-8. But 3 additional supervisions help get the highest PCKh. CU-Net-4 and CU-Net-8 are much deeper than the CU-Net-2. The coupling connections still could not compensate the gradient vanishing due to the long distance propagation. Thus, adding some intermediate supervisions could further improve the accuracy. The CU-Net-8 is twice deeper than the CU-Net-4, thereby requiring more additional intermediate supervisions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison of CU-Net with Naive Dense U-Net</head><p>We design the CU-Net after analyzing the drawbacks of the naive dense U-Net. In this experiment, we compare them to validate the design. The overall PCKh comparison of naive dense U-Net, CU-Net and CU-Net with intermediate supervisions are shown in <ref type="figure">Figure 5</ref>. It shows three groups of comparisons with 2, 4 and 8 U-Nets. Note that the dense U-Net is always a single U-Net. For fair comparison, we add one layer in each dense block of the dense U-Net every time we increase one U-Net in the CU-Net. According to <ref type="figure">Figure 5</ref>, the CU-Net obviously outperforms the dense U-Net by 1.0%, 0.5% and 0.5% from the left to the right. This demonstrates the multi-stage top-down bottom-up inference in the CU-Net could improve the accuracy. And the CU-Net and dense U-Net have the same number of parameters in the three settings. Further, adding intermediate supervisions could improve the accuracy except for the 2 U-Nets setting. Because larger networks requires more supervisions to help the training. This proves that the CU-Net has the flexibility to use intermediate supervisions. It is worth pointing out that both the repeated top-down and bottom-up processing and the intermediate supervisions do not require extra parameters.</p><p>We also show their PCKh curves under the three settings in <ref type="figure">Figures 4, 6 and 7</ref>. The converged PCKh gaps are consistent with those in <ref type="figure">Figure 5</ref>. Besides, the PCKh curve fluctuates more when adding the intermediate supervisions. The model learner would make larger steps on the training set with more supervisions. Due to the distribution shift of training and validation sets, it is easier to over-shoot the local minimas in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-art Methods</head><p>In this experiment, we compare 8 coupled U-Nets (CU-Net-8) with state-of-the-art approaches for human pose estimation. Based on above experiments, we choose hyper-parameters m = 128 and n = 32 and use intermediate supervisions with the CU-Net. More specifically, we use supervisions for the 2 nd , 4 th , 6 th and 8 th U-Nets.   <ref type="table">Table 4</ref> shows comparisons of human pose estimation on MPII and LSP test sets. The CU-Net-8 achieves comparable PCKhs as state-of-the-art methods. In contrast, as shown in <ref type="table">Table 3</ref>, the CU-Net-8 has only 17%-40% parameters of other recent state-of-the-art methods. It is worth highlighting that Newell et al. <ref type="bibr" target="#b18">[18]</ref> use 8 stacked U-Nets. The CU-Net-8 could obtain comparable PCKhs but with only 40% parameters.</p><p>The CU-Net is simple and effective. Other state-of-the-art methods use stacked U-Nets with either sophisticated modules <ref type="bibr" target="#b31">[31]</ref>, graphical models <ref type="bibr" target="#b6">[6]</ref> or extra adversarial networks <ref type="bibr" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed the CU-Net, a new architecture based on the U-Net. We connect the same semantic blocks of several stacked U-Nets. Each U-Net pair are coupled since they are connected at multiple resolutions. Compared with the naive dense U-Net, the CU-Net has the advantages of multi-stage top-down and bottom-up inference and intermediate supervisions.</p><p>Compared with the stacked U-Nets, it is more parameter efficient benefiting from the feature reuse across U-Nets. Experiments on MPII and LSP benchmark datasets show that it could achieve state-of-the-art accuracy but using at most 40% model parameters of other methods. <ref type="table">Table 3</ref>: Comparison of model parameter numbers with state-of-the-art human pose estimators. The CU-Net-8 uses only 17%-40% parameters of other methods. In particular, the CU-Net-8 has 40% parameters of 8 stacked U-Nets <ref type="bibr" target="#b18">[18]</ref>. The coupling connections makes the CU-Net much more parameter efficient.  <ref type="table">Table 4</ref>: PCKh comparison on MPII (Top) and LSP (Bottom) test sets. The CU-Net-8 could achieve comparable performance as state-of-the-art methods. More importantly, it is concise and simple. In contrast, recent state-of-the-arts approaches using more sophisticated modules, graphical models or additional adversarial networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>StackedFigure 1 :</head><label>1</label><figDesc>arXiv:1808.06521v1 [cs.CV] 20 Aug 2018 Illustration of naive dense U-Net, stacked U-Nets and coupled U-Nets (CU-Net).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Unit 1 :Figure 2 :</head><label>12</label><figDesc>Batch Norm + ReLU + Conv1x1 Unit 2: Batch Norm + ReLU + Conv3x3 Unit 1 Diagram of coupled U-Nets. 3 U-Nets are coupled together by the red dot lines. The red dot lines with the same labels are connected. The same semantic blocks in different U-Nets are connected directly. For simplicity, we only show 2 top-down and bottom-up semantic blocks in each U-Net. The connectivity is similar for more semantic blocks. An advantage of stacked U-Nets is its repeated top-down and bottom-up inference. In one U-Net, the input goes through the top-down and bottom-up pipeline once. The U-Net could capture some spatial relationships of the predictions. However, sometimes they may be not enough for the accurate predictions. For instance, in human pose estimation, the relations of upper and lower body joints are complex. Adding a U-Net on top of another could help capture higher order spatial relationships, resulting in higher prediction accuracy. Besides, the stacked U-Nets make it very easy to add intermediate supervisions. Each U-Net could extend a side path to make its own prediction. We could the same groundtruth for each prediction. It does not affect the feature flow in the main U-Nets cascade. However, it is not straightforward to use intermediate supervisions in a single U-Net. The intermediate supervisions on its top-down blocks encourage predictions ignorant of global cues in the lower resolutions. Similarly, the intermediate supervisions in its bottom-up blocks cannot evaluate the feature effectiveness in the higher resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>coupled U-Nets still have a main feature flow along the U-Nets cascade. Let m denote the feature number in the main flow and n represent the generated feature number at each block of U-Net. For each block of the i th (i ≥ 0) U-Net, its inputs contain the m features in the main flow and another n × i features from the shortcut connections of previous U-Nets. They are concatenated channel-wise to m + n × i features. Then a 1 × 1 convolution compresses them to 4 × m features. A following 3 × 3 convolution produces n new features. At last, the m + n × i input features and n generated features are concatenated. Another 1 × 1 convolution compresses them to m output features, flowing into the next block.Intuitively, the coupled U-Nets are stacked U-Nets plus the shortcut connections among the semantic blocks. Therefore, the coupled U-Nets still possess the two advantages of stacked U-Nets: multiple stages top-down and bottom-up inference and the effective intermediate supervisions. Moreover, the additional shortcut connections largely boost the information flow across U-Nets.The proposed coupling helps not only feature reuse but also the gradient backpropagation. The intermediate supervisions are known to provide additional gradients. Hence, they have an overlapping function. It is interesting to investigate how they cooperate with each other. Empirically, coupled U-Nets with moderate intermediate supervisions would achieve the highest prediction accuracy. However, the stacked U-Nets usually work the best with full intermediate supervisions. The coupling makes some intermediate supervisions not necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 : 2 Figure 4 :</head><label>324</label><figDesc>Curves of validation PCKh under different hyper-parameters m and n. The converged curve reaches higher for larger m and n. But the gap between adjacent curves becomes smaller. Larger m and n also make the curve smoother, indicating more stable training. Validation PCKh curves of a dense U-Net, 2 coupled U-Nets (CU-Net-2) with 1 and 2 supervisions. The CU-Net-2 outperforms the dense U-Net. The CU-Net-2 with 2 supervisions does not further improve the PCKh because CU-Net-2 is not deep enough.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : 2 Figure 6 : 4 Figure 7 :</head><label>52647</label><figDesc>CU-Net v.s. naive dense U-Net measured by the PCKh on the MPII validation set. In the three comparisons, the CU-Net has 2, 4 and 8 U-Nets. The naive dense U-Net is a single U-Net with equivalent sizes. The CU-Net outperforms the dense U-Net obviously. Adding intermediate supervisions could help further improve the PCKh for deep CU-Net. -u-net-4 cu-net-4-loss-1 cu-net-4-loss-Validation PCKh curves of a dense U-Net and 4 coupled U-Nets (CU-Net-4) with 1 and 2 supervisions. There are clear gaps between the three converged curves. The intermediate supervision makes the curve fluctuate more from about 30 to 90 training epochs. -u-net-8 cu-net-8-loss-1 cu-net-8-loss-Validation PCKh curves of a dense U-Net and 8 coupled U-Nets (CU-Net-8) with 1 and 4 supervisions. CU-Net-8 and CU-Net-4 with 3 and 1 intermediate supervisions gets the highest PCKhs. Deeper CU-Net usually benefits from more intermediate supervisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different hyper-parameters m and n measured by the model parameter number and the PCKh on the MPII validation set. The PCKh increase becomes less from the left to the right while the parameter number growly consistently. A good trade-off between the PCKh and parameter number is m=128 and n=32.</figDesc><table><row><cell>m</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>192</cell><cell>192</cell></row><row><cell>n</cell><cell>16</cell><cell>16</cell><cell>24</cell><cell>32</cell><cell>24</cell><cell>32</cell></row><row><cell># Parameters</cell><cell cols="6">0.5M 1.0M 1.4M 1.9M 2.4M 2.9M</cell></row><row><cell cols="2">PCKh@0.5 (%) 81.6</cell><cell>84.2</cell><cell>85.6</cell><cell>86.0</cell><cell>86.3</cell><cell>86.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PCKhs of the CU-Net with varied intermediate supervisions on the MPII validation</figDesc><table><row><cell cols="11">set. CU-Net-2 denotes a CU-Net with 2 U-Nets. The intermediate supervisions lower the</cell></row><row><cell cols="11">PCKh of CU-Net-2. However, it improves the PCKh of deeper networks CU-Net-4 and</cell></row><row><cell cols="11">CU-Net-8. Deeper CU-Net requires more intermediate supervisions to get the highest PCKh.</cell></row><row><cell cols="6">But full intermediate supervisions are not the optimal.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CU-Net-2</cell><cell></cell><cell cols="2">CU-Net-4</cell><cell></cell><cell></cell><cell cols="2">CU-Net-8</cell><cell></cell></row><row><cell># Supervisions</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="3">PCKh@0.5 (%) 86.0 85.8</cell><cell cols="4">87.6 88.1 88.0 87.8</cell><cell cols="4">88.6 89.3 89.4 89.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Net et al.[31] et al.[30] et al.[3] et al.[6] et al.[18] (8 U-Nets) # Parameters 28.0M 29.7M 58.1M 58.1M 25.5M 10.1M</figDesc><table><row><cell>Method</cell><cell>Yang</cell><cell>Wei</cell><cell>Bulat</cell><cell>Chu</cell><cell>Newell</cell><cell>CU-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This work is partly supported by the Air Force Office of Scientific Research (AFOSR) under the Dynamic Data-Driven Application Systems Program, NSF 1763523, 1747778, 1733843 and 1703883 Awards.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Method</forename><surname>Head Sho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wri</surname></persName>
		</author>
		<title level="m">Hip Knee Ank. Mean Belagiannis et al. FG&apos;17</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multicontext attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<title level="m">Chi-Wing Fu, and Pheng Ann Heng. H-denseunet: Hybrid densely connected unet for liver and liver tumor segmentation from ct volumes. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ita</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A recurrent encoderdecoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bjoern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mykhaylo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for largescale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><forename type="middle">S</forename><surname>Greff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cr-gan: Learning complete representations for multi-view generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to forecast and refine residual motion for image-to-video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
