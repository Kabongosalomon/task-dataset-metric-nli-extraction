<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NTU60-X: TOWARDS SKELETON-BASED RECOGNITION OF SUBTLE HUMAN ACTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran Sarvadevabhatla</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology IIIT Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NTU60-X: TOWARDS SKELETON-BASED RECOGNITION OF SUBTLE HUMAN ACTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-human action recognition</term>
					<term>skeleton</term>
					<term>dataset</term>
					<term>human activity recognition</term>
					<term>pose estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The lack of fine-grained joints such as hand fingers is a fundamental performance bottleneck for state of the art skeleton action recognition models trained on the largest action recognition dataset, NTU-RGBD. To address this bottleneck, we introduce a new skeleton based human action dataset -NTU60-X. In addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X dataset includes finger and facial joints, enabling a richer skeleton representation. We appropriately modify the state of the art approaches to enable training using the introduced dataset. Our results demonstrate the effectiveness of NTU60-X in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on hitherto worst performing action categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Understanding human actions from visual data is crucial for applications related to surveillance, interactive user interfaces and multimedia systems. This task is usually accomplished with RGB videos as input <ref type="bibr">[1]</ref>. However, advances in technologies have enabled use of other modalities (e.g. depth <ref type="bibr">[2]</ref>) and systems such as Microsoft Kinect which can provide skeleton-like human pose representations <ref type="bibr">[3]</ref>. In particular, the introduction of large-scale skeleton-based human action datasets NTU RGB+D <ref type="bibr">[4,</ref><ref type="bibr">5]</ref> has shifted the focus towards skeleton-based human action recognition approaches. In contrast to full-frame RGB-based representations, 3D skeleton joints encode human body dynamics in a computationally efficient manner, preserve privacy and can offer greater robustness to view and illumination.</p><p>The recent adoption of graph neural networks which process the skeleton action sequence as a spatio-temporal graph has enabled a steady rise in performance scores for skeleton action recognition <ref type="bibr" target="#b0">[6,</ref><ref type="bibr" target="#b1">7,</ref><ref type="bibr" target="#b2">8,</ref><ref type="bibr" target="#b3">9]</ref>. However, a closer examination of the scores reveals a performance bottleneck across the approaches -see  involve the usage of fingers. The reason is that hand joints in Kinect-based skeletons are represented by just two finger joints ( <ref type="figure" target="#fig_0">Figure 1-d)</ref>. As a result, actions involving subtle finger movements (e.g. 'eating', 'writing') often fail to be recognized correctly. Sometimes, even the non-hand, main body joints are often localized poorly by the Kinect-based capture system, as <ref type="figure" target="#fig_1">Figure 2</ref> shows.</p><p>To address these issues, we introduce NTU60-X, a curated and extended version of the existing NTU dataset. Obtained from RGB videos, the skeletons in the new dataset include 42 finger joints (21 for each hand), 51 facial keypoint joints and 25 body joints similar to those present in Kinect-based NTU-60, for a total of 118 joints per skeleton (see <ref type="figure" target="#fig_0">Figure 1</ref>). Additionally, we modify the state of the art approaches to enable experimental evaluation and benchmark the modified variants on NTU60-X. The results demonstrate the benefit of the newly introduced dataset for overcoming the performance bottleneck mentioned earlier and enabling recognition of subtle human actions involving hand-based joints.</p><p>For resources (pre-trained models, analysis and videos) arXiv:2101.11529v2 [cs.CV] 29 Jan 2021 . Note that RGB frame is included only for reference and is not part of skeleton data. The three classes mentioned -'eat meal', 'writing' and 'reading' are the most confused classes for NTU60 dataset (see <ref type="table" target="#tab_0">Table 2</ref>). As the zoomed insets illustrate, the quality of joints captured by NTU60-X dataset is better compared to NTU60.</p><p>related to NTU60-X, please visit https://github.com/ skelemoa/ntu-x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Prior to creation of NTU60 dataset, a number of datasets enabled progress for skeleton-based human action recognition. MSR-Action3d <ref type="bibr" target="#b4">[10]</ref> was one of the first action recognition datasets which provided depth and skeleton joint modalities, albeit from a single viewpoint. However, it only covered a limited set of gaming actions (e.g. forward punching, side boxing). The Northwestern-UCLA dataset <ref type="bibr" target="#b5">[11]</ref> scaled up the diversity to include videos from multiple views and with actions performed by 10 different actors. The NTU RGB-D dataset [4, 5] is the largest and widely used skeleton action dataset with a large diversity in terms of action classes, performers and capture viewpoints. Varying-view RGB-D Action Dataset (VAD) <ref type="bibr" target="#b6">[12]</ref> comprises view-varying Kinect captured sequences covering the entire 360Â°view angles, containing 40 actions that are performed by 118 distinct performers. Unfortunately, the full dataset is not publicly available (as of current). Notably, the datasets mentioned above do not provide fine-grained joints for hands and faces which limits their utility for certain actions as mentioned previously. An alternative approach for skeleton estimation infers the joints from RGB video frames without requiring specialized capture equipment. In the Kinetics-skeleton dataset <ref type="bibr" target="#b0">[6]</ref>, the 2D skeleton joint coordinates predicted from RGB frames are combined with the joint estimation confidence to obtain a Dataset Body Face Fingers Sequences Classes Joints MSR-Action3D <ref type="bibr" target="#b4">[10]</ref> 567 20 20 Northwestern-UCLA <ref type="bibr" target="#b5">[11]</ref> 1,475 10 21 VAD <ref type="bibr" target="#b6">[12]</ref> 25,600 40 25 NTU RGB+D <ref type="bibr">[4]</ref> 56,880 60 25 NTU60-X (Ours) 56,148 60 118 <ref type="table">Table 1</ref>. Comparison between NTU-X and some of the other publicly available skeleton-action recognition datasets. We are one of the first datasets to include body, face and hands joints in 3D for multi-person and occlusion case as well.</p><p>pseudo 3D skeleton representation on videos from Kinetics-400 action dataset <ref type="bibr" target="#b7">[13]</ref>. However, the resulting skeleton dataset contains many invalid sequences <ref type="bibr" target="#b8">[14]</ref>. We summarize the salient aspects of these datasets and our proposed NTU60-X in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NTU60-X</head><p>The NTU dataset [4] provides RGB videos along with 3D Kinect skeleton data. We first extract the RGB frames from the videos at the frame rate of 30 FPS. We estimate 3D poses from RGB frames using SMPL-X <ref type="bibr" target="#b9">[15]</ref>. SMPL-X uses strong 2D pose priors estimated using Openpose <ref type="bibr" target="#b10">[16]</ref> on each RGB frame. However, SMPL-X based pose estimation is rather slow and is reliant on optimization heuristics. It also fails on blurred images and in the presence of light occlusion. To compensate for these issues, we use ExPose <ref type="bibr" target="#b11">[17]</ref>. ExPose uses a part-wise attention-based model that feeds high reso-  <ref type="table" target="#tab_0">Table 2</ref>. The columns correspond to the state of the art models on the NTU60 dataset. In the first row, the action classes with the lowest accuracy are shown. The second row is similar, except that the results correspond to our newly introduced dataset NTU60-X (Section 3), with models being modified appropriately (Section 4). Thanks to the availability of additional finger joints on hands in NTU60-X, performances for lowest accuracy classes improve by 10-20% on average. lution patches of the corresponding body parts to their dedicated refinement module. Unlike SMPL-X, ExPose estimates the full 3D pose (body, finger and face joints) from the RGB image without relying on 2D pose prior and is much faster compared to SMPL-X.</p><p>Since it is difficult to automatically select between SMPL-X and ExPose pose representations, we employ a semiautomatic approach to curate the final dataset. We use Openpose <ref type="bibr" target="#b10">[16]</ref> toolbox to estimate the 2D pose and associated confidence for the full-body joints. Openpose provides total 70 joints for face out of which we use 51 major joints as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref> to make the final skeleton of 118 joints (25 body + 21 Ã 2 fingers + 51 face) for each frame of the clips. Keeping the intra-view and intra-subject variance of the NTU dataset in mind, we sample random videos covering each view per class of NTU and estimate the SMPL-X and ExPose outputs. We examine the quality of the skeleton backprojected to RGB frame and use the accuracy of alignment to select between ExPose and SMPL-X. Empirically, we observe that ExPose and SMPL-X perform equally well for single-person actions but SMPL-X, though slow, provides better pose estimates for multi-person action class sequences.</p><p>To ensure good dataset quality, we remove corrupted videos from the original dataset, using a procedure similar to one adopted for the original dataset <ref type="bibr">[4]</ref>. We also omit videos in which people are completely absent. Additionally, for some classes such as 'hugging', OpenPose provides poor estimates when one of the two people involved are in close proximity. We discard instances of such videos as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>To evaluate the impact of NTU60-X on overall performance, we benchmarked models with state-of-the-art performance on model NTU60 NTU60-X MS-G3D <ref type="bibr" target="#b1">[7]</ref> 91.5 91.76 PA-ResGCN <ref type="bibr" target="#b3">[9]</ref> 90.9 91.64 4s-ShiftGCN <ref type="bibr" target="#b2">[8]</ref> 90.7 91.78 <ref type="table">Table 3</ref>. Results of top performing models of NTU60 dataset on NTU60-X dataset (with finger joints) -see Section 4.1.</p><p>NTU60. For the models MS-G3D <ref type="bibr" target="#b1">[7]</ref> and 4s-ShiftGCN <ref type="bibr" target="#b2">[8]</ref>, we updated the graph structure of the skeletons to incorporate the newly introduced joints.</p><p>PA-ResGCN <ref type="bibr" target="#b3">[9]</ref>, being a semantic part-based model, required more significant modification. Along with changes in input skeleton graph structure as done for the other two models, we defined new parts to incorporate the newly introduced joints and thus enable richer feature extraction. Since this model learns attentive weights for each of the input skeleton joints by dividing the skeleton into different parts, the definition of parts were also changed based on the NTU60-X skeleton. In case of NTU60 skeleton, PA-ResGCN defines total 5 parts: torso, left arm, right arm, left leg and right leg. In the new NTU60-X skeleton, 3 additional parts were defined for 67 joints (body + fingers) skeleton: left fingers, right fingers and head, resulting in a total of 8 parts. And for 118 joints (body + fingers + face) skeleton along with these 3 additional parts, one more part of face was added resulting in a total of 9 parts. The source code and pre-trained models are available in our github repository.  <ref type="table">Table 4</ref>. Results on different variants of NTU60-X dataset to understand the contribution of the additional joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>The results of training the three models on the new NTU60-X dataset with finger joints included for Cross Subject protocol are shown in <ref type="table">Table 3</ref>. From the results, it is evident that our modified MS-G3D, 4s-Shift-GCN and PA-ResGCN outperform their counterparts' performance on the original NTU dataset.</p><p>To further demonstrate the importance of including finger joints in the input skeleton, we analyze the worst performing classes for MS-G3D, PA-ResGCN and 4s-ShiftGCN on both original NTU60 and new NTU60-X dataset. As <ref type="table" target="#tab_0">Table 2</ref> shows, the performance for the worst classes (e.g. 'writing', 'reading', 'eat meal') , involving subtle finger-based movements, improves by 10-20% on average when trained on NTU60-X dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To examine the importance of body joints, finger joints and face joints individually, we also perform experiments with only body joints (25 joints), body + finger joints (67 joints) and body + fingers + face joints (118 joints) as well. The results of ablation study are shown in <ref type="table">Table 4</ref>. The performance degrades when face joints are included with the body and finger joints. One reason for this could be that the actions in NTU dataset do not involve significant facial motion. Hence, the additional joints of the face make the skeleton graph larger than necessary and difficult for model optimization. Another possible reason could be that the existing models do not have a suitable architecture to handle the dense subgraph arising from the presence of facial keypoints.</p><p>The poor results of models trained with only body joints (25 joints) are also in line with our hypothesis that the inclusion of finger joints in the input skeleton is crucial for better performance. In other words, the performance gain is not merely due to the shift from Kinect-based to RGB-based skeleton generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we have shown that the lack of hand-level joints is a fundamental performance bottleneck in the skeleton data of the largest action recognition dataset, NTU-RGBD. To address this bottleneck, we contribute a carefully curated skele-ton dataset which provides finger-level hand joints and facial keypoint joints.</p><p>We appropriately modify the state of the art approaches to enable training using the introduced dataset. Our results demonstrate the effectiveness of the proposed dataset in enabling the modified approaches to overcome the aforementioned bottleneck and improve their performance, overall and on hitherto worst performing action categories. We also perform experiments to evaluate the relative importance of the introduced joints.</p><p>Going forward, we plan to expand our dataset to include the full complement of 120 action classes present in NTU-RGBD collection <ref type="bibr">[5]</ref>. We also expect the research community to devise novel and efficient approaches for tackling dense skeleton representations present in our dataset.</p><p>For resources (pre-trained models, analysis and videos) related to NTU60-X, please visit https://github.com/ skelemoa/ntu-x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head><p>We would like to acknowledge Google Cloud Platform Education Grants Team for providing Google Cloud Platform credits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">REFERENCES</head><p>[1] Ronald Poppe, "Poppe, r.: A survey on vision-based human action recognition. image and vision computing 28 <ref type="formula">(6)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) The 118 joint skeleton introduced in the new NTU60-X dataset. The 25 body joints are indicated by red dots.(b) 51 facial joints (c) 21 finger joints (d) 25 body joints present in NTU-60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Sample skeletons from original NTU60 Kinect dataset (blue background) and proposed NTU60-X dataset (pink background)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>(top). The worst performing classes</figDesc><table /><note>* Equal contribution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, 976-990," Image Vision Comput., vol. 28, pp.</figDesc><table><row><cell>976-990, 06 2010. 1</cell></row><row><cell>[2] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan,</cell></row><row><cell>"Mining actionlet ensemble for action recognition with</cell></row><row><cell>depth cameras," June 2012, IEEE International Con-</cell></row><row><cell>ference on Computer Vision and Pattern Recognition</cell></row><row><cell>(CVPR). 1</cell></row><row><cell>[3] Bin Ren, Mengyuan Liu, Runwei Ding, and Hong Liu,</cell></row><row><cell>"A survey on 3d skeleton-based action recognition using</cell></row><row><cell>learning method," 2020. 1</cell></row><row><cell>[4] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang</cell></row><row><cell>Wang, "Ntu rgb+d: A large scale dataset for 3d human</cell></row><row><cell>activity analysis," in The IEEE Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition (CVPR), June 2016. 1,</cell></row><row><cell>2, 3</cell></row></table><note>[5] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot, "Ntu rgb+d 120: A large-scale benchmark for 3d human activity under- standing," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 1, 2, 4</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACMMM)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Jiang Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A largescale varying-view rgb-d action dataset for arbitraryview human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JoÃ£o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Quo vadis, skeleton action recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubh</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02072</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
