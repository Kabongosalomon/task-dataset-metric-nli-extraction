<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GradNet: Gradient-Guided Network for Visual Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
							<email>bychen@mail.dlut.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Science IntelliCloud Technology Co</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country>Australia, ‡ China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
							<email>xiaoyun.yang@intellicloud.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GradNet: Gradient-Guided Network for Visual Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fully-convolutional siamese network based on template matching has shown great potentials in visual tracking. During testing, the template is fixed with the initial target feature and the performance totally relies on the general matching ability of the siamese network. However, this manner cannot capture the temporal variations of targets or background clutter. In this work, we propose a novel gradient-guided network to exploit the discriminative information in gradients and update the template in the siamese network through feed-forward and backward operations. To be specific, the algorithm can utilize the information from the gradient to update the template in the current frame. In addition, a template generalization training method is proposed to better use gradient information and avoid overfitting. To our knowledge, this work is the first attempt to exploit the information in the gradient for template update in siamese-based trackers. Extensive experiments on recent benchmarks demonstrate that our method achieves better performance than other state-of-the-art trackers. The source codes are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is an important topic in computer vision, where the target object is identified in the initial video frame and successively tracked in subsequent frames. In recent years, deep networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> have significantly improved the tracking performance due to their representation prowess.</p><p>There are two groups of deep-learning-based trackers. The first group <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4]</ref> improves the discriminative ability of deep networks by frequent online update. They utilize the first frame to initialize the model and update it * Corresponding Author: Dr. Dong Wang <ref type="figure">Figure 1</ref>. The motivation of our algorithm. Images in the first and third column are target patches in SiameseFC. The other images show absolute values of their gradients, where the red regions have large gradient. As we can see, the gradient values can reflect the target variations and background clutter. every few frames. Timely online update enables trackers to capture target variations but also requires more computational time. Therefore, the speed of these trackers generally cannot meet the real-time requirements.</p><p>Siamese-based trackers are representative in the second group <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> which is totally based on offline training. They learn the similarity between objects in different frames through massive offline training. During online testing, the initial target feature is regarded as template and used to search the target in the following frames. These methods need no online updating, thus, they usually run at real-time speeds. However, these methods cannot adapt to appearance variations of target without important online adaptability, thereby increasing the risk of tracking drift. To solve this problem, many researches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40]</ref> present different mechanisms to update template features. However, these methods only focus on combining the previous target features, ignoring the discriminative information in background clutter. This results in a big accuracy gap between the siamese-based trackers and those with online update.</p><p>Generally, gradients are calculated through the final loss which considers both positive and negative candidates. <ref type="table">Table 1</ref>. The number of backward iterations to update the template of SiameseFC. 'LR' means learning rate; 'n×' means n times the basic learning rate; 'ITERs' means the needed iterations to converge. There is no proper step to converge by one iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR</head><p>1× <ref type="table" target="#tab_2">3× 5× 7× 9× 10× 30× 50× 70× 90× 100× 500× 1000× 3000× 5000×  ITERs 449 136 77 64 60  58  59  51  54  56  55  54  61</ref> 67 ∞ Thus, gradients contain the discriminative information to reflect the target variations and distinguish the target from background clutter. As shown in <ref type="figure">Figure 1</ref>, when objects are occluded with noise or similar objects coexist at the neighborhood of the target, the absolute value of gradients at these locations are prone to be higher. The high value in gradients can force the template to focus on these regions and capture the core discriminative information.</p><p>Most gradient-based trackers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref> concentrate on handdesigned optimization algorithms, such as momentum <ref type="bibr" target="#b33">[34]</ref>, Adagrad <ref type="bibr" target="#b10">[11]</ref>, ADAM <ref type="bibr" target="#b19">[20]</ref> and so on. These algorithms need hundreds of iterations to converge, which lead to more computation and a lower speed. How to take a trade-off between the speed and accuracy of update is still a problem.</p><p>If we expect to reduce the number of training iterations but still keep online update through gradients, the extreme case is to adapt the template through one backward propagation. However, training by one backward propagation is a difficult task. As shown in <ref type="table">Table 1</ref>, there is no proper learning rate to make the template of SiameseFC converge through one iteration. Generally, even with the optimal step length, moving according to the gradient at only one iteration cannot update the template properly, because the normal gradient-based optimization is a nonlinear process. On the other hand, we can learn a nonlinear function by CNNs, which simulates the non-linear gradient-based optimization by exploring the rich information in gradients. Therefore, we propose a gradient-guided network (GradNet) to perform gradient-guided adaptation in visual tracking. The GradNet integrates the adaptation process that consists of two feed-forward and one backward calculation, simplifying the process of gradient-based optimization.</p><p>It is a very tough task to train a robust GradNet due to two main reasons. The first reason is that the network is prone to use the appearance of the template instead of using the gradient for tracking (details can be found in Section 3.3), because learning to use the gradients is more difficult than learning to use appearance. The second reason is that the network is prone to overfit. As shown in <ref type="figure">Figure</ref> 2, the model with normal training (Ours-T) can quickly get a low distance error but its test accuracy is not promising, compared with our model. To handle these issues, we propose a template generalization method to effectively explore gradient information and avoid overfitting.</p><p>The major contributions can be summarized as follows:</p><p>• A GradNet is proposed to conduct gradient-guided template updating for visual tracking.  <ref type="figure">Figure 2</ref>. The training and testing plots of models through normal training (Ours-T) and our training method (Ours). The left map shows the error between the predicted map and the real map during training and the right map shows the accuracy during testing.</p><p>• A template generalization method is proposed to ensure strong adaptation ability and avoid overfitting.</p><p>• Extensive experiments conducted on four popular benchmarks show that the proposed tracker achieves promising results at a real-time speed of 80fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Siamese Network based Tracking</head><p>SiameseFC <ref type="bibr" target="#b2">[3]</ref> is the most representative trackers based on template matching. Bertinetto et al. <ref type="bibr" target="#b2">[3]</ref> present a siamese network with two shared branches to extract features of both the target and the search region. During online tracking, the template is fixed as the initial target feature and the tracking performance mainly relies on the discriminative ability of the offline-trained network. Without online updating, the tracker achieves beyond real-time speed. Similarly, SINT <ref type="bibr" target="#b32">[33]</ref> also designs a network to match the initial target with candidates in a new frame. Its speed is much lower because hundreds of candidate patches are sent into the network instead of one search image. Another siamesebased tracker is GOTURN <ref type="bibr" target="#b16">[17]</ref> which proposes a siamese network to regress the target bounding box with a speed of 100fps. All these methods are lack of important online updating. The fixed model cannot adapt to appearance variations, which makes the tracker easily disturbed by similar instances or background noise. In this paper, we choose SiameseFC as our basic model and propose a gradient-guided method to update the template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Updating in Tracking</head><p>Timely updating is essential to keep trackers robust. There are three main dominant strategies of model updating, including template combination, gradient-descent based and correlation-based strategies.</p><p>Template Combination. Algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">45]</ref> based on template combination aim to effectively combine the target features from previous frames. Guo et al. <ref type="bibr" target="#b15">[16]</ref> propose a fast transformation learning model to enable effective online learning from previous frames. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> utilize the optical flow information to convert templates and integrate them according to their weights. All these methods focus on using the information of templates, which ignore the background clutter. Different from these methods, we take full use of the discriminative information in backward gradients instead of just integrating previous templates.</p><p>Gradient-descent based approaches. Deep trackers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref> based on gradient descent explore the discriminative information in backward gradients to update the model through hundreds of iterations. Wang et al. <ref type="bibr" target="#b35">[36]</ref> train two separate convolutional layers to regress Gaussian maps with the initial frame and update these layers every few frames. Similarly, Song et al. <ref type="bibr" target="#b31">[32]</ref> also utilize a number of gradient descent iterations in initialization and online update procedures. These trackers need many training iterations to capture the appearance variations of the target, which makes the tracker less effective and far from real-time requirements. We propose a GradNet that needs only one backward propagation and two forward propagations to update the template effectively. Besides, our template generalization method for handling overfitting is not investigated in existing works.</p><p>Correlation based Tracking. Correlation based trackers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref> train classifier through circular convolution, which can be quickly calculated in Fourier domain. The final classifier is trained and updated by solving the closed-form solution of the optimization function. The classifier training cannot be simulated totally by deep networks, so most correlation based trackers just utilize deep networks to extract robust features. Differently, our method aims to update the template in an end-to-end network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gradient Exploiting</head><p>Currently, most deep neural networks adopt gradients in offline training based on hand-designed optimization strategies, such as momentum <ref type="bibr" target="#b33">[34]</ref>, Adagrad <ref type="bibr" target="#b10">[11]</ref>, ADAM <ref type="bibr" target="#b19">[20]</ref> and so on. These methods usually need expensive computation and large-scale data sets. How to accelerate the training of deep networks is a hot topic in computer vision.</p><p>Meta Learning. Meta learning approaches can be broadly divided into different categories, including optimizationbased methods <ref type="bibr" target="#b0">[1]</ref>, memory-based methods <ref type="bibr" target="#b30">[31]</ref>, variablebased methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24]</ref> and so on. Our algorithm can be seen as an improved version of the optimization-based method <ref type="bibr" target="#b0">[1]</ref> to adapt to the update task in visual tracking. Our approach has three main differences compared with <ref type="bibr" target="#b0">[1]</ref>. First, ours only learns to update template, but not the network branch of search region. This is specifically designed for the tracking task. Second, our update process only contains one iteration instead of multiple iterations. Finally, our training of the optimizer includes second-order gradient which is not used in <ref type="bibr" target="#b0">[1]</ref>. Meta Learning for Tracking. Despite the popularity of meta learning in many fields, there are few works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29]</ref> applying it to visual tracking. Yang et al. <ref type="bibr" target="#b39">[40]</ref> design a memory structure to dynamically write and read previous templates for model updating. Differently, we focus on exploring the discriminative information of gradients. Eunbyung et al. <ref type="bibr" target="#b28">[29]</ref> train the initialization parameters of filters with pixel-wise learning rate offline and utilize a matrix multiplication to update the filters. The update is a linear process. While, our template update is a non-linear process with convolutional layers and Relu. Besides, we use the target feature as the prior information to speed up the update process by providing a good initial value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>The whole pipeline of GradNet is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which consists of two branches. One branch extracts features of the search region X and the other branch generates the template according to the target information and gradients, detailed in Section 3.2. The template generation process consists of initial embedding, gradient calculation and template updating. First, the shallow target feature f 2 (Z) is sent to one sub-net U 1 (shown in purple in <ref type="figure" target="#fig_2">Figure 3</ref>) to obtain an initial template β which is used to calculate the initial loss L. Second, the gradient of the shallow target feature is calculated through backward propagation, and sent to the other sub-net U 2 (shown in orange in <ref type="figure" target="#fig_2">Figure 3</ref>) for being non-linearly converted to better gradient representation. Finally, the converted gradient is added to the shallow target feature to get an updated target feature which is sent to the sub-net U 1 again to output the optimal template. It should be noted that the two sub-nets in the initial embedding and template update process share parameters. The optimal template is used to search targets on search regions through cross correlation convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Tracker</head><p>We adopt SiameseFC <ref type="bibr" target="#b2">[3]</ref> as the basic tracker. f x (.) is used to model the feature extraction branch for search region, f z (.) is used to model the feature extraction branch for target region. We assume that the movement of the target is smooth between two consecutive frames. Thus, we can crop a search region X which is larger than the target patch Z in the current frame, centered at the target's position in the last frame. The final score map is calculated by:</p><formula xml:id="formula_0">S = β * f x (X),<label>(1)</label></formula><p>where β is the template to perform an exhaustive search over the search region X, * means cross correlation convo-  lution, S denotes the score map to find the target. In Siame-seFC, the template β is defined as the deep target feature:</p><formula xml:id="formula_1">β sia = f z (Z),<label>(2)</label></formula><p>where Z is the target patch in the first frame. In order to improve the discriminative ability of the template β during online tracking, we design the update branch U (α) to explore the rich information in gradients:</p><formula xml:id="formula_2">β our = U (Z, X, α),<label>(3)</label></formula><p>where α is the parameter of the update branch which can not only capture the template information in Z but also the background information in X through gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Template Generation</head><p>Initial Embedding. Given the image pair (X, Z), we want to get the optimal template β which is suitable to distinguish the target from the background in search region X. First, we get the target feature f 2 (Z) (using two convolutional layers) and sent f 2 (Z) to the sub-net U 1 to get the initial template β:</p><formula xml:id="formula_3">β = U 1 (f 2 (Z), α 1 ),<label>(4)</label></formula><p>where α 1 is the parameter of U 1 . The initial template only contains template information without background information. Thus, we need to explore the discriminative information in gradient to make it more robust. After getting β, the initial score map S is calculated through equation <ref type="formula" target="#formula_0">(1)</ref>.</p><p>Gradient Calculation. Based on the initial score map S and the training label Y, we can get the initial loss L by:</p><formula xml:id="formula_4">L = l (S, Y),<label>(5)</label></formula><p>where l(.) is logistic loss function. We utilize this loss to calculate the gradient of f 2 (Z) and added it to f 2 (Z). Then, the updated target feature is obtained by:</p><formula xml:id="formula_5">h 2 (Z) = f 2 (Z) + U 2 ( ∂L ∂f 2 (Z) , α 2 ),<label>(6)</label></formula><p>where α 2 is the parameter of U 2 . Here, the gradient is related to U 1 and used as the input of the sub-net U 2 to calculate the final loss, so the second-order guidance is introduced in the parameter training of the sub-net U 1 .</p><p>Template Update. Finally, we send the updated target feature h 2 (Z) to the sub-net U 1 again to obtain the optimal template β and the final score map S by:</p><formula xml:id="formula_6">β = U 1 (h 2 (Z), α 1 ), S = β * f x (X).<label>(7)</label></formula><p>The optimal score map S is utilized to estimate the target position. Our goal is to let S have the highest value at the target position and lower values at other positions. Thus, we utilize the loss which is calculated by S to train the update branch:</p><formula xml:id="formula_7">arg min α l (S , Y).<label>(8)</label></formula><p>To our knowledge, this work is the first attempt to exploit the discriminative information of gradients to update the template in SiameseFC. To simplify the introduction of template generation process, we just utilize one image pair here. In the next subsection, we will discuss the training method more generally and detailedly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Template Generalization</head><p>Problem of Basic Optimization. Image pairs from different videos and their training labels form the training set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight Ratio Distribution</head><p>Ours</p><p>Ours-T 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.010 0.011 <ref type="figure">Figure 4</ref>. The distribution of weight ratio between gradients and features. The weight ration is calculated by the absolute value of α2, which reflects the proportion of the gradient during the template update. The rectangles at different positions represent the number of points in those ranges.</p><formula xml:id="formula_8">T = {(X 1 , Z 1 , Y 1 ), (X 2 , Z 2 , Y 2 ), . . . , (X n , Z n , Y n )}, X i is search region which is larger than target patch Z i , Y i</formula><p>is training label and n is the number of training samples. It should be noted that X i and Z i are from different frames of the same video, while X i and X j (i = j) are from different videos. One simple idea to train our network is to utilize image pairs (X i , Z i , Y i ) in the training set T to get optimal template β i and final score maps S i by equations (4−7). The update branch is trained through:</p><formula xml:id="formula_9">arg min α n i=1 l (S i , Y i ).<label>(9)</label></formula><p>This method has two main problems according to our experiment. The first one is that the update branch of the network is prone to focus on the template appearance instead of the gradient, because learning to use the gradient is harder than modeling the similarity metric. As shown in <ref type="figure">Figure 4</ref>, the network trained without template generalization has lower weight ratio of gradients. This means that the network focuses less on gradients. The second one is that the network cannot avoid overfitting under this training process as shown in <ref type="figure">Figure 2</ref>.</p><p>Template Generalization. Our goal is forcing the update branch to focus on gradients and avoiding overfitting. Based on these requirements, we propose a template generalization method which adopts search regions from different videos to obtain a versatile template and make it perform well on all search regions in each training batch. We show the training process of our model without template generalization (a) and our model with template generalization (b) in <ref type="figure" target="#fig_3">Figure 5</ref> based on four image pairs. The main difference is that we utilize one template (instead of four templates) to search targets on four images from different videos. We choose k (k = 4 in <ref type="figure" target="#fig_3">Figure 5</ref>) training image pairs from the training set T to form a training batch and utilize the target patch Z 1 in the first image pair to calculate the target feature f 2 (Z 1 ). The initial template β 1 can be obtained by equation <ref type="formula" target="#formula_3">(4)</ref>. Here, β 1 means the template which is calculated through Z 1 . Then, we utilize β 1 to find the target on all search regions:</p><formula xml:id="formula_10">S i = β 1 * f x (X i ), i = 1, 2, ...k.<label>(10)</label></formula><p>Then, we can obtain the initial loss by equation <ref type="formula" target="#formula_4">(5)</ref> and update the template β 1 through equations <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b6">7)</ref>. After obtaining the updated template β 1 , we utilize it to search the target in all search regions (X 1 , X 2 , ..., X k ) and train the update branch through equation <ref type="bibr" target="#b8">(9)</ref>. In this way, the β 1 is required to track the targets in X 1 , X 2 , ..., X k simultaneously. To clarify, we show the details in Algorithm 1. The template generalization offers the target feature with multiple search regions and aims to obtain a general template feature which performs well on all search regions. This strategy can force the network to focus on the gradients during offline training, because the initial target features are misaligned and the gradients are aligned. The sub-nets U 1 and U 2 need to correct the initial misaligned template according to the gradients and thereby obtaining a great power to update templates according to gradients. As shown in <ref type="figure">Figures 2 and 4</ref>, the template generalization algorithm can effectively avoid overfitting and pay attention on gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Online Tracking</head><p>After offline training, the update branch is totally fixed and used for initialization and update during online testing . Initialization. Given the ground truth in the first frame, we crop a target patch Z 1 and a search region X 1 as inputs of the network. Then, we can obtain the optimal template β according to equations (4−7). Besides, the updated target features h 2 (Z 1 ) is calculated through equation <ref type="bibr" target="#b5">(6)</ref> and used to update the template in the following frames. Online Update. We update the template β with one reliable training sample through one iteration. We save the reliable sample (X i , Y i ) according to tracking results and use it to update the current template β based on equations (4−7) ( replacing f 2 (Z), X, Y with h 2 (Z), X i , Y i ). Namely, we obtain updated feature h 2 (Z 1 ) through the initial frame. Then, the update branch of network is used Algorithm 1 Offline training the update branch Input: Training samples (I 1 , I 2 , . . . , I n ) from different videos and gaussian maps (Y 1 , Y 2 , . . . , Y n ) Output: Trained weights α for the update branch.</p><p>Initialize the update branch with weights α 0 . Initialize the feature extraction part of the tracker with parameters from SiameseFC <ref type="bibr" target="#b2">[3]</ref>. Crop template images Z and search regions X from the training samples to construct the training set T =</p><formula xml:id="formula_11">{(X 1 , Z 1 , Y 1 ), (X 2 , Z 2 , Y 2 ), . . . , (X n , Z n , Y n )}.</formula><p>while not converged do 1. Randomly select k training samples from T . 2. Utilize the update branch to get β 1 and β 1 .</p><formula xml:id="formula_12">for i ∈ 0, . . . , k do (a). β 1 = U 1 (f 2 (Z 1 ), α 1 ) (b). S i = β 1 * f x (X i ) (c). L = k i=1 l (S i , Y i ) (d).</formula><p>Get h 2 (Z 1 ) according to equation <ref type="formula" target="#formula_5">(6)</ref>. (e). β 1 = U 1 (h 2 (Z 1 ), α 1 ) end for 3. Train the update branch by minimizing the loss. for i ∈ 0, . . . , k do (a).</p><formula xml:id="formula_13">S i = β 1 * f x (X i ) (b). L = k i=1 l (S i , Y i ) (c)</formula><p>. Minimize L to update α 0 by SGD. end for end while to update h 2 (Z 1 ) according to the reliable sample (X i , Y i ) and produce optimal templates β for the regression part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>The feature extraction f x (.) for the search region consists of five convolutional layers with the same structure and parameters as SiameseFC <ref type="bibr" target="#b2">[3]</ref>. The shallow target features f 2 (.) are from the second convolutional layers of Siame-seFC. There are three convolutional layers in U 1 which have the same structure with the last three layers of SiameseFC. The kernel size of the convolutional layer in U 2 is 3×3. The size of template β and β is 6×6 and the size of score map is 17×17. During tracking, we update the template β every 5 frames. The reliable training sample is chosen according to the max value of the score map. We set the max value of the score map in the first frame as a threshold thre. If the max value of the current score map is larger than thre * 0.5, we think that the result is accurate and crop the training sample X t as the reliable training sample. The scale evaluate, learning rate and training epoch in the proposed method are the same as those in SiameseFC <ref type="bibr" target="#b2">[3]</ref>. To take the trade-off between the fast adaptation and error accumulation, the final template is obtained by combining the initial template     and β . We only train the network on ILSVRC2014 VID dataset and the whole network is fixed during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our tracker is implemented in Python with the Pytorch framework, which runs at 80fps with an intel i7 3.2GHz CPU with 32G memory and a Nvidia 1080ti GPU with 11G memory. We compare our tracker with many state-of-theart trackers with real-time performance (i.e., their speeds are faster than 25fps) on recent benchmarks, including OTB-2015 <ref type="bibr" target="#b37">[38]</ref>, TC-128 <ref type="bibr" target="#b24">[25]</ref>, VOT-2017 <ref type="bibr" target="#b20">[21]</ref> and LaSOT <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on the OTB-2015 dataset</head><p>The OTB-2015 <ref type="bibr" target="#b37">[38]</ref> dataset is one of the most popular benchmarks, which consists of 100 challenging video clips annotated with 11 different attributes. We refer the reader to <ref type="bibr" target="#b37">[38]</ref> for more detailed information. Here, we adopt both success and precision plots to evaluate different trackers on OTB-2015. The precision plot reports the percentages that the center location errors are smaller than certain thresholds. Whereas the success plot reports the percentages of   frames where the overlap between the predicted and the ground truth bounding boxes is higher than a series of given ratios. We compare our algorithm with twelve state-of-theart trackers including nine real-time deep trackers (ACT <ref type="bibr" target="#b4">[5]</ref>, StructSiam <ref type="bibr" target="#b43">[44]</ref>, SiamRPN <ref type="bibr" target="#b21">[22]</ref>, ECO-HC <ref type="bibr" target="#b6">[7]</ref>, PTAV <ref type="bibr" target="#b12">[13]</ref>, CFNet <ref type="bibr" target="#b34">[35]</ref>, Dsiam <ref type="bibr" target="#b15">[16]</ref>, LCT <ref type="bibr" target="#b26">[27]</ref>, SiameFC <ref type="bibr" target="#b2">[3]</ref>) and three traditional trackers (Staple <ref type="bibr" target="#b1">[2]</ref>, DSST <ref type="bibr" target="#b7">[8]</ref>, KCF <ref type="bibr" target="#b17">[18]</ref>). <ref type="figure" target="#fig_4">Figure 6</ref> illustrates the precision and success plots of all compared trackers over OTB-2015, which shows the proposed tracker achieves very good performance (merely a slightly lower than ECO-HC in success). Especially, our tracker performs significantly better than the baseline model (SiameseFC) by almost 8% in precision and 6% in success. To facilitate more detailed analysis, we demonstrate the visual results of some representative methods in <ref type="figure" target="#fig_9">Figure 9</ref>. From these figures, we can see that our method can well handle various challenging factors and consistently achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the TC-128 dataset</head><p>The TC128 <ref type="bibr" target="#b24">[25]</ref> dataset consists of 128 fully-annotated image sequences with 11 various challenging factors, which is larger than OTB-2015 and focuses more on color information. We also adopt both success and precision plots to evaluate different trackers (the same evaluation protocol as OTB-2015). We compare our algorithm with eleven trackers, including ACT <ref type="bibr" target="#b4">[5]</ref>, PTAV <ref type="bibr" target="#b12">[13]</ref>, Dsiam <ref type="bibr" target="#b15">[16]</ref>, SiameFC <ref type="bibr" target="#b2">[3]</ref>, HCFT <ref type="bibr" target="#b25">[26]</ref>, FCNT <ref type="bibr" target="#b35">[36]</ref>, STCT <ref type="bibr" target="#b36">[37]</ref>, BACF <ref type="bibr" target="#b14">[15]</ref>, SRDCF <ref type="bibr" target="#b8">[9]</ref>, KCF <ref type="bibr" target="#b17">[18]</ref> and MEEM <ref type="bibr" target="#b40">[41]</ref>. <ref type="figure" target="#fig_5">Figure 7</ref> shows that our tracker achieves the best results in terms of both precision and success criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on the VOT2017 dataset</head><p>The VOT2017 <ref type="bibr" target="#b20">[21]</ref> dataset contains 60 short sequences annotated with 6 different attributes. According to its evaluation protocol, the tested tracker is re-initialized whenever a tracking failure is detected. In this benchmark, the accuracy (A) and robustness (R) as well as expected average overlap (EAO) are three important criterion. Different trackers are ranked based on the EAO criterion. We refer the reader to <ref type="bibr" target="#b20">[21]</ref> for more detailed information. In this subsection, we compare our algorithm with top ten trackers reported in the VOT2017 real-time Challenge <ref type="bibr" target="#b20">[21]</ref> and another state-of-the-art tracker SiamRPN <ref type="bibr" target="#b21">[22]</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows that our tracker achieves the best performance in terms of EAO while maintaining a very competitive accuracy and robustness. The EAO of our tracker is higher than the winner (CSRDCF++) of the VOT2017 real-time Challenge by 3.5%. Our tracker can also perform better than SiamRPN whose training data (over 100,000 videos) is much larger than ours (about 4,000 videos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on the LaSOT dataset</head><p>The LaSOT <ref type="bibr" target="#b11">[12]</ref> dataset is a very large-scale dataset consisting of 1, 400 sequences with 70 categories and more than 3.5M frames in total. The average frame length of this dataset is more than 2, 500 frames. Up to now, this dataset is the largest for visual tracking. Following onepass evaluation, different trackers are compared based on three criteria including precision, normalized precision and success. We also adopt precision and success plots to compare 35 trackers and show the performance of the top 12 trackers in <ref type="figure" target="#fig_7">Figure 8</ref> (more compared results are presented in the supplementary material). From <ref type="figure" target="#fig_7">Figure 8</ref>, we can see that our tracker performs the third-best in this dataset. Although MDNet and VITAL achieve better accuracies than our tracking algorithm, their speeds are far from the realtime requirement (MDNet, 1fps and VITAL, 1.5fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Analysis</head><p>Self-comparison. To verify the contribution of each component in our algorithm, we implement and evaluate several variations of our approach (Ours) on OTB-2015. These versions include: <ref type="formula" target="#formula_0">(1)</ref>   The performance of all variations and our final method is reported in <ref type="table" target="#tab_2">Table 3</ref>, from which we can see that all components facilitate improving the tracking accuracy. For examples, the comparison of the 'Ours w/o M' and final methods demonstrates the template generalization training method could effectively learn an expected GradNet. With the same amount of training data, 'Ours' improves the precision and IOU score of 'Ours-baseline' about 9% and 5% respectively, which demonstrates the effectiveness of the GradNet.</p><p>Training Analysis. To further analyze the template generalization, we show the initial score map S and the optimal score map S of two different training methods in <ref type="figure" target="#fig_11">Figure 10</ref>. The initial score maps of the model with template generalization (a) are noisy score maps where the approximate area of all objects has high response values. After the template updating based on gradients, the promising score maps (b) only have a high response at the target position. Differently, the model without template generalization is likely to output initial score maps (c) with a high response at the target position directly. Thus, we think the model trained by template generalization learns different tasks in the initial embedding and template update processes. During initial embedding, it learns a general template to detect the target and background clutter. This manner provides the model more discriminative gradients. Then, the model learns to update the template based on these gradients in the template update process. The discriminative gradients enable the fast adaptation of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a GradNet for template update, achieving accurate tracking with a high speed. The two sub-nets in GradNet exploits the discriminative information in gradients through feed-forward and backward operations and speeds up the hand-designed optimization process. To take full use of gradients and obtain versatile templates, a template generalization method is applied during offline training, which can force the update branch to concentrate on the gradient and avoid overfitting. Experiments on four benchmarks show that our method significantly improves the tracking performance compared with other realtime trackers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The pipeline of the proposed algorithm, which consists of two branches. The bottom branch extracts the feature of search region X and the top branch (named update branch) is responsible for template generation. The two purple trapezoids in the figure represent sub-nets with shared parameters; the solid and dotted line represents forward and backward propagation respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of 'Ours-T' and 'Ours' on exploiting templates. 'Ours-T' denotes training without template generalization; 'Ours' represents training through template generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Precision and success plots on the OTB-2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Precision and success plots on the TC128 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Precision and success plots on the LaSOT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Representative visual results of different tracking algorithms on the OTB-2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>'Ours w/o M': GradNet without template generalization training process; (2) 'Ours w/o MG': Grad-Net removed template generalization training process and gradient application. It can be seen as SiameseFC with two unshared branches; (3) 'Ours w/o U': the proposed method without template update; (4) 'Ours w 2U': the two sub-nets (in purple) in Figure 3 do not share parameters; (5) 'Oursbaseline': SiameseFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>The first row displays the search regions from different videos. (a) and (b) shows S and S of our model; (c) and (d) shows S and S of the model without template generalization. Model through template generalization can get general initial score maps S and optimal final score maps S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">The accuracy (A), robustness (R) and expected average</cell></row><row><cell cols="4">overlap (EAO) scores of different trackers on VOT2017.</cell></row><row><cell>Trackers</cell><cell>A</cell><cell>R</cell><cell>EAO</cell></row><row><cell>Ours</cell><cell>0.507</cell><cell>0.375</cell><cell>0.247</cell></row><row><cell>SiamRPN</cell><cell>0.490</cell><cell>0.460</cell><cell>0.244</cell></row><row><cell>CSRDCF++</cell><cell>0.459</cell><cell>0.398</cell><cell>0.212</cell></row><row><cell>SiamFC</cell><cell>0.502</cell><cell>0.604</cell><cell>0.182</cell></row><row><cell>ECO HC</cell><cell>0.494</cell><cell>0.571</cell><cell>0.177</cell></row><row><cell>Staple</cell><cell>0.530</cell><cell>0.688</cell><cell>0.170</cell></row><row><cell>KFebT</cell><cell>0.451</cell><cell>0.684</cell><cell>0.169</cell></row><row><cell>SSKCF</cell><cell>0.530</cell><cell>0.656</cell><cell>0.164</cell></row><row><cell>CSRDCFf</cell><cell>0.475</cell><cell>0.646</cell><cell>0.158</cell></row><row><cell>UCT</cell><cell>0.490</cell><cell>0.777</cell><cell>0.145</cell></row><row><cell>MOSSEca</cell><cell>0.400</cell><cell>0.810</cell><cell>0.139</cell></row><row><cell>SiamDCF</cell><cell>0.503</cell><cell>0.988</cell><cell>0.135</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Precision and success scores on OTB-2015 for different variations of our algorithm.</figDesc><table><row><cell>Variations</cell><cell>PRE</cell><cell>IOU</cell><cell>FPS</cell></row><row><cell>Ours</cell><cell>0.861</cell><cell>0.639</cell><cell>80</cell></row><row><cell>Ours w/o M</cell><cell>0.823</cell><cell>0.615</cell><cell>80</cell></row><row><cell>Ours w/o MG</cell><cell>0.717</cell><cell>0.524</cell><cell>94</cell></row><row><cell>Ours w/o U</cell><cell>0.775</cell><cell>0.552</cell><cell>85</cell></row><row><cell>Ours w 2U</cell><cell>0.833</cell><cell>0.628</cell><cell>80</cell></row><row><cell>Ours-baseline</cell><cell>0.771</cell><cell>0.582</cell><cell>94</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi attention module for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="80" to="93" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime &apos;actor-critic&apos; tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive spatially-regularized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno>abs/1809.07845</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jiri Matas. The visual object tracking VOT2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep visual tracking: Review and experimental comparison. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="323" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<idno>abs/1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CREST: convolutional residual learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An incremental gradient(-projection) method with momentum term and adaptive stepsize rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="506" to="531" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">skimming-perusal&apos; tracking: A framework for real-time and robust long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Correlation particle filter for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2676" to="2687" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning multi-task correlation particle filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
