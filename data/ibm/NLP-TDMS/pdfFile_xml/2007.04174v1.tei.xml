<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Re-Identification by Multiple Views Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-08">8 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">AImageLab</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Re-Identification by Multiple Views Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-08">8 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Re-Identification</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Angelo Porrello [0000−0002−9022−8484] , Luca Bergamini [0000−0003−1221−8640] , Simone Calderara [0000−0001−9056−1538]</p><p>Abstract. To achieve robustness in Re-Identification, standard methods leverage tracking information in a Video-To-Video fashion. However, these solutions face a large drop in performance for single image queries (e.g., Image-To-Video setting). Recent works address this severe degradation by transferring temporal information from a Video-based network to an Image-based one. In this work, we devise a training strategy that allows the transfer of a superior knowledge, arising from a set of views depicting the target object. Our proposal -Views Knowledge Distillation (VKD) -pins this visual variety as a supervision signal within a teacher-student framework, where the teacher educates a student who observes fewer views. As a result, the student outperforms not only its teacher but also the current state-of-the-art in Image-To-Video by a wide margin (6.3% mAP on MARS, 8.6% on Duke and 5% on VeRi-776). A thorough analysis -on Person, Vehicle and Animal Re-ID -investigates the properties of VKD from a qualitatively and quantitatively perspective. Code is available at https://github.com/aimagelab/VKD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances on Metric Learning <ref type="bibr">[38,</ref><ref type="bibr">41,</ref><ref type="bibr">47,</ref><ref type="bibr">45]</ref> give to researchers the foundation for computing suitable distance metrics between data points. In this context, Re-Identification (Re-ID) has greatly benefited in diverse domains [56, <ref type="bibr" target="#b15">16,</ref><ref type="bibr">37]</ref>, as the common paradigm requires distance measures exhibiting robustness to variations in background clutters, as well as different viewpoints. To meet these criteria, various deep learning based approaches leverage videos to provide detailed descriptions for both query and gallery items. However, such a setting -known as Video-To-Video (V2V) Re-ID -does not represent a viable option in many scenarios (e.g. surveillance) <ref type="bibr">[54,</ref><ref type="bibr">50,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b9">10]</ref>, where the query comprises a single image (Image-To-Video, I2V). As observed in <ref type="bibr" target="#b9">[10]</ref>, a large gap in Re-ID performance still subsists between V2V and I2V, highlighting the number of query images as a critical factor in achieving good results. Contrarily, we advise the learnt representation should not be heavily affected when few images are shown to the network (e.g. only   one). To bridge such a gap, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref> propose a teacher-student paradigm, in which the student -in contrast with the teacher -has access to a small fraction of the frames in the video. Since the student is educated to mimic the output space of its teacher, it will show higher generalisation properties than its teacher when a single frame is available. It is noted that these approaches rely on transferring temporal information: as datasets often come with tracking annotation, they can guide the transfer from a tracklet into one of its frames. In this respect, we argue the limits of transferring temporal information: in fact, it is reasonable to assume an high correlation between frames from the same tracklet ( <ref type="figure" target="#fig_2">Fig. 1a</ref>), which may potentially underexploit the transfer. Moreover, limiting the analysis to the temporal domain does not guarantee robustness to variation in background appearances.</p><p>Here, we make a step forward and consider which information to transfer, shifting the paradigm from time to views: we argue that more valuable information arises when ensembling diverse views of the same target <ref type="figure" target="#fig_2">(Fig. 1c</ref>). This information often comes for free, as various datasets <ref type="bibr">[55,</ref><ref type="bibr">49,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b3">4]</ref> provide images capturing the same target from different camera viewpoints. To support our claim, <ref type="figure" target="#fig_2">Fig. 1</ref> (right) reports pairwise distances computed on top of ResNet-50, when trained on Person and Vehicle Re-ID. In more details: matrices from <ref type="figure" target="#fig_2">Fig. 1b</ref> visualise the distances when tracklets are provided as input, whereas <ref type="figure" target="#fig_2">Fig. 1d</ref> shows the same for sets of views. As one can see, leveraging different views leads to a more distinctive blockwise pattern: namely, activations from the same identity are more consistent if compared to the ones computed in the tracklet scenario. As shown in <ref type="bibr">[44]</ref>, this reflects a higher capacity to capture the semantics of the dataset, and therefore a graceful knowledge a teacher can transfer to a student. Based on the above, we propose Views Knowledge Distillation (VKD), which transfers the knowledge lying in several views in a teacher-student fashion. VKD devises a two-stage procedure, which pins the visual variety as a teaching signal for a student who has to recover it using fewer views. We remark the following contributions: i) the student outperforms its teacher by a large margin, especially in the Image-To-Video setting; ii) a thorough investigation shows that the student focuses more on the target compared to its teacher and discards uninformative details; iii) importantly, we do not limit our analysis to a single domain, but instead achieve strong results on Person, Vehicle and Animal Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Image-To-Video Re-Identification The I2V Re-ID task has been successfully applied to multiple domains. the teacher to a student with the same architecture, with the aim of improving the overall performance at the end of the training. Here, we get a step ahead and introduce an asymmetry between the teacher and student, which has access to fewer frames. In this respect, our work closely relates to what <ref type="bibr" target="#b4">[5]</ref> devises for Video Classification. Besides facing another task, a key difference subsists: while <ref type="bibr" target="#b4">[5]</ref> limits the transfer along the temporal axis, our proposal advocates for distilling many views into fewer ones. On this latter point, we shall show that the teaching signal can be further enhanced when opening to diverse camera viewpoints. In the Re-Identification field, Temporal Knowledge Propagation (TKP) <ref type="bibr" target="#b9">[10]</ref> similarly exploits intra-tracklet information to encourage the imagelevel representations to approach the video-level ones. In contrast with TKP: i) we do not rely on matching internal representations but instead their distances solely, thus making our proposal viable for cross-architecture transfer too; ii) at inference time, we make use of a single shared network to deal with both image and video domains, thus halving the number of parameters; iii) during transfer, we benefit from a larger visual variety, emerging from several viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We purse the aim of learning a function F θ (S) mapping a set of images S = (s 1 , s 2 , ..., s n ) into a representative embedding space. Specifically, S is a sequence of bounding boxes crops depicting a target (e.g. a person or a car), for which we are interested in inferring its corresponding identity. We take advantage of Convolutional Neural Networks (CNNs) for modelling F θ (S). Here, we look for two distinctive properties, aspiring to representations that are i) invariant to differences in background and viewpoint and ii) robust to a reduction in the number of query images. To achieve this, our proposal frames the training algorithm as a two-stage procedure, as follows:</p><p>-First step (Sec. 3.1): the backbone network is trained for the standard Video-To-Video setting. -Second step (Sec. 3.2): we appoint it as the teacher and freeze its parameters. Then, a new network with the role of the student is instantiated. As depicted in <ref type="figure" target="#fig_8">Fig. 2</ref>, we feed frames representing different views as input to the teacher and ask the student to mimic the same outputs from fewer frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Teacher Network</head><p>Without loss of generality, we will refer to ResNet-50 <ref type="bibr" target="#b10">[11]</ref> as the backbone network, namely a module f θ : R W×H×3 → R D mapping each image s i from S to a fixed-size representation d i (in this case D = 2048). Following previous works [28,10], we initialise the network weights on ImageNet and additionally include few amendments [28] to the architecture. First, we discard both the last ReLU activation function and final classification layer in favour of the BNNeck one [28] (i.e. batch normalisation followed by a linear layer). Second: to benefit from fine-grained spatial details, the stride of the last residual block is decreased from 2 to 1.</p><p>Set representation Given a set of images S, several solutions [27,54,22] may be assessed for designing the aggregation module, which fuses a variable-length set of representations d 1 , d 2 , . . . , d n into a single one. Here, we naively compute the set-level embedding F(S) through a temporal average pooling. While we acknowledge better aggregation modules exist, we do not place our focus on devising a new one, but instead on improving the earlier features extractor.</p><p>Teacher optimisation We train the base network -which will be the teacher during the following stage -combining a classification term L CE (cross-entropy) with the triplet loss L TR 1 . The first can be formulated as:</p><formula xml:id="formula_0">L CE = −y logŷ<label>(1)</label></formula><p>where y andŷ represent the one-hot labels (identities) and the output of the softmax respectively. The second term L TR encourages distance constraints in feature space, moving closer representations from the same target and pulling away ones from different targets. Formally:</p><formula xml:id="formula_1">L TR = ln(1 + e D(F θ (S i a ),F θ (S i p ))−D(F θ (S i a ),F θ (S j n )) ),<label>(2)</label></formula><p>where S p and S n are the hardest positive and negative for an anchor S a within the batch. In doing so, we rely on the batch hard strategy <ref type="bibr" target="#b11">[12]</ref> and include P identities coupled with K samples in each batch. Importantly, each set S i comprises images drawn from the same tracklet <ref type="bibr">[22,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Views Knowledge Distillation (VKD)</head><p>After training the teacher, we propose to enrich its representation capabilities, especially when only few images are made available to the model. To achieve this, our proposal bets on the knowledge we can gather from different views, depicting the same object under different conditions. When facing re-identification tasks, one can often exploit camera viewpoints [55,33,25] to provide a larger variety of appearances for the target identity. Ideally, we would like to teach a new network to recover such a variety even from a single image. Since this information may not be inferred from a single frame, this can lead to an ill-posed task. Still, one can underpin this knowledge as a supervision signal, encouraging the student to focus on important details and favourably discover new ones. On this latter point, we refer the reader to Section 4.4 for a comprehensive discussion. Views Knowledge Distillation (VKD) stresses this idea by forcing a student network F θ S (·) to match the outputs of the teacher F θ T (·). In doing so, we: i) allow the teacher to access framesŜ T = (ŝ 1 ,ŝ 2 , . . . ,ŝ N ) from different viewpoints; ii) force the student to mimic the teacher output starting from a sub-setŜ S = (ŝ 1 ,ŝ 2 , . . . ,ŝ M ) ⊂Ŝ T with cardinality M &lt; N (in our experiments, M = 2 and N = 8). The frames inŜ S are uniformly sampled fromŜ T without replacement. This asymmetry between the teacher and the student leads to a self-distillation objective, where the latter can achieve better solutions despite inheriting the same architecture of the former. To accomplish this, VKD exploits the Knowledge Distillation loss <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_2">L KD = τ 2 KL(y T y S )<label>(3)</label></formula><p>where y T = softmax(h T /τ ) and y S = softmax(h S /τ ) are the distributionssmoothed by a temperature τ -we attempt to match 2 . Since the student experiences a different task from the teacher one, Eq. 3 resembles the regularisation term imposed by [19] to relieve catastrophic forgetting. In a similar vein, we intend to strengthen the model in the presence of few images, whilst not deteriorating the capabilities it achieved with longer sequences. In addition to fitting the output distribution of the teacher (Eq. 3), our proposal devises additional constraints on the embedding space learnt by the student. In details, VKD encourages the student to mirror the pairwise distances spanned by the teacher.</p><formula xml:id="formula_3">Indicating with D T [i, j] ≡ D(F θ T (Ŝ T [i]), F θ T (Ŝ T [j]</formula><p>)) the distance induced by the teacher between the i-th and j-th sets (the same notation D S [i, j] also holds for the student), VKD seeks to minimise:</p><formula xml:id="formula_4">L DP = (i,j)∈ ( B 2 ) (D T [i, j] − D S [i, j]) 2 ,<label>(4)</label></formula><p>where B equals the batch size. Since the teacher has access to several viewpoints, we argue that distances spanned in its space yield a powerful description of corresponding identities. From the student perspective, distances preservation provides additional semantic knowledge. Therefore, this holds an effective supervision signal, whose optimisation is made more challenging since fewer images are available to the student. Even thought VKD focuses on self-distillation, we highlight that both L KD and L DP allow to match models with different embedding size, which would not be viable under the minimisation performed by <ref type="bibr" target="#b9">[10]</ref>. As an example, it is still possible to distill ResNet-101 (D = 2048) into MobileNet-V2 [36] (D = 1280).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student optimisation</head><p>The VKD overall objective combines the distillation terms (L KD and L DP ) with the ones optimised by the teacher -L CE and L TR -that promote higher conditional likelihood w.r.t. ground truth labels. To sum up, VKD aims at strengthening the features of a CNN in Re-ID settings through the following optimisation problem:</p><formula xml:id="formula_5">argmin θ S L VKD ≡ L CE + L TR + αL KD + βL DP ,<label>(5)</label></formula><p>where α and β are two hyperparameters balancing the contributions to the total loss L VKD . We conclude with a final note on the student initialisation: we empirically found beneficial to start from the teacher weights θ T except for the last convolutional block, which is reinitialised according to the ImageNet pretraining. We argue this represents a good compromise between exploring new configurations and exploiting the abilities already achieved by the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Evaluation Protocols We indicate the query-gallery matching as x2x, where both x terms are features that can be generated by either a single (I) or multiple frames (V). In the Image-to-Image (I2I) setting features extracted from a query set image are matched against features from individual images in the gallery. This protocol -which has been amply employed for person Re-ID and face recognition -has a light impact in terms of resources footprint. However, a single image captures only a single view of the identity, which may not be enough for identities exhibiting multi-modal distributions. Contrarily, the Video-to-Video (V2V) setting enables to capture and combine different modes in the input, but with a significant increase in the number of operations and memory. Finally, the Image-to-Video (I2V) setting [58,59,24,48,26] represents a good compromise: building the gallery may be slow, but it is often performed offline. Moreover, matchings perform extremely fast, as a query comprise only a single image. We remark that i) We adopt the standard "Cross Camera Validation" protocol, not considering examples of the gallery from the same camera of the query at evaluation and ii) even if VKD relies on frames from different camera during train, we strictly adhere to the common schema and switch to trackletbased inputs at evaluation time.</p><p>Evaluation Metrics While settings vary between different dataset, evaluation metrics for Re-Identification are shared by the vast majority of works in the field. In the followings, we report performance in terms of top-k accuracy and Mean Average Precision (mAP). By combining them, we evaluate VKD both in terms of accuracy and ranking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Person Implementation details Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">22]</ref> we adopt the following hyperparameters for MARS and Duke: i) each batch contains P = 8 identities with K = 4 samples each; ii) each sample comprises 8 images equally spaced in a tracklet. Differently, for image-based datasets (ATRW and VeRi-776) we increase P to 18 and use a single image at a time. All the teacher networks are trained for 300 epoch using Adam <ref type="bibr" target="#b16">[17]</ref>, setting the learning rate to 10 −4 and multiplying it by 0.1 every 100 epochs. During the distillation stage, we feed N = 8 images to the teacher and M = 2 ones (picked at random) to the student. We found beneficial  to train the student longer: so, we set the number of epochs to 500 and the learning rate decay steps at 300 and 450. We keep fixed τ = 10 (Eq. 3), α = 10 −1 and β = 10 −4 (Eq. 5) in all experiments. To improve generalisation, we apply data augmentation as described in <ref type="bibr">[28]</ref>. Finally, we put the teacher in training mode during distillation (consequently, batch normalisation <ref type="bibr" target="#b14">[15]</ref> statistics are computed on a batch basis): as observed in <ref type="bibr">[2]</ref>, this provides more accurate teacher labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-Distillation</head><p>In this section we show the benefits of self-distillation for person and vehicle re-id. We indicate the teacher with the name of the backbone (e.g. ResNet-50) and append "VKD" for its student (e.g. ResVKD-50). To validate our ideas, we do not limit the analysis on ResNet-*; contrarily, we test self-distillation on DenseNet-121 <ref type="bibr" target="#b13">[14]</ref> and MobileNet-V2 1.0X <ref type="bibr">[36]</ref>. Since learning what and where to look represents an appealing property when dealing with Re-ID tasks <ref type="bibr" target="#b7">[8]</ref>, we additionally conduct experiments on ResNet-50 coupled with Bottleneck Attention Modules [31] (ResNet-50bam).   <ref type="table" target="#tab_2">Table 1</ref> reports the comparisons for different backbones: in the vast majority of the settings, the student outperforms its teacher. Such a finding is particularly evident when looking at the I2V setting, where the mAP metric gains 4.04% on average. The same holds for the I2I setting on VeRi-776, and in part also on V2V. We draw the following remarks: i) in accordance with the objective the student seeks to optimise, our proposal leads to greater improvements when few images are available; ii) bridging the gap between I2V and V2V does not imply a significant information loss when more frames are available; on the contrary it sometimes results in superior performance; iii) the previous considerations hold true across different architectures. As an additional proof, plots from <ref type="figure" target="#fig_3">Figure 3</ref> draw a comparison between models before and after distillation. VKD improves metrics considerably on all three dataset, as highlighted by the bias between the teachers and their corresponding students. Surprisingly, this often applies when comparing lighter students with deeper teachers: as an example, ResVKD-34 scores better than even ResNet-101 on VeRi-776, regardless of the number of images sampled for a gallery tracklet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-Of-The-Art</head><p>Image-To-Video Tables 2, 3 and 4 report a thorough comparison with current state-of-the-art (SOTA) methods, on MARS, Duke and VeRi-776 respectively. As common practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">32]</ref>, we focus our analysis on ResNet-50, and in particular on its distilled variants ResVKD-50 and ResVKD-50bam. Our method clearly outperforms other competitors, with an increase in mAP w.r.t. top-scorers of 6.3% on MARS, 8.6% on Duke and 5% on VeRi-776. This results is totally in line with our goal of conferring robustness when just a single image is provided as query. In doing so, we do not make any task-specific assumption, thus rendering our proposal easily applicable to both person and vehicle Re-ID.</p><p>Video-To-Video Analogously, we conduct experiments on the V2V setting and report results in <ref type="table">Table 5</ref> (MARS) and <ref type="table">Table 6</ref> (Duke) <ref type="bibr" target="#b3">4</ref> . Here, VKD yields the following results: on the one hand, on MARS it pushes a baseline architecture as  ResVKD-50 close to NVAN and STE-NVAN [22], the latter being tailored for the V2V setting. Moreover -when exploiting spatial attention modules (ResVKD-50bam) -it establishes new SOTA results, suggesting that a positive transfer occurs when matching tracklets also. On the other hand, the same does not hold true for Duke, where exploiting video features as in STA <ref type="bibr" target="#b7">[8]</ref> and NVAN appears rewarding. We leave the investigation of further improvements on V2V to future works. As of today, our proposals is the only one guaranteeing consistent and stable results under both I2V and V2V settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis on VKD</head><p>In the absence of camera information. Here, we address the setting where we do not have access to camera information. As an example, when dealing with animal re-id this information often lacks and datasets come with images and labels solely: can VKD still provide any improvement? We think so, as one can still exploit the visual diversity lying in a bag of randomly sampled images. To demonstrate our claim, we test our proposal on Amur Tigers re-identification (ATRW), which was conceived as an Image-To-Image dataset. During comparisons: i) since other works do not conform to a unique backbone, here we opt for ResNet-101; ii) as common practice in this benchmark [21,23,52], we leverage reranking [57]. <ref type="table" target="#tab_6">Table 7</ref> compares VKD against the top scorers in the "Computer Vision for Wildlife Conservation 2019" competition. Importantly, the student ResVKD-101 improves over its teacher (1.5% on mAP and 2.9% on top 5 ) and places second behind [21], confirming its effectiveness in a challenging scenario. Moreover, we remark that the top-scorer requires additional annotations -such as body parts and pose information -which we do not exploit.</p><p>Distilling viewpoints vs time. <ref type="figure">Figure 4</ref> shows results of distilling knowledge from multiple views against time (i.e. multiple frames from a tracklet). On one side, as multiple views hold more "visual variety", the student builds a more invariant representation for the identity. On the opposite, a student trained with tracklets still considerably outperforms the teacher. This shows that, albeit the visual variety is reduced, our distillation approach still successfully exploits it.</p><p>VKD reduces the camera bias. As pointed out in [43], the appearance encoded by a CNN is heavily affected by external factors surrounding the target  object (e.g. different backgrounds, viewpoints, illumination . . . ). In this respect, is our proposal effective for reducing such a bias? To investigate this aspect, we perform a camera classification test on both the teacher (e.g. ResNet-34) and the student network (e.g. ResVKD-34) by fitting a linear classifier on top of their features, with the aim of predicting the camera the picture is taken from. We freeze all backbone layers and train for 300 epochs (lr = 10 −3 and halved every 50 epochs). <ref type="table" target="#tab_7">Table 8</ref> reports performance on the gallery set for different teachers and students. To provide a better understanding, we include a baseline that computes predictions by sampling from the cameras prior distribution. As expected: i) the teacher outperforms the baseline, suggesting it is in fact biased towards background conditions; ii) the student consistently reduces the bias, confirming VKD encourages the student to focus on identities features and drops viewpointspecific information. Finally, it is noted that time-based distillation does not yield the bias reduction we observe for VKD (see supplementary materials).</p><p>Can performance of the student be obtained without distillation? To highlight the advantages of the two-stage procedure above discussed, we here consider a teacher (ResNet-50) trained straightly using few frames (N = 2) only. First two rows of <ref type="table" target="#tab_8">Table 9</ref> show the performance achieved by this baseline (using tracklets and views respectively). Results show that major improvements come from the teacher-student paradigm we devise (third row), instead of simply reducing the number of input images available to the teacher.</p><p>Student explanation. To further assess the differences between teachers and students, we leverage GradCam [39] to highlight the input regions that have been considered paramount for predicting the identity. <ref type="figure" target="#fig_4">Figure 5</ref>  VKD for various examples from MARS, VeRi-776 and ATRW. In general, the student network pays more attention to the subject of interest compared to its teacher. For person and animal Re-ID, background features are suppressed (third and last columns) while attention tends to spread to the whole subject (first and fourth columns). When dealing with vehicle Re-ID, one can appreciate how the attention becomes equally distributed on symmetric parts, such as front and rear lights (second, seventh and last columns). Please see supplementary materials for more examples, as well as a qualitative analysis of some of our model errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Distillation.</head><p>Differently from other approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, VKD is not confined to self-distillation, but instead allows the knowledge transfer from a complex architecture (e.g. ResNet-101) into a simpler one, such as MobileNet-V2 or ResNet-34 (cross-distillation). Here, drawing inspirations from the model compression area, we attempt to reduce the network complexity but, at the same time, increase the profit we already achieve through self-distillation. In this respect, <ref type="table" target="#tab_2">Table 11</ref> shows results of cross-distillation, for various combinations of a teacher and a student. It appears that better the teacher, better the student: as  an example, ResVKD-34 gains an additional 3% mAP on Duke when educated by ResNet-101 rather than "itself".</p><p>On the impact of loss terms. We perform a thorough ablation study (Table 10) on the student loss (Eq. 5). It is noted that leveraging ground truth solely (second row) hurts performance. Differently, best performance for both metrics are obtained exploiting teacher signal (from the third row onward), with particular emphasis to L DP , which proves to be a fundamental component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>An effective Re-ID method requires visual descriptors robust to changes in both background appearances and viewpoints. Moreover, its effectiveness should be ensured even for queries composed of a single image. To accomplish these, we proposed Views Knowledge Distillation (VKD), a teacher-student approach where the student observes only a small subset of input views. This strategy encourages the student to discover better representations: as a result, it outperforms its teacher at the end of the training. Importantly, VKD shows robustness on diverse domains (person, vehicle and animal), surpassing by a wide margin the state of the art in I2V. Thanks to extensive analysis, we highlight that the student presents stronger focus on the target and reduces the camera bias. Neural Information Processing Systems (2016) 42. Tang, Z., Naphade, M., Birchfield, S., Tremblay, J., Hodge, W., Kumar, R., Wang, S., Yang, X.: Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data. In: IEEE International Conference on Computer Vision (2019) 43. Tian, M., Yi, S., Li, H., Li, S., Zhang, X., Shi, J., Yan, J., Wang, X.: Eliminating background-bias for robust person re-identification. Distilling viewpoints vs time: impact on camera bias. As discussed in the main paper (Introduction and Sec. 4.4), limiting the teacher-student transfer to the temporal axis does not explicitly encourage invariance and robustness to different viewpoints. To further prove such a claim, we again measure the camera bias lying in high-level features, in the same manner as described in Sec. 4.4 of the main paper. This time, though, we focus on a student accessing fewer frames from the same tracklet, thus being educated to capture time information solely. <ref type="table" target="#tab_2">Table 1</ref> compares this strategy (third row) with our proposal (fourth row), which instead forces the transfer at viewpoint level. As expected: i) time-based distillation performs similarly to the teacher, confirming its poor ability to confer robustness to shifts in background appearance; ii) as advocated by our work, a student shows a lower camera bias when trained on different viewpoints instead of using temporal information only.</p><p>Student explanation -other examples. In Sec. 4.4 of the main paper, we investigate which regions the student focuses on, showing that it pays higher attention to foreground details when compared to its teacher. We observe that this happens systematically, especially when dealing with person Re-ID. <ref type="figure" target="#fig_2">Figure 1</ref> reports additional comparisons between the explanations provided by the teacher and its student on Duke-Video-ReID <ref type="bibr">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Errors Analysis</head><p>We provide here some visual examples of the errors of our method and try to investigate their nature. With reference to the Video-To-Video setting on MARS <ref type="bibr">[2]</ref>, our model (ResVKD-50) misidentifies 223 out of <ref type="table" target="#tab_2">Table 1</ref>. Analysis on camera bias -in terms of viewpoint classification accuracyfor different methods. We indicate with "ResTKD-50" a student restricted to time information solely. 1980 top-1 matchings. From an analysis computed on top of these 223 cases, we identify four different categories of errors. We also asked two external researchers to annotate the errors according to these four classes as follows: a) True errors: the network associates the query to a wrong identity from the gallery set <ref type="figure" target="#fig_8">(Figure 2a</ref>). This often happens when similar clothes and appearances between the two identities fool the network. Out of 223, 103 (46.2%) were identified as true errors; b) Wrong ID Annotations: the ground truth indicates that the network associates the query to a wrong identity from the gallery set. However -for a limited set of queries -this does not hold true when visually inspecting the gallery identity. This is due to annotation errors, probably caused by a drift in the tracker <ref type="figure" target="#fig_8">(Figure 2b</ref>). Out of 223, 29 (13.0%) were identified as true errors; c) Couples of People: some crops depict more than one subject (e.g. two) but only one can be associated with the tracklet id ( <ref type="figure" target="#fig_8">Figure 2c</ref>). Out of 223, 37 (16.6%) were identified as errors involving frames with more than one person; d) Misleading Distractors: cases in which the subject has been correctly identified, but the gallery tracklet was erroneously indicated as a distractor. Again, because this set has not been manually checked, some distractors are valid as they depict people <ref type="figure" target="#fig_8">(Figure 2d</ref>). Out of 223, 54 (24.2%) were identified as misleading distractors;</p><p>It is worth noting that the presence of the last three types of errors places a limit on the maximum score a method can obtain.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Two examples of tracklets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>MARS VeRi- 776 (</head><label>776</label><figDesc>b) Distances between tracklets features. (c) Two examples of multiview sets. (d) Distances when ensambling views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Visual comparison between tracklets and viewpoints variety, on person (MARS [55]) and vehicle (VeRi-776 [25]) re-id. Right: pairwise distances computed on top of features from ResNet-50. Inputs batches comprise 192 sets from 16 different identities, grouped by ground truth identity along each axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Performance (mAP) in the Image-To-Video setting when changing at evaluation time the number of frames in each gallery tracklet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Model explanation via GradCam[39] on ResNet-50 (teacher) and ResVKD-50 (student). The student favours visual details characterising the target, discarding external and uninformative patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>18. Li, S., Li, J., Lin, W., Tang, H.: Amur tiger re-identification in the wild. arXiv preprint arXiv:1906.05586 (2019) 19. Li, Z., Hoiem, D.: Learning without forgetting. In: European Conference on Computer Vision (2016) 20. Liao, S., Hu, Y., Zhu, X., Li, S.Z.: Person re-identification by local maximal occurrence representation and metric learning. In: IEEE International Conference on Computer Vision and Pattern Recognition (2015) 21. Liu, C., Zhang, R., Guo, L.: Part-pose guided amur tiger re-identification. In: IEEE International Conference on Computer Vision Workshops (2019) 22. Liu, C.T., Wu, C.W., Wang, Y.C.F., Chien, S.Y.: Spatially and temporally efficient non-local attention network for video-based person re-identification. In: British Machine Vision Conference (2019) 23. Liu, N., Zhao, Q., Zhang, N., Cheng, X., Zhu, J.: Pose-guided complementary features learning for amur tiger re-identification. In: IEEE International Conference on Computer Vision Workshops (2019) 24. Liu, X., Zhang, S., Huang, Q., Gao, W.: Ram: a region-aware deep model for vehicle re-identification. In: IEEE International Conference on Multimedia and Expo (ICME) (2018) 25. Liu, X., Liu, W., Mei, T., Ma, H.: A deep learning-based approach to progressive vehicle re-identification for urban surveillance. In: European Conference on Computer Vision (2016) 26. Liu, X., Liu, W., Mei, T., Ma, H.: Provid: Progressive and multimodal vehicle reidentification for large-scale urban surveillance. IEEE Transactions on Multimedia (2017) 27. Liu, Y., Junjie, Y., Ouyang, W.: Quality aware network for set to set recognition. In: IEEE International Conference on Computer Vision (2017) 28. Luo, H., Gu, Y., Liao, X., Lai, S., Jiang, W.: Bag of tricks and a strong baseline for deep person re-identification. In: IEEE International Conference on Computer Vision and Pattern Recognition Workshops (2019) 29. Matiyali, N., Sharma, G.: Video person re-identification using learned clip similarity aggregation. In: The IEEE Winter Conference on Applications of Computer Vision (2020) 30. Nguyen, T.B., Le, T.L., Nguyen, D.D., Pham, D.T.: A reliable image-to-video person re-identification based on feature fusion. In: Asian conference on intelligent information and database systems (2018) 31. Park, J., Woo, S., Lee, J., Kweon, I.S.: BAM: bottleneck attention module. In: British Machine Vision Conference (2018) 32. Qian, J., Jiang, W., Luo, H., Yu, H.: Stripe-based and attribute-aware network: A two-branch deep model for vehicle re-identification. arXiv preprint arXiv:1910.05549 (2019) 33. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures and a data set for multi-target, multi-camera tracking. In: European Conference on Computer Vision (2016) 34. Ristani, E., Tomasi, C.: Features for multi-target multi-camera tracking and reidentification. In: IEEE International Conference on Computer Vision and Pattern Recognition (2018) 35. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. International Conference on Learning Representations (2015) 36. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: IEEE International Conference on Computer Vision and Pattern Recognition (2018) 37. Schneider, S., Taylor, G.W., Linquist, S., Kremer, S.C.: Past, present and future approaches using computer vision for animal re-identification from camera trap data. Methods in Ecology and Evolution (2019) 38. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: IEEE International Conference on Computer Vision and Pattern Recognition (2015) 39. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-cam: Visual explanations from deep networks via gradient-based localization.In: IEEE International Conference on Computer Vision and Pattern Recognition (2017) 40. Si, J., Zhang, H., Li, C.G., Kuen, J., Kong, X., Kot, A.C., Wang, G.: Dual attention matching network for context-aware feature sequence based person reidentification. In: IEEE International Conference on Computer Vision and Pattern Recognition (2018) 41. Sohn, K.: Improved deep metric learning with multi-class n-pair loss objective. In:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Model explanation (Duke-Video-ReID) on ResNet-50 (teacher) and ResVKD-50 (student).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 .</head><label>2</label><figDesc>Different categories of errors on MARS. While almost half of them can be attributed to our method misidentifying between similar appearances (a), the other half are due to the automatic annotation process. In particular, wrong annotation caused by tracking drift (b), more than one identity in the same tracklet (c) and misleading distractors (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Re-ID: MARS [55] comprises 19680 tracklets from 6 different cameras, capturing 1260 different identities (split between 625 for the training set, 626 for the gallery and 622 for the query) with 59 frames per tracklet on average. MARS has shown to be a challenging dataset because it has been automatically annotated, leading to errors and false detections [56]. The Duke [33] dataset was first introduced for multi-target and multi-camera surveillance purposes, and then expanded to include person attributes and identities (414 ones). Consistently with<ref type="bibr" target="#b9">[10,</ref>40,22,29], we use the Duke-Video-ReID [49] variant, where identities have been manually annotated from tracking information 3 . It comprises 5534 video tracklets from 8 different cameras, with 167 frames per tracklet on average. Following<ref type="bibr" target="#b9">[10]</ref>, we extract the first frame of every tracklet when testing in the I2V setting, for both MARS and Duke. Vehicle Re-ID: VeRi-776 [25] has been collected from 20 fixed cameras, capturing vehicles moving on a circular road in a 1.0 km 2 area. It contains 18397 tracklets with an average number of 6 frames per tracklet, capturing 775 identities split between train (575) and gallery (200). The query set shares identities consistently with the gallery, but differently from the other two sets it includes only a single image for each couple (id, camera). Consequently, all recent methods perform the evaluation following the I2V setting. Animal Re-ID: The Amur Tiger [18] Re-Identification in the Wild (ATRW) is a recently introduced dataset collected from a diverse set of wild zoos. The training set includes 107 subjects and 17.6 images on average per identity; no information is provided to aggregate images into tracklets. It is possible to evaluate only the I2I setting through a remote http server. As done in [21], we horizontally flip the training images to duplicate the number of identities available, thus resulting in 214 training identities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Self-Distillation results across datasets, settings and architectures. 70.74 86.67 78.03 81.34 78.70 93.45 91.88 92.97 70.30 93.80 75.01 ResVKD-34 82.17 73.68 87.83 79.50 83.33 80.60 93.73 91.62 95.29 75.97 94.76 79.02 ResNet-50 82.22 73.38 87.88 81.13 82.34 80.19 95.01 94.17 93.50 73.19 93.33 77.88 ResVKD-50 83.89 77.27 88.74 82.22 85.61 83.81 95.01 93.41 95.23 79.17 95.17 82.16 ResNet-101 82.78 74.94 88.59 81.66 83.76 82.89 96.01 94.73 94.28 74.27 94.46 78.20 ResVKD-101 85.91 77.64 89.60 82.65 86.32 85.11 95.44 93.67 95.53 80.62 96.07 83.26 ResNet-50bam 82.58 74.11 88.54 81.19 82.48 80.24 94.87 93.82 93.33 72.73 93.80 77.14 ResVKD-50bam 84.34 78.13 89.39 83.07 86.18 84.54 95.16 93.45 96.01 78.67 95.71 81.57 DenseNet-121 82.68 74.34 89.75 81.93 82.91 80.26 93.73 91.73 91.24 69.24 91.84 74.52 DenseVKD-121 84.04 77.09 89.80 82.84 86.47 84.14 95.44 93.54 94.34 76.23 93.80 79.76 MobileNet-V2 78.64 67.94 85.96 77.10 78.06 74.73 93.30 91.56 88.80 64.68 89.81 69.90 MobileVKD-V2 83.33 73.95 88.13 79.62 83.76 80.83 94.30 92.51 92.85 70.93 92.61 75.27</figDesc><table><row><cell></cell><cell>MARS</cell><cell></cell><cell>Duke</cell><cell></cell><cell>VeRi-776</cell></row><row><cell></cell><cell>I2V</cell><cell>V2V</cell><cell>I2V</cell><cell>V2V</cell><cell>I2I</cell><cell>I2V</cell></row><row><cell></cell><cell cols="6">cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP</cell></row><row><cell>ResNet-34</cell><cell>80.81</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>MARS I2V</figDesc><table><row><cell>Method</cell><cell cols="2">top1 top5 mAP</cell></row><row><cell>P2SNet[46]</cell><cell cols="2">55.3 72.9 -</cell></row><row><cell>Zhang[54]</cell><cell cols="2">56.5 70.6 -</cell></row><row><cell>XQDA[20]</cell><cell cols="2">67.2 81.9 54.9</cell></row><row><cell>TKP[10]</cell><cell cols="2">75.6 87.6 65.1</cell></row><row><cell cols="2">STE-NVAN[22] 80.3 -</cell><cell>68.8</cell></row><row><cell>NVAN[22]</cell><cell>80.1 -</cell><cell>70.2</cell></row><row><cell>MGAT[3]</cell><cell cols="2">81.1 92.2 71.8</cell></row><row><cell>ResVKD-50</cell><cell cols="2">83.9 93.2 77.3</cell></row><row><cell cols="3">ResVKD-50bam 84.3 93.5 78.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Duke I2V VeRi-776 I2V</figDesc><table><row><cell>Method</cell><cell>top1 top5 mAP</cell><cell>Method</cell><cell>top1 top5 mAP</cell></row><row><cell cols="2">STE-NVAN[22] 42.2 -TKP[10] 77.9 -NVAN[22] 78.4 -ResVKD-50 85.6 93.9 83.8 41.3 75.9 76.7 ResVKD-50bam 86.2 94.2 84.5</cell><cell>PROVID[26] VFL-LSTM[1] RAM[24] VANet[7] PAMTRI[42] SAN[32]</cell><cell>76.8 91.4 48.5 88.0 94.6 59.2 88.6 -61.5 89.8 96.0 66.3 92.9 92.9 71.9 93.3 97.1 72.5</cell></row><row><cell></cell><cell></cell><cell cols="2">PROVID-BOT[26] 96.1 97.9 77.2</cell></row><row><cell></cell><cell></cell><cell>ResVKD-50</cell><cell>95.2 98.0 82.2</cell></row><row><cell></cell><cell></cell><cell cols="2">ResVKD-50bam 95.7 98.0 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>MARS V2V Duke V2V</figDesc><table><row><cell>Method</cell><cell cols="2">top1 top5 mAP</cell><cell>Method</cell><cell cols="2">top1 top5 mAP</cell></row><row><cell>DuATN[40]</cell><cell cols="2">81.2 92.5 67.7</cell><cell>DuATN[40]</cell><cell cols="2">81.2 92.5 67.7</cell></row><row><cell>TKP[10]</cell><cell cols="2">84.0 93.7 73.3</cell><cell>Matiyali[29]</cell><cell cols="2">89.3 98.3 88.5</cell></row><row><cell cols="3">CSACSE+OF[6] 86.3 94.7 76.1</cell><cell>TKP[10]</cell><cell>94.0 -</cell><cell>91.7</cell></row><row><cell>STA[8]</cell><cell cols="2">86.3 95.7 80.8</cell><cell cols="2">STE-NVAN[22] 95.2 -</cell><cell>93.5</cell></row><row><cell cols="2">STE-NVAN[22] 88.9 -</cell><cell>81.2</cell><cell>STA[8]</cell><cell cols="2">96.2 99.3 94.9</cell></row><row><cell>NVAN[22]</cell><cell>90.0 -</cell><cell>82.8</cell><cell>NVAN[22]</cell><cell>96.3 -</cell><cell>94.9</cell></row><row><cell>ResVKD-50</cell><cell cols="2">88.7 96.1 82.2</cell><cell>ResVKD-50</cell><cell cols="2">95.0 98.9 93.4</cell></row><row><cell cols="3">ResVKD-50bam 89.4 96.8 83.1</cell><cell cols="3">ResVKD-50bam 95.2 98.6 93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>ATRW I2I</figDesc><table><row><cell>Method</cell><cell>top1 top5 mAP</cell></row><row><cell cols="2">PPbM-a [18] 82.5 93.7 62.9</cell></row><row><cell cols="2">PPbM-b [18] 83.3 93.2 60.3</cell></row><row><cell cols="2">NWPU [52] 94.7 96.7 75.1</cell></row><row><cell>BRL [23]</cell><cell>94.0 96.7 77.0</cell></row><row><cell>NBU [21]</cell><cell>95.6 97.9 81.6</cell></row><row><cell cols="2">ResNet-101 92.3 93.5 75.7</cell></row><row><cell cols="2">ResVKD-101 92.0 96.4 77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Analysis on camera bias, in terms of viewpoint classification accuracy.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VeRi-776</cell><cell>MARS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell>77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76</cell></row><row><cell>Prior Class. ResNet-34 ResVKD-34</cell><cell cols="3">MARS Duke VeRi-776 0.19 0.14 0.06 0.61 0.73 0.55 0.40 0.67 0.51</cell><cell>mAP(%)</cell><cell>80 82 81</cell><cell>76 76 74 75</cell><cell>Viewpoints KD Tracklet KD</cell></row><row><cell>ResNet-101 ResVKD-101</cell><cell>0.71 0.51</cell><cell>0.72 0.70</cell><cell>0.73 0.68</cell><cell cols="3">2 3 4 5 6 7 8 #gallery images Fig. 4. Comparison between time and 2 3 4 5 6 7 8 #gallery images</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">viewpoints distillation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Analysis on different modalities for training the teacher.</figDesc><table><row><cell></cell><cell></cell><cell>MARS</cell><cell></cell><cell>Duke</cell></row><row><cell></cell><cell>Input Bags</cell><cell>I2V</cell><cell>V2V</cell><cell>I2V</cell><cell>V2V</cell></row><row><cell></cell><cell></cell><cell cols="4">cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP</cell></row><row><cell cols="6">ResNet-50 Viewpoints (N = 2) 80.05 71.16 84.70 76.99 77.21 75.19 89.17 87.70</cell></row><row><cell>ResNet-50</cell><cell cols="5">Tracklets (N = 2) 82.32 73.69 87.32 79.91 81.77 80.34 93.73 92.88</cell></row><row><cell cols="6">ResVKD-50 Viewpoints (N = 2) 83.89 77.27 88.74 82.22 85.61 83.81 95.01 93.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Ablation study questioning the impact of each loss term. .26 85.71 77.45 82.62 81.03 94.73 93.29 92.61 70.06 92.31 74.82 84.09 77.37 88.33 82.06 84.90 83.56 95.30 93.79 95.29 79.35 95.29 82.26 83.54 75.18 88.43 80.77 83.90 82.34 94.30 92.97 95.41 78.01 95.17 81.32 84.29 76.82 88.69 81.82 85.33 83.45 95.44 93.90 94.40 77.41 94.87 80.93 83.89 77.27 88.74 82.22 85.61 83.81 95.01 93.41 95.23 79.17 95.17 82.16</figDesc><table><row><cell></cell><cell></cell><cell>MARS</cell><cell>Duke</cell><cell>VeRi-776</cell></row><row><cell></cell><cell>LCE LTR LKL LDP</cell><cell cols="2">I2V cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP cmc1 mAP V2V I2V V2V I2I I2V</cell></row><row><cell></cell><cell cols="3">ResNet-50 (teacher) 82.22 73.38 87.88 81.13 82.34 80.19 95.01 94.17 93.50 73.19 93.33 77.88</cell></row><row><cell>ResVKD-50</cell><cell>(students)</cell><cell>80.25 71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Measuring the benefit of VKD for cross-architecture transfer. MobileNet-V2 (2.2M) 83.33 73.95 83.76 80.83 92.61 75.27 ResNet-101 (42.5M) 83.38 74.72 83.76 81.36 93.03 76.38</figDesc><table><row><cell></cell><cell></cell><cell>MARS</cell><cell>Duke</cell><cell>VeRi-776</cell></row><row><cell>Student (#params)</cell><cell>Teacher (#params)</cell><cell>I2V</cell><cell>I2V</cell><cell>I2V</cell></row><row><cell></cell><cell></cell><cell cols="3">cmc1 mAP cmc1 mAP cmc1 mAP</cell></row><row><cell></cell><cell>ResNet-34 (21.2M)</cell><cell cols="3">82.17 73.68 83.33 80.60 94.76 79.02</cell></row><row><cell>ResNet-34 (21.2M)</cell><cell>ResNet-50 (23.5M)</cell><cell cols="3">83.08 75.45 84.05 82.61 95.05 80.05</cell></row><row><cell></cell><cell cols="4">ResNet-101 (42.5M) 83.43 75.47 85.75 83.65 94.87 80.41</cell></row><row><cell>ResNet-50 (23.5M)</cell><cell cols="4">ResNet-50 (23.5M) ResNet-101 (42.5M) 84.49 77.47 85.90 84.34 95.41 82.99 83.89 77.27 85.61 83.81 95.17 82.16</cell></row><row><cell>MobileNet-V2 (2.2M)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Tang, L., Liu, X., Yao, Z., Yi, S., Shao, J., Yan, J., Wang, S., Li, H., Wang, X.: Orientation invariant feature embedding and spatial temporal regularization for vehicle re-identification. In: IEEE International Conference on Computer Vision (2017) 49. Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., Yang, Y.: Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning. In: IEEE International Conference on Computer Vision and Pattern Recognition (2018) 50. Xie, Z., Li, L., Zhong, X., Zhong, L., Xiang, J.: Image-to-video person reidentification with cross-modal embeddings. Pattern Recognition Letters (2019) 51. Yang, C., Xie, L., Qiao, S., Yuille, A.: Knowledge distillation in generations: More tolerant teachers educate better students. arXiv preprint arXiv:1805.05551 (2018) 52. Yu, J., Su, H., Liu, J., Yang, Z., Zhang, Z., Zhu, Y., Yang, L., Jiao, B.: A strong baseline for tiger re-id and its bag of tricks. In: IEEE International Conference on Computer Vision Workshops (2019) 53. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In: International Conference on Learning Representations (2017) 54. Zhang, D., Wu, W., Cheng, H., Zhang, R., Dong, Z., Cai, Z.: Image-to-video person re-identification with temporally memorized similarity learning. IEEE Transactions on Circuits and Systems for Video Technology (2017) 55. Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S., Tian, Q.: Mars: A video benchmark for large-scale person re-identification. In: European Conference on AImageLab, University of Modena and Reggio Emilia {angelo.porrello, luca.bergamini24, simone.calderara}@unimore.it</figDesc><table><row><cell>Robust Re-Identification by Multiple Views</cell></row><row><cell>Knowledge Distillation -Supplementary</cell></row><row><cell>Material</cell></row><row><cell>Angelo Porrello [0000−0002−9022−8484] , Luca Bergamini [0000−0003−1221−8640] ,</cell></row><row><cell>In: IEEE International Con-ference on Computer Vision and Pattern Recognition (2018) 44. Tung, F., Mori, G.: Similarity-preserving knowledge distillation. In: IEEE Interna-tional Conference on Computer Vision (2019) 45. Ustinova, E., Lempitsky, V.: Learning deep embeddings with histogram loss. In: Neural Information Processing Systems (2016) 46. Wang, G., Lai, J., Xie, X.: P2snet: can an image match a video for person re-identification in an end-to-end way? IEEE Transactions on Circuits and Systems for Video Technology (2017) 47. Wang, J., Zhou, F., Wen, S., Liu, X., Lin, Y.: Deep metric learning with angular loss. In: IEEE International Conference on Computer Vision and Pattern Recog-nition (2017) 56. Zheng, L., Yang, Y., Hauptmann, A.G.: Person re-identification: Past, present and future. arXiv preprint arXiv:1610.02984 (2016) 57. Zhong, Z., Zheng, L., Cao, D., Li, S.: Re-ranking person re-identification with k-reciprocal encoding. In: IEEE International Conference on Computer Vision and Pattern Recognition (2017) 58. Zhou, Y., Liu, L., Shao, L.: Vehicle re-identification by deep hidden multi-view inference. IEEE Transactions on Image Processing (2018) 59. Zhou, Y., Shao, L.: Aware attentive multi-view inference for vehicle re-identification. In: IEEE International Conference on Computer Vision and Pattern Recognition (2018) 48. Wang, Z., Computer Vision (2016) Simone Calderara [0000−0001−9056−1538]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the sake of clarity, all the loss terms are referred to one single example. In the implementation, we extend the penalties to a batch by averaging.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since the teacher parameters are fixed, its entropy is constant and the objective of Eq. 3 reduces to the cross-entropy between yT and yS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In the following, we refer to Duke-Video-ReID simply as Duke. Another variant of Duke named Duke-ReID exists [34], but it does not come with query tracklets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since VeRi-776 does not include any tracklet information in the query set, following all other competitors we limit experiments to the I2V setting only.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The authors would like to acknowledge Farm4Trade for its financial and technical support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational representation learning for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A S</forename><surname>Alfasly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<title level="m">Label refinery: Improving imagenet classification through label progression</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Masked graph attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-views embedding for cattle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Dondona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Del Negro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Signal-Image Technology &amp; Internet-Based Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient video classification using fewer frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vehicle re-identification with viewpoint-aware metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sta: Spatial-temporal attention for largescale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<title level="m">Born again neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal knowledge propagation for image-to-video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
