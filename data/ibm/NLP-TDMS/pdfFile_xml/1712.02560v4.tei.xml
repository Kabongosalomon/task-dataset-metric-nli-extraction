<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
							<email>k-saito@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>2 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
							<email>watanabe@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>2 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
							<email>ushiku@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>2 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>2 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics.</p><p>To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The classification accuracy of images has improved substantially with the advent of deep convolutional neural networks (CNN) which utilize numerous labeled samples <ref type="bibr" target="#b15">[16]</ref>. However, collecting numerous labeled samples in various domains is expensive and time-consuming.</p><p>Domain adaptation (DA) tackles this problem by transferring knowledge from a label-rich domain (i.e., source domain) to a label-scarce domain (i.e., target domain). DA aims to train a classifier using source samples that generalize well to the target domain. However, each domain's samples have different characteristics, which makes the prob- Our proposed method attempts to detect target samples outside the support of the source distribution using task-specific classifiers.</p><p>lem difficult to solve. Consider neural networks trained on labeled source images collected from the Web. Although such neural networks perform well on the source images, correctly recognizing target images collected from a real camera is difficult for them. This is because the target images can have different characteristics from the source images, such as change of light, noise, and angle in which the image is captured. Furthermore, regarding unsupervised DA (UDA), we have access to labeled source samples and only unlabeled target samples. We must construct a model that works well on target samples despite the absence of their labels during training. UDA is the most challenging situation, and we propose a method for UDA in this study.</p><p>Many UDA algorithms, particularly those for training neural networks, attempt to match the distribution of the source features with that of the target without considering the category of the samples <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref>. In particular, domain classifier-based adaptation algorithms have been applied to many tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref>. The methods utilize two players to align distributions in an adversarial manner: domain classifier (i.e., a discriminator) and feature generator. Source and target samples are input to the same feature generator.</p><p>Features from the feature generator are shared by the discriminator and a task-specific classifier. The discriminator is trained to discriminate the domain labels of the features generated by the generator whereas the generator is trained to fool it. The generator aims to match distributions between the source and target because such distributions will mimic the discriminator. They assume that such target features are classified correctly by the task-specific classifier because they are aligned with the source samples.</p><p>However, this method should fail to extract discriminative features because it does not consider the relationship between target samples and the task-specific decision boundary when aligning distributions. As shown in the left side of <ref type="figure" target="#fig_0">Fig. 1</ref>, the generator can generate ambiguous features near the boundary because it simply tries to make the two distributions similar.</p><p>To overcome both problems, we propose to align distributions of features from source and target domain by using the classifier's output for the target samples.</p><p>We introduce a new adversarial learning method that utilizes two types of players: task-specific classifiers and a feature generator. task-specific classifiers denotes the classifiers trained for each task such as object classification or semantic segmentation. Two classifiers take features from the generator. Two classifiers try to classify source samples correctly and, simultaneously, are trained to detect the target samples that are far from the support of the source. The samples existing far from the support do not have discriminative features because they are not clearly categorized into some classes. Thus, our method utilizes the task-specific classifiers as a discriminator. Generator tries to fool the classifiers. In other words, it is trained to generate target features near the support while considering classifiers' output for target samples. Thus, our method allows the generator to generate discriminative features for target samples because it considers the relationship between the decision boundary and target samples. This training is achieved in an adversarial manner. In addition, please note that we do not use domain labels in our method.</p><p>We evaluate our method on image recognition and semantic segmentation. In many settings, our method outperforms other methods by a large margin. The contributions of our paper are summarized as follows:</p><p>• We propose a novel adversarial training method for domain adaptation that tries to align the distribution of a target domain by considering task-specific decision boundaries.</p><p>• We confirm the behavior of our method through a toy problem.</p><p>• We extensively evaluate our method on various tasks: digit classification, object classification, and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Training CNN for DA can be realized through various strategies. Ghifary et al. proposed using an autoencoder for the target domain to obtain domain-invariant features <ref type="bibr" target="#b8">[9]</ref>. Sener et al. proposed using clustering techniques and pseudo-labels to obtain discriminative features <ref type="bibr" target="#b32">[33]</ref>. Taigman et al. proposed cross-domain image translation methods <ref type="bibr" target="#b37">[38]</ref>. Matching distributions of the middle features in CNN is considered to be effective in realizing an accurate adaptation. To this end, numerous methods have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The representative method of distribution matching involves training a domain classifier using the middle features and generating the features that deceive the domain classifier <ref type="bibr" target="#b7">[8]</ref>. This method utilizes the techniques used in generative adversarial networks <ref type="bibr" target="#b9">[10]</ref>. The domain classifier is trained to predict the domain of each input, and the category classifier is trained to predict the task-specific category labels. Feature extraction layers are shared by the two classifiers. The layers are trained to correctly predict the label of source samples as well as to deceive the domain classifier. Thus, the distributions of the middle features of the target and source samples are made similar. Some methods utilize maximum mean discrepancy (MMD) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, which can be applied to measure the divergence in high-dimensional space between different domains. This approach can train the CNN to simultaneously minimize both the divergence and category loss for the source domain. These methods are based on the theory proposed by <ref type="bibr" target="#b1">[2]</ref>, which states that the error on the target domain is bounded by the divergence of the distributions. To our understanding, these distribution aligning methods using GAN or MMD do not consider the relationship between target samples and decision boundaries. To tackle these problems, we propose a novel approach using task-specific classifiers as a discriminator.</p><p>Consensus regularization is a technique used in multisource domain adaptation and multi-view learning, in which multiple classifiers are trained to maximize the consensus of their outputs <ref type="bibr" target="#b22">[23]</ref>. In our method, we address a training step that minimizes the consensus of two classifiers, which is totally different from consensus regularization. Consensus regularization utilizes samples of multi-source domains to construct different classifiers as in <ref type="bibr" target="#b22">[23]</ref>. In order to construct different classifiers, it relies on the different characteristics of samples in different source domains. By contrast, our method can construct different classifiers from only one source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present the detail of our proposed method. First, we give the overall idea of our method in Section 3.1. Second, we explain about the loss function we  <ref type="figure">Figure 2</ref>. (Best viewed in color.) Example of two classifiers with an overview of the proposed method. Discrepancy refers to the disagreement between the predictions of two classifiers. First, we can see that the target samples outside the support of the source can be measured by two different classifiers (Leftmost, Two different classifiers). Second, regarding the training procedure, we solve a minimax problem in which we find two classifiers that maximize the discrepancy on the target sample, and then generate features that minimize this discrepancy.</p><p>used in experiments in Section 3.2. Finally, we explain the entire training procedure of our method in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Idea</head><p>We have access to a labeled source image x s and a corresponding label y s drawn from a set of labeled source images {X s , Y s }, as well as an unlabeled target image x t drawn from unlabeled target images X t . We train a feature generator network G, which takes inputs x s or x t , and classifier networks F 1 and F 2 , which take features from G. F 1 and F 2 classify them into K classes, that is, they output a Kdimensional vector of logits. We obtain class probabilities by applying the softmax function for the vector. We use the notation p 1 (y|x), p 2 (y|x) to denote the K-dimensional probabilistic outputs for input x obtained by F 1 and F 2 respectively.</p><p>The goal of our method is to align source and target features by utilizing the task-specific classifiers as a discriminator in order to consider the relationship between class boundaries and target samples. For this objective, we have to detect target samples far from the support of the source. The question is how to detect target samples far from the support. These target samples are likely to be misclassified by the classifier learned from source samples because they are near the class boundaries. Then, in order to detect these target samples, we propose to utilize the disagreement of the two classifiers on the prediction for target samples. Consider two classifiers (F 1 and F 2 ) that have different characteristics in the leftmost side of <ref type="figure">Fig. 2</ref>. We assume that the two classifiers can classify source samples correctly. This assumption is realistic because we have access to labeled source samples in the setting of UDA. In addition, please note that F 1 and F 2 are initialized differently to obtain different classifiers from the beginning of training. Here, we have the key intuition that target samples outside the support of the source are likely to be classified differently by the two distinct classifiers. This region is denoted by black lines in the leftmost side of <ref type="figure">Fig. 2</ref> (Discrepancy Region). Conversely, if we can measure the disagreement between the two classifiers and train the generator to minimize the disagreement, the generator will avoid generating target features outside the support of the source. Here, we consider measuring the difference for a target sample using the following equation, d(p 1 (y|x t ), p 2 (y|x t )) where d denotes the function measuring divergence between two probabilistic outputs. This term indicates how the two classifiers disagree on their predictions and, hereafter, we call the term as discrepancy. Our goal is to obtain a feature generator that can minimize the discrepancy on target samples.</p><p>In order to effectively detect target samples outside the support of the source, we propose to train discriminators (F 1 and F 2 ) to maximize the discrepancy given target features (Maximize Discrepancy in <ref type="figure">Fig. 2</ref>). Without this operation, the two classifiers can be very similar ones and cannot detect target samples outside the support of the source. We then train the generator to fool the discriminator, that is, by minimizing the discrepancy (Minimize Discrepancy in <ref type="figure">Fig. 2</ref>). This operation encourages the target samples to be generated inside the support of the source. This adversarial learning steps are repeated in our method. Our goal is to obtain the features, in which the support of the target is included by that of the source (Obtained Distributions in <ref type="figure">Fig.  2</ref>). We show the loss function used for discrepancy loss in the next section. Then, we detail the training procedure.</p><p>Step C : Minimize discrepancy on target (Fix F 1 , F 2 )</p><p>Step B : Maximize discrepancy on target (Fix G)</p><formula xml:id="formula_0">d(p1(y|xt), p2(y|xt)) G x t d(p1(y|xt), p2(y|xt)) F 1 F 2 Discrepancy Loss Discrepancy Loss Update ! Fix Class Predictions p1(y|xt) p2(y|xt) G x t F 1 F 2 Update ! Fix p1(y|xt) p2(y|xt)</formula><p>Target sample</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Predictions</head><p>Target sample <ref type="figure">Figure 3</ref>. Adversarial training steps of our method. We separate the network into two modules: generator (G) and classifiers (F1 , F2 ).</p><p>The classifiers learn to maximize the discrepancy Step B on the target samples, and the generator learns to minimize the discrepancy</p><p>Step C. Please note that we employ a training Step A to ensure the discriminative features for source samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discrepancy Loss</head><p>In this study, we utilize the absolute values of the difference between the two classifiers' probabilistic outputs as discrepancy loss:</p><formula xml:id="formula_1">d(p 1 , p 2 ) = 1 K K k=1 |p 1k − p 2k |,<label>(1)</label></formula><p>where the p 1k and p 2k denote probability output of p 1 and p 2 for class k respectively. The choice for L1-distance is based on the Theorem . Additionally, we experimentally found that L2-distance does not work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Steps</head><p>To sum up the previous discussion in Section 3.1, we need to train two classifiers, which take inputs from the generator and maximize d(p 1 (y|x t ), p 2 (y|x t )), and the generator which tries to mimic the classifiers. Both the classifiers and generator must classify source samples correctly. We will show the manner in which to achieve this. We solve this problem in three steps.</p><p>Step A First, we train both classifiers and generator to classify the source samples correctly. In order to make classifiers and generator obtain task-specific discriminative features, this step is crucial. We train the networks to minimize softmax cross entropy. The objective is as follows:</p><formula xml:id="formula_2">min G,F1,F2 L(X s , Y s ).</formula><p>(2)</p><formula xml:id="formula_3">L(X s , Y s ) = −E (xs,ys)∼(Xs,Ys) K k=1 1l [k=ys] log p(y|x s ) (3)</formula><p>Step B In this step, we train the classifiers (F 1 , F 2 ) as a discriminator for a fixed generator (G). By training the classifiers to increase the discrepancy, they can detect the target samples excluded by the support of the source. This step corresponds to Step B in <ref type="figure">Fig. 3</ref>. We add a classification loss on the source samples. Without this loss, we experimentally found that our algorithm's performance drops significantly. We use the same number of source and target samples to update the model. The objective is as follows:</p><formula xml:id="formula_4">min F1,F2 L(X s , Y s ) − L adv (X t ).</formula><p>(4)</p><formula xml:id="formula_5">L adv (X t ) = E xt∼Xt [d(p 1 (y|x t ), p 2 (y|x t ))]<label>(5)</label></formula><p>Step C We train the generator to minimize the discrepancy for fixed classifiers. This step corresponds to Step C in <ref type="figure">Fig. 3</ref>. The number n indicates the number of times we repeat this for the same mini-batch. This number is a hyperparameter of our method. This term denotes the trade-off between the generator and the classifiers. The objective is as follows: min</p><formula xml:id="formula_6">G L adv (X t ).<label>(6)</label></formula><p>These three steps are repeated in our method. To our understanding, the order of the three steps is not important. Instead, our major concern is to train the classifiers and generator in an adversarial manner under the condition that they can classify source samples correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Theoretical Insight</head><p>Since our method is motivated by the theory proposed by Ben-David et al. <ref type="bibr" target="#b0">[1]</ref>, we want to show the relationship between our method and the theory in this section.</p><p>Ben-David et al. <ref type="bibr" target="#b0">[1]</ref> proposed the theory that bounds the expected error on the target samples, R T (h), by using three terms: (i) expected error on the source domain, R S (h); (ii) H∆H-distance (d H∆H (S, T )), which is measured as the discrepancy between two classifiers; and (iii) the shared error of the ideal joint hypothesis, λ. S and T denote source and target domain respectively. Another theory <ref type="bibr" target="#b1">[2]</ref> bounds the error on the target domain, which introduced H-distance (d H (S, T )) for domain divergence. The two theories and their relationships can be explained as follows.</p><p>Theorem 1 Let H be the hypothesis class. Given two domains S and T , we have</p><formula xml:id="formula_7">∀h ∈ H, R T (h) ≤ R S (h) + 1 2 d H∆H (S, T ) + λ ≤ R S (h) + 1 2 d H (S, T ) + λ<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">d H∆H (S, T ) = 2 sup (h,h )∈H 2 E x∼S I h(x) = h (x) − E x∼T I h(x) = h (x) d H (S, T ) = 2 sup h∈H E x∼S I h(x) = 1 − E x∼T I h(x) = 1 , λ = min [R S (h) + R T (h)]</formula><p>Here, R T (h) is the error of hypothesis h on the target domain, and R S (h) is the corresponding error on the source domain. I[a] is the indicator function, which is 1 if predicate a is true and 0 otherwise.</p><p>H-distance is shown to be empirically measured by the error of the domain classifier, which is trained to discriminate the domain of features. λ is a constant-the shared error of the ideal joint hypothesis-which is considered sufficiently low to achieve an accurate adaptation. Earlier studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref> attempted to measure and minimize Hdistance in order to realize the adaptation. As this inequality suggests, H-distance upper-bounds the H∆H-distance. We will show the relationship between our method and H∆Hdistance.</p><p>Regarding d H∆H (S, T ), if we consider that h and h can classify source samples correctly, the term</p><formula xml:id="formula_9">E x∼S I h(x) = h (x)</formula><p>is assumed to be very low. h and h should agree on their predictions on source samples. Thus, d H∆H (S, T ) is approximately calculated as</p><formula xml:id="formula_10">sup (h,h )∈H 2 E x∼T I h(x) = h (x)</formula><p>, which denotes the supremum of the expected disagreement of two classifiers' predictions on target samples.</p><p>We assume that h and h share the feature extraction part. Then, we decompose the hypothesis h into G and F 1 , and h into G and F 2 . G, F 1 and F 2 correspond to the network in our method. If we substitute these notations into the sup</p><formula xml:id="formula_11">(h,h )∈H 2 E x∼T I h(x) = h (x) and for fixed G, the term will become sup F1,F2 E x∼T I [F 1 • G(x) = F 2 • G(x)].<label>(8)</label></formula><p>Furthermore, if we replace sup with max and minimize the term with respect to G, we obtain</p><formula xml:id="formula_12">min G max F1,F2 E x∼T I [F 1 • G(x) = F 2 • G(x)].<label>(9)</label></formula><p>This equation is very similar to the mini-max problem we solve in our method, in which classifiers are trained to maximize their discrepancy on target samples and generator tries to minimize it. Although we must train all networks to minimize the classification loss on source samples, we can see the connection to the theory proposed by <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Classification</head><p>First, we observed the behavior of our model on toy problem. Then, we performed an extensive evaluation of the proposed methods on the following datasets: digits, traffic signs, and object classification.   <ref type="figure" target="#fig_2">Fig. 4(a)</ref> is the model trained only on source samples. <ref type="figure" target="#fig_2">Fig. 4(b)</ref> is the model trained to increase discrepancy of the two classifiers on target samples without using</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of three decision boundaries</head><p>Step C. <ref type="figure" target="#fig_2">Fig. 4(c)</ref> shows our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Toy Datasets</head><p>In the first experiment, we observed the behavior of the proposed method on inter twinning moons 2D problems, in which we used scikit-learn <ref type="bibr" target="#b26">[27]</ref> to generate the target samples by rotating the source samples. The goal of the experiment was to observe the learned classifiers' boundary. For the source samples, we generated a lower moon and an upper moon, labeled 0 and 1, respectively. Target samples were generated by rotating the angle of the distribution of the source samples. We generated 300 source and target samples per class as the training samples. In this experiment, we compared the decision boundary obtained from our method with that obtained from both the model trained only on source samples and from that trained only to increase the discrepancy. In order to train the second comparable model, we simply skipped Step C in Section 3.3 during training. We tested the method on 1000 target samples and visualized the learned decision boundary with source and target samples. Other details including the network architecture used in this experiment are provided in our supplementary material. As we expected, when we trained the two classifiers to increase the discrepancy on the target samples, two classifiers largely disagreed on their predictions on target samples ( <ref type="figure" target="#fig_2">Fig. 4(b)</ref>). This is clear when compared to the source only model ( <ref type="figure" target="#fig_2">Fig. 4(a)</ref>). Two classifiers were trained on the source samples without adaptation, and the boundaries seemed to be nearly the same. Then, our proposed method attempted to generate target samples that reduce the discrepancy. Therefore, we could expect that the two classifiers will be similar. <ref type="figure" target="#fig_2">Fig. 4(c)</ref> demonstrates the assumption. The decision boundaries are drawn considering the target samples. The two classifiers output nearly the same prediction for target samples, and they classified most target samples correctly.  <ref type="figure">Figure 5</ref>. (Best viewed in color.) Left: Relationship between discrepancy loss (blue line) and accuracy (red and green lines) during training. As discrepancy loss decreased, accuracy improved. Right: Visualization of features obtained from last pooling layer of the generator in adaptation from SYN SIGNS to GTSRB using t-SNE <ref type="bibr" target="#b23">[24]</ref>. Red and blue points indicate the target and source samples, respectively. All samples are testing samples. We can see that applying our method makes the target samples discriminative.  <ref type="table">Table 1</ref>. Results of the visual DA experiment on the digits and traffic signs datasets. The results are cited from each study. The score of MMD is cited from DSN <ref type="bibr" target="#b3">[4]</ref>. Please note that † means that the method used a few labeled target samples as validation, which is different from our setting. We repeated each experiment 5 times and report the average and the standard deviation of the accuracy. The accuracy was obtained from classifier F1. Including the methods that used the labeled target samples for validation, our method achieved good performance. MNIST* and USPS* mean that we used all of the training samples to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Digits Datasets</head><p>In this experiment, we evaluate the adaptation of the model on three scenarios. The example datasets are presented in the supplementary material.</p><p>We assessed four types of adaptation scenarios by using the digits datasets, namely MNIST <ref type="bibr" target="#b16">[17]</ref>, Street View House Numbers (SVHN) <ref type="bibr" target="#b25">[26]</ref>, and USPS <ref type="bibr" target="#b13">[14]</ref>. We further evaluated our method on the traffic sign datasets, Synthetic Traffic Signs (SYN SIGNS) <ref type="bibr" target="#b24">[25]</ref> and the German Traffic Signs Recognition Benchmark <ref type="bibr" target="#b34">[35]</ref> (GTSRB). In this experiment, we employed the CNN architecture used in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b2">[3]</ref>. We added batch normalization to each layer in these models. We used Adam <ref type="bibr" target="#b14">[15]</ref> to optimize our model and set the learn-ing rate as 2.0 × 10 −4 in all experiments. We set the batch size to 128 in all experiments. The hyper-parameter peculiar to our method was n, which denotes the number of times we update the feature generator to mimic classifiers. We varied the value of n from 2 to 4 in our experiment and observed the sensitivity to the hyper-parameter. We followed the protocol of unsupervised domain adaptation and did not use validation samples to tune hyper-parameters. The other details are provided in our supplementary material due to a limit of space.</p><p>SVHN→MNIST SVHN <ref type="bibr" target="#b25">[26]</ref> and MNIST <ref type="bibr" target="#b16">[17]</ref> have distinct properties because SVHN datasets contain images with a colored background, multiple digits, and extremely blurred digits, meaning that the domain divergence is very large between these datasets.</p><p>SYN SIGNS→GTSRB In this experiment, we evaluated the adaptation from synthesized traffic signs datasets (SYN SIGNS dataset <ref type="bibr" target="#b6">[7]</ref>) to real-world signs datasets (GTSRB dataset <ref type="bibr" target="#b34">[35]</ref>). These datasets contain 43 types of classes.</p><p>MNIST↔USPS We also evaluate our method on MNIST and USPS datasets <ref type="bibr" target="#b16">[17]</ref> to compare our method with other methods. We followed the different protocols provided by the paper, ADDA <ref type="bibr" target="#b38">[39]</ref> and PixelDA <ref type="bibr" target="#b2">[3]</ref>.</p><p>Results <ref type="table">Table 1</ref> lists the accuracies for the target samples, and <ref type="figure">Fig. 5</ref>(a) and 5(b) show the relationship between the discrepancy loss and accuracy during training. For the source only model, we used the same network architecture as used in our method. Details are provided in the supplementary material. We extensively compared our methods with distribution matching-based methods as shown in <ref type="table">Table 1</ref>. The proposed method outperformed these methods in all settings. The performance improved as we increased the value of n. Although other methods such as ATDA <ref type="bibr" target="#b31">[32]</ref> performed better than our method in some situations, the method utilized a few labeled target samples to decide hyper-parameters for each dataset. The performance of our method will improve too if we can choose the best hyper-parameters for each dataset. As <ref type="figure">Fig. 5</ref>(a) and 5(b) show, as the discrepancy loss diminishes, the accuracy improves, confirming that minimizing the discrepancy for target samples can result in accurate adaptation. We visualized learned features as shown in <ref type="figure">Fig. 5</ref>(c) and 5(d). Our method did not match the distributions of source and target completely as shown in <ref type="figure">Fig. 5(d)</ref>. However, the target samples seemed to be aligned with each class of source samples. Although the target samples did not separate well in the non-adapted situation, they did separate clearly as do source samples in the adapted situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on VisDA Classification Dataset</head><p>We further evaluated our method on an object classification setting. The VisDA dataset <ref type="bibr" target="#b27">[28]</ref> was used in this experiment, which evaluated adaptation from synthetic-object to real-object images. To date, this dataset represents the largest for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation, and testing domains. The source images were generated by rendering 3D models of the same object categories as in the real data from different angles and under different lighting conditions. It contains 152,397 synthetic images. The validation images were collected from MSCOCO <ref type="bibr" target="#b17">[18]</ref> and they amount to 55,388 in total. In our experiment, we considered the images of validation splits as the target domain and trained models in unsupervised domain adaptation settings. We evaluate the performance of ResNet101 <ref type="bibr" target="#b11">[12]</ref> model pre-trained on Imagenet <ref type="bibr" target="#b5">[6]</ref>. The final fully-connected layer was removed and all layers were updated with the same learning rate because this dataset has abundant source and target samples. We regarded the pre-trained model as a generator network and we used three-layered fully-connected networks for classification networks. The batch size was set to 32 and we used SGD with learning rate 1.0 × 10 −3 to optimize the model. We report the accuracy after 10 epochs. The training details for baseline methods are written in our supplementary material due to the limit of space.</p><p>Results Our method achieved an accuracy much better than other distribution matching based methods ( <ref type="table" target="#tab_2">Table 2</ref>). In addition, our method performed better than the source only model in all classes, whereas MMD and DANN perform worse than the source only model in some classes such as car and plant. We can clearly see the clear effectiveness of our method in this regard. In this experiment, as the value of n increase, the performance improved. We think that it was because of the large domain difference between synthetic objects and real images. The generator had to be updated many times to align such distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Semantic Segmentation</head><p>We further applied our method to semantic segmentation. Considering a huge annotation cost for semantic segmentation datasets, adaptation between different domains is an important problem in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Detail</head><p>We used the publicly available synthetic dataset GTA5 <ref type="bibr" target="#b29">[30]</ref> or Synthia <ref type="bibr" target="#b30">[31]</ref> as the source domain dataset and real dataset Cityscapes <ref type="bibr" target="#b4">[5]</ref> as the target domain dataset. Following the work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, the Cityscapes validation set was used as our test set. As our training set, the Cityscapes train set was used. During training, we randomly sampled just a single sample (setting the batch size to 1 because of the GPU memory limit) from both the images (and their labels) of the source dataset and the remaining images of the target dataset but with no labels.</p><p>We applied our method to VGG-16 <ref type="bibr" target="#b33">[34]</ref> based FCN-8s <ref type="bibr" target="#b19">[20]</ref> and DRN-D-105 <ref type="bibr" target="#b41">[42]</ref> to evaluate our method. The details of models, including their architecture and other hyper-parameters, are described in the supplementary material.</p><p>We used Momentum SGD to optimize our model and set the momentum rate to 0.9 and the learning rate to 1.0 × 10 −3 in all experiments. The image size was resized to 1024×512. Here, we report the output of F 1 after 50,000 iterations.</p><p>Results <ref type="table" target="#tab_4">Table 3</ref>, <ref type="table">Table 4</ref>, and <ref type="figure">Fig. 6</ref> show quantitative and qualitative results, respectively. These results illustrate that even with a large domain difference between synthetic to real images, our method is capable of improving the performance. Considering the mIoU of the model trained only on source samples, we can see the clear effectiveness of our adaptation method. Also, compared to the score of DANN, our method shows clearly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Source Only DANN Ours <ref type="figure">Figure 6</ref>. Qualitative results on adaptation from GTA5 to Cityscapes. DRN-105 is used to obtain these results.   <ref type="table">Table 4</ref>. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a new approach for UDA, which utilizes task-specific classifiers to align distributions. We propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to fool the classifiers. Since the generator uses feedback from task-specific classifiers, it will avoid generating target features near class boundaries. We extensively evaluated our method on image classifica-tion and semantic segmentation datasets. In almost all experiments, our method outperformed state-of-the-art methods. We provide the results when applying gradient reversal layer <ref type="bibr" target="#b6">[7]</ref> in the supplementary material, which enables to update parameters of the model in one step.</p><p>We would like to show supplementary information for our main paper. First, we introduce the detail of the experiments. Finally, we show some additional results of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Toy Dataset Experiment</head><p>We show the detail of experiments on toy dataset in main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detail on experimental setting</head><p>The detail of experiment on toy dataset is shown in this section. When generating target samples, we set the rotation angle 30 in experiments of our main paper. We used Adam with learning rate 2.0 × 10 −4 to optimizer the model. The batch size was set to 200. For a feature generator, we used 3-layered fully-connected networks with 15 neurons in hidden layer, in which ReLU is used as the activation function. For classifiers, we used three-layed fully-connected networks with 15 neurons in hidden layer and 2 neurons in output layer. The decision boundary shown in the main paper is obtained when we rotate the source samples 30 degrees to generate target samples. We set n to 3 in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment on Digit Dataset</head><p>We report the accuracy after training 20,000 iterations except for the adaptation between MNIST and USPS. Due to the lack of training samples of the datasets, we stopped training after 200 epochs (13 iterations per one epoch) to prevent over-fitting. We followed the protocol presented by <ref type="bibr" target="#b6">[7]</ref> in the following three adaptation scenarios. SVHN→MNIST In this adaptation scenario, we used the standard training set as training samples, and testing set as testing samples both for source and target samples.</p><p>SYN DIGITS→SVHN We used 479400 source samples and 73257 target samples for training, 26032 samples for testing.</p><p>SYN SIGNS→GTSRB We randomly selected 31367 samples for target training and evaluated the accuracy on the rest.</p><p>MNIST↔USPS In this setting, we followed the different protocols provided by the paper, ADDA <ref type="bibr" target="#b38">[39]</ref> and Pix-elDA <ref type="bibr" target="#b2">[3]</ref>. The former protocol provides the setting where a part of training samples are utilized during training. 2,000 training samples are picked up for MNIST and 1,800 samples are used for USPS. The latter one allows to utilize all training samples during training. We utilized the architecture used as a classification network in PixelDA <ref type="bibr" target="#b2">[3]</ref>. We added Batch Normalization layer to the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment on VisDA Classification Dataset</head><p>The detail of architecture we used and the detail of other methods are shown in this section.</p><p>Class Balance Loss In addition to feature alignment loss, we used a class balance loss to improve the accuracy in this experiment. Please note that we incorporated this loss in comparable methods too. We aimed to assign the target samples to each classes equally. Without this loss, the target samples can be aligned in an unbalanced way. The loss is calculated as follows:</p><formula xml:id="formula_13">E xt∼Xt K k=1 log p(y = k|x t )<label>(10)</label></formula><p>The constant term λ = 0.01 was multiplied to the loss and add this loss in Step 2 and Step 3 of our method. This loss was also introduced in MMD and DANN too when updating parameters of the networks. For the fully-connected layers of classification networks, we set the number of neurons to 1000. In order to fairly compare our method with others, we used the exact the same architecture for other methods.</p><p>MMD We calculated the maximum mean discrepancy (MMD) <ref type="bibr" target="#b20">[21]</ref>, namely the last layer of feature generator networks. We used RBF kernels to calculate the loss. We used the the following standard deviation parameters: σ = [0.1, 0.05, 0.01, 0.0001, 0.00001]</p><p>We changed the number of the kernels and their parameters, but we could not observe significant performance difference. We report the performance after 5 epochs. We could not see any improvement after the epoch. DANN To train a model ( <ref type="bibr" target="#b6">[7]</ref>), we used two-layered domain classification networks. We set the number of neurons in the hidden layer as 100. We also used Batch Normalization, ReLU and dropout layer. Experimentally, we did not see any improvement when the network architecture is changed. According to the original method ( <ref type="bibr" target="#b6">[7]</ref>), learning rate is decreased every iteration. However, in our experiment, we could not see improvement, thus, we fixed learning rate 1.0 × 10 −3 . In addition, we did not introduce gradient reversal layer for our model. We separately update discriminator and generator. We report the accuracy after 1 epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Semantic Segmentation</head><p>We describe the details of our experiments on semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details</head><p>Datasets GTA <ref type="bibr" target="#b29">[30]</ref>, Synthia <ref type="bibr" target="#b30">[31]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref> are vehicle-egocentric image datasets but GTA and Synthia are synthetic and Cityscapes is real world dataset. GTA is collected from the open world in the realistically rendered computer game Grand Theft Auto V (GTA, or GTA5). It contains 24,996 images, whose semantic segmentation annotations are fully compatible with the classes used in Cityscapes. Cityscapes is collected in 50 cities in Germany and nearby countries. We only used dense pixel-level annotated dataset collected in 27 cities. It contains 2,975 training set, 500 validation set, and 1525 test set. We used training and validation set. Please note that the labels of Cityscapes are just used for evaluation and never used in training. Similarly, we used the training splits of Synthia dataset to train our model.</p><p>Training Details When training, we ignored the pixelwise loss that is annotated backward (void). Therefore, when testing, no predicted backward label existed. The weight decay ratio was set to 2 × 10 −5 and we used no augmentation methods.</p><p>Network Architecture We applied our method to FCN-8s based on VGG-16 network. Convolution layers in original VGG-16 networks are used as generator and fullyconnected layers are used as classifiers. For DRN-D-105, we followed the implementation of https://github. com/fyu/drn. We applied our method to dilated residual networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> for base networks. We used DRN-D-105 model. We used the last convolution networks as classifier networks. All of lower layers are used as a generator.</p><p>Evaluation Metrics As evaluation metrics, we use intersection-over-union (IoU) and pixel accuracy. We use the evaluation code 1 released along with VisDA challenge <ref type="bibr" target="#b27">[28]</ref>. It calculates the PASCAL VOC intersection-overunion, i.e., IoU = TP TP+FP+FN , where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. For further discussing our result, we also compute pixel accuracy, pixelAcc. = Σinii Σiti , where n ii denotes number of pixels of class i predicted to belong to class j and t i denotes total number of pixels of class i in ground truth segmentation. <ref type="bibr" target="#b0">1</ref> https://github.com/VisionLearningGroup/taskcv-2017public/blob/master/segmentation/eval.py</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training via Gradient Reversal Layer</head><p>In our main paper, we provide the training procedure that consists of three training steps and the number of updating generator (k) is a hyper-parameter in our method. We found that introducing gradient reversal layer (GRL) <ref type="bibr" target="#b6">[7]</ref> enables to update our model in only one step and works well in many settings. This improvement makes training faster and deletes hyper-parameter in our method. We provide the detail of the improvement and some experimental results here.</p><p>Training Procedure We simply applied gradient reversal layer when updating classifiers and generator in an adversarial manner. The layer flips the sign of gradients when back-propagating the gradient. Therefore, update for maximizing the discrepancy via classifier and minimizing it via generator was conducted simultaneously. We publicize the code with this implementation.</p><p>Results The experimental results on semantic segmentation are shown in <ref type="table" target="#tab_5">Table 5</ref>,6, and <ref type="figure" target="#fig_4">Fig. 7</ref>. Our model with GRL shows the same level of performance compared to the model trained with our proposed training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to Hyper-Parameter</head><p>The number of updating generator is the hyperparameter peculiar to our method. Therefore, we show additional experimental results related to it. We employed the adaptation from SVHN to MNIST and conducted experiments where n = 5, 6. The accuracy was 96.0% and 96.2% on average. The accuracy seems to increase as we increase the value though it saturates. Training time required to obtain high accuracy can increase too. However, considering the results of GRL on semantic segmentation, the relationship between the accuracy and the number of n seems to depend on which datasets to adapt.  <ref type="table">Table 6</ref>. Adaptation results on the semantic segmentation. We evaluate adaptation from Synthia to Cityscapes dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(Best viewed in color.) Comparison of previous and the proposed distribution matching methods.. Left: Previous methods try to match different distributions by mimicing the domain classifier. They do not consider the decision boundary. Right:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(Best viewed in color.) Red and green points indicate the source samples of class 0 and 1, respectively. Blue points are target samples generated by rotating source samples. The dashed and normal lines are two decision boundaries in our method. The pink and light green regions are where the results of both classifiers are class 0 and 1, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) SVHN to MNIST (b) SYN SIGN to GTSRB (c) Source Only (d) Adapted (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on adaptation from GTA5 to Cityscapes. From top to bottom, input, ground truth, result of source only model, DANN, and our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>= 2) 94.2±2.6 93.5±0.4 92.1±0.8 93.1±1.9 90.0±1.4 Ours (n = 3) 95.9±0.5 94.0±0.4 93.8±0.8 95.6±0.9 91.8±0.9 Ours (n = 4) 96.2±0.4 94.4±0.3 94.2±0.7 96.5±0.3 94.1±0.3</figDesc><table><row><cell></cell><cell>SVHN</cell><cell>SYNSIG</cell><cell>MNIST</cell><cell>MNIST*</cell><cell>USPS</cell></row><row><cell>METHOD</cell><cell>to</cell><cell>to</cell><cell>to</cell><cell>to</cell><cell>to</cell></row><row><cell></cell><cell>MNIST</cell><cell>GTSRB</cell><cell>USPS</cell><cell>USPS*</cell><cell>MNIST</cell></row><row><cell>Source Only</cell><cell>67.1</cell><cell>85.1</cell><cell>76.7</cell><cell>79.4</cell><cell>63.4</cell></row><row><cell></cell><cell cols="4">Distribution Matching based Methods</cell><cell></cell></row><row><cell>MMD  † [21] DANN  † [7] DSN  † [4] ADDA [39]</cell><cell>71.1 71.1 82.7 76.0±1.8</cell><cell>91.1 88.7 93.1 -</cell><cell>-77.1±1.8 91.3 89.4±0.2</cell><cell>81.1 85.1 --</cell><cell>-73.0±0.2 -90.1±0.8</cell></row><row><cell>CoGAN [19]</cell><cell>-</cell><cell>-</cell><cell>91.2±0.8</cell><cell>-</cell><cell>89.1±0.8</cell></row><row><cell>PixelDA [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.9</cell><cell>-</cell></row><row><cell cols="4">Ours (n Other Methods</cell><cell></cell><cell></cell></row><row><cell>ATDA  † [32] ASSC [11]</cell><cell cols="2">86.2 95.7±1.5 82.8±1.3 96.2</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>DRCN [9]</cell><cell>82.0±0.1</cell><cell>-</cell><cell>91.8±0.09</cell><cell>-</cell><cell>73.7±0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of ResNet101 model fine-tuned on the VisDA dataset. The reported accuracy was obtained after 10 epoch updates.</figDesc><table><row><cell>Method</cell><cell cols="2">plane bcycl bus</cell><cell cols="9">car horse knife mcycl person plant sktbrd train truck mean</cell></row><row><cell>Source Only</cell><cell>55.1</cell><cell cols="2">53.3 61.9 59.1 80.6</cell><cell>17.9</cell><cell>79.7</cell><cell>31.2</cell><cell>81.0</cell><cell>26.5</cell><cell>73.5</cell><cell>8.5</cell><cell>52.4</cell></row><row><cell>MMD [21]</cell><cell>87.1</cell><cell cols="2">63.0 76.5 42.0 90.3</cell><cell>42.9</cell><cell>85.9</cell><cell>53.1</cell><cell>49.7</cell><cell>36.3</cell><cell cols="2">85.8 20.7</cell><cell>61.1</cell></row><row><cell>DANN [7]</cell><cell>81.9</cell><cell cols="2">77.7 82.8 44.3 81.2</cell><cell>29.5</cell><cell>65.1</cell><cell>28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>Ours (n = 2)</cell><cell>81.1</cell><cell cols="2">55.3 83.6 65.7 87.6</cell><cell>72.7</cell><cell>83.1</cell><cell>73.9</cell><cell>85.3</cell><cell>47.7</cell><cell cols="2">73.2 27.1</cell><cell>69.7</cell></row><row><cell>Ours (n = 3)</cell><cell>90.3</cell><cell cols="2">49.3 82.1 62.9 91.8</cell><cell>69.4</cell><cell>83.8</cell><cell>72.8</cell><cell>79.8</cell><cell>53.3</cell><cell cols="2">81.5 29.7</cell><cell>70.6</cell></row><row><cell>Ours (n = 4)</cell><cell>87.0</cell><cell cols="2">60.9 83.7 64.0 88.9</cell><cell>79.6</cell><cell>84.7</cell><cell>76.9</cell><cell>88.6</cell><cell>40.3</cell><cell cols="2">83.0 25.8</cell><cell>71.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.</figDesc><table><row><cell>Network</cell><cell>method</cell><cell cols="9">mIoU road sdwlk bldng wall fence pole light sign vgttn sky prsn ridr</cell><cell>car</cell><cell cols="2">bus mcycl bcycl</cell></row><row><cell>VGG-16</cell><cell>Source Only [43]</cell><cell>22.0</cell><cell>5.6</cell><cell>11.2</cell><cell>59.6</cell><cell>0.8</cell><cell>0.5</cell><cell>21.5 8.0</cell><cell>5.3</cell><cell cols="3">72.4 75.6 35.1 9.0 23.6 4.5</cell><cell>0.5</cell><cell>18.0</cell></row><row><cell></cell><cell>FCN Wld [13]</cell><cell>20.2</cell><cell>11.5</cell><cell>19.6</cell><cell>30.8</cell><cell>4.4</cell><cell>0.0</cell><cell cols="5">20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2</cell><cell>0.2</cell><cell>0.6</cell></row><row><cell></cell><cell>CDA (I+SP) [43]</cell><cell>29.0</cell><cell>65.2</cell><cell>26.1</cell><cell>74.9</cell><cell>0.1</cell><cell>0.5</cell><cell>10.7 3.7</cell><cell>3.0</cell><cell cols="3">76.1 70.6 47.1 8.2 43.2 20.7</cell><cell>0.7</cell><cell>13.1</cell></row><row><cell cols="2">DRN 105 Source Only</cell><cell>23.4</cell><cell>14.9</cell><cell>11.4</cell><cell>58.7</cell><cell>1.9</cell><cell>0.0</cell><cell>24.1 1.2</cell><cell>6.0</cell><cell cols="3">68.8 76.0 54.3 7.1 34.2 15.0</cell><cell>0.8</cell><cell>0.0</cell></row><row><cell></cell><cell>DANN [7]</cell><cell>32.5</cell><cell>67.0</cell><cell>29.1</cell><cell cols="2">71.5 14.3</cell><cell>0.1</cell><cell cols="5">28.1 12.6 10.3 72.7 76.7 48.3 12.7 62.5 11.3</cell><cell>2.7</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (k=2)</cell><cell>36.3</cell><cell>83.5</cell><cell>40.9</cell><cell>77.6</cell><cell>6.0</cell><cell>0.1</cell><cell>27.9 6.2</cell><cell>6.0</cell><cell cols="3">83.1 83.5 51.5 11.8 78.9 19.8</cell><cell>4.6</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (k=3)</cell><cell>37.3</cell><cell>84.8</cell><cell>43.6</cell><cell>79.0</cell><cell>3.9</cell><cell>0.2</cell><cell>29.1 7.2</cell><cell>5.5</cell><cell cols="3">83.8 83.1 51.0 11.7 79.9 27.2</cell><cell>6.2</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (k=4)</cell><cell>37.2</cell><cell>88.1</cell><cell>43.2</cell><cell>79.1</cell><cell>2.4</cell><cell>0.1</cell><cell>27.3 7.4</cell><cell>4.9</cell><cell cols="3">83.4 81.1 51.3 10.9 82.1 29.0</cell><cell>5.7</cell><cell>0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Network methodmIoU road sdwk bldng wall fence pole light sign vgttn trrn sky person rider car truck bus train mcycl bcycl Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.</figDesc><table><row><cell>VGG-16</cell><cell>Source Only</cell><cell></cell><cell>24.9</cell><cell cols="2">25.9 10.9</cell><cell>50.5</cell><cell>3.3</cell><cell cols="5">12.2 25.4 28.6 13.0 78.3</cell><cell cols="2">7.3 63.9</cell><cell>52.1</cell><cell>7.9</cell><cell>66.3</cell><cell>5.2</cell><cell>7.8</cell><cell>0.9</cell><cell>13.7</cell><cell>0.7</cell></row><row><cell></cell><cell cols="2">FCN Wld [13]</cell><cell>27.1</cell><cell cols="2">70.4 32.4</cell><cell cols="2">62.1 14.9</cell><cell>5.4</cell><cell cols="3">10.9 14.2 2.7</cell><cell cols="3">79.2 21.3 64.6</cell><cell>44.1</cell><cell>4.2</cell><cell>70.4</cell><cell>8.0</cell><cell>7.3</cell><cell>0.0</cell><cell>3.5</cell><cell>0.0</cell></row><row><cell></cell><cell cols="2">CDA (I) [43]</cell><cell>23.1</cell><cell cols="2">26.4 10.8</cell><cell cols="2">69.7 10.2</cell><cell>9.4</cell><cell cols="4">20.2 13.6 14.0 56.9</cell><cell cols="2">2.8 63.8</cell><cell>31.8</cell><cell>10.6 60.5 10.9</cell><cell>3.4 10.9</cell><cell>3.8</cell><cell>9.5</cell></row><row><cell></cell><cell>Ours (k=2)</cell><cell></cell><cell>28.0</cell><cell cols="2">87.4 15.4</cell><cell cols="2">75.5 17.4</cell><cell>9.9</cell><cell cols="3">16.2 11.9 0.6</cell><cell cols="3">80.6 28.1 60.2</cell><cell>32.5</cell><cell>0.9</cell><cell>75.4 13.6</cell><cell>4.8</cell><cell>0.1</cell><cell>0.7</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (k=3)</cell><cell></cell><cell>27.3</cell><cell cols="2">86.0 10.5</cell><cell cols="2">75.1 20.0</cell><cell>2.9</cell><cell cols="2">19.4 8.4</cell><cell>0.7</cell><cell cols="3">78.4 19.4 74.8</cell><cell>23.2</cell><cell>0.3</cell><cell>74.1 14.3 10.4 0.2</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (k=4)</cell><cell></cell><cell>28.8</cell><cell>86.4</cell><cell>8.5</cell><cell cols="2">76.1 18.6</cell><cell>9.7</cell><cell cols="2">14.9 7.8</cell><cell>0.6</cell><cell cols="3">82.8 32.7 71.4</cell><cell>25.2</cell><cell>1.1</cell><cell>76.3 16.1 17.1 1.4</cell><cell>0.2</cell><cell>0.0</cell></row><row><cell></cell><cell>Ours (GRL)</cell><cell></cell><cell>27.3</cell><cell cols="2">86.2 16.1</cell><cell cols="2">74.4 20.7</cell><cell>9.5</cell><cell cols="3">21.5 14.8 0.1</cell><cell cols="3">80.4 27.8 50.3</cell><cell>33.9</cell><cell>1.2</cell><cell>67.6 10.8</cell><cell>3.0</cell><cell>0.2</cell><cell>0.9</cell><cell>0.0</cell></row><row><cell>DRN-105</cell><cell>Source Only</cell><cell></cell><cell>22.2</cell><cell cols="2">36.4 14.2</cell><cell cols="5">67.4 16.4 12.0 20.1 8.7</cell><cell>0.7</cell><cell cols="3">69.8 13.3 56.9</cell><cell>37.0</cell><cell>0.4</cell><cell>53.6 10.6</cell><cell>3.2</cell><cell>0.2</cell><cell>0.9</cell><cell>0.0</cell></row><row><cell></cell><cell>DANN [7]</cell><cell></cell><cell>32.8</cell><cell cols="2">64.3 23.2</cell><cell cols="9">73.4 11.3 18.6 29.0 31.8 14.9 82.0 16.8 73.2</cell><cell>53.9</cell><cell>12.4 53.3 20.4 11.0 5.0</cell><cell>18.7</cell><cell>9.8</cell></row><row><cell></cell><cell>Ours (k=2)</cell><cell></cell><cell>39.7</cell><cell cols="2">90.3 31.0</cell><cell cols="9">78.5 19.7 17.3 28.6 30.9 16.1 83.7 30.0 69.1</cell><cell>58.5</cell><cell>19.6 81.5 23.8 30.0 5.7</cell><cell>25.7</cell><cell>14.3</cell></row><row><cell></cell><cell>Ours (k=3)</cell><cell></cell><cell>38.9</cell><cell cols="2">90.8 35.6</cell><cell cols="9">80.5 22.9 15.5 27.5 24.9 15.1 84.2 31.8 77.4</cell><cell>54.6</cell><cell>17.2 82.0 21.6 29.0 1.3</cell><cell>21.8</cell><cell>5.3</cell></row><row><cell></cell><cell>Ours (k=4)</cell><cell></cell><cell>38.1</cell><cell cols="2">89.2 23.2</cell><cell cols="6">80.2 23.6 18.1 27.7 25.0 9.3</cell><cell cols="3">84.4 34.6 79.5</cell><cell>53.2</cell><cell>16.0 84.1 26.0 22.5 5.2</cell><cell>16.7</cell><cell>4.8</cell></row><row><cell></cell><cell>Ours (GRL)</cell><cell></cell><cell>39.9</cell><cell cols="2">90.4 34.5</cell><cell cols="9">79.3 20.4 20.9 33.1 28.3 18.5 82.4 22.6 75.5</cell><cell>57.6</cell><cell>18.6 82.7 24.1 25.6 7.6</cell><cell>23.9</cell><cell>12.3</cell></row><row><cell></cell><cell>Network</cell><cell cols="2">method</cell><cell></cell><cell cols="12">mIoU road sdwlk bldng wall fence pole light sign vgttn sky prsn ridr</cell><cell>car</cell><cell>bus mcycl bcycl</cell></row><row><cell></cell><cell>VGG-16</cell><cell cols="3">Source Only [43]</cell><cell>22.0</cell><cell>5.6</cell><cell>11.2</cell><cell>59.6</cell><cell>0.8</cell><cell>0.5</cell><cell cols="2">21.5 8.0</cell><cell>5.3</cell><cell cols="3">72.4 75.6 35.1 9.0 23.6 4.5</cell><cell>0.5</cell><cell>18.0</cell></row><row><cell></cell><cell></cell><cell cols="3">FCN Wld [13]</cell><cell>20.2</cell><cell>11.5</cell><cell>19.6</cell><cell>30.8</cell><cell>4.4</cell><cell>0.0</cell><cell cols="6">20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2</cell><cell>0.2</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell cols="3">CDA (I+SP) [43]</cell><cell>29.0</cell><cell>65.2</cell><cell>26.1</cell><cell>74.9</cell><cell>0.1</cell><cell>0.5</cell><cell cols="2">10.7 3.7</cell><cell>3.0</cell><cell cols="3">76.1 70.6 47.1 8.2 43.2 20.7</cell><cell>0.7</cell><cell>13.1</cell></row><row><cell></cell><cell cols="3">DRN 105 Source Only</cell><cell></cell><cell>23.4</cell><cell>14.9</cell><cell>11.4</cell><cell>58.7</cell><cell>1.9</cell><cell>0.0</cell><cell cols="2">24.1 1.2</cell><cell>6.0</cell><cell cols="3">68.8 76.0 54.3 7.1 34.2 15.0</cell><cell>0.8</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">DANN [7]</cell><cell></cell><cell>32.5</cell><cell>67.0</cell><cell>29.1</cell><cell cols="2">71.5 14.3</cell><cell>0.1</cell><cell cols="6">28.1 12.6 10.3 72.7 76.7 48.3 12.7 62.5 11.3</cell><cell>2.7</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (k=2)</cell><cell></cell><cell>36.3</cell><cell>83.5</cell><cell>40.9</cell><cell>77.6</cell><cell>6.0</cell><cell>0.1</cell><cell cols="2">27.9 6.2</cell><cell>6.0</cell><cell cols="3">83.1 83.5 51.5 11.8 78.9 19.8</cell><cell>4.6</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (k=3)</cell><cell></cell><cell>37.3</cell><cell>84.8</cell><cell>43.6</cell><cell>79.0</cell><cell>3.9</cell><cell>0.2</cell><cell cols="2">29.1 7.2</cell><cell>5.5</cell><cell cols="3">83.8 83.1 51.0 11.7 79.9 27.2</cell><cell>6.2</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (k=4)</cell><cell></cell><cell>37.2</cell><cell>88.1</cell><cell>43.2</cell><cell>79.1</cell><cell>2.4</cell><cell>0.1</cell><cell cols="2">27.3 7.4</cell><cell>4.9</cell><cell cols="3">83.4 81.1 51.3 10.9 82.1 29.0</cell><cell>5.7</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (GRL)</cell><cell></cell><cell>34.8</cell><cell>74.7</cell><cell>35.5</cell><cell>75.9</cell><cell>6.2</cell><cell>0.1</cell><cell cols="2">29.0 7.4</cell><cell>6.1</cell><cell cols="3">82.9 83.4 47.8 9.2 71.7 19.3</cell><cell>7.0</cell><cell>0.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The work was partially supported by CREST, JST, and was partially funded by the ImPACT Program of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="550" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer learning from multiple source domains via consensus regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of traffic sign recognition methods trained on synthetically generated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACIVS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">Visda: The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational recurrent adversarial deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nilanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
