<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Face Anti-Spoofing by 3D Virtual Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchuan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genxun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">First Research Institute</orgName>
								<orgName type="department" key="dep2">Ministry of Public Security of PRC</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Face Anti-Spoofing by 3D Virtual Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face anti-spoofing is crucial for the security of face recognition systems. Learning based methods especially deep learning based methods need large-scale training samples to reduce overfitting. However, acquiring spoof data is very expensive since the live faces should be reprinted and re-captured in many views. In this paper, we present a method to synthesize virtual spoof data in 3D space to alleviate this problem. Specifically, we consider a printed photo as a flat surface and mesh it into a 3D object, which is then randomly bent and rotated in 3D space. Afterward, the transformed 3D photo is rendered through perspective projection as a virtual sample. The synthetic virtual samples can significantly boost the anti-spoofing performance when combined with a proposed data balancing strategy. Our promising results open up new possibilities for advancing face anti-spoofing using cheap and largescale synthetic data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the intrinsic distinctiveness and convenience of biometrics, biometric-based systems are widely used in our daily life for person authentication. The most common applications cover phone unlock (e.g., iPhone X), access control, surveillance, and security. Face, as one of the biometric modalities, gains increasing popularity in academic and industry community <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>. Face recognition has achieved great success in terms of verification and identification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. However, spoof faces can be easily obtained by printers (i.e., print attack) and digital camera devices (i.e., replay attack). These spoofs can be very similar to genuine faces in appearance with proper handlings, like bending and rotating. Therefore, it is important to equip the face recognition system with robust presentation attack detection (PAD) algorithms. Print and replay attacks are the most common presentation attack (PA) ways and have been well studied in the academic field. Prior works can be roughly divided into three categories: cue-based, texture-based and deep learning based methods. Cue-based methods attempt to detect liveness motions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> such as eye blinking, lip, and head movements. Texture-based methods aim to exploit discriminative patterns between live and spoof faces, by adopting hand-crafted features such as HOG and LBP. Deep learning based methods mainly consist of two types. The first one treats PA as a binary classification or pseudo-depth regression problem <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1]</ref>. The other one tries to utilize temporal information of the video, such as applying the RNN-based structure <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>In practical applications, the replay attack can be easily detected using specialized sensors like depth or Near In-fraRed (NIR) cameras, because the captured depth values of the spoof face lie in one flat surface, which is easily distinguished from live faces. While for the print attack, an imposter will try his best to fool the system, such as bending and rotating the printed photo. However, most of the published databases miss the transformations, so that the trained models are easily spoofed by photo bending and rotating.</p><p>On the other hand, learning based methods especially deep learning based methods for face anti-spoofing need a large number of training samples to reduce overfitting. However, it is very expensive to acquire spoof data since the live faces should be re-printed and re-captured in many views. It is worth noting that, several recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24]</ref> have shown that synthesized images are effective for training CNN-based models in various tasks.</p><p>Motivated by these previous works, we propose to address these issues in a virtual synthesis manner. First, the high-fidelity virtual spoof samples with bending and outof-plane rotating are synthesized through rendering from the transformed mesh in 3D space. Several synthetic examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Second, deep models are trained on these synthetic samples with a data balancing method. To validate the effectiveness of our method, we design our experiments in two aspects: intra-database and inter-database testing. The intra-database testing is evaluated on CASIA-MFSD <ref type="bibr" target="#b38">[39]</ref> and Replay-Attack <ref type="bibr" target="#b4">[5]</ref> for fair comparisons with other methods. For inter-database testing, we choose CASIA-MFSD <ref type="bibr" target="#b38">[39]</ref> as our training dataset and CASIA-RFS <ref type="bibr" target="#b19">[20]</ref> as our testing set, since CASIA-RFS contain rotated and bent spoof faces series, which is more challenging. Besides, we rebuild the protocol on CASIA-RFS for better quantitative comparisons.</p><p>The main contributions of our work include:</p><p>• A virtual synthesis method to generate bent and outof-plane rotated spoofs is proposed. Large scales of spoof training data can be generated for training deep neural networks.</p><p>• To train CNN from the large-scale synthetic spoof samples, a data balancing method is proposed to improve generalization of the face anti-spoofing model.</p><p>• We achieve the state-of-the-art performances on the CASIA-MFSD and Replay-Attack databases and obtain great improvement of generalization on the CASIA-RFS database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review related works from two perspectives: CNNbased methods for face anti-spoofing and data synthesis for CNN training.</p><p>CNN-based Methods for Face Anti-spoofing. Many CNN-based methods for face anti-spoofing have been recently proposed, which can be categorized into two groups: texture-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref> and series-based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Most of the texture-based methods treat face antispoofing as a binary classification problem. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, it uses the CaffeNet or VGG model pre-trained on ImageNet as initialization and then fine-tunes it on face-spoofing data. The SVM is finally applied for face spoofing detection. In <ref type="bibr" target="#b20">[21]</ref>, different kinds of face features (e.g., multi-scale faces or hand-crafted features) are designed to feed into CNN. In <ref type="bibr" target="#b16">[17]</ref>, a two-step training method is proposed to learn local and global features. Recently, several studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> indicate that the depth supervised based methods perform better than binary supervised. Atoum et al. <ref type="bibr" target="#b0">[1]</ref> propose to use the pseudo-depth map as supervised signals. A novel two-stream CNN-based approach for face anti-spoofing by extracting the local features and holistic depth maps from the face images is proposed. The fusion of the scores of two-steam CNNs leads to the final predicted class of live vs. spoof. Liu et al. <ref type="bibr" target="#b21">[22]</ref> fuse the estimated depth and the rPPG signals to distinguish live vs. spoof faces. They argue that auxiliary supervision such as pseudo-depth map and rPPG signals are important to guide the learning toward discriminative and generalizable cues. One of the most recent work <ref type="bibr" target="#b34">[35]</ref> proposes a depth supervised face anti-spoofing model in both spatial and temporal domains thus more robust and discriminative features can be extracted to classify live and spoof faces.</p><p>Series-based methods aim to fully utilize the temporal information of serialized frames. Feng et al. <ref type="bibr" target="#b6">[7]</ref> feed both optical flow map and Shearlet feature to CNN. The work <ref type="bibr" target="#b35">[36]</ref> proposes a deep neural network architecture combining Long Short-Term Memory (LSTM) units with CNN. In <ref type="bibr" target="#b7">[8]</ref>, 3D convolution network is adopted in short video frame level to distinguish live vs. spoof face. Besides, the rPPG signals extracted from serialized frames are used as auxiliary supervision for classification in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Virtual Synthesis for CNN Training. Generally, CNNbased methods need a large scale of training data to reduce overfitting. However, training data is difficult to collect in many cases. Several recent works focus on creating synthetic images to augment the training data, e.g. face alignment <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref>, face recognition <ref type="bibr" target="#b12">[13]</ref>, 3D human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>, pedestrian detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> and action recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr">Zhu et al. [?,</ref><ref type="bibr" target="#b40">41]</ref> attempt to utilize 3D information to synthesize face images in large poses to provide abundant samples for training. Similarly, Guo et al. <ref type="bibr" target="#b12">[13]</ref> synthesize high-fidelity face images with eyeglasses as training data based on 3D face model and 3D eyeglasses and achieve better face recognition performance on real eyeglass face testset. Pishchulin et al. <ref type="bibr" target="#b27">[28]</ref> generate synthetic images with a game engine. In <ref type="bibr" target="#b28">[29]</ref>, they deform 2D images with a 3D model. In <ref type="bibr" target="#b29">[30]</ref>, action recognition is addressed with synthetic human trajectories from MoCap data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D Virtual Synthesis</head><p>In this section, we demonstrate how to synthesize virtual spoof face data. The purpose of synthesis is trying to simulate the behaviors of bending and out-of-plane rotating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Meshing and Deformation</head><p>We assume that a printed photo has a simple 3D structure, which is a flat surface so that we can easily transfer the printed photo to a 3D object and manipulate its appearance in 3D space. First, we label four corner anchors ( <ref type="figure">Fig. 2(a)</ref>) to crop the printed photo region. Second, the anchors are uniformly sampled on cropped region ( <ref type="figure">Fig. 2(b)</ref>). Their depths are set the same since we treat the printed photo as a plane. Finally, the delaunay algorithm is applied to triangulate these anchors to mesh the printed photo into a virtual 3D object ( <ref type="figure">Fig. 2(c)</ref>). The 3D view of virtual 3D printed photo is are shown in <ref type="figure">Fig. 2(d)</ref> and <ref type="figure">Fig. 2</ref>(e).</p><p>After 3D meshing, the 3D transformation operations such as rotating and bending can be applied. We use a rotation matrix R to rotate the 3D object. Let V xyz representing the sampled anchors, the rotating operation can be repre- The bending operation involves non-rigid transformation. In order to simulate the bending operation, we deform the 3D planar mesh to a cylinder, with the length along the horizontal and vertical directions preserved. As shown in <ref type="figure">Fig. 3</ref>, the bending angle θ measures the degree of vertical bending. We show the 2D aerial view of vertical bending in <ref type="figure" target="#fig_2">Fig. 4</ref>, where P is an anchor point on original 3D planar mesh and P is the deformed anchor point on cylinder surface. The radius of the circle on cylinder mesh surface can be first calculated by r = l/θ, where l denotes the width of 3D planar mesh. The radian of arc P C can be calculated by φ = d/l · θ, where d = x − l/2 is the distance from anchor point P to the mesh center C (or from P to C ). The new position of anchor P can be formulated as follows:</p><formula xml:id="formula_0">sented by V = R * V xyz . (a) (b) (c) (d) (e)</formula><formula xml:id="formula_1">x θ = r sin φ = l θ · sin ( x − l/2 l · θ), z θ = r cos φ − r cos θ 2 = l θ · cos ( x − l/2 l · θ) − cos θ 2 ,<label>(1)</label></formula><p>where θ is the bending angle. Horizontal bending is similar to vertical bending. Besides, the rotating and bending can be composed to synthesize more varied samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Perspective Projection</head><p>When a printed photo is captured, the object in the distance should appear smaller than the object close by. How-ever, this effect is always ignored and the weak perspective projection is adopted in many virtual synthesis techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. In this work, we use perspective projection for more realistic synthesis.</p><p>To perform the perspective transformation, we must first approximate the physical size of the printed photo. We assume the pixel distance and real distance between two eyes centers as d p and d r respectively, then the scale factor of the transformation from image space to world coordinated system can be approximated as:</p><formula xml:id="formula_2">s = d p /d r ,<label>(2)</label></formula><p>where s is the scale factor of transformation from image space to world coordinate system. Then the perspective projection can be applied by</p><formula xml:id="formula_3">b x = v x · (f /d z ), b y = v y · (f /d z ).<label>(3)</label></formula><p>where f is the focal length, d z indicates the physical depth distance from camera to photo, v x , v y are anchor vertices in world coordinate system, and b x , b y are projected 2D vertices by perspective transformation. For convenience, the real distance between eyes centers d r , the focal length f and the depth d z are approximated by constant values based on prior.</p><p>After the mesh deformation and projection, synthetic samples can be rendered by Z-buffer. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the difference between weak perspective projection and perspective projection. The image synthesized by perspective projection in <ref type="figure" target="#fig_3">Fig. 5(c)</ref> is more realistic than the one by weak perspective projection in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Post-processing</head><p>Due to the mesh deformation and perspective projection, the size of synthetic photos will be changed (see <ref type="figure" target="#fig_4">Fig. 6(a)</ref>).</p><p>To improve the fidelity of the final synthetic sample, we try to make sure the synthetic printed photo fully overlaps the originally printed photo region (see <ref type="figure" target="#fig_4">Fig. 6</ref>). Besides, the gaussian image filter is applied to make the fused edge smoother. Several final synthetic spoof results are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Network Training on Synthetic Data</head><p>In this section, we present our approach to training from synthetic spoof data. <ref type="figure" target="#fig_5">Fig. 7</ref> shows a high-level illustration of our training pipeline. Virtual synthetic spoof samples are used to supervise the network training with our data balancing methods.</p><p>Data Balancing. We can generate as many spoof samples as we want by the synthesis method, but the amount of live samples is fixed. As a result, the CNN model trained has a bias to virtual spoofs due to the imbalance of live and virtual spoof samples.</p><p>There are two methods to mitigate the impact of data imbalance: balanced sampling strategy and importing external live samples. To perform balanced sampling, the ratio of sampled live and spoof instances in each min-batch is kept fixed during training after augmenting dataset. Besides, the live samples are much easier to acquire than the spoof ones in practical applications, such as the samples from face recognition databases <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, unlimited external live samples can be imported to balance the distribution of training data.</p><p>How to Treat Synthetic Samples. The previous synthesis-based work <ref type="bibr" target="#b3">[4]</ref> first uses the synthetic samples to train one initialization model and then fine-tunes it on the real data. Since our synthesis is high-fidelity and possesses much more variations than the real data, we treat equally the synthetic spoof samples with the real ones and directly train models on the joint data of synthetic and real data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the effectiveness of learning with virtual synthetic data from two aspects: CASIA-MFSD <ref type="bibr" target="#b38">[39]</ref> and Replay-Attack <ref type="bibr" target="#b4">[5]</ref> for intra-database evaluation and CASIA-RFS <ref type="bibr" target="#b19">[20]</ref> for inter-database evaluation. Performance evaluations on CASIA-MFSD and Replay-Attack databases are for fair comparisons with other previous methods. The CASIA-RFS database is much more challenging since it contains rotated live and spoof faces with various poses. We use it as the testing set and the entire CASIA-MFSD database as the training set to carry out the inter-database testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Database and Protocols</head><p>CASIA-MFSD. This database contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing.</p><p>Replay-Attack. This database consists of 1,300 videos from 50 subjects. These videos are collected under controlled and adverse conditions and are divided into training, development and testing sets with 15, 15 and 20 subjects respectively.</p><p>CASIA-RFS. This database contains 785 videos for genuine faces and 1950 videos for spoof faces. The videos are collected using three devices with different resolutions: digital camera (DC), mobile phone (MP) and web camera (WC). Two kinds of spoof attacks including planar and bent photo attacks are designed. Protocols.</p><p>For CASIA-MFSD and Replay-Attack databases, our experiments follow the associated protocols, the EER and HTER indicators are reported. The Replay-Attack database has already provided a development set thus our HTER threshold is determined on it. As for interdatabase evaluation, we build the protocols following <ref type="bibr" target="#b41">[42]</ref>. Attack Presentation Classification Error Rate (APCER), Bona Fide Presentation Classification Error Rate (BPCER), Average Classification Error Rate (ACER), Top-1 accuracy are all evaluated. Particularly, the APCER here represents the highest error among plane printed and bending printed attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Network Structure. Our network structure is modified from ResNet <ref type="bibr" target="#b17">[18]</ref>. The input size of original ResNet is 224 × 224, while ours is 120 × 120. As a result, the original 7 × 7 convolution in the first layer is replaced by 5 × 5 and follows one 3 × 3 convolution layer to preserve the dimension of feature map output. Finally, one 15 layers ResNet (ResNet-15) structure is designed for our task and shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Training. Our experiments are based on PyTorch framework and GeForce GTX TITAN X GPU devices. All training images are cropped and aligned to the size of 120 × 120 by similar transformation, then being normalized by subtracting 127.5 and being divided by 128. For all three databases, we use SGD with a mini-batch size of 64 to optimize the network, with the weight decay of 0.0005 and momentum of 0.9. Image horizontal flipping is adopted as standard augmentation. The weight parameters of ResNet-15 model are randomly initialized. We train 30 epochs for each experiment. We set the initial learning rate of 0.1, then  Synthesis. For each printed spoof instance in CASIA-MFSD and Replay-Attack, we generate ten synthetic samples, in which five samples are rotated and bent, another five ones are only rotated. For each replay spoof sample, we generate five synthetic samples without bending, since the replay devices cannot be bent. During rotating, the yaw angle is uniformly drawn from the interval [0, 40], the pitch angle is from [-10, 10] and the bending angle is from <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">60]</ref>. We show one example in <ref type="figure" target="#fig_6">Fig. 8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Ablation Study</head><p>We evaluate the performance of different projections, synthetic data, and data balancing in this section. Projection. We explore the weak perspective and perspective projections of the spoof data synthesis. From results in <ref type="table" target="#tab_1">Table 2</ref>, we can see that the perspective projection has a significant improvement over the weak perspective projection with an EER 2.22% and an HTER 1.67%. Moreover, ACER improves from 3.33% to 2.22% and Top-1 accuracy rises from 97.78% to 98.61%. The results indicate that perspective projection is more suitable for virtual spoof data synthesis.</p><p>Synthetic Data &amp; Balanced Sampling (BS). For CASIA-MFSD and Replay-Attack in <ref type="table" target="#tab_2">Table 3</ref>, we compare three configurations: (i) the original dataset is used without the synthetic spoof data and BS; (ii) the synthetic spoof data is added; (iii) both the synthetic data and BS are adopted. The results in <ref type="table" target="#tab_2">Table 3</ref> indicate that (ii) is considerably better than (i) in ACER, Top-1 accuracy, EER, and HTER on both CASIA-MFSD and Replay-Attack. It shows the synthetic spoof data is effective for training CNN. Besides, (iii) achieves the best performance on both CASIA-MFSD and Replay-Attack, which validates the effectiveness of our BS and the advantage of combining synthetic samples and BS.</p><p>External Live Samples. For CASIA-RFS in <ref type="table" target="#tab_3">Table 4</ref>, we compare three configurations: (i) neither synthetic data nor MultiPIE is used (baseline); (ii) the synthetic spoof data is used; (iii) both MultiPIE and the synthetic spoof data are used. The balanced sampling is applied in (ii) and (iii). Compared with the baseline, the addition of synthetic data makes APCER decrease from 20.96% to 4.01%, but Method EER (%) HTER (%) Fine-tuned VGG-Face <ref type="bibr" target="#b20">[21]</ref> 5.20 -DPCNN <ref type="bibr" target="#b20">[21]</ref> 4.50 -Multi-Scale <ref type="bibr" target="#b36">[37]</ref> 4.92 -CNN <ref type="bibr" target="#b35">[36]</ref> 6.20 7.34 LSTM-CNN <ref type="bibr" target="#b35">[36]</ref> 5.17 5.93 YCbCr+HSV-LBP <ref type="bibr" target="#b1">[2]</ref> 6.20 -Feature Fusion <ref type="bibr" target="#b32">[33]</ref> 3.14 -Fisher Vector <ref type="bibr" target="#b2">[3]</ref> 2.80 -Patch-based CNN <ref type="bibr">[</ref>  BPCER becomes higher. It shows that the spoof face classification accuracy gets better but the live face classification precision drops. We think it is because live faces in CASIA-RFS have a wide range of poses, while such variation in CASIA-MFSD is limited. Once MultiPIE is used for training, BPCER drops obviously from 50.23% to 18.75% and APCER almost unchanges. Finally, the Top-1 and EER indicators on CASIA-RFS get improved by 9.85% and 9.67% respectively compared with the baseline. The above results show that external live data, e.g. MultiPIE, improves the generalization by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Intra-database Testing</head><p>The intra-database testing is performed on CASIA-MFSD and Replay-Attack. <ref type="table" target="#tab_4">Table 5</ref> shows the comparisons of our proposed synthesis-based method with state-of-the-art methods. As shown in <ref type="table" target="#tab_4">Table 5</ref>, our synthesis-based method outperforms other methods in both EER and HTER. For Replay-Attack database, we perform comparisons in <ref type="table" target="#tab_6">Table 6</ref>. We can see the proposed method also outperforms other state-of-the-art methods. Though our method has sim- ilar EER with several methods, the HTER is smaller than theirs, which means we have lower false acceptance and false rejection rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Inter-database Testing</head><p>We perform the inter-database testing on CASIA-RFS, in which CASIA-MFSD is used for training. In <ref type="table" target="#tab_7">Table 7</ref>, we compare our method with other CNN-based methods and our baseline (no additional synthetic data or external live data). The other CNN-based methods are re-implemented following the descriptions in original papers. The results show that our method outperforms other CNN-based methods and the baseline model. The inter-database testing also validates the effectiveness of synthetic spoof data and the data balancing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have shown successful large-scale training of CNNs from synthetically generated spoof data. For the data imbalance brought by the spoof data, we exploit two methods for balancing it: balanced sampling and adding external live samples. Experimental results show that our synthetic spoof data and data balancing methods greatly promote the performance for face anti-spoofing. The promising performance shows the great potential for advancing face anti-spoofing using large-scale synthetic data. Besides, more realistic and more kinds of spoof synthesis are future directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Virtual synthetic samples from CASIA-MFSD. Left column: the original printed photos. Right column: synthetic samples with different vertical/horizontal bending and rotating angles. These synthetic samples can significantly improve the face anti-spoofing performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>3D Image Meshing. (a) The input source image marked with corner anchors. (b) The cropped image and sampled anchors. (c) The triangle mesh overlapped with the cropped image (2D view). (d) The triangle mesh overlapped with cropped image (3D view) (e) 3D view of the triangle mesh. Vertical bending of the planar triangle mesh. Left is 3D planar mesh and right is 3D bending mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>2D aerial view of vertical bending.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Different projections. (a) Texture of the cropped printed photo region. (b) Rendered texture by weak perspective projection. (c) Rendered texture by perspective projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Post-processing of the synthetic printed photo. (a) Fused image without full overlapping. (b) Fused image with full overlapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The pipeline of our proposed deep network training with synthetic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Synthetic samples before post-processing. The first row: five synthesized planar samples. The second row: five synthesized vertically bent samples. The values in brackets indicate yaw, pitch, and bending angle successively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Our ResNet-15 network structure. Conv3.x, Conv4.x, and Conv5.x indicate convolution units which contain multiple convolution layers, and residual blocks are shown in double-column brackets. For instance, [3 × 3, 128] × 3 denotes 3 cascaded convolution layers with 128 feature maps with filters of size 3 × 3, and S2 denotes stride 2.</figDesc><table><row><cell>Layers</cell><cell cols="2">15-layer CNN</cell></row><row><cell>Conv1.x</cell><cell cols="2">[5×5, 32]×1, S2</cell></row><row><cell>Conv2.x</cell><cell cols="2">[3×3, 64]×1, S1</cell></row><row><cell>Conv3.x</cell><cell>3 × 3, 128 3 × 3, 128</cell><cell>× 2, S2</cell></row><row><cell>Conv4.x</cell><cell>3 × 3, 256 3 × 3, 226</cell><cell>× 2, S2</cell></row><row><cell>Conv5.x</cell><cell>3 × 3, 512 3 × 3, 512</cell><cell>× 2, S2</cell></row><row><cell>Global Pooling &amp; FC</cell><cell>512</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ACER(%), Top-1(%), EER(%) and HTER(%) on CASIA-MFSD with different projections.</figDesc><table><row><cell cols="2">Projection</cell><cell cols="4">ACER Top-1 EER HTER (%) (%) (%) (%)</cell></row><row><cell cols="2">Weak Perspective</cell><cell>3.33</cell><cell cols="2">97.78</cell><cell>2.59</cell><cell>2.41</cell></row><row><cell cols="2">Perspective</cell><cell>2.22</cell><cell cols="2">98.61</cell><cell>2.22</cell><cell>1.67</cell></row><row><cell cols="6">decrease it by multiplying 0.1 in 10th and 20th epoch re-</cell></row><row><cell>spectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">The ratios of the number of live/spoof samples on the</cell></row><row><cell cols="6">CASIA-MFSD and Replay-Attack databases are all about</cell></row><row><cell cols="6">1:3. We keep this ratio in each mini-batch sampling on</cell></row><row><cell cols="6">augmented training data when applying balanced sampling.</cell></row><row><cell cols="6">More comparisons can be referred to Sec.5.3. For inter-</cell></row><row><cell cols="6">database testing in Sec.5.3.3, we use part of CMU Multi-</cell></row><row><cell cols="6">PIE Face Database (MultiPIE) [10]. Totally, 8,120 face im-</cell></row><row><cell cols="6">ages with various poses are introduced as external data.</cell></row><row><cell>[8, 9, 0]</cell><cell>[16, 8.4, 0]</cell><cell cols="2">[24, -8.9, 0]</cell><cell cols="2">[32, 4.8, 0]</cell><cell>[40, -4.6, 0]</cell></row><row><cell></cell><cell></cell><cell>Plane</cell><cell></cell><cell></cell></row><row><cell cols="6">[8, -1.5, 49.9] [16, 0.9, 35.3] [24, 8.9, 33.8] [32, -1.6, 9.7] [40, 9.7, 35.1]</cell></row><row><cell></cell><cell></cell><cell>Bending</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>ACER(%), Top-1(%), EER(%) and HTER(%) on CASIA-MFSD and Replay-Attack. Syn indicates synthetic data and BS indicates the balanced sampling.</figDesc><table><row><cell>Database</cell><cell>Method</cell><cell>ACER (%)</cell><cell>Top-1 (%)</cell><cell>EER (%)</cell><cell>HTER (%)</cell></row><row><cell></cell><cell>Baseline</cell><cell>5</cell><cell>96.67</cell><cell>4.44</cell><cell>3.89</cell></row><row><cell>CASIA-MFSD</cell><cell>Syn w/o BS</cell><cell>4.44</cell><cell>97.78</cell><cell>3.33</cell><cell>2.78</cell></row><row><cell></cell><cell>Syn w/ BS</cell><cell>2.22</cell><cell>98.61</cell><cell>2.22</cell><cell>1.67</cell></row><row><cell></cell><cell>Baseline</cell><cell>4.17</cell><cell>98.13</cell><cell>2.50</cell><cell>3.50</cell></row><row><cell>Replay-Attack</cell><cell>Syn w/o BS</cell><cell>2.50</cell><cell>98.14</cell><cell>1.25</cell><cell>1.75</cell></row><row><cell></cell><cell>Syn w/ BS</cell><cell>0.21</cell><cell>99.79</cell><cell>0.25</cell><cell>0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: APCER(%), BPCER(%), ACER (%) and Top-</cell></row><row><cell cols="5">1(%) on CASIA-RFS. Syn indicates synthetic data.</cell></row><row><cell>Method</cell><cell cols="4">APCER BPCER ACER Top-1 (%) (%) (%) (%)</cell></row><row><cell>Baseline</cell><cell>20.96</cell><cell>23.98</cell><cell>22.47</cell><cell>81.83</cell></row><row><cell>Syn w/o MultiPIE</cell><cell>4.01</cell><cell>50.23</cell><cell>27.12</cell><cell>82.63</cell></row><row><cell>Syn w/ MultiPIE</cell><cell>4.68</cell><cell>18.75</cell><cell>11.72</cell><cell>91.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>EER (%) and HTER (%) on CASIA-MFSD. BS indicates the balanced sampling strategy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>EER (%) and HTER (%) on Replay-Attack. BS indicates the balanced sampling strategy.</figDesc><table><row><cell>Method</cell><cell cols="2">EER (%) HTER (%)</cell></row><row><cell>Fine-tuned VGG-Face [21]</cell><cell>8.40</cell><cell>4.30</cell></row><row><cell>DPCNN [21]</cell><cell>2.90</cell><cell>6.10</cell></row><row><cell>Multi-Scale [37]</cell><cell>2.14</cell><cell>-</cell></row><row><cell>YCbCr+HSV-LBP [2]</cell><cell>0.40</cell><cell>2.90</cell></row><row><cell>Fisher Vector [3]</cell><cell>0.10</cell><cell>2.20</cell></row><row><cell>Moire pattern [27]</cell><cell>-</cell><cell>3.30</cell></row><row><cell>Patch-based CNN [1]</cell><cell>4.44</cell><cell>3.78</cell></row><row><cell>Depth-based CNN [1]</cell><cell>3.78</cell><cell>2.52</cell></row><row><cell>Patch&amp;Depth Fusion [1]</cell><cell>0.79</cell><cell>0.72</cell></row><row><cell>FASNet [23]</cell><cell>-</cell><cell>1.20</cell></row><row><cell>Ours (Syn w/ BS)</cell><cell>0.25</cell><cell>0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>APCER (%), BPCER (%), ACER(%) and Top-1(%) on CASIA-RFS for inter-database testing.</figDesc><table><row><cell>Method</cell><cell cols="4">APCER BPCER ACER Top-1 (%) (%) (%) (%)</cell></row><row><cell>FASNet [23]</cell><cell>1.23</cell><cell>47.27</cell><cell>24.25</cell><cell>84.48</cell></row><row><cell>Patch-based CNN [1]</cell><cell>12.93</cell><cell>29.89</cell><cell>21.41</cell><cell>83.54</cell></row><row><cell>Baseline</cell><cell>20.96</cell><cell>23.98</cell><cell>22.47</cell><cell>81.83</cell></row><row><cell>Ours</cell><cell>4.68</cell><cell>18.75</cell><cell>11.72</cell><cell>91.68</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the Chinese National Natural Science Foundation Projects #61876178, #61806196, #61872367, #61572501, #61806203.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face antispoofing using patch and depth-based cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face antispoofing based on color texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face antispoofing using speeded-up robust features and fisher vector encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Komulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the effectiveness of local binary patterns in face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chingovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIOSIG</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integration of image quality and motion cues for face anti-spoofing: A neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d convolutional neural network based on face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning camera viewpoint using cnn to improve 3d body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Ghezelghieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dominant and complementary emotion recognition from still images of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-modality network with visual and geometrical information for micro emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Face synthesis for eyeglass-robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01196</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3DMA: A Multi-modality 3D Mask Face Anti-spoofing Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning meta face recognition in unseen domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain Balancing: Face Recognition on Long-Tailed Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the learning of deep local features for robust face spoofing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Countermeasures to face photo spoofing attacks by exploiting structure and texture information from rotated face sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Biometrics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An original face anti-spoofing approach using partial convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Boulkenafet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPTA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep models for face anti-spoofing: Binary or auxiliary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transfer learning using convolutional neural networks for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lucena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Moia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep exemplar 2d-3d detection by adapting from real to rendered views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Eyeblink-based antispoofing in face recognition from a generic webcamera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-database face antispoofing with robust feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCBR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Live face video vs. spoof face video: Use of moiré patterns to detect replay video attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning people detection models from few training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face anti-spoofing with multifeature videolet aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploiting temporal and depth information for multi-frame face antispoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05118</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning temporal features using lstm-cnn architecture for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learn convolutional neural network for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5601</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Face liveness detection with component dependent descriptor. ICB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Fast, Accurate and Stable 3D Dense Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Oulunpu: a mobile face presentation attack database with real-world variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zinelabidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jukka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISBA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
