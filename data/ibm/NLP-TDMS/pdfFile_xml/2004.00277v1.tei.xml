<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Structured Network for Image-Text Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04">Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
							<email>liuchunxiao@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
							<email>maozhendong2008@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<email>wangbin11@xiaomi.com</email>
							<affiliation key="aff3">
								<orgName type="department">Xiaomi AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Structured Network for Image-Text Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04">Apr 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-text matching has received growing interest since it bridges vision and language. The key challenge lies in how to learn correspondence between image and text. Existing works learn coarse correspondence based on object co-occurrence statistics, while failing to learn finegrained phrase correspondence. In this paper, we present a novel Graph Structured Matching Network (GSMN) to learn fine-grained correspondence. The GSMN explicitly models object, relation and attribute as a structured phrase, which not only allows to learn correspondence of object, relation and attribute separately, but also benefits to learn fine-grained correspondence of structured phrase. This is achieved by node-level matching and structure-level matching. The node-level matching associates each node with its relevant nodes from another modality, where the node can be object, relation or attribute. The associated nodes then jointly infer fine-grained correspondence by fusing neighborhood associations at structure-level matching. Comprehensive experiments show that GSMN outperforms state-of-the-art methods on benchmarks, with relative Re-call@1 improvements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code will be released at: https://github.com/CrossmodalGroup/GSMN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-text matching is an emerging task that matches instance from one modality with instance from another modality. This enables to bridge vision and language, which has potential to improve the performance of other multimodal applications. The key challenge in image-text matching lies in learning correspondence of image and text, such that can reflect similarity of image-text pairs accurately. * Zhendong Mao is the corresponding author. <ref type="figure">Figure 1</ref>: Illustration of coarse and fine-grained correspondence. In the left figure, the two dogs are coarsely correlated with the word "dog", while neglecting their relation and attribute (bite or being bitten? gray or brown?). In the right figure, the gray and brown dogs are fine-grained correlated with finer textual details, which is achieved by learning phrase correspondence using a graph-based method.</p><p>Existing approaches either focus on learning global correspondence or local region-word correspondence. The general framework of global correspondence learning methods is to jointly project the whole image and text into a common latent space, where corresponding image and text can be unified into similar representations. Techniques to common space projection range from designing specific networks <ref type="bibr" target="#b22">[23]</ref> to adding constraints, such as triplet loss <ref type="bibr" target="#b28">[29]</ref>, adversarial loss <ref type="bibr" target="#b26">[27]</ref> and classification loss <ref type="bibr" target="#b14">[15]</ref>. Another branch of image-text matching learns local region-word correspondence, which is used to infer the global similarity of image-text pairs. Some researchers focus on learning local correspondence between salient regions and keywords. For example, Ji et al. <ref type="bibr" target="#b9">[10]</ref> present to correlate words with partial salient regions detected by a lightweight saliency model, which demands external saliency dataset as a supervision. Recent works discover all possible region-word correspondences. For instance, Lee et al. <ref type="bibr" target="#b13">[14]</ref> propose to correlate each word with all the regions with different weights, and vice versa. Following this work, wang et al. <ref type="bibr" target="#b29">[30]</ref> integrate positional embedding to guide the correspondence learning and Liu et al. <ref type="bibr" target="#b17">[18]</ref> present to eliminate partial irrelevant words and regions in correspondence learning.</p><p>However, existing works only learn coarse correspondence based on object co-occurrence statistics, while failing to learn fine-grained correspondence of structured object, relation and attribute. As a result, they suffer from two limitations: <ref type="bibr" target="#b0">(1)</ref> it is hard to learn correspondences of the relation and attribute as they are overwhelmed by object correspondence. <ref type="bibr" target="#b1">(2)</ref> objects are prone to correspond to wrong categories without the guidance of descriptive relation and attribute. As shown in <ref type="figure">Figure 1</ref>, the coarse correspondence will incorrectly correlate the word "dog" with all the dogs in the image, while neglecting dogs are with finer details, i.e. brown or gray. By contrast, the fine-grained correspondence explicitly models the object "dog", relation "bite" and attribute "brown" as a phrase. Therefore, the relation "bite" and attribute "brown" can also correlate to a specific region, and they will further promote identifying finegrained phrase "brown dog bite".</p><p>To learn fine-grained correspondence, we propose a Graph Structured Matching Network (GSMN) that explicitly models object, relation and attribute as a phrase, and jointly infer fine-grained correspondence by performing matching on these localized phrases. This unions the correspondence learning of object, relation and attribute in a mutually enforced way. On the one hand, relation correspondence and attribute correspondence can guide the finegrained object correspondence learning. On the other hand, the fine-grained object correspondence forces the network to learn relation correspondence and attribute correspondence explicitly. Concretely, the proposed network constructs graph for image and text, respectively. The graph node consists of the object, relation and attribute, the graph edge exists if any two nodes interact with each other (e.g. the node of an object will connect with the node of its relations or attributes). Then we perform node-level and structure-level matching on both visual and textual graphs. The node-level matching associates each node with nodes from another modality differentially, which are then propagated to neighborhoods at structure-level matching. The phrase correspondence can be inferred with the guidance of node correspondence. Moreover, the correspondence of object node can be updated as long as its neighboring relation and attribute point to a same object. At last, the updated correspondence is used for predicting the global similarity of image-text pairs, which jointly considers correspondence of all the individual phrases.</p><p>The main contributions of this paper are summarized as: <ref type="bibr" target="#b0">(1)</ref> We propose a Graph Structured Matching Network that explicitly constructs the graph structure for image and text, and performs matching by learning fine-grained phrase correspondence. To the best of our knowledge, this is the first framework that performs image-text matching on heterogeneous visual and textual graphs. (2) To the best of our knowledge, this is the first work that uses graph convolutional layer to propagate node correspondence, and uses it to infer fine-grained phrase correspondence. (3) We conduct extensive experiments on Flickr30K and MSCOCO, showing our superiority over state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing works learn correspondence of image and text based on object co-occurence, which is roughly categorized into two types: global correspondence and local correspondence learning methods, where the former learns the correspondence between the whole image and sentence, and the latter learns that between local region and word.</p><p>The main goal of global correspondence learning methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref> is to maximize similarity of matched image-text pairs. A main line of research on this field is to first represent image and text as feature vectors, and then project them into a common space optimized by a ranking loss. Some works focus on designing specific networks. For instance, Liu et al. <ref type="bibr" target="#b18">[19]</ref> propose to densely correlate image and text exploiting residual blocks. Gu et al. <ref type="bibr" target="#b3">[4]</ref> imagine what the matched instance should look like, and improve the correspondence of target instance to this imagined instance. Some works focus on optimization, Wang et al. <ref type="bibr" target="#b28">[29]</ref> point out that the correspondence within the same modality should also be preserved while learning correspondence in different modalities. Based on this observation, Wu et al. <ref type="bibr" target="#b30">[31]</ref> preserve graph structure among neighborhood images or texts. Such global correspondence learning methods cannot learn correspondence of image and text accurately, because primary objects play the dominant role in the global representation of image-text pairs while secondary objects are mostly ignored.</p><p>The local correspondence learning methods learn regionword correspondence. Some works focus on learning correspondence of salient objects. Karparthy et al. <ref type="bibr" target="#b11">[12]</ref> make the first attempt by optimizing correspondence of the most <ref type="figure">Figure 2</ref>: An overview of our approach, which consists of three modules: (a) Feature Extraction: Faster-RCNN <ref type="bibr" target="#b25">[26]</ref> and Stanford CoreNLP <ref type="bibr" target="#b20">[21]</ref> are employed to detect salient regions, and parse the semantic dependency, respectively. (b) Graph Construction: The node of graph is object, relation or attribute, the edge exists if any two nodes are semantically dependent. (c1) Node-level Matching: learn correspondence of object, relation and attribute separately. (c2) Structure-level Matching: Propagating the learned correspondence to neighbors to jointly infer fine-grained phrase correspondence. and hence the region-word can be correlated more accurately. A lightweight saliency model is employed using an external saliency dataset as a supervision. The local correspondence policy has also been widely used in other fields <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref>, like <ref type="bibr" target="#b16">[17]</ref> that learns distinction and connection among multi-tasks. Another branch of researches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> present to discover all possible regionword correspondence. Ma et al. <ref type="bibr" target="#b19">[20]</ref> present to jointly map global image and text, local regions and words into a common space, which can implicitly learn region-word correspondence. A recent approach SCAN <ref type="bibr" target="#b13">[14]</ref> greatly improves the matching performance, which is most relevant to our work. They learn region-word correspondence using attention mechanism, where each region corresponds to multiple words and vice versa. These works learn correspondence based on object co-occurrence, and have achieved much progress in image-text matching. Nonetheless, these only learn coarse correspondence since they mostly rely on correspondence of salient objects, while neglecting the correspondence of relation and attribute is as important as object correspondence. Moreover, the correspondence of relation and attribute can benefit object to correspond to a specific type with a finer detail. By contrast, we explicitly model the image and text as graph structures, and learn finegrained phrase correspondence. Instead of transforming the image and text as scene graphs using rule-based <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11]</ref> or classifier-based <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref> methods, we only need to identify whether nodes are interact with each other, which avoids the loss of information caused by scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overview of our proposed network is illustrated in <ref type="figure">Figure 2</ref>. We first extract features of image and text, and then construct visual and textual graph. Next, the nodelevel matching learns node correspondence, and propagate to neighbors in structure-level matching, in which the correspondences of object, relation and attribute are fused to infer the fine-grained phrase correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Construction</head><p>Textual Graph. Formally, we seek to construct an undirected sparse graph G 1 = (V 1 , E 1 ) for each text, we use matrix A to represent the adjacent matrix of each node, and add self-loops. The edge weight is denoted as a matrix W e , which shows the semantic dependency of nodes.</p><p>To construct the textual graph, we first identify the semantic dependency within the text using off-the-shelf Stanford CoreNLP <ref type="bibr" target="#b20">[21]</ref>. This can not only parse the object (nouns), relation (verbs) and attribute (adjectives or quantifiers) in a sentence, but also parse their semantic dependencies. For example, given a text "A brown dog bite a gray dog ear", "A", "brown" are attributes for the first object "dog", and the "bite" is its relation. They are semantically dependent since all of them describe the same object. Based on this observation, we set each word as the graph node, and there exists graph edge between nodes if they are semantically dependent. Then we compute the similarity matrix S of word representations u as</p><formula xml:id="formula_0">s ij = exp(λu T i u j ) m j=0 exp(λu T i u j ) .<label>(1)</label></formula><p>where the s ij indicates the similarity between i-th and j-th node. λ is a scaling factor. The weight matrix W e can be obtained by a Hadamard product between similarity matrix and adjacent matrix, followed by L 2 normalization, i.e.</p><formula xml:id="formula_1">W e = S • A 2 .<label>(2)</label></formula><p>Additionally, we also implement the textual graph as a fully-connected graph. In contrast to sparse graph that employs semantic dependency of words, it can exploit implicit dependencies. We find the sparse and dense graphs are complementary to each other, and can greatly improve the performance, see section 4.2.1. Visual Graph. To construct the visual graph G 2 = (V 2 , E 2 ), we represent each image as an undirected fullyconnected graph, where the node is set as salient regions detected by Faster-RCNN <ref type="bibr" target="#b25">[26]</ref>, and each node is associated with all the other nodes. Inspired by <ref type="bibr" target="#b23">[24]</ref> in visual question answering, we use the polar coordinate to model the spatial relation of each image, which disentangles the orientation and distance of pair-wise regions. This can capture both semantic and spatial relationships among different regions, since the relation and attribute are expected to close to object, and the direction information allows to estimate the type of relations. For example, the relations "on" and "under" show opposite relative position to the object "desk". To get edge weight for this fully-connected graph, we compute polar coordinate (ρ, θ) based on the centres of the bounding boxes of pair-wise regions, and set the edge weight matrix W e as pair-wise polar coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multimodal Graph Matching</head><p>Given a textual graph G 1 = (V 1 , E 1 ) of a text, and a visual graph G 2 = (V 2 , E 2 ) of an image, our goal is to match two graphs to learn fine-grained correspondence, producing similarity g(G 1 , G 2 ) as global similarity of an image-text pair. We define the node representation of textual graph as U α ∈ R m×d , and the node representation of visual graph as V β ∈ R n×d . Here, m and n denotes the node number of textual and visual graph, d is the representation dimension. To compute the similarity of these heterogeneous graphs, we first perform node-level matching to associate each node with nodes from another modality graph, i.e. learning node correspondence, and then perform structure-level matching i.e. learning phrase correspondence, by propagating associated nodes to neighbors, which jointly infer fine-grained correspondence of structured object, relation and attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Node-level Matching</head><p>Each node in the textual and visual graphs will match with nodes from another modality graph to learn node correspondence. We first depict the node-level matching on textual graph in details, and then roughly describe that on visual graph since this operation is symmetric on two kinds of graphs. Concretely, we first compute similarities between visual and textual nodes, denoted as U α V T β , followed by a softmax function along the visual axis. The similarity value measures how the visual node corresponds to each textual node. Then, we aggregate all the visual nodes as a weighted combination of their feature vectors, where the weight is the computed similarities. This process can be formulated as:</p><formula xml:id="formula_2">C t→i = softmax β (λU α V T β )V β .<label>(3)</label></formula><p>where λ is a scaling factor to focus on matched nodes. Unlike previous approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> that uses the learned correspondence to compute the global similarity, we present a multi-block module that computes block-wise similarity of the textual node and the aggregated visual node C t→i . This is computational efficiency and converts the similarity from a scalar into a vector for subsequent operations. Also, this allows different blocks to play different roles in matching. Concretely, we split the i-th feature of the textual node and the its corresponding aggregated visual nodes into t blocks, represented as [u i1 , u i2 , · · · , u it ] and [c i1 , c i2 , · · · , c it ], respectively. The multi-block similarity is computed within pair-wise blocks. For instance, the similarity in j-th blocks is calculated as x ij = cos(u ij , c ij ).</p><p>Here, x ij is a scalar value, cos(·) denotes cosine similarity. The matching vector of i-th textual node can be obtained by concatenating the similarity of all the blocks, that is</p><formula xml:id="formula_3">x i = x i1 || x i2 || · · · || x it .<label>(4)</label></formula><p>where "||" indicates concatenation. In this way, each textual node is associated with its matched visual nodes, which will be propagated to its neighbors at structure-level matching to guide neighbors learn fine-grained phrase correspondence. Symmetrically, when given a visual graph, the nodelevel matching is proceeded on each visual node. The corresponding textual nodes will be associated differentially</p><formula xml:id="formula_4">C i→t = softmax α (λV β U T α )U α<label>(5)</label></formula><p>Then each visual node, together with its associated textual nodes, will be processed by the multi-block module, producing the matching vector x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Structure-level Matching</head><p>The structure-level matching takes the node-level matching vectors as input, and propagates these vectors to neighbors along with the graph edge. Such a design benefits to learn fine-grained phrase correspondence as neighboring nodes guide that. For example, a sentence "A brown dog bite a gray dog ear", the first "dog" will correspond to the visual brown dog in a finer level, because its neighbors "bite" and "brown" point to the brown dog, and hence the "dog" prefer to correlate with the correct dog in the image. To be specific, the matching vector of each node is updated by integrating neighborhood matching vectors using GCN. The GCN layer will apply K kernels that learn how to integrate neighborhood matching vectors, formulated aŝ</p><formula xml:id="formula_5">x i = || K k=1 σ   j∈Ni W e W k x j + b   .<label>(6)</label></formula><p>where N i denotes the neighborhood of i-th node, W e indicates the edge weight depicted in section 3.1, W k and b are the parameters to be learned of k-th kernel. Note that k kernels are applied, the output of the spatial convolution is defined as a concatenation over the output of k kernels, producing convolved vector that reflects the correspondence of connected nodes. These nodes form the localized phrase. The phrase correspondence can be inferred by propagating neighboring node correspondence, which can be used to reason the overall matching score of image-text pair. Here, we feed the convolved vectors into a multi-layer perceptron (MLP) to jointly consider the learned correspondence of all the phrases, and infer the global matching score. This represents how much one structured graph matches another structured graph. This process is formulated as</p><formula xml:id="formula_6">s t→i = 1 n i W u s (σ(W u hxi + b u h )) + b u s ,<label>(7)</label></formula><formula xml:id="formula_7">s i→t = 1 m j W v s (σ(W v hxj + b v h )) + b v s .<label>(8)</label></formula><p>where W s, bs denote parameters of MLP, which includes two fully-connected layers, the function σ(·) indicates the tanh activation. Note that we perform structure-level matching on both visual and textual graphs, which can learn phrase correspondence complement to each other. The overall matching score of an image-text pair is computed as the sum of matching score at two directions</p><formula xml:id="formula_8">g(G 1 , G 2 ) = s t→i + s i→t .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Objective Function</head><p>Following previous approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30]</ref>, we employ the triplet loss as the objective function. When using the text T as query, we sample its matched images and mismatched images at each mini-batch, which form positive pairs and negative pairs. The similarity in positive pairs should be higher than that in negative pairs by a margin γ. Analogously, when using the image I as query, the negative sample should be a text that mismatches the given query, their similarity relative to positive pairs should also satisfy the above constraints. We focus on optimizing hard negative samples that produce the highest loss, that is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Representation</head><p>Visual Representation. Given an image I, we represent its feature as a combination of its n salient regions, which are detected by Faster-RCNN pretrained on Visual Genome <ref type="bibr" target="#b12">[13]</ref>. The detected regions are feed into pretrained ResNet-101 <ref type="bibr" target="#b4">[5]</ref> to extract features, and then transformed into a ddimensional feature space using a fully connected layer:</p><formula xml:id="formula_9">v i = W m [CN N (I i )] + b m .<label>(11)</label></formula><p>where CN N (·) encodes each region within bounding box as a region feature, W m , b m are parameters of the fully connected layer that transforms the feature into the common space. These region features form the image representation, denoted as [v 1 , v 2 , · · · , v n ]. Textual Representation. Given a text T that contains m words, we represent its feature as [u 1 , u 2 , · · · , u m ], where each word is associated with a feature vector. We first represent each word as a one-hot vector, and then embed it into ddimensional feature space using a Bidirectional Gated Recurrent Unit (BiGRU), which enables to integrated forward and backward contextual information into text embeddings. The representation of i-th word is obtained by averaging the hidden state of forward and backward GRU at i-th time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>To validate the effectiveness of our proposed method, we evaluate it on two most widely used benchmarks, Flickr30K <ref type="bibr" target="#b24">[25]</ref> and MSCOCO <ref type="bibr" target="#b15">[16]</ref>. Each benchmark contains multiple image-text pairs, where each image is described by five corresponding sentences. Flickr30K collects 31,000 images and 31,000 × 5 = 155,000 sentences in total. Following the settings in previous works <ref type="bibr" target="#b11">[12]</ref>, this benchmark is split into 29,000 training images, 1,000 validation images, and 1,000 testing images. A large-scale benchmark MSCOCO contains 123,287 images and 123,287 × 5 = 616,435 sentences, we use 113,287 images for training, both the validation and testing sets contain 5,000 instances. The evaluation result is calculated on 5-folds of testing images.</p><p>The commonly used evaluation metrics for image-text matching are Recall@K (K=1,5,10), denoted as R@1, R@5, and R@10, which depict the percentage of ground truth being retrieved at top 1, 5, 10 results, respectively. The higher Recall@K indicates better performance. Additionally, to show the overall matching performance, we also compute the sum of all the Recall values (rSum) at imageto-text and text-to-image directions, that is  (12) As for implementation details, we train the proposed network on training set and validate it at each epoch on validation set, selecting the model with the highest rSum to be test. We train the proposed method on 1 Titan Xp GPU  with 30 and 20 epochs for Flickr30K and MSCOCO, respectively. The Adam optimizer is employed with mini batch size 64. The initial learning rate is set as 0.0002 with decaying 10% every 15 epochs on Flickr30K, and 0.0005 with decaying 10% every 5 epochs on MSCOCO. We set the dimension of word embeddings as 300, which are then feed into Bi-GRU to get 1024-diemensioanl word representation. As for image feature, each image contains 36 regions that are most salient, and extract 2048-dimensional features for each region. The region feature is then transformed into a 1024-dimensional visual representation by a fully-connected layer. At the structure-level matching, we use one spatial graph convolution layer with 8 kernels, each of which are 32-dimensional. After that, we feed each node in the graph into two fully-connected layers followed by a tanh activation to reason the matching score. The scaling factor λ setting is investigated at section 4.2.3. As for optimization, the margin γ is empirically set as 0.2.</p><formula xml:id="formula_10">rSum = R@1 + R@5 + R@10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparisons with state-of-the-arts</head><p>Baselines. we make a comparison with several networks in image-text matching, including (1) typical works m-CNN <ref type="bibr" target="#b19">[20]</ref>, DSPE <ref type="bibr" target="#b28">[29]</ref> and DANs <ref type="bibr" target="#b22">[23]</ref> that learn global imagetext correspondence by designing different network blocks.</p><p>(2) VSE++ <ref type="bibr" target="#b1">[2]</ref>, DPC <ref type="bibr" target="#b34">[35]</ref> and TIMAM <ref type="bibr" target="#b26">[27]</ref> that learn correspondence using different optimization.</p><p>(3) SCO <ref type="bibr" target="#b8">[9]</ref>, GXN <ref type="bibr" target="#b3">[4]</ref> that learn region-word correspondence by designing specific networks. (4) state-of-the-art methods SCAN <ref type="bibr" target="#b13">[14]</ref>, BFAN <ref type="bibr" target="#b17">[18]</ref>, PFAN <ref type="bibr" target="#b29">[30]</ref>. Quantitative Analysis. We provide two versions of our approach, one models the text as a sparse graph and another one models it as a dense graph. We ensemble them by averaging their similarity of image-text pairs, and find that can greatly improve the performance. Note that state-of-the-art  methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18</ref>] also use ensemble model. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can observe that the proposed network outperforms state-of-the-arts with respect to all the evaluation metrics on Flickr30K. Compared with the state-of-the-art method PFAN <ref type="bibr" target="#b29">[30]</ref> that also utilizing the position information of salient regions, our approach obtains relative R@1 gains with 6.4% and 7% at image-to-text and text-to-image matching. Differs from PFAN <ref type="bibr" target="#b29">[30]</ref> that embeds position information into visual representation, our approach employs it as the weight of graph edge. The improvement indicates that structured models object, relation and attribute can greatly improve the matching performance. Although a previous approach SCAN <ref type="bibr" target="#b13">[14]</ref> uses similar method to learn object correspondence, our approach achieves more improvement, with nearly 10% R@1 gain, since it ignores to explicitly learn correspondence of the relation and attribute. In addition, our single model also outperforms their ensemble model by a large margin, and the dense model is better than sparse one as it can discover latent dependencies. The quantitative results on a larger and more complicated dataset MSCOCO is shown at Table 2. We can observe that our approach can outperform state-of-the-art methods with nearly 2% improvement in terms of Recall@1, which is more concerned by users in real applications. Our Re-call@10 in image-to-text matching is slightly lower than PFAN since noise exists. Compared with SCAN that is most relevant to our work, we suppress it in terms of all the evaluation metrics, getting over 5.5% and 4.5% relative Recall@1 improvements on two directions. Note that the sparse model performs better than the dense model, it mainly arises from the sentence in this dataset is more complicated, and thus might incorrectly correlate totally irrelevant words if a fully-connected graph is built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Impact of different network structures</head><p>To validate the impact of different network structures, we conduct ablation studies incrementally on Flickr30K. We compare the full dense model and full sparse model with five models: (1) GSMN-w/o graph, which only performs node-level matching. (2) GSMN-w/o i2t, which only applies the node-level matching and structure-level matching on image-to-text direction. (3) GSMN-w/o t2i, which only applies the node-level matching and structure-level matching on text-to-image direction. (4) GSMN-2GCN, its depth of GCN layer is set as 2. (5) GSMN-GRU, a network that only uses GRU instead of Bi-GRU as the text encoder. As shown in <ref type="table" target="#tab_2">Table 3</ref>, The two full models outperform all these types of networks, and they largely exceed the network that only performs matching on single direction. Note that GSMN-2GCN requires more computational cost and GPU memory, results show that a deeper network will drop the performance as it additionally considers indirectly connected nodes, which will disturb the learned correspondence. Compared with GSMN-GRU, our approach achieves more improvement on text-to-image graph, it derives from the Bi-GRU can better model the semantic dependency among object, relation and attribute than GRU, and hence the edge weight of textual graph can be accurately reflected. Note that GSMN-w/o i2t gets better performance than GSMN-w/o t2i, because the implicit relation among regions is difficult to be discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Impact of different parameters</head><p>To validate the impact of different parameters, we conduct extensive experiments on two benchmarks. In this work, the most sensitive parameter is the scaling factor λ that determines the relative weight of different nodes in nodelevel matching, and the edge weight of textual graph. A large λ will filter out extensive nodes, and only preserve little nodes that are highly relevant to the specific node. A  small λ is unable to distinguish relevant nodes from irrelevant ones. Hence, an appropriate parameter is important in our proposed network. Here, we investigate the matching performance with setting the λ as 5, 10 and 20, see <ref type="figure" target="#fig_4">figure  3</ref>. We observe the Recall@1 on validation set at each training epoch. The top two subfigures are on Flickr30K, it is obvious that when λ = 20, the proposed network yields better Recall@1 on two matching directions, and there is just little difference when the parameter is set as 5 and 10. The bottom two subfigures are on MSCOCO, showing that λ = 10 is much better. The different parameter setting on two datasets might be caused by different data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Case Study</head><p>We provide a visualization to show the learned node correspondence and phrase correspondence in <ref type="figure" target="#fig_5">Figure 4</ref>. Note that we only show the most relevant region for each textual node, it shows different kinds of nodes can associate with their corresponding regions with relatively higher scores. Moreover, we can infer phrase correspondence enclosed by multiple bounding boxes, and their scores are greatly improved. Also, we visualize the text-to-image and image-totext matching results on Flickr30K, shown in <ref type="figure" target="#fig_6">Figure 5</ref> and <ref type="figure">Figure 6</ref>. These show our approach always retrieves the <ref type="figure">Figure 6</ref>: Visualization of image-to-text matching on Flickr30K. For each image query, we show top 5 ranked texts, where mismatched texts are marked as red.</p><p>ground truth with a high rank. In addition, our approach is able to learn fine-grained correspondence of the relation and attribute. For example, for the first text query in <ref type="figure" target="#fig_6">Figure  5</ref>, our network can distinguish different kinds of hats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Conclusion</head><p>In this paper, we propose a graph structured matching network for image-text matching, which performs matching on heterogeneous visual and textual graphs. This is achieved by node-level matching and structure-level matching that infer fine-grained correspondence by propagating node correspondence along the graph edge. Moreover, such a design can learn correspondence of relation and attribute, which are mostly ignored by previous works. With the guidance of relation and attribute, the object correspondence can be greatly improved. Extensive experiments demonstrate the superiority of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>g(I, T )+g(I, T ′ )] + +[γ−g(I, T )+g(I ′ , T )] + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>′ are hard negatives, the function [·] + is equivalent to max[·, 0], and g(·) is the global similarity of an image-text pair computed by equation 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>+</head><label></label><figDesc>R@1 + R@5 + R@10T ext as query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of Recall@1 results on Flickr30K and MSCOCO with different λ settings. (a) Image-to-text on Flickr30K. (b) Text-to-image on Flickr30K. (c) Imageto-text on MSCOCO. (d) Text-to-image on MSCOCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of node correspondence and phrase correspondence with score inside the box. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of text-to-image matching on Flickr30K. For each text query, we show top 3 ranked images from left to right, where mismatched images are with red boxes and matched images are with green boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image-text matching results on Flickr30K, ′ f t ′ and ′ f ixed ′ are fine-tuning and no fine-tuning. The bests are in bold.</figDesc><table><row><cell>Image-to-Text</cell><cell>Text-to-Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Image-text matching results on MSCOCO, ′ f t ′ and ′ f ixed ′ are fine-tuning and no fine-tuning. The bests are in bold.</figDesc><table><row><cell>Image-to-Text</cell><cell>Text-to-Image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The ablation study on Flickr30K to investigate the effect of different network structures.</figDesc><table><row><cell></cell><cell cols="4">Image-to-Text Text-to-Image</cell></row><row><cell>Model</cell><cell cols="4">R@1 R@10 R@1 R@10</cell></row><row><cell>GSMN-w/o graph</cell><cell>63.2</cell><cell>94.5</cell><cell>48.7</cell><cell>84.5</cell></row><row><cell>GSMN-w/o t2i</cell><cell>64.6</cell><cell>93.5</cell><cell>45.8</cell><cell>82.6</cell></row><row><cell>GSMN-w/o i2t</cell><cell>67.0</cell><cell>95.5</cell><cell>52.3</cell><cell>86.3</cell></row><row><cell>GSMN-2GCN</cell><cell>68.4</cell><cell>94.8</cell><cell>51.5</cell><cell>86.0</cell></row><row><cell>GSMN-GRU</cell><cell>71.1</cell><cell>95.3</cell><cell>50.9</cell><cell>85.6</cell></row><row><cell cols="2">GSMN-full (sparse) 71.4</cell><cell>96.1</cell><cell>53.9</cell><cell>87.1</cell></row><row><cell>GSMN-full (dense)</cell><cell>72.6</cell><cell>96.8</cell><cell>53.7</cell><cell>87.0</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">similar region-word pairs. Huang et al.<ref type="bibr" target="#b8">[9]</ref> present to order semantic concepts and composite them to infer correspondence. Similarly, Huang et al.<ref type="bibr" target="#b7">[8]</ref> propose to recurrently select corresponding region-word pairs. Ji et al.<ref type="bibr" target="#b9">[10]</ref> exploit saliency model to localize salient regions,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1855" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stacked latent attention for multimodal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1072" to="1080" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Look, imagine and match: Improving textualvisual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>abs/1711.06420</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7211" to="7221" />
		</imprint>
	</monogr>
	<note>Jonathan Berant, and Amir Globerson</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-directional spatial-semantic attention networks for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09471</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1908" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical clustering multi-task learning for joint human action grouping and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Zhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focus your attention: A bidirectional focal attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4127" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Webly supervised joint embedding for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stathis</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2641" to="2649" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial representation learning for text-to-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10534</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth workshop on vision and language</title>
		<meeting>the fourth workshop on vision and language</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="407" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning semantic structure-preserved embeddings for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="825" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-level policy and reward-based deep reinforcement learning framework for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<title level="m">Dual-path convolutional imagetext embedding with instance loss</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
