<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Neural Machine Translation with SMT as Posterior Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Ma</surname></persName>
							<email>mashuai@buaa.edu.cn‡zrustc11@gmail.com§shujliu</email>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Neural Machine Translation with SMT as Posterior Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Without real bilingual corpus available, unsupervised Neural Machine Translation (NMT) typically requires pseudo parallel data generated with the back-translation method for the model training. However, due to weak supervision, the pseudo data inevitably contain noises and errors that will be accumulated and reinforced in the subsequent training process, leading to bad translation performance. To address this issue, we introduce phrase based Statistic Machine Translation (SMT) models which are robust to noisy data, as posterior regularizations to guide the training of unsupervised NMT models in the iterative back-translation process. Our method starts from SMT models built with pre-trained language models and word-level translation tables inferred from cross-lingual embeddings. Then SMT and NMT models are optimized jointly and boost each other incrementally in a unified EM framework. In this way, (1) the negative effect caused by errors in the iterative back-translation process can be alleviated timely by SMT filtering noises from its phrase tables; meanwhile, (2) NMT can compensate for the deficiency of fluency inherent in SMT. Experiments conducted on en-fr and en-de translation tasks show that our method outperforms the strong baseline and achieves new state-of-the-art unsupervised machine translation performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the rise and success of Neural Machine Translation (NMT) <ref type="bibr">(</ref> sentences in two languages are mapped into the same latent space with a shared encoder, which is expected to be the internal information representation irrelevant to the languages themselves. From that target sentences are generated by a shared or different decoders. Some of them also use denoising auto-encoders <ref type="bibr" target="#b22">(Vincent et al. 2010</ref>) and adversarial training. Despite the differences in structures and training methods, they reach a consensus to use the pseudo parallel data generated iteratively with the back-translation method (Sennrich, Haddow, and Birch 2016; Zhang et al. <ref type="bibr">2018a</ref>) to train their unsupervised NMT models, i.e. they use monolingual data in the target language and a target-to-source translation model to generate source sentences, then use the pseudo parallel data of generated sources and real targets to train the source-to-target model, and vice versa.</p><p>However, since the pseudo data are generated by unsupervised models, random errors and noises are inevitably introduced, such as redundant or unaligned words deviating from the meaning of source sentences. Due to the lack of supervision, those infrequent errors will be accumulated and reinforced by NMT models into frequent patterns during the training iterations, leading to bad translation performance. For instance in <ref type="figure">Figure 1</ref>, the French word "malade" is mistakenly translated into the English word "ill-fated" in the first training sample. With strong abilities to identify and memorize patterns, NMT models mistakenly translate this word into "ill-fated" when "old" (similar to "grandmother" in the first training sample) occurs in the test. Even so, there are also many good translation patterns (such as "malade" → "ill" or "sick" in the second and third training samples), which could have been extracted in time to guide the NMT models into the correct training direction. The extraction and guidance can be well carried out by Statistical Machine Translation (SMT). As is pointed out by <ref type="bibr" target="#b7">Khayrallah and Koehn (2018)</ref>, SMT performs better than NMT in tackling noisy data by constructing a strong phrase table with good and frequent translation patterns and filtering out infrequent errors and noises. This gives the motivation that if we incorporate SMT in the training process, unsupervised NMT could benefit from the robustness of SMT to noisy data.</p><p>In this paper, we propose to leverage SMT to denoise and guide the training of unsupervised NMT models in the iterative back-translation process. Different from previous work <ref type="bibr" target="#b5">(He et al. 2016;</ref><ref type="bibr" target="#b20">Tang et al. 2016;</ref><ref type="bibr" target="#b23">Wang et al. 2017)</ref> introducing SMT into NMT by changing model structures in supervised scenarios, we adopt the framework of posterior regularization <ref type="bibr" target="#b3">(Ganchev et al. 2010)</ref> to leave model structures unchanged. Our method starts from initial SMT models built with pre-trained language models and word-level translation tables inferred from cross-lingual embeddings. Then SMT models and NMT models are trained jointly in a unified Expectation Maximization (EM) training framework. In each iteration, as desired distributions, SMT models are expected to correct NMT models timely with denoised pseudo data generated in a constrained search space of reliable translation patterns. Based on that, enhanced NMT models can generate better pseudo data for SMT to extract phrases of higher quality, so that they can benefit from each other incrementally. In this way, infrequent errors in NMT models can be eliminated with the constraints exerted by SMT features, while NMT can compensate for the deficiency in smoothness inherent in SMT models. Experiments conducted on en-fr and en-de translation tasks show that our method significantly outperforms the strong baseline <ref type="bibr" target="#b10">(Lample et al. 2018)</ref> and achieves the new state-of-the-art translation performance in unsupervised machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>Given a source sentence x = (x 1 , x 2 , ..., x l ) and a target one y = (y 1 , y 2 , ..., y m ), Neural Machine Translation (NMT) directly models the word-level translation probability with parameters θ as:</p><formula xml:id="formula_0">p(y i |x, y &lt;i ; θ) = softmax(g(h yi , h y&lt;i , c i ; θ)) (1)</formula><p>in which g(·) denotes a non-linear function extracting features to predict the target word y i from the decoder states (h yi and h y&lt;i ) and the context vector c i calculated with the encoder and attention mechanism. Then the sentence-level translation probability p(y|x; θ) is calculated by p(y|x; θ) = m i=1 p(y i |x, y &lt;i ; θ). As for training, given a parallel corpus {(x n , y n )} N n=1 , the objective function is to maximize log p(y n |x n ; θ) over the whole training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phrase-based Statistic Machine Translation</head><p>The current approach of Statistic Machine Translation (SMT) is typically based on the log-linear model proposed by <ref type="bibr" target="#b14">Och and Ney (2002)</ref>. According to it, the translation probability from sentence x to sentence y is formulated as:</p><formula xml:id="formula_1">p(y|x; λ M 1 ) = exp [ M m=1 λ m h m (x, y)] ỹ exp [ M m=1 λ m h m (x,ỹ)]</formula><p>(2)</p><p>where h m (x, y) = log φ m (x, y) denotes the m th feature.</p><p>In phrase based SMT (PBSMT) <ref type="bibr" target="#b9">(Koehn, Och, and Marcu 2003)</ref>, the sentence pair is segmented into a sequence of phrasesx I 1 andȳ J 1 , where I and J are the counts of phrases. During training, given a bilingual corpus, PBSMT first infers word alignment, based on which phrase pairs are derived and stored in the phrase </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Posterior Regularization</head><p>Posterior regularization <ref type="bibr" target="#b3">(Ganchev et al. 2010</ref>) is a framework for structured, weakly supervised learning, which incorporates indirect supervision from a desired distribution q(y) via constraints on posterior distribution p(y|x n ; θ) imposed by a Kullback-Leible (KL) divergence as follows:</p><formula xml:id="formula_2">F (q; θ) = L(θ) − N n=1 min q∈Q KL(q(y)||p(y|x n ; θ)) (3)</formula><p>where L(θ) is the original likelihood of model p(y|x; θ), and Q is a constraint posterior set satisfying: </p><formula xml:id="formula_3">Q = {q(y) : E q [φ(x, y)] ≤ b}</formula><formula xml:id="formula_4">M : θ t+1 = arg max θ L(θ) + E q t+1 [log p(y|x n ; θ)]<label>(5)</label></formula><p>However, there may be a problem as pointed out by Zhang et al. <ref type="bibr">(2017)</ref> that it is hard to set a reasonable bound b if we directly apply posterior regularization to NMT. To solve this problem, we follow their practice of representing the desired distribution q(y) as the log-linear model described in <ref type="figure">Eq.</ref>(2). In this way, SMT models directly act as the posterior regularization to constrain NMT models p(y|x n ; θ t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Due to the lack of supervision, noises and infrequent errors in the pseudo data generated by unsupervised NMT models will be accumulated and reinforced in the iterative backtranslation process (shown in the shadow area in <ref type="figure" target="#fig_2">Figure 2</ref>). To address this issue, we introduce SMT as posterior regularization (the red frame above that) to denoise and guide the training of NMT, thus the noises being eliminated timely.</p><p>The whole procedure of our method mainly consists of two parts shown in the left and right of <ref type="figure" target="#fig_2">Figure 2</ref>. Given a language pair X-Y, for model initialization, we build two initial SMT models with language models pre-trained using monolingual data, and word translation tables inferred from cross-lingual embeddings according to the approach in 3.2. Then the initial SMT models will generate pseudo data to warm up two NMT models. Note that the NMT models are trained using not only the pseudo data generated by SMT models, but those generated by reverse NMT models with the iterative back-translation method. After that, the NMT-generated pseudo data are fed to SMT models. As posterior regularization (PR), SMT models timely filter out noises and infrequent errors by constructing strong phrase tables with good and frequent translation patterns, and then generate denoised pseudo data to guide the subsequent NMT training. Benefiting from that, NMT then produces better pseudo data for SMT to extract phrases of higher quality, meanwhile compensating for the deficiency in smoothness inherent in SMT via back-translation. Those two steps are unified in the EM framework described in 3.3, where NMT and SMT models are trained jointly and boost each other incrementally until final convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initialization</head><p>Our initial SMT models are built with word-based phrase tables and two pre-trained language models via Moses 1 . For the word translation table, we first train word embeddings using monolingual corpora for two languages respectively. Based on that, we adopt the method proposed by Artetxe et al. <ref type="bibr">(2018)</ref>  Then the word translation probability from word x i to y j is:</p><formula xml:id="formula_5">p(y j |x i ) = exp [λ cos(e xi , e yj )] k exp [λ cos(e xi , e y k )]<label>(6)</label></formula><p>where λ is a hyper-parameter to control the peakiness of the distribution. The calculation of p(x i |y j ) is similar to Eq.(6). Based on the above, we choose top-k translation candidates for each word in our initial phrase table. We only use two features in our initial phrase tables, i.e. translation probabilities and inverse translation probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised NMT with SMT as PR</head><p>As is mentioned in 3.1, SMT plays a role in denoising and is leveraged as posterior regularization for NMT. Therefore, we replace the posterior regularization term q(y) in Eq.</p><p>(3) with the SMT models (x → y) and (y → x) in <ref type="figure" target="#fig_2">Figure 2</ref>, which will be denoted by − → p s (y|x) and ← − p s (x|y). By the way, the NMT models (x → y) and (y → x) will be denoted by − → p n (y|x; θ x→y ) and ← − p n (x|y; θ x←y ), where θ x→y and θ x←y are parameters. Then, given monolingual corpora</p><formula xml:id="formula_6">{x i } M i=1</formula><p>and {y j } N j=1 , we formulate the training objective as:</p><formula xml:id="formula_7">J (θ x→y , θ x←y , − → p s , ← − p s ) =L(θ x→y , θ x←y ) − M i=1 min − → ps KL( − → p s (y|x i )|| − → p n (y|x i ; θ x→y )) − N j=1 min ← − ps KL( ← − p s (x|y j )|| ← − p n (x|y j ; θ x←y ))<label>(7)</label></formula><p>whereL(θ x→y , θ x←y ) corresponds to the training objective of iterative back-translation for NMT models, which is</p><formula xml:id="formula_8">L(θ x→y , θ x←y ) = M i=1 E y∼ − → pn(y|xi;θx→y) [log ← − p n (x i |y; θ x←y )] + N j=1 E x∼ ← − pn(x|yj ;θx←y) [log − → p n (y j |x; θ x→y )]<label>(8)</label></formula><p>and two Kullback-Leibler divergence (KL) terms denote the posterior regularizations for two NMT models respectively. Based on that, the training processes of iterative backtranslation for NMT and SMT models as posterior regularization are unified into a single objective J . Then, we modulate the EM algorithm in Eq.(5) to optimize it as follows:</p><formula xml:id="formula_9">E : ← − p s t+1 = arg max ← − ps J (θ x→y , θ x←y , − → p s , ← − p s ) = arg min ← − ps KL( ← − p s (x|y j )|| ← − p n (x|y j ; θ t x←y )) − → p s t+1 = arg max − → ps J (θ x→y , θ x←y , − → p s , ← − p s ) = arg min − → ps KL( − → p s (y|x i )|| − → p n (y|x i ; θ t x→y )) M : θ t+1 x←y = arg max θx←y J (θ x→y , θ x←y , − → p s , ← − p s ) = arg max θx←y {E← − ps t+1 [log ← − p n (x|y j ; θ x←y )] + E− → pn(y|xi;θ t x→y ) [log ← − p n (x i |y; θ x←y )]} θ t+1 x→y = arg max θx→y J (θ x→y , θ x←y , − → p s , ← − p s ) = arg max θx→y E− → ps t+1 [log − → p n (y|x i ; θ x→y )] + E← − pn(x|yj ;θ t x←y ) [log − → p n (y j |x; θ x→y )]</formula><p>(9) Briefly speaking, in the E-step, we optimize the desired distributions represented by SMT to minimize the KL distance between SMT models and NMT models. In the Mstep, we optimize NMT models using the pseudo data generated by SMT models and the corresponding reverse NMT models to fit the desired distributions and meanwhile perform back-translation iterations. We will give the specific equation for updating parameters in 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Algorithm</head><p>We combine the model initialization and the whole training procedure into Algorithm 1 as follows.</p><p>According to Eq.(9), in the E-step, we need to minimize the gap between SMT models and NMT models. However, this step cannot be done by traditional gradient descent methods. Approximately, we train SMT models using the pseudo data generated by the corresponding NMT models to fit the mode of NMT posterior distributions. Thus the KL divergence between them is diminished. This step corresponds to the the 7 th and 8 th lines in Algorithm 1, meaning SMT extracts good and frequent translation patterns from the data generated by current NMT models to finish denoising.</p><p>In the M-step, we optimize two NMT models with gradient descent methods. We formulate the updating for θ x←y in Eq.(10), to which that for θ x→y is similar.</p><formula xml:id="formula_10">∇ θx←y J (θ x→y , θ x←y , − → p s , ← − p s ) = E x∼ ← − ps(x|yj ) ∇ θx←y log ← − p n (x|y j ; θ x←y ) + E y∼ − → pn(y|xi;θx→y) ∇ θx←y log ← − p n (x i |y; θ x←y )<label>(10)</label></formula><p>Algorithm 1: Unsupervised NMT with SMT as PR</p><formula xml:id="formula_11">Input: Monolingual data X = {xi} M i=1 and Y = {yj} N j=1</formula><p>Output: Parameters of two NMT models: θx→y, θx←y 1 Train language models lx and ly using X and Y 2 Infer word translation tables txy and tyx as in 3. This step corresponds to lines 14 to 17 in Algorithm 1. A difficulty here is the exponential search space of the translation candidates. To address it, we leverage the sampling method <ref type="bibr" target="#b18">(Shen et al. 2015)</ref> and simply generate the top target sentence for approximation in our experiments. Note that in the 11 th line, NMT models are trained using the denoised pseudo data generated by SMT models only, while in the 13 th line, the mixed data of those and the pseudo data generated by the reverse NMT models are used. The intention here is to first use the denoised pseudo data to correct the NMT models established before, and then apply iterative back-translation to boost NMT models under the guide of the denoised data. NMT also makes up for the deficiency in smoothness of SMT in this step. In this way, SMT and NMT models can benefit from each other in the EM iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Dataset In our experiments, we consider two language pairs, English-French and English-German. For each language, we use 50 million monolingual sentences in NewsCrawl, a monolingual dataset from WMT, which is the same as the previous work (Artetxe et al. <ref type="bibr" target="#b10">Lample et al. 2018)</ref>. For the convenience of comparison, we use newstest 2014 as the test set for the English-French pair, and newstest 2014 as well as newstest 2016 for the English-German pair.</p><p>Preprocess We use Moses scripts for word tokenization and truecasing. In model initialization, we use the public im-Method fr-en en-fr de-en en-de de-en en-de <ref type="bibr">(2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison</head><p>Baselines Our proposed method is compared with four baselines of unsupervised machine translation listed in the upper area of <ref type="table">Table 1</ref>, among which the fourth baseline contains several methods. Given a language pair, the first two baselines (Artetxe et al. 2017; Lample, Denoyer, and Ranzato 2017) use a shared encoder and different decoders for the two languages. The third baseline <ref type="bibr" target="#b25">(Yang et al. 2018</ref>) uses different encoders and decoders, and introduces a weight sharing mechanism. The fourth baseline <ref type="bibr" target="#b10">(Lample et al. 2018</ref>) uses a shared encoder and decoder in their NMT systems. As for the training method, the second and third baselines use adversarial training. All of the four baselines use denoising auto-encoder and iterative back-translation.</p><p>Note that the fourth baseline contains four methods. "NMT" means unsupervised NMT models, while "PBSMT" denotes unsupervised SMT models with the back-translation method performed by SMT. "NMT+PBSMT" and "PB-SMT+NMT" simply combine the best pseudo data that the former generates into the final iteration of the latter. Different from our proposed method, the training processes of NMT and SMT models in their methods are independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>The comparison results are reported in <ref type="table">Table 1</ref>. The BLEU scores are calculated by multibleu.pl. From the table, we find that our method significantly outperforms all the baselines even the strong one <ref type="bibr" target="#b10">(Lample et al. 2018)</ref>. We elaborate the reasons as follows.</p><p>(1) Our proposed method significantly improves the performance over the "NMT" and "PBSMT" of <ref type="bibr" target="#b10">(Lample et al. 2018)</ref>. This is because unsupervised NMT methods suffer from the noise problem while PBSMT is inherently deficient in fluency just as the case study in 4.5 shows. Our method can compensate for the deficiencies of them by combining the training processes of them.</p><p>(2) Notice that "NMT+PBSMT" performs even worse than pure "PBSMT", which may be caused by accumulated errors in the iterations of NMT models. Due to the lack of timely denoising methods, infrequent errors and noises are repeated and reinforced as frequent ones by unsupervised NMT, so that even PBSMT could not distinguish them from good patterns in the last iteration.</p><p>(3) The performance gained by "PBSMT+NMT" verifies combining data of high quality into NMT training could be a better choice. But the simple combination in their method is not able to make the best of both models. In their method, NMT and SMT models are trained independently so that the bad patterns within the models themselves cannot be well removed due to weak supervision. In contrast, our proposed method integrates the training of NMT and SMT models in a unified EM framework where they can boost each other incrementally. The noises and errors generated by NMT models can be reduced in time by SMT as posterior regularization, while NMT can compensate for the deficiency of smoothness inherent in SMT models. Therefore, our proposed method still outperforms "PBSMT+NMT".</p><p>Apart from SMT as posterior regularization, our framework can be easily extended to incorporate other posterior regularization methods without changing model structures, such as the target-bidirectional agreement regularization <ref type="bibr" target="#b28">(Zhang et al. 2018b</ref>). This regularization can help deal with the problem of exposure bias in supervised NMT, where another "reversed" NMT model is trained using data of reversed sentences from left to right. Then the "reversed" NMT model is leveraged to generate pseudo data for training the original NMT model. Specifically, we introduce the R2L regularization after the final training iteration of NMT models (i.e., NMT2 in <ref type="table" target="#tab_5">Table 2</ref>). With this extension, we achieve  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Evolution</head><p>We conduct several EM iterations in our experiments, and record the test BLEU scores on newstest 2014 after each Estep (SMT) and M-step (NMT) in <ref type="table" target="#tab_5">Table 2</ref>. We have tried more steps but the models do converge after three EM iterations. For the convenience of comparison, in the last column of the table, we also list the average improvement of four translation models after each step. From the table, first, we find NMT and SMT models improve incrementally after each iteration, which accords with our proposed motivation. Note that the improvements between adjacent NMT steps are exactly contributions made by SMT as posterior regularization. Second, the models improve the most in the first EM iteration and nearly converge at the third EM iteration.</p><p>Additionally, we also compare the translation performance on sentences of different lengths as iteration steps progress. We group the sentences in the fr-en test set by length as shown by the three curves in <ref type="figure" target="#fig_3">Figure 3</ref>. Then, we record the BLEU scores of different groups after each step. From the figure, we find the models converge much slower on longer sentences, which indicates that it is easier for the models to learn shorter sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion on Initialization</head><p>In this subsection, we delve into the initialization stage which is crucial to our method. In that stage, there are three hyper parameters described in 3.2 that should be taken into account, i.e., the peakiness controller λ, the vocabulary size S or T , and the number of translation candidates k for each word. Since the performance of initialization can be evaluated by SMT0, we adjust the hyper-parameters and measure the fr-en test BLEU of SMT0 models accordingly. For brevity, we let S = T = V in our experiments. The results are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. From this figure, we find that k and V have much bigger impacts on the initial model SMT0 than λ. With the value of λ increasing, the performance of SMT0 gradually improves but starts to decline a bit after around 20. This is because the larger λ will make the distribution in Eq.(6) sharper, severely restricting the search spaces of SMT models. Similarly, the performance of SMT0 improves in accord with the value of k or V going up. But the improvement stops after certain thresholds (about 80 of k and 50000 of V ). The reason may be the useful information provided by word-translation tables is saturated after those.</p><p>We also tried other initialization methods in our experiments, such as directly using the pseudo parallel data constructed from word-by-word translation to warm up NMT models. We compare NMT0 models warmed up with this method (without SMT0) to NMT0 in our proposed method (with SMT0) in the following table, which stresses the necessity of SMT0 and the importance of good initialization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>To further demonstrate the effectiveness of our method, we select some cases from translation results (fr-en) and compare the translations generated by models of different training steps. The results are listed in <ref type="table" target="#tab_8">Table 4</ref>. In the first case, which is exactly the example in the Introduction, the word "malade" in French is wrongly translated into "ill-fated" in English by NMT0. As we can see, this error has been corrected in NMT1 after the guidance of SMT1. In the second case, apart from the wrongly aligned word "bâtisse-là" to "canopy-back business" by NMT1, there is also a redundant phrase "plenty of" generated by it. Those errors are both corrected after the regularization of SMT1. In the third case, we also reach the same conclusion that NMT1 can benefit from SMT1 and rectify the mistake on "rendu visiteà". There is also an interesting phenomenon from case three of NMT adhering to "from" which makes the sentence more fluent, even though this word is missed by SMT models. In a Source J'ai eu des relations difficiles avec lui jusqu'à ce qu'il devienne vieux, malade. SMT0</p><p>I've gotten of difficult relations with him until he will become old, sick. NMT0</p><p>I've had difficult relations with him until he's become old, ill-fated. SMT1</p><p>I've had difficult relationships with him until he became old, sick. NMT1 I had difficult relations with him until he became old and sick. Reference I had a difficult relationship with him until he became old and ill.</p><p>Source Le fonds d'investissement quiétait propriétaire de cette bâtisse-là avait des choixà faire. SMT0</p><p>The owner of this underlinebuilding, so had to make a choice of which was an investment fund. NMT0</p><p>The investment fund that was an owner of that canopy-back business had plenty of choice to do. SMT1</p><p>The investment fund that was the owner of this building just had to make choices. NMT1</p><p>The investment fund that was the owner of this building had choices to make. Reference The investment fund that owned the building had to make a choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>M. Dutton a rendu visiteà Mme Plibersek pour garantir qu'aucun dollar du plan de sauvetage ne sera dépensé en bureaucratie supplémentaire. SMT0</p><p>Mr Dutton paid a visit to Ms Plibersek to guarantee that the greenback no rescue plan of not be spent in extra bureaucracy. NMT0</p><p>Mr Dutton said Ms Plibersek'visit to guarantee any dollar from the rescue plan will be spent in extra bureaucracy. SMT1</p><p>Mr Dutton was visiting Ms Plibersek to guarantee that no dollar rescue plan will be spent on additional bureaucracy. NMT1</p><p>Mr Dutton paid a visit to Ms Plibersek to guarantee that no dollar from the rescue plan will be spent on extra bureaucracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Mr Dutton called on Ms Plibersek to guarantee that not one dollar out of the rescue package would be spent on additional bureaucracy.  <ref type="table" target="#tab_5">Table 2</ref>.</p><p>word, the above analysis verifies that noises and errors in unsupervised NMT models can be eliminated timely by SMT models as posterior regularization with our method . From these cases, we find that SMT can also benefit from NMT models. Even though the meanings of the key words could be captured by SMT, the outputs of SMT0 are not fluent especially in the second case. This problem is relieved in SMT1, after SMT is fed with more fluent pseudo data generated by NMT0, which validates that SMT and NMT can incrementally boost each other with our method. Denoising auto-encoder <ref type="bibr" target="#b22">(Vincent et al. 2010</ref>) and adversarial training methods are also leveraged to improve the ability of encoders. Besides, iterative back-translation is applied to generated pseudo parallel data for cross-lingual training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>After that, Lample et al. <ref type="bibr">(2018)</ref> summarize three principles for unsupervised machine translation, which are initialization, language modeling and iterative back-translation, and propose some effective methods with simplified training procedures. Four methods are leveraged in their work, including unsupervised NMT, unsupervised PBSMT and two combinations of them. Our method is different from them. In their methods, SMT and NMT are treated as independent models so that they suffer from respective deficiencies and cannot benefit from each other in their training processes. In contrast, we combine them into a unified EM training frame-work and enable them to improve jointly and boost each other incrementally, where NMT models are responsible for smoothing and fluency, while SMT models are responsible for denoising and guiding NMT models.</p><p>Moreover, there has been some work exploiting SMT features to improve supervised NMT. In He et al. <ref type="bibr">(2016)</ref>, the probability calculated by NMT is integrated as a feature into a log-linear model. After that, Tang et al. <ref type="bibr">(2016)</ref> and Wang et al. <ref type="bibr">(2017)</ref> leverage gate mechanisms to introduce a phrase table or candidates provided by SMT into NMT models. Different from them, we leave the model structures unchanged via the framework of posterior regularization. Zhang et al. <ref type="bibr">(2017)</ref> also integrate more prior knowledge into the training of NMT with the help of posterior regularization. But there is a major difference that we introduce the successful practice of iterative back-translation into this framework with a unified EM training algorithm, where SMT and NMT models can benefit from each other. Additionally, in unsupervised scenarios, our SMT features are learned from scratch and improved incrementally, rather than pre-trained from real bilingual data and fixed during the whole procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce SMT models as posterior regularization to denoise and guide unsupervised NMT models with the ability of constructing more reliable phrase tables and eliminating the infrequent and bad patterns generated in the back-translation iterations of NMT. We unify SMT and NMT models within the EM training algorithm where they can be trained jointly and benefit from each other incrementally. In the experiments conducted on en-fr and en-de language pairs, our method significantly outperforms previous methods, and achieves the new state-of-the-art performance of unsupervised machine translation, which demon-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014; Luong, Pham, and Manning 2015; Wu et al. 2016; Vaswani et al. 2017; Hassan et al. 2018). However, NMT relies heavily on large in-domain parallel data, resulting in poor performance on low-resource language pairs (Koehn and Knowles 2017). For some low-resource pairs without any bilingual corpus, how to train NMT models with only a monolingual corpus is a popular and interesting topic. Existing methods for unsupervised machine translation (Artetxe et al. 2017; Lample, Denoyer, and Ranzato 2017; Yang et al. 2018; Lample et al. 2018) are mainly the modifications of encoder-decoder schema. In their work, source Figure 1: The effect of noisy training data. The first training sample contains the noise ("malade" in French means "ill", not "ill-fated"), leading to the wrong test result (sys).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>features φ(x, y) are bounded by b. To maximize F (q; θ), Ganchev et al. (2010) propose an EM framework (McLachlan and Krishnan 2007) as: E : q t+1 = arg min q∈Q KL(q(y)||p(y|x n ; θ t ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Method overview. The whole procedure mainly consists of two parts as the left and the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Test BLEU on sentences grouped by length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Test of initial models with various hyper-params.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Previous unsupervised neural machine translation systems (Artetxe et al. 2017; Lample, Denoyer, and Ranzato 2017; Yang et al. 2018) are mainly the modifications of the current encoder-decoder structure. To constrain outputs of encoders for two languages into a same latent space, Artetxe et al. (2017), and Lample et al. (2017) use a shared encoder, while Yang et al. (2018) use a weight sharing mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to obtain respective cross-lingual embeddings {e</figDesc><table /><note>xi } S i=1 and {e yj } T j=1 , where S and T are vocabulary sizes.1 https://github.com/moses-smt/mosesdecoder</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Initialize − → ps 0 and ← − ps 0 using lx, ly, txy and tyx</figDesc><table><row><cell></cell><cell>2</cell></row><row><cell cols="2">3 t := 0</cell></row><row><cell></cell><cell>while not convergence do</cell></row><row><cell>5</cell><cell>// E-step:</cell></row><row><cell></cell><cell>if t = 0 then</cell></row><row><cell></cell><cell>else</cell></row><row><cell>7</cell><cell>Generate pseudo data {(xt, y + t )} and {(x + t , yt)} using models − → pn t and ← − pn t respectively</cell></row><row><cell>9</cell><cell>// M-step:</cell></row><row><cell>10 11</cell><cell>Generate denoised pseudo data {(xt, y  *  t )} and {(x  *  t , yt)} using − → ps t and ← − ps t Train − → pn t and ← − pn t using {(xt, y  *  t )} and {(x  *  t , yt)}</cell></row></table><note>4 Sample data {xt} ∈ X and {yt} ∈ Y68 Train − → ps t and ← − pst using (xt, y + t ) and (x +t , yt)12 Generate pseudo data {(xt, y + t )} and {(x + t , yt)} using − → pnt and ← − pn t respectively13 Train − → pn t and ← − pn t using {(x + t , yt)} ∪ {(xt, y * t )} and {(xt, y + t )} ∪ {(x * t , y)}14 t := t + 115 return θx→y, θx←y</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>NMT0) 24.06 24.82 16.29 12.88 +7.95 E-step (SMT1) 26.49 27.64 17.34 14.81 +2.06 M-step (NMT1) 28.29 29.02 19.61 16.02 +1.67 E-step (SMT2) 28.64 29.21 19.87 16.29 +0.23 M-step (NMT2) 28.79 29.17 20.04 16.43 +0.11</figDesc><table><row><cell>Steps</cell><cell>fr-en</cell><cell>en-fr</cell><cell cols="2">de-en en-de</cell><cell>ave</cell></row><row><cell>E-step (SMT0)</cell><cell cols="3">15.34 11.74 11.03</cell><cell>8.14</cell><cell>11.56</cell></row><row><cell>M-step (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Test BLEU on newstest 2014 in different steps.higher performance (+R2L regularization inTable 1).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The necessity of SMT0 in model initialization. The numbers in this table are BLEU scores on newstest 2014.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Cases of translation results from French to English in newstest 2014. The models of SMT0, NMT0, SMT1 and NMT1 are corresponding to the steps in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tmikolov/word2vec 3 https://github.com/artetxem/vecmap 4 https://github.com/tensorflow/tensor2tensor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>strates the effectiveness of our method. In the future, we may delve into the initialization stage, which is crucial to the final performance of the proposed method.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5012" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05567</idno>
		<title level="m">Achieving human parity on automatic chinese to english news translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with smt features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving translation quality by discarding most of the phrasetable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the impact of various types of noise on neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12282</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The EM algorithm and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01792</idno>
		<title level="m">Neural machine translation with external phrase memory</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation advised by statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3330" to="3336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation with weight sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09057</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prior knowledge integration for neural machine translation using posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1514" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint training for neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regularizing neural machine translation by target-bidirectional agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1808.04064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
