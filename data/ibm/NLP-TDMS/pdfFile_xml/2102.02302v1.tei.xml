<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cleora: A Simple, Strong and Scalable Graph Embedding Scheme</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bąbel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Gołuchowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Michałowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dąbrowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cleora: A Simple, Strong and Scalable Graph Embedding Scheme</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The area of graph embeddings is currently dominated by contrastive learning methods, which demand formulation of an explicit objective function and sampling of positive and negative examples. This creates a conceptual and computational overhead. Simple, classic unsupervised approaches like Multidimensional Scaling (MSD) or the Laplacian eigenmap skip the necessity of tedious objective optimization, directly exploiting data geometry. Unfortunately, their reliance on very costly operations such as matrix eigendecomposition make them unable to scale to large graphs that are common in today's digital world. In this paper we present Cleora: an algorithm which gets the best of two worlds, being both unsupervised and highly scalable. We show that high quality embeddings can be produced without the popular step-wise learning framework with example sampling. An intuitive learning objective of our algorithm is that a node should be similar to its neighbors, without explicitly pushing disconnected nodes apart. The objective is achieved by iterative weighted averaging of node neigbors' embeddings, followed by normalization across dimensions. Thanks to the averaging operation the algorithm makes rapid strides across the embedding space and usually reaches optimal embeddings in just a few iterations. Cleora runs faster than other state-of-the-art CPU algorithms and produces embeddings of competitive quality as measured on downstream tasks: link prediction and node classification. We show that Cleora learns a data abstraction that is similar to contrastive methods, yet at much lower computational cost. We open-source Cleora under the MIT license allowing commercial use under https://github.com/Synerise/cleora.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graphs are data structures which are extremely useful for modeling real-life interaction structures. A graph is represented by sets of nodes and edges, where each node represents an entity from the graph domain, and each edge represents the relationship between two or more nodes. Graph structures are found for example in biology as interconnected sets of amino acids building proteins <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, road networks <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b47">[48]</ref>, as well as social networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b46">[47]</ref>, citation networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b33">[34]</ref>, or web data <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In most machine learning applications it is crucial to represent graph nodes as node embeddings -structures expressing node properties as an input to downstream machine learning algorithms. A simple node adjacency matrix is usually not feasible due to its large size (quadratic with respect to the number of nodes) and lack of easily accessible representation of node properties. A number of elaborate node embedding methods have been proposed, with configurable embedding dimensionality and node similarity defined in terms of a selected distance metric. However, most of these approaches do not scale to the real-world large graphs consisting of millions of nodes and billions of edges. Methods such as Deepwalk <ref type="bibr" target="#b31">[32]</ref> or Node2Vec <ref type="bibr" target="#b14">[15]</ref>, require hours or even days of training even on medium-sized graphs to arrive at a representation of reasonable quality. Scalable models have also been proposed, such as PyTorch BigGraph (PBG) <ref type="bibr" target="#b20">[21]</ref> or GOSH <ref type="bibr" target="#b2">[3]</ref>, however such optimization methods demand significant model complexity to parallelize the computations or/and coarsen the graph into a smaller structure.</p><p>In this paper we present Cleora: a very simple yet competitive graph embedding scheme. Cleora relies on multiple iterations of normalized, weighted averaging of each node's neighbor embeddings. Each embedding dimension is optimized independently from other dimensions (with the exception of normalization, which takes into account all dimensions). As such, Cleora can be thought of as an ensemble of 1-D optimizers rather than a single optimizer of N-D data. The operation of averaging can make rapid, substantial changes to embeddings (as compared to step-wise optimization based on an objective), making the algorithm reach optimal embeddings in just four to five iterations on average. Cleora is purely unsupervised. No explicit learning objective is formulated and no sampling of positive or negative examples is performed.</p><p>Cleora has only two configurable parameters: the number of iterations and dimensionality of resulting embeddings, which makes it easy to search for an optimal configuration. Contrastive learning methods will usually have many more configurable parameters, for example PBG has at least 18 parameters which directly define the training process. Moreover, some methods (PBG included) define a number of loss functions which optimize embeddings for various purposes separately (e.g. node ranking or classification). With Cleora the obtained embeddings are versatile as no explicit objective is defined.</p><p>Cleora scales well to massive graph sizes and can embed various types of edges: undirected, directed and/or weighted. We evaluate Cleora on various real-world datasets of medium to large size, in order to show that the quality of produced embeddings is competitive to recent state-of-the-art methods, without the overhead of complex architecture. We show that Cleora is faster than other recent CPU-based methods, thanks to the simplicity and the ease of parallelization.</p><p>Moreover, our algorithm possesses two interesting properties which are a consequence of the weighted averaging approach and may turn up useful in practical scenarios:</p><p>• Additivity. The embedding procedure can be conducted on chunked graph and the embedded chunks can be merged with a single Cleora step, without any extra solution. As the method is based on simple weighted averaging, an average of all chunks' embeddings produces the final embeddings. This simply repeats the operation which would have happened on the full graph if embedded directly. Cleora can embed massive graphs which do not fit into RAM/disk space at once. • Inductivity. Embedding of nodes added after the computation of full graph embedding is a natural operation, requiring a single iteration of the algorithm on just the newly added nodes. None of the competitors we evaluate have this ability. Cleora is an algorithm which processes data from large retailers in our production environment, with graphs comprised of millions of nodes and billions of edges. Embedding times for our biggest e-commerce datasets are below 2 hours on a single Azure virtual machine of type Standard E32s v3 32 vCPUs/16 cores and 256 GB RAM. Cleora has already been in use for 1 year.</p><p>We release Cleora as open-source software. We offer an easy to run, highly optimized implementation written in Rust. We release the code under the permissive MIT license, which allows commercial use 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The first attempts at graph embeddings were purely unsupervised. Methods such as classic PCA <ref type="bibr" target="#b17">[18]</ref>, Laplacian eigenmap <ref type="bibr" target="#b6">[7]</ref>, MSD <ref type="bibr" target="#b11">[12]</ref>, and IsoMap <ref type="bibr" target="#b39">[40]</ref> typically exploit spectral properties of various matrix representations of graphs, such as the Laplacian and the adjacency matrix. In essence, these methods preform dimensionality reduction while preserving distance relations between nodes (viewed as path distances in graphs). Unfortunately, their complexity is at least quadratic to the number of nodes, which makes these methods far too slow for today's volumes of graph data.</p><p>A subsequent, essential line of work was based on random walks between graph nodes. The classic DeepWalk model <ref type="bibr" target="#b31">[32]</ref> uses random walks to create a set of node sequences fed to a skip-gram model, taking an inspiration from learning representations of tokens in Word2Vec <ref type="bibr" target="#b27">[28]</ref> from the area of natural language processing. Node2vec <ref type="bibr" target="#b14">[15]</ref> is a variation of DeepWalk where the level of random walk exploration (depth-first search) versus exploitation (breadth-first search) is controlled with parameters. Other more recent approaches make the random walk strategy more flexible at the cost of increased complexity <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b32">[33]</ref>. These models perform well in practice, but cannot scale to graphs with millions of nodes.</p><p>LINE <ref type="bibr" target="#b38">[39]</ref> is yet another approach, which aims to preserve the first-order or second-order proximity separately by the use of KL-divergence minimization. After optimizing the loss functions, it concatenates both representations. LINE is optimized to handle large graphs, but its GPU-based implementation -Graphvite <ref type="bibr" target="#b48">[49]</ref> -cannot embed a graph when the total size of the embedding is larger than the total available 1 https://github.com/Synerise/cleora GPU memory. With GPU memory size usually much smaller than available RAM, this is a serious limitation.</p><p>Another established line of work focuses on embedding graphs with the use of graph convolutional neural networks <ref type="bibr" target="#b18">[19]</ref>. Methods such as GraphSAGE <ref type="bibr" target="#b15">[16]</ref> are designed for cases where the graph nodes are accompanied by additional feature information and incorporation of no-feature nodes might be complicated or demand additional measures. In contrast, our approach focuses on pure graph structure.</p><p>Works closest to our aims in terms of scalability and speed are able to embed massive graphs in reasonable time without the requirement of node features or any kind of side information. These fast embedding methods often boil down to older methods implemented in parallelizable and highly efficient architectures. Pytorch BigGraph (PBG) <ref type="bibr" target="#b20">[21]</ref> uses ideas such as adjacency matrix partitioning and reusing examples within one batch to train models analogous to successful but less scalable RESCAL <ref type="bibr" target="#b30">[31]</ref>, DistMult <ref type="bibr" target="#b43">[44]</ref>, TransE <ref type="bibr" target="#b7">[8]</ref>,and ComplEx <ref type="bibr" target="#b40">[41]</ref>, thus making these models applicable to large graphs. GOSH <ref type="bibr" target="#b2">[3]</ref> is a GPU-based approach which trains a parallelized version of Verse <ref type="bibr" target="#b41">[42]</ref> on a number of smaller, coarsened graphs. Embeddings are computed with Noise Contrastive Estimation between positive and negative samples, first on smaller, coarsened graphs and projected to bigger ones. In contrast to these methods, Cleora is not a reimplementation of an older approach but a conceptually new algorithm. Moreover, the listed methods are complex, demand step-wise training via gradient descent, and do not naturally generalize to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLEORA EMBEDDINGS A. Preliminaries</head><p>A graph G is a pair (V, E) where V denotes a set of nodes (also called vertices) and E ⊆ (V × V ) as the set of edges connecting the nodes. We consider undirected graphs, where an edge is an unordered pair of nodes. An embedding of a graph G = (V, E) is a |V | × d matrix T , where d is the dimension of the embedding. The i-th row of matrix T (denoted as T i, * ) corresponds to a node i ∈ V and each value j ∈ {1, ..., d} in the vector T i,j captures a different feature of node i.</p><p>Hypergraphs are a generalization of graphs where the relationships between nodes can be not only pairwise, but also higher-order. For example, the relation of Customer c buying a Product p in Store s is a hyperedge composed of nodes c, p, s. Hyperedge width k is defined as the number of nodes contained in the hyperedge. For example, the hyperedge (c, p, s) has width 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Algorithm</head><p>Hypergraph Expansion. In order to produce hypergraph embedding, Cleora needs to break down all existing hyperedges into edges as the algorithm relies on the pairwise notion of node transition. Hypergraph expansion to graph is done using two alternative strategies:</p><p>• Clique Expansion. Each hyperedge is transformed into a clique -a subgraph where each pair of nodes is connected with an edge. Space/time complexity of the whole embedding procedure in this approach is given by</p><formula xml:id="formula_0">O(|V | × d + |E| × k 2 )</formula><p>where |E| is the number of hyperedges and k is the maximal width of hyperedge from the hypergraph. With the usage of cliques the number of created edges can be significant but guarantees better fidelity to the original hyperedge relationship. We apply this scheme to smaller graphs. • Star Expansion. An extra node is introduced which links to the original nodes contained by a hyperedge. Space/time complexity of the whole embedding procedure in this approach is given by O((|V |+|E|)×d+|E|k).</p><p>Here we must include the time and space needed to embed an extra entity for each hyperedge, but we save on the number of created edges, which would be only k for each hyperedge. This approach is recommended for large graphs or graphs with large hyperedges.</p><p>Embedding. With all hyperedges broken down into pairwise edges, we proceed to embed the graph. Given an interaction network (e.g. between users and items) represented as a graph G = (V, E), we define the random walk transition</p><formula xml:id="formula_1">matrix M ∈ R V ×V where M ab = e ab deg ( va) for ab ∈ E,</formula><p>where e ab is the number of edges running from node a to b, and deg(v a ) is the degree of node a. For ab pairs which do not exist in the graph, we set e ab = 0. If edges have attached weights, e ab is the sum of the weights of all participating edges.</p><p>We initialize the embedding matrix T 0 ∈ R |V |×d ∼ U (−1, 1), where d is the embedding dimensionality. Then for I iterations, we multiply matrix M and T i , and normalize rows to keep constant L 2 norm. T I is our final embedding matrix. If the graph is too big to be embedded at once due to memory constraints, it can be split and merged in a natural way by weighted averaging of the composing parts' embeddings. The full procedure (including graph splitting and averaging) is show in detail in Algorithm 1.</p><p>Initial vectors in matrix T 0 need to 1) be different from each other so that the algorithm does not collapse to the same representation 2) have similar pairwise distances in order to avoid 'false friends' which are accidentally close in the embedding space. Matrix initialization from the uniform distribution creates vectors which fulfill these conditions in high dimensional space.</p><p>Cleora works analogously to an ensemble of d models, as each dimension is optimized separately from others (see the column-wise matrix multiplication operation in <ref type="figure" target="#fig_1">Figure 2</ref>. The only operation which takes into account all dimensions (and as such, allows some information sharing) is the L 2 normalization. The normalization step ensures numeric stability, preventing the norm of embedding vectors from collapsing towards zero during repeated multiplication by the transition matrix, of which the determinant is ≤ 1.</p><p>The method can be interpreted as iterated L 2 -normalized weighted averaging of neighboring nodes' representations. After just one iteration, nodes with similar 1-hop neighborhoods in the network will have similar representations. Further iterations extend the same principle to q-hop neighborhoods.</p><p>Data: Graph G = (V, E) with set of nodes V and set of edges E, iteration number I, chunk number Q, embedding dimensionality d Result: Embedding matrix T ∈ R |V |×d Divide graph G into Q chunks. Let G q be the q-th chunk with edge set E q ; for q from 1 to Q do For graph chunk G q compute random walk transition matrix M q = e ab deg(va) for ab ∈ E q , where e ab is the number of edges running from node a to b ; Initialize chunk embedding matrix T q 0 ∼ U (−1, 1) ; for i from 1 to I do</p><formula xml:id="formula_2">T q i = M q · T q i−1 ; L 2 normalize rows of T q i ; end T q = T q I ; end for v from 1 to |V | do T v, * = Q q=1 w qv × T q v, * where w qv is the node weight w qv = |n∈Vq:n=nv| Q k=1 |n∈V k :n=nv| end</formula><p>Algorithm 1: Cleora algorithm.</p><p>Thanks to the weighted averaging approach two useful operations become natural:</p><p>• Creation of new node embedding by weighted averaging of all neighbors' representations. With I being the maximum iteration number, the representations should ideally come from (I − 1)th iteration to mirror the last matrix multiplication and average. Using representations from Ith iteration is not an error either, but caution is advised as the embedding quality can deteriorate quickly with increasing iteration number (see Section x). • Obtaining node embedding from a large partitioned graph by weighted averaging of the representation from each slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head><p>Cleora is implemented in Rust in a highly parallel architecture, using multithreading and adequate data arrangement for fast CPU access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modes of Operation</head><p>Cleora ingests text files with relational tables of rows representing a heterogeneous hypergraph. The same input will be treated differently depending on parameters, which define the meaning of particular columns, and as such -the actual graph that will be created. Each column of nodes in the input file can be defined with a number of properties represented with column modifiers. Following column modifiers are available:</p><p>• transient -the field is virtual -it is considered during embedding process, no entity is written for the column • complex -the field is composite, containing multiple entity identifiers separated by space in TSV or an array in JSON • reflexive -the field is reflexive, which means that it interacts with itself, additional output file is written for every such field • ignore -the field is ignored, no output file is written for the field <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of the most common combinations of column modifiers, with resulting graph structures and the outputs which will be written.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Embedding Computation</head><p>We exemplify the embedding procedure in <ref type="figure" target="#fig_1">Figure 2</ref>, using a very general example of multiple relations in one graph. Embedding is done in two basic steps: graph construction and training. Such multi-relation configuration is allowed and will result in computation of a separate embedding matrix for each relation pair.</p><p>For maximum efficiency we created a custom implementation of a sparse matrix data structure -the SparseMatrix struct. It follows the sparse matrix coordinate list format (COO). Its purpose is to save space by holding only the coordinates and values of nonzero entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph construction</head><p>Graph construction starts with the creation of a helper matrix P object as a regular 2-D Rust array, which represents the input graph edges according to the selected expansion method (clique, star, or no expansion). An example involving clique expansion is presented in <ref type="figure" target="#fig_1">Figure 2</ref> -a Cartesian product (all combinations) of all columns is created. If there are more than 2 relations in the graph, multiple separate M matrices will be created for each relation pair.</p><p>Each entity identifier from the original input file is hashed with xxhash 2 -a fast and efficient hashing method. We hash the identifiers to store them in a unified, small data format.</p><p>Subsequently, for each relation pair from matrix P we create a separate matrix M as a SparseMatrix object (the matrices M will usually hold mostly zeros). Each matrix M object is produced in a separate thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>In this step training proceeds separately for each matrix M, so we will now refer to a single object M. The matrix M is multiplied by a freshly initialized 2-D array representing matrix T 0 -this array will represent node embeddings. Multiplication is done against each column of matrix T 0 object in a separate thread. The obtained columns of the new matrix T 1 object are subsequently merged into the full matrix. T 1 matrix is L2-normalized, again in a multithreaded fashion across matrix columns. The appropriate matrix representation lets us accelerate memory access taking advantage of CPU caching.</p><p>Finally, depending on the target iteration number, the T 1 matrix object is either returned as program output and printed to file, or fed for next iterations of multiplication against the matrix M object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Memory Consumption</head><p>Memory consumption is linear to the number of nodes and edges. To be precise, during training we need to allocate space for the following:</p><p>• |V | objects of 40 bytes to store the matrix P; • 2×|E| objects of 24 bytes (in undirected graphs we need to count an edge in both directions) to store the matrix M; • 2 × d × |V | objects of 4 bytes to store the embedding matrix T. As such, the total memory complexity is given by O(|V |(1+ 2d) + 2|E|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To evaluate the quality of Cleora embeddings, we study their performance on two popular tasks: link prediction <ref type="bibr" target="#b23">[24]</ref> and node classification. As competitors we consider recent stateof-the-art methods designed for big data: PBG and GOSH, as well as classic models: LINE and Deepwalk. The details on the competitors' algorithms are described in Section II (Related Work). For each competitor, we use its original author's implementation. We train each model with the best parameter configurations reported in their original papers or     <ref type="bibr" target="#b20">[21]</ref> 0.0817* 0.2133* 0.0321* 0.0640* 0.8717* 0.9106* 0.5669* 0.6730* -** -** GOSH <ref type="bibr" target="#b2">[3]</ref> 0.0924* 0.2319* 0.0280* 0.0590* 0.8756* 0.8977* 0.2242* 0.4012* -** -** Non-scalable methods Deepwalk <ref type="bibr" target="#b31">[32]</ref> 0.0803* 0.1451* 0.1045* 0.1805* 0.9626* 0.9715* timeout timeout timeout timeout LINE <ref type="bibr" target="#b37">[38]</ref> 0.0749* 0.1923* 0.1064* 0.1813* 0.9628* 0.9833* 0.5663* 0.6670* -** -**  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We use 5 public datasets, summarized in <ref type="table" target="#tab_1">Table I</ref>. We focus on picking the most referential datasets, which are already popular benchmarks. Moreover, we take care to include datasets of various sizes, spanning from medium-sized (Facebook) to massive (Twitter). For each experiment we randomly sample out 80% of the edges found in each dataset as trainset (both for the embeddings and the proceeding proxy tasks) and the rest serves as validation/test dataset.</p><p>• Facebook <ref type="bibr" target="#b35">[36]</ref>. In this graph, the nodes represent official Facebook pages while the links are mutual likes between sites. All nodes belong to one of 4 classes defined by Facebook: politicians, governmental organizations, television shows and companies. • Youtube <ref type="bibr" target="#b28">[29]</ref>. This graph represents mutual friendships in the Youtube social networks. Nodes are also characterized by class assignments representing membership in groups. • RoadNet <ref type="bibr" target="#b22">[23]</ref>. A road network of California. Intersections and endpoints are represented by nodes and the roads connecting these intersections or road endpoints are represented by edges. • LiveJournal <ref type="bibr" target="#b5">[6]</ref>. LiveJournal is a free online community with almost 10 million members. The graph represents friendships among members. • Twitter <ref type="bibr" target="#b19">[20]</ref>. A subset of the Twitter network with directed follower-following social network. This graph is marked by low reciprocity of relations and a strong presence of influential nodes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Embedding Computation</head><p>We use the following configurations for each of the models:</p><p>• Cleora: all embeddings are trained with iteration number i = 4 and embedding dimensionality d = 1024. We treat each row of the adjacency matrix as a hyperedge. Hyperedges are expanded with the clique expansion approach in Facebook and RoadNet datasets, and with star expansion in YouTube and LiveJournal datasets (due to the appearance of very large clusters). • PBG <ref type="bibr" target="#b20">[21]</ref>: We use the code from original author repository <ref type="bibr" target="#b2">3</ref> . We use the following parameter configurations which we find to give the best results: -LiveJournal dataset: we reuse the original configuration from the author repository. -Facebook, Roadnet datasets: dimension=1024, global_emb=False, num_epochs=30, lr=0.001, regularization_coef=1e-3, comparator="dot" -Youtube dataset: same as above, with num_epochs=40 For link prediction we use loss_fn="ranking" and for classification loss_fn="softmax" as suggested in the documentation. • LINE <ref type="bibr" target="#b37">[38]</ref>: We use the code from Graphvite 4 , which includes a parallelized GPU-based implementation of the original algorithm. We use the configuration for Youtube dataset from author repository for embedding this dataset. We reuse this configuration for other datasets with epoch number e = 4000. We concatenate the base embeddings with context embeddings as recommended by the authors. • GOSH <ref type="bibr" target="#b2">[3]</ref>: We use the original author code 5 . The model is trained with learning rate lr = 0.045 and epochs e = 1000, defined in the original paper as the optimal configuration for our graph sizes (without any graph coarsening for maximum accuracy). • Deepwalk <ref type="bibr" target="#b31">[32]</ref>: We use the original author code <ref type="bibr" target="#b5">6</ref> .</p><p>We reuse the optimal configuration from author paper: -workers 30, -representation-size 128, -number-walks 80, -walk-length 40 We compare the results of scalable methods (Cleora, PBG, GOSH), contrasting them with unscalable methods (Deepwalk, LINE). We deem LINE unscalable as its implementation cannot embed a graph when the total size of the embedding is larger than the total available GPU memory.</p><p>Embedding training times are displayed in <ref type="table" target="#tab_1">Table II</ref>. We include only CPU-based methods for fair comparison. PBG is the fastest high-performing CPU-based method we are aware of, and Deepwalk serves as a baseline example of the nonscalable methods. Cleora is shown to be significantly faster than PBG in each case. Training is up to 5 times faster than PBG and over 200 times faster than Deepwalk.</p><p>We face technical difficulties with training the embeddings on the Twitter graph. Both GOSH and PBG crash during or just after loading the data into memory due to exhausting the available hardware resources. This demonstrates that Cleora can be trained on hardware which fails for other fast methods (note, though, that our hardware configuration is not small but rather standard). Also, technical errors can stem directly from complex model architecture (e.g. heavy and complicated parallelism in PBG), a problem which does not exist in Cleora due to the simplicity of the algorithm. As a consequence, the only model we evaluate on Twitter is Cleora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Check</head><p>Link Prediction. Link prediction is one of the most common proxy tasks for embedding quality evaluation <ref type="bibr" target="#b24">[25]</ref>. It consists in determining whether two nodes should be connected with an edge.</p><p>We collect the embeddings for all positive pairs (real edges) and for each such pair we generate a negative pair with random sampling. We compute the Hadamard product for each pair, which is a standard procedure observed to work well for performance comparison <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Then, we feed the products to a Logistic Regression classifier which predicts the existence of edges. As the graphs we use are mostly large-scale, we resort to step-wise training with the SGDClassifier module from scikit-learn with a Logistic Regression solver.</p><p>For evaluation, we reuse the setting from <ref type="bibr" target="#b20">[21]</ref>: for each positive node pair, we pick 10,000 most popular graph nodes, which are then paired with the start node of each positive pair to create negative examples. We rank the pairs, noting the position of the real pair among the 10,001 total examples. In this setting we are able to compute popular ranking-based metrics:</p><p>• MRR (mean reciprocal rank). This is the average of the reciprocals of the ranks of all positives. The standard formulation of MRR requires that only the single highest rank be summed for each example i (denoted as rank i ) from the whole query set Q:</p><formula xml:id="formula_3">M RR = 1 |Q| |Q| i=1 1 rank i .</formula><p>As we consider all edge pairs separately, this is true by default in our setting. Higher score is better, the best is 1.0. • HitRate@10. This gives the percentage of valid end nodes which rank in top 10 predictions among their negatives. Higher score is better, the best score is 1.0. Due to a large size of graphs, we evaluate on 100,000 sampled examples from the testset. The variances of scores among samples drawn with various seeds are very low, on the level of 1e−8 to 1e−12, so the sampling error will have no visible influence on model ranking.</p><p>Results of our evaluation versus the competitors are presented in <ref type="table" target="#tab_1">Table III</ref>. The table shows that in spite of its extreme simplicity, the quality of Cleora embeddings is generally on par with the scalable competitors. Cleora performs worse than the competitors on the smallest graph -Facebook, but gets better with increasing graph size. In some cases Cleora reaches results that are better than other fast algorithms aligned for big data -PBG and GOSH. Cleora performs especially well in the MRR metric, which means that it positions the correct node pair high enough in the result list as to avoid punishment with the harmonically declining scores.</p><p>Apart from score rankings, another important issue is embedding versatility. This can be problematic in particular for PBG, which defines a number of loss functions linked to the downstream task. Indeed, we note that in order to achieve the top results we observe, it is necessary to train 2 versions of the embeddings: a version for link prediction task with loss_fn="ranking" and a version for classification task with loss_fn="softmax". Whenever training with the classification objective, we observe significant drops of performance in link prediction (e.g. up to 5 times lower MRR and HR for Youtube dataset). The other models, including Cleora, do not require separate task-specific parameters.</p><p>We are unable to compare our results on the Twitter graph to competitors, thus presenting the results only for Cleora in <ref type="table" target="#tab_1">Table III</ref>. Due to large size of the graph and the computed embedding file (over 500 GB, far exceeding our available memory size), we compute the results on a large connected component of the graph, containing 4,182,829 nodes and 8,197,949 edges.</p><p>Classification. We evaluate node classification on two datasets which have node labels: Facebook and YouTube. In Facebook, we use the 4 available classes. In YouTube, the number of classes (representing interest group numbers) is on the scale of thousands. We select 47 most numerous classes for our classification objective (following <ref type="bibr" target="#b37">[38]</ref>). As performance measures we use micro-F1 and macro-F1 scores from scikit-learn library.</p><p>The results are displayed in <ref type="table" target="#tab_1">Table IV</ref>. The findings confirm the results from link prediction. Cleora is again ranking high, this time usually beating the scalable competitors, and getting close to non-scalable ones. <ref type="figure">Figure 4</ref> shows Cleora, GOSH and PBG vectors learned on Facebook and projected to 2-D space using t-SNE <ref type="bibr" target="#b49">[50]</ref>. The vectors are colored by their class assignments. It can be observed that Cleora learns an essentially similar data abstraction as the contrastive methods. Additionally, a number of outliers appear in Cleora's visualization, which are nodes from small, strongly interconnected clusters. Such nodes' embeddings get extremely similar to each other and at the same time distant from the rest of the embeddings. As such, Cleora clearly separates strongly connected components of the graph.</p><p>Node Reconstruction (Inductivity). We also evaluate the quality of embeddings of new nodes which are added to the graph after the computation of the full graph embedding table.</p><p>To this end, we split the Facebook and Youtube datasets into a 'seen node' set comprised of 30% of each dataset, and 'new node' set comprised of the remaining 70% of each dataset. This setting is challenging as the vast majority of nodes will need to be reconstructed. At the same time, it gives us enough data to study embedding quality of two kinds of reconstruction: 1-hop reconstruction, where a new node embedding is created based on 'seen node' embeddings, which have been learned directly from interaction data, and 2-hop reconstruction, where a new node embedding is created based on reconstructed node embeddings. 2-hop reconstruction is a much harder task, as errors from previous reconstruction step will be accumulated. <ref type="table" target="#tab_6">Table V</ref> shows the results of the reconstruction quality check. We use the previous measures of micro-F1 and macro-F1 in the Node Classification task, also taking into account the MR (Mean Rank) measure, which is a simple average of obtained ranks in the Link Prediction task. The measure is a simplified abstraction of the MRR and HR metrics. 1-hop and 2-hop reconstruction results are computed on the reconstructed embeddings only. We observe that the task of classification on Facebook dataset notes only a 5% micro-F1 drop in 1-hop node generation, and a 14.5% micro-F1 drop in 2-hop node generation. Youtube faced a stronger drop of 18% and 40%, respectively. A reversed tendency is observed for link prediction: the MR drops are severe for Facebook (MR falling by a half when reconstructed), while being very mild for Youtube (absolute difference in MR being only 266). This suggests that quality of reconstruction is heavily dependent on dataset. Even with 2-hop reconstruction, it is possible to obtain embeddings of high relative quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimal iteration number</head><p>The iteration number defines the breadth of neighborhood on which a single node is averaged: iteration number i means that nodes with similar i-hop neighborhoods will have similar representations. We show the influence of i on classification performance in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>The iteration number is related to the concept of average path length from the area of graph topology. The average path length is defined as the average number of steps along the shortest paths for all possible pairs of network nodes. If the iteration number reaches the average path length, an average node will likely have access to all other nodes. Thus, iteration number slightly exceeding the average path length can be deemed optimal. For example, the average path length for the Facebook graph equals 5.18 and the best i is found to be 5-7 according to <ref type="figure" target="#fig_2">Figure 3</ref>. In practice however the computation of average path length is significantly slower than the computation of Cleora embeddings. An optimal solution to determining the iteration number is to verify iteration number empirically on a downstream task.</p><p>Too large iteration number will make all embeddings gradually more similar to each other, eventually collapsing to an exact same representation. This behavior might be rather slow or abrupt after passing the optimal point depending on the dataset, as evidenced in <ref type="figure" target="#fig_2">Figure 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Homophily and Structural Equivalence</head><p>The idea of node similarity can be considered under various formulations. <ref type="bibr" target="#b14">[15]</ref> propose to focus on two such aspects: homophily <ref type="bibr" target="#b26">[27]</ref> and structural equivalence. Under the homophily criterion, node embeddings should be similar when the nodes are highly interconnected. Structural equivalence, on the other hand, stresses that similarity of node embeddings should be based on their role within a graph (e.g. two nodes which are centers of large clusters should be similar, or two leaf nodes should be similar). Under this definition, Cleora leans strongly towards the homophily criterion. An extreme example of this behavior would be the embedding of a leaf node. Leaf nodes have only one neighbor, so their embedding is always equal to the embedding of their single neighbor from the previous iteration. As such, two leaf nodes will have very different embeddings if their geodesic distance is higher than the iteration number. With rising iteration number, more structural knowledge flows into the embeddings.</p><p>We exemplify this property contrasting Cleora with Word2Vec <ref type="bibr" target="#b27">[28]</ref> in <ref type="table" target="#tab_1">Table VI</ref>. Both models are trained on the enwik8 dataset 7 . Cleora is trained with clique expansion approach on token sequences with window sizes of <ref type="bibr">5,7,9.</ref> Representations computed with each window size were concatenated.</p><p>We query each set of embeddings with selected nouns to find their nearest neighbors. Word2Vec embeddings exhibit strong structural equivalence, usually keeping to the part of speech (PoS) of the query. PoS is an important structural characteristic of a word, defining its role within a sentence. On the other hand, Cleora i = 4 usually returns similar nouns but sometimes also related words of other PoS, e.g elephant is related to hunting. Cleora i = 1 is an extreme case of homophily in close neighborhoods, returning words which appear in one phrase, e.g. madama butterfly, elephant dumbo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Complement and Substitute Prediction</head><p>Node similarity problem can be cast as complement versus substitute identification <ref type="bibr" target="#b25">[26]</ref>, which is especially interesting in e-commerce applications. Substitutes are defined as products 7 https://deepai.org/dataset/enwik8 that can be purchased instead of each other, while complements are products that can be purchased in addition to each other (they often appear together in the same basket). As such, complements are expected to be similar if their 1hop neighborhoods are similar, boiling down to neighborhood prediction. Substitutes should be close in terms of a broader notion of semantic similarity of nodes.</p><p>Thanks to the influence of the iteration number, Cleora can represent either the complementary or the substitute relation. To exemplify this property, we preprocess the "Complete Journey" Dunnhumby dataset <ref type="bibr" target="#b7">8</ref> . The dataset includes transactions of 2,500 households within all categories in the store, gathered over 2 years. We embed the product baskets with Cleora, expanding each basket with the clique expansion approach. We use two configurations: iteration number i = 1 and i = 4, and compute the nearest neighbors for each product using cosine similarity.</p><p>Extracts from our results are displayed in <ref type="table" target="#tab_1">Table VII</ref>. With i = 1 the closest embeddings mirror the complement relation. For example, instant Ramen soups seem to be often bought together with automotive parts, dips and sweets which suggests that they might be bought by drivers stopping by. Regarding bread, there seems to be a marked difference in the perception of regular white bread and French bread. White bread is associated with everyday meals, while French bread is bought together with greeting cards, candy and gifts. As such, it is much more strongly connected to special occasions such as celebration of Valentine's Day and parties. In the case of i = 4, products are more closely aligned with similar products (possible substitutes): ramen soups with other ramen soups of various kinds (only the product IDs differ here). Breads are close to cereal, pies, sandwiches and other bakery products but also meats.</p><p>Note that nearest neighbor search between i = 1 embeddings does not achieve identical goals as algorithms of association rule mining <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Due to the averaging operation over neighbors, embeddings of items appearing in small baskets will be much more similar to each other than if they appeared in bigger baskets. Thus, nearest neigbor search with Cleora strongly promotes smaller, more selective clusters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SUMMARY</head><p>We have presented Cleora -a simple, purely unsupervised embedding algorithm which learns representations analogous to contrastive methods. Cleora is much faster than other CPU-based methods. Moreover, it has useful extra properties, such as node embedding inductivity and the ability to compute partial embeddings on chunked graph, which can be subsequently merged. The iteration number allows to obtain various functionalities, for example complement or substitute prediction. We open-source Cleora to the community in order to aid reproducibility and allow a wide use of our method, including commercial use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Examples of various embedding modes and the resulting graph structures and output embeddings. Fractional numbers given next to nodes show values form transition matrix M for each node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Cleora Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The influence of iteration number on embedding quality.(a) Cleora (b) GOSH (c) PBG Fig. 4: t-SNE projections of embeddings learned on Facebook dataset, colored by class assignment. The displayed methods are: a) Cleora b) GOSH c) PBG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>× 10 −4 4.7 × 10 −6 1.4 × 10 −6 3.4 × 10 −6 9.1 × 10 −7</figDesc><table><row><cell>Name</cell><cell cols="2">Facebook</cell><cell>YouTube</cell><cell>RoadNet</cell><cell>LiveJournal</cell><cell>Twitter</cell></row><row><cell>#Nodes</cell><cell></cell><cell>22,470</cell><cell>1,134,890</cell><cell>1,965,206</cell><cell>4,847,571</cell><cell>41,652,230</cell></row><row><cell>#Edges</cell><cell cols="2">171,002</cell><cell>2,987,624</cell><cell>2,766,607</cell><cell>68,993,773</cell><cell>1,468,365,182</cell></row><row><cell>Average Degree</cell><cell></cell><cell>12</cell><cell>5</cell><cell>3</cell><cell>16</cell><cell>36</cell></row><row><cell cols="2">Density 6 #Classes</cell><cell>4</cell><cell>47</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Directed</cell><cell></cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Dataset characteristics.</figDesc><table><row><cell cols="2">Algorithm Facebook</cell><cell>YouTube</cell><cell>RoadNet</cell><cell>LiveJournal</cell><cell>Twitter</cell></row><row><cell></cell><cell></cell><cell cols="2">Total embedding time</cell><cell></cell></row><row><cell>Cleora</cell><cell cols="3">00:00:43 h 00:12:07 h 00:24:15 h</cell><cell>01:35:40 h</cell><cell>25:34:18 h</cell></row><row><cell>PBG</cell><cell cols="3">00:04.33 h 00:54:35 h 00:37:41 h</cell><cell>10:38:03 h</cell><cell>-*</cell></row><row><cell cols="4">Deepwalk 00:36:51 h 28:33:52 h 53:29:13 h</cell><cell>timeout</cell><cell>timeout</cell></row><row><cell></cell><cell></cell><cell cols="2">Training time</cell><cell></cell></row><row><cell>Cleora</cell><cell cols="3">00:00:25 h 00:11:46 h 00:04:14 h</cell><cell>01:31:42 h</cell><cell>17:14:01 h</cell></row><row><cell>PBG</cell><cell cols="3">00:02:03 h 00:24:15 h 00:31:11 h</cell><cell>07:10:00 h</cell><cell>-*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Calculation times of CPU-based methods: Cleora, PBG and Deepwalk. Total embedding times encompass the whole training procedure, including data loading and preprocessing. Training times encompass the training procedure itself, excluding data loading and preprocessing. * -training crashed due to excessive resource consumption.</figDesc><table><row><cell>Algorithm</cell><cell cols="9">Facebook MRR HR@10 MRR YouTube HR@10 MRR RoadNet HR@10 MRR LiveJournal HR@10 MRR</cell><cell cols="2">Twitter HR@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Scalable methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cleora</cell><cell>0.0724</cell><cell>0.1761</cell><cell>0.0471</cell><cell>0.0618</cell><cell>0.9243</cell><cell>0.9429</cell><cell>0.6079</cell><cell>0.6665</cell><cell cols="2">0.0355</cell><cell>0.076</cell></row><row><cell>PBG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>Algorithm</cell><cell cols="4">Facebook Micro-F1 Macro-F1 Micro-F1 Macro-F1 YouTube</cell></row><row><cell></cell><cell></cell><cell cols="2">Scalable methods</cell><cell></cell></row><row><cell>Cleora</cell><cell>0.9165</cell><cell>0.9166</cell><cell>0.3859</cell><cell>0.3077</cell></row><row><cell>PBG</cell><cell>0.9258</cell><cell>0.9262</cell><cell>0.3567*</cell><cell>0.2459*</cell></row><row><cell>GOSH</cell><cell>0.8312*</cell><cell>0.8305*</cell><cell>0.3166*</cell><cell>0.2245*</cell></row><row><cell></cell><cell cols="3">Non-scalable methods</cell><cell></cell></row><row><cell cols="2">Deepwalk 0.9349*</cell><cell>0.9354*</cell><cell>0.3166*</cell><cell>0.2245*</cell></row><row><cell>LINE</cell><cell>0.9442*</cell><cell>0.9446*</cell><cell>0.4008*</cell><cell>0.3338*</cell></row></table><note>: Link prediction performance results. * -results with statistically significant differences to Cleora according to the Wilcoxon two-sided paired test (p-value lower than 0.05). ** -training crashed due to excessive resource consumption/unscalable architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Classification performance results. * -results with statistically significant differences to Cleora according to the Wilcoxon two-sided paired test (p-value lower than 0.05).</figDesc><table><row><cell>repositories per each dataset. For datasets which do not have</cell></row><row><cell>a per-model best configuration, we perform an experimental</cell></row><row><cell>grid search.</cell></row><row><cell>We conduct all experiments on two machines: 1) Standard</cell></row><row><cell>E32s v3 Azure (32 vCPUs/16 cores) and 256 GB RAM for</cell></row><row><cell>CPU-based methods, and 2) 128 GB RAM, 14 core (28 HT</cell></row><row><cell>threads) Intel Core i9-9940X 3.30GHz CPUs and GeForce</cell></row><row><cell>RTX 2080 Ti 11GB GPU for GPU-based methods.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Quality check of reconstructed nodes in a challenging setting where only 30% of all nodes are learned directly and 70% are reconstructed. 1-hop reconstruction nodes are computed from original nodes. 2-hop reconstruction nodes are computed from 1-hop reconstruction nodes.</figDesc><table /><note>(a) Facebook Dataset -Classification Task. (b) Youtube Dataset -Classification Task (c) Facebook Dataset -Link Prediction Task. (d) Youtube Dataset -Link Prediction Task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Results of the homophily vs. structural equivalence experiment. We contrast Cleora with Word2Vec<ref type="bibr" target="#b27">[28]</ref> showing varied levels of homophily and structural equivalence depending on iteration number.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>CT SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ 2. PROCESSED DIPS 15.5 OZ SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ 3. SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ 4. J-HOOKS JHOOK -HOUSEWARE SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ 5. PACKAGED CANDY BAGS-CHOCOLATE 11 OZ SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ BAKED BREAD/BUNS/ROLLS MAINSTREAM WHITE BREAD 20 OZ</figDesc><table><row><cell></cell><cell cols="2">SOUP RAMEN NOODLES/RAMEN CUPS 3 OZ</cell></row><row><cell></cell><cell>1 iteration (Complement)</cell><cell>4 iterations (Substitute)</cell></row><row><cell>1.</cell><cell>AUTOMOTIVE PRODUCTS 4 1 iteration (Complement)</cell><cell>4 iterations (Substitute)</cell></row><row><cell>1.</cell><cell cols="2">BAKED BREAD/BUNS/ROLLS DINNER ROLLS 11 OZ SMOKED MEATS MARINATED</cell></row><row><cell>2.</cell><cell>CHIPS&amp;SNACKS MISC 3.5 OZ</cell><cell>PICKLE/RELISH/PKLD VEG PICKLES</cell></row><row><cell>3.</cell><cell>SPRING/SUMMER SEASONAL SALLY HANSEN</cell><cell>PNT BTR/JELLY/JAMS JELLY</cell></row><row><cell>4.</cell><cell>DRY NOODLES/PASTA SPAGHETTI DRY 16 OZ</cell><cell>COLD CEREAL KIDS CEREAL</cell></row><row><cell>5.</cell><cell>BEERS/ALES BEERALEMALT LIQUORS 40 OZ</cell><cell>BREAKFAST SAUSAGE/SANDWICHES PATTIES</cell></row><row><cell></cell><cell cols="2">BREAD BREAD:ITALIAN/FRENCH</cell></row><row><cell></cell><cell>1 iteration (Complement)</cell><cell>4 iterations (Substitute)</cell></row><row><cell>1.</cell><cell>LUNCHMEAT PEPPERONI/SALAMI 3 OZ</cell><cell>REFRIGERATED DOUGH PRODUCTS ROLLS</cell></row><row><cell>2.</cell><cell>CANDY BAGS-NON CHOCOLATE 4.25 OZ</cell><cell>SEAFOOD -FROZEN SEAFOOD-FRZ-RW-ALL</cell></row><row><cell>3.</cell><cell>GREETING CARDS/WRAP/PARTY SPLY PARTY</cell><cell>BAKED SWEET GOODS SNACK CAKE -PACK 5.7 OZ</cell></row><row><cell>4.</cell><cell>VALENTINE VALENTINE GIFTWARE/DECOR 5 CT</cell><cell>PIES PIES: PUMPKIN/CUSTARD</cell></row><row><cell>5.</cell><cell>CANDY -CHECKLANE CANDY BARS (SINGLES)</cell><cell>LUNCHMEAT HAM 9 OZ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Examples of complement vs. substitute prediction on shopping baskets from Dunnhumby dataset. We show that low iteration numbers produce embeddings which reflect the complement relation. On the other hand, high iteration numbers produce embeddings which reflect the substitute relation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cyan4973.github.io/xxHash/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/PyTorch-BigGraph/ 4 https://github.com/DeepGraphLearning/graphvite 5 https://github.com/SabanciParallelComputing/GOSH 6 https://github.com/phanein/deepwalk</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://www.dunnhumby.com/source-files/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th VLDB Conference</title>
		<meeting>of the 20th VLDB Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining associations between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gosh: Embedding big graphs on small hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amro</forename><surname>Taha Atahan Akyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamer</forename><surname>Alabsi Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th International Conference on Parallel Processing -ICPP, ICPP &apos;20</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting twitter user socioeconomic attributes with network and language information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th on Hypertext and Social Media</title>
		<meeting>the 29th on Hypertext and Social Media</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting trends in academic research from a citation network using network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimitaka</forename><surname>Asatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanao</forename><surname>Ochi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">197260</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: Membership, growth, and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Biased graph walks for rdf graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 7th International Conference on Web Intelligence, Mining and Semantics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards interpretation of node embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayushi</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018, WWW &apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficiently using prefix-trees in mining frequent itemsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gösta</forename><surname>Grahne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FIMI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15820" to="15831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principal Component Analysis and Factor Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="115" to="128" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations, ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What is Twitter, a social network or a news media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haewoon</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;10: Proceedings of the 19th international conference on World wide web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Largescale Graph Embedding System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
		<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring networks of substitutable and complementary products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measurement and Analysis of Online Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/Usenix Internet Measurement Conference (IMC&apos;07)</title>
		<meeting>the 5th ACM/Usenix Internet Measurement Conference (IMC&apos;07)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
	<note>Peter Druschel, and Bobby Bhattacharjee</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering protein drug targets using knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vít</forename><surname>Sameh K Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayah</forename><surname>Nováček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nounu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Don&apos;t walk, skip! online learning of multi-scale network embeddings. ASONAM &apos;17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="258" to="265" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convcn: A cnn-based citation network embedding algorithm towards citation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanathip</forename><surname>Pornprasit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natthawut</forename><surname>Kertkeidkachorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung-Sook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries in 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
	<note>Thanapon Noraset, and Suppawong Tuarob</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rdf2vec: Rdf graph embeddings for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrikus</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Lemmerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Frantzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Klabunde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<title level="m">The effects of randomness on the stability of node embeddings. ArXiv, abs</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15</title>
		<meeting>the 24th International Conference on World Wide Web, WWW &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
		<respStmt>
			<orgName>Republic and Canton of Geneva</orgName>
		</respStmt>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Vin De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page">2319</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Verse: Versatile graph embeddings from similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning effective road network representation with hierarchical graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Wayne</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accurately detecting protein complexes by graph embedding and combining functions with interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="777" to="787" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph embedding on biomedical networks: methods, applications and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Moosavinasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yungui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1241" to="1251" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cosine: Communitypreserving social network embedding from information diffusion cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gman: A graph multi-attention network for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanpan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graphvite: A high-performance cpu-gpu hybrid system for node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
