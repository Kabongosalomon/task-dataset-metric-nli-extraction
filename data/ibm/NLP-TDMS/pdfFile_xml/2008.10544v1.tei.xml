<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TORNADO-NET: MULTIVIEW TOTAL VARIATION SEMANTIC SEGMENTATION WITH DIAMOND INCEPTION MODULE A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Gerdzhev</surname></persName>
							<email>martin.gerdzhev@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada</orgName>
								<address>
									<settlement>Toronto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
							<email>ryan.razani@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada</orgName>
								<address>
									<settlement>Toronto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
							<email>ehsan.taghavi@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada</orgName>
								<address>
									<settlement>Toronto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu Noah&amp;apos;s Ark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada</orgName>
								<address>
									<settlement>Toronto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada</orgName>
								<address>
									<settlement>Toronto</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TORNADO-NET: MULTIVIEW TOTAL VARIATION SEMANTIC SEGMENTATION WITH DIAMOND INCEPTION MODULE A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Robotics</term>
					<term>Autonomous Driving</term>
					<term>Spherical Transformation</term>
					<term>Bird&apos;s Eye view</term>
					<term>Multi-view fusion (MVF)</term>
					<term>Semantic Segmentation</term>
					<term>LiDAR point cloud</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of point clouds is a key component of scene understanding for robotics and autonomous driving. In this paper, we introduce TORNADO-Net -a neural network for 3D LiDAR point cloud semantic segmentation. We incorporate a multi-view (bird-eye and range) projection feature extraction with an encoder-decoder ResNet architecture with a novel diamond context block. Current projection-based methods do not take into account that neighboring points usually belong to the same class. To better utilize this local neighbourhood information and reduce noisy predictions, we introduce a combination of Total Variation, Lovász-Softmax, and Weighted Cross-Entropy losses. We also take advantage of the fact that the LiDAR data encompasses 360 • field of view and uses circular padding. We demonstrate state-of-the-art results on the SemanticKITTI dataset and also provide thorough quantitative evaluations and ablation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation of point clouds, specially on data collected from LiDARs is becoming a crucial task in many applications such as robotics, autonomous driving, etc. Semantic segmentation is a key component of a larger scope of tasks called scene understanding. The role of scene understanding is even more pronounced for autonomous systems such as self-driving cars, where safety is paramount and errors in the perception stack can lead to bad planning and accidents.</p><p>While semantic segmentation has been used on images in other domains, such as medical imaging and surveillance systems, it has seen limited applications in self-driving due to the difficulty of data annotation of the different sensors. An autonomous vehicle is equipped with many primary sensors, including cameras, LiDARs, RADARs and possibly other secondary sensors such as sonars, which help the task of perception for an autonomous system. The earliest works on data labeling in robotics and autonomous driving have been done on cameras and images due to their wide availability and somewhat easy labeling procedure.</p><p>To enable 3D data collected from LiDARs to be fully utilized in perception modules, one needs a labeled point cloud at the point level (semantic) to be available. To this date, SemanticKITTI <ref type="bibr" target="#b0">[1]</ref> proved to be one of the best LiDAR datasets with point wise labels available to researchers and industries. Due to this reason, the focus of this work is mainly on literature available on LiDAR semantic segmentation using SemanticKITTI <ref type="bibr" target="#b0">[1]</ref> as a benchmark dataset. It is worth noting that, due to wider field of view, precise distance measurements and light-invariance, most autonomous car platforms use LiDAR sensors for scene understanding on the road.</p><p>LiDAR point clouds present a number of challenges as compared to images -they are unstructured, sparse and their density varies with distance. Some methods try to tackle the problem by operating on the point clouds directly <ref type="bibr" target="#b1">[2]</ref>, while others try to utilize the approaches from the image domain, by projecting the point clouds onto images (Bird Eye View, or Frontal projection) or 3D voxels, and applying convolutions on the structured representation <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2008.10544v1 [cs.CV] 24 Aug 2020</head><p>This work builds upon several approaches like multiple views projections similar to MVF <ref type="bibr" target="#b3">[4]</ref> and encoder-decoder networks like SalsaNext <ref type="bibr" target="#b4">[5]</ref> to propose an end-to-end model that achieves state of the art results on SemanticKITTI <ref type="bibr" target="#b0">[1]</ref>. With TORNADO-Net we introduce the following contributions:</p><p>• A pillar based learning module that learns and extracts features on BEV data representation of LiDAR point clouds;</p><p>• A novel global context module, named Diamond feature extractor, that processes the spherical range-image LiDAR data and extracts rich features suitable for semantic segmentation;</p><p>• A novel loss function based on total variation denoising techniques that improves the overall accuracy of point cloud semantic segmentation models;</p><p>• Circular padding to account for the LiDAR data with 360 • horizontal field of view;</p><p>• An analysis on the semantic segmentation performance using different architectures through an extensive ablation study;</p><p>The rest of the paper is organized as follows. In Section 2, a brief review of recent and related works is given. Section 3 describes the proposed neural network model and the new loss functions in detail. Training details and experimental results including qualitative, quantitative, and ablation studies can be found in Section 4, along with the benchmarks from all available and published methods in the literature for reference. Finally, conclusions and future work are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Until recently, there have been few approaches that focus on LiDAR point cloud based semantic segmentation due to the lack of large-scale labeled datasets. Some of these early methods include PointNet <ref type="bibr" target="#b1">[2]</ref>, SqueezeSeg <ref type="bibr" target="#b2">[3]</ref> and DeepTemporalSeg <ref type="bibr" target="#b5">[6]</ref>. The introduction of the SemanticKITTI dataset <ref type="bibr" target="#b0">[1]</ref> has spurred the development of novel LiDAR-based segmentation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Based on how the point cloud is represented, methods can be grouped into methods that operate on points directly (point-wise), or methods that project the point cloud into a different, easier to work with structure (projectionbased). Point-wise methods like PointNet <ref type="bibr" target="#b1">[2]</ref>, KPConv <ref type="bibr" target="#b6">[7]</ref> and RandLA-Net <ref type="bibr" target="#b14">[15]</ref> don't require any preprocessing or transformation. Although capable of processing smaller point clouds, they are less useful for large point clouds (specially those which collect 360 • data) due to large memory requirements and slow inference speeds. To address some of these problems, some methods like SPG <ref type="bibr" target="#b7">[8]</ref> utilize a superpoint graph, which is formed by geometrically consistent elements. Some of the more prevailing methods, however, aim to project the pointcloud into either a 3D voxel grid, or a 2D image (Bird-Eye-View (BEV) or spherical Range View (RV)), and utilize convolutional operators that work on structured data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. The projection-based methods usually have achieved higher accuracy, while maintaining a much faster inference time. Therefore, TORNADONet aims at solving the problem of LiDAR semantic segmentation using projection-based techniques that combines multiple projections -first to BEV, and then to RV similar to <ref type="bibr" target="#b15">[16]</ref> to extract complementary features and achieve state-of-the-art results.</p><p>In addition to the neural network model that is designed to address a specific problem, the choice of a loss function can play a crucial role to the accuracy of the model. For semantic segmentation tasks, cross entropy (CE) loss is one of the most widely used loss functions. However, since there can be a big class imbalance with many classes being over-represented in the data, the loss functions that take the class frequency into account, such as weighted cross entropy (WCE) or Focal loss <ref type="bibr" target="#b16">[17]</ref>, can improve the performance. While these losses work well in a variety of tasks, they do not optimize the same criterion that is used to evaluate the performance of semantic segmentation, i.e., intersection-over-union or Jaccard Index (IoU).</p><p>Since IoU is a discrete function and cannot be optimized for directly, surrogate functions that are differentiable have been proposed. Lovász-Softmax loss <ref type="bibr" target="#b17">[18]</ref> and Dice loss <ref type="bibr" target="#b18">[19]</ref>, optimize the Jaccard and Dice coefficients, respectively in order to maximize the mean IoU(mIoU).</p><p>These losses however do not take into account the local neighbourhoods of the pixels/points and can lead to noisy predictions. To address this problem, regularization functions, such as, the Total Variation regularizer, have been used to regularize the overall loss by the smoothness of the prediction over neighboring pixels or data points. More recently, the TV regularizer was used to address the denoising problem in image applications <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. A major target in image denoising is to preserve important image features, such as edges, while removing noise. In our work, we use this concept and propose a novel TV loss function that takes into account neighbouring information.</p><p>To achieve better results, it is also common to use a weighted combination of multiple loss functions. For example, SalsaNext <ref type="bibr" target="#b9">[10]</ref> uses a combination of WCE loss and Lovász-Softmax loss. In this paper, a combination of WCE loss, Lovász-Softmax loss and the novel TV loss is proposed to help TORNADO-Net achieve state-of-the-art accuracy in LiDAR semantic segmentation.</p><p>3 TORNADONet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>In this paper, a novel and intuitive NN architecture is introduced to solve the problem of LiDAR semantic segmentation benefiting from information extraction in different views. Although the encoder-decoder model processes range image similar to <ref type="bibr" target="#b4">[5]</ref>, the pillar-projection-learning module (PPL) learns and extracts information in the Bird's Eye View (BEV). This series of different projections as shown in <ref type="figure">Fig</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Pillar Projection Learning</head><p>The proposed model starts off with the PPL module (see <ref type="figure">Figure 2</ref>) in which the raw LiDAR point cloud P is processed by mapping the unordered points to BEV. We follow the approach similar to <ref type="bibr" target="#b15">[16]</ref> where points are grouped into pillars and their normalized x, y, z pillar coordinates are appended to the point cloud features. The point cloud goes through a FC layer to extract better features and is then mapped to the BEV pillars. All points within a pillar are processed through another FC layer and then after a pooling operation each pillar is represented by a single feature vector. The projected pillars then undergo a series of strided convolutions followed by upsampling layers in order to capture neighbouring information. All points are then augmented with the features from the pillar that they belong to.</p><p>The PPL thus generates rich point features that can be used for different applications. For the purpose of LiDAR semantic segmentation, range images have shown better and more accurate results with reasonable computational complexity. Because of this reason, in the proposed model and after applying PPL, the features are projected onto the range-image for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: PPL Block</head><p>To accomplish this, one can use a transformation from point cloud to image as follows for</p><formula xml:id="formula_0">R 3 → R 2 u v = 1 2 [1 − arctan(y, x)π −1 ]W [1 − (arcsin(zr −1 ) + f up ) 1 f ]H (1)</formula><p>where (u, v) are the coordinates of a given point cloud (x, y, z) in spherically transformed data, hereafter range-image, W is the desired horizontal resolution and H is the desired vertical resolution. Moreover, the vertical field of view of the sensor can be described as f = f up + f down . In its simplest form, the channels of the new range-image can be filled with (x, y, z, rem, r), where rem is remission reading of points and r is the range. If desired, other channels can be added to the range-image to address the much needed features for a specific task. As the point cloud is already processed using a PPL block, in this NN model, we use extracted features C D and project them onto the range-image. Empty pixels in the transformed data can be masked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Diamond contextual block</head><p>Here we propose a new global context module to process the output of PPL, named diamond context block (DCB). DCB uses regular 2D convolutions and provides context at different scales with efficient computation that can be used in various CNN models without loss. A generic block diagram of DCB is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. DCB consists of three diamond shape convolution blocks. Each block is a combination of 3 × 3, 5 × 5 and 7 × 7 2D convolutions. Moreover, to carry the local features, a skip connection with 1 × 1 2D convolution connects the input to the output of the second diamond block. Finally, the third diamond block is concatenated with the skip connection of its input to generate the final feature tensor for further processing. Our ablation studies show that DCB enhances semantic segmentation on SemanticKITTI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Encoder-Decoder</head><p>After the raw point cloud is processed by PPL and DCB, the feature tensor is fed to an encoder-decoder CNN similar to what is proposed in <ref type="bibr" target="#b9">[10]</ref>. The architecture of the proposed encoder-decoder is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The input to the network is the spherical projection of the extracted features from PPL+DCB in section 3.1.2. The encoder-decoder part of TORNADONet is built upon the base SalsaNet model <ref type="bibr" target="#b9">[10]</ref> which follows the standard encoder-decoder architecture with a bottleneck compression rate of 16. As opposed to the original implementation of SalsaNet with series of ResNet blocks <ref type="bibr" target="#b21">[22]</ref>, we use the blocks introduced in <ref type="bibr" target="#b4">[5]</ref> with dilations in the convolutions both on the decoder and encoder parts of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss function</head><p>One of the major contributions of this paper comes in the design of a new loss function using ideas introduced in [23], <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b23">[24]</ref>. In this section, we first briefly review the common loss functions used in semantic segmentation tasks. Then the proposed loss function is introduced.</p><p>The weighted cross entropy loss <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> can be written as,</p><formula xml:id="formula_1">L wce (y,ŷ) = − i a i P (y i ) log P (ŷ i ), a i = 1/ (ν i )<label>(2)</label></formula><p>where ν i is the frequency of each class, and P (ŷ i ) and P (y i ) are the corresponding predicted and ground truth probability. Note that P (y i ) acts as indicator for the correct label class.</p><p>This loss is suitable where a NN model deals with multiclass classification problem much like semantic segmentation. Moreover, because it minimizes the distance between the two probability distributions, namely, predicted and actual, WCE makes sure that the difference between P (ŷ i ) and P (y i ) is being minimized.</p><p>The Lovász-Softmax loss <ref type="bibr" target="#b17">[18]</ref> can be expressed as:</p><formula xml:id="formula_2">L ls = 1 |C| c∈C J(e(c))<label>(3)</label></formula><p>where J is the Lovász extension of IoU, e(c) is the vector of errors for class c, e(c) ∈ [0, 1] p , and p is the number of pixels considered. J denotes a piece-wise linear function, with a global minimum. It has been shown in various works such <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>, that Lovász loss is an effective additional loss term that can be used for different machine learning tasks such as object detection and segmentation. Hence, in the process of training the proposed model, Lovász loss will be combined with other losses to achieve better overall accuracy of the trained model.</p><p>Total variation (TV) regularization has been used in the literature to address the denoising problem in images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. This technique forms a significant preliminary step in many computer vision tasks, such as object detection and object recognition (i.e. object localization and classification). A major concern in designing image denoising models is to preserve important image features, such as edges, while removing noise from a digital image.</p><p>The same problem can be recognized in machine learning when one tries to identify pixel or point level classification for images or point cloud data points. Although the task is not directly denoising, classifying a pixel in an image or a data point in a point cloud relies heavily on the information provided by the neighboring pixels or data points. Hence, designing a loss function based on TV regularization seems viable. In <ref type="bibr" target="#b23">[24]</ref>, a total variation regularizer was introduced, which regularizes the overall loss by the smoothness of the prediction over neighboring pixels or data points, resulting in a better optimization for any machine learning task such as image or point cloud semantic segmentation.</p><p>Although the technique introduced in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b26">[27]</ref> is mathematically sound and effective, it is only a regularization term. In this implementation, the TV regularizer is modified to a new loss function. Moreover, in combination with a standard CNN for semantic segmentation its effectiveness is shown for the task of LiDAR semantic segmentation.</p><p>Given an image-like prediction and label for the ground truth, one can write the TV loss as follows:</p><formula xml:id="formula_3">L tv (y,ŷ) = ∆i,∆j Y (∆i),(j) −Ŷ (∆i),(j) p,q + Y (i),(∆j) −Ŷ (i),(∆j) p,q , ∀i, j, ∆i, ∆j ∈ Z, p, q ≥ 1<label>(4)</label></formula><p>where i, j are indexes for the pixel location, ∆i and ∆j are the step sizes in row and column directions, respectively. The ground truth label is denoted by y andŷ is the prediction of the network. In this expression, the quantities Y (∆i),(j) and Y (i),(∆j) represent the difference in pixel values of the current pixel with its vertical and horizontal neighbours, respectively. They are computed as,</p><formula xml:id="formula_4">Y (∆i),(j) = |y (i+∆i),(j) − y i,j |, Y (i),(∆j) = |y (i),(j+∆j) − y i,j |, ∀i, j, ∆i, ∆j ∈ Z<label>(5)</label></formula><p>For the task of LiDAR semantic segmentation on range-image, equation 4 can be simplified to the following for L 1,1 norm and one sided neighbours, i.e., ∆i, ∆j = 1, p, q = 1:</p><formula xml:id="formula_5">L tv (y,ŷ) = i,j ||y i+1,j − y i,j | − |ŷ i+1,j −ŷ i,j || + ||y i,j+1 − y i,j | − |ŷ i,j+1 −ŷ i,j ||<label>(6)</label></formula><p>The total loss that is used to train the proposed model is a combination of Lovász, WCE, and TV losses introduced above. In order to balance the effect of each loss in the training, different weights are introduced for each loss term that can be accomodated as hyperparameters. The final loss can be formulated as</p><formula xml:id="formula_6">L total = β ls L ls + β wce L wce + β tv L tv<label>(7)</label></formula><p>where β ls , β wce and β tv are weights for Lovász loss, WCE loss, and TV loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Details</head><p>Before feeding the raw pointclouds P into the model, they are truncated to the range [-80, 80], <ref type="bibr">[-80, 80]</ref>, <ref type="bibr">[-5,5]</ref> in the x, y, and z directions respectively. This is followed by a series of augmentations. We adopt an augmentation scheme similar to <ref type="bibr" target="#b4">[5]</ref>, where we do random point dropping, global pointcloud rotation and translation, and flipping along the x-axis. We drop up to 20% of the points. For the rotations, we randomly sample angles in the range The weights for the losses were set to β ls = 1.5, β wce = 1.0, and β tv = 7.5.</p><p>In the post-processing stage, the KNN used a kernel size of 5 for the low-res model and kernel size of 11 for the hi-res model, K = 5, σ = 1, and a cutoff of 1m for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We use the SemanticKITTI dataset <ref type="bibr" target="#b0">[1]</ref> to evaluate the performance of our model and compare it with the state-of-the-art methods. It provides over 43K frames with dense point-wise annotations for the entire KITTI Odometry Benchmark across 22 sequences. The dataset consists of 22 distinct semantic classes and is divided into two sets. The first set, referred to as the training and validation set, includes the sequences of (00 -10), while the second set referred to as the test set, includes the sequences of <ref type="bibr">(11 -21)</ref>. The validation set is sequence 08 upon which the ablation studies are based upon. The point-wise labels for the first set are publicly available, however, the labels for the second set are not provided and are kept hidden for competition purposes.</p><p>In order to evaluate the results of the trained methods, the mIoU metric is used. mIoU is the most popular metric for evaluating semantic point cloud segmentation. It can be formalized as,</p><formula xml:id="formula_7">mIoU = 1 n n c=1 T P c T P c + F P c + F N c<label>(8)</label></formula><p>where T P c is the number of true positive points for class c, F P c is the number of false positives, and F N c is the number of false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Analysis</head><p>We compare the numerical experiments of our work with the existing methods in <ref type="table">Table 1</ref>. It demonstrates the class wise IoU, Frames Per Second (FPS), and mean IoU for different approaches. We categorized the methods into two classes of point-wise and projection-based methods. In each category, the best IoU per class is selected. As shown, the proposed method achieves the state-of-the-art result, outperforming all the previous methods in mIoU and almost all the classes in its category. While the proposed method achieves high accuracy it is also faster than most point-based methods, making it applicable to the real-time systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Analysis</head><p>In order to better understand the results produced by the proposed method, TORNADO-Net, in this section, four different samples from the SemanticKITTI <ref type="bibr" target="#b0">[1]</ref> validation set are provided in <ref type="figure">Figure 4</ref>. The results are presented along with ground truth labels for each case presenting the quality of the results in different situations where different objects/scene are available. Case 1 shows how well TORNADO-Net can perform where there exist many dynamic objects such as cars (parked or otherwise). Cases 2 and 3 depicts the successful prediction for road, vegetation, pole and other structural objects. As it is shown, TORNADO-Net can handle most of the cases well and in many cases, distinguishing between ground truth and prediction is hard.    <ref type="table">Table 1</ref>: IoU results on the Semantic-Kitti dataset test split. FPS measurements were taken using a single GTX 2080Ti GPU, or approximated if a runtime comparison was made on another GPU. Note that a FPS of 10 or more is considered real-time, since the acquisition frequency of the Velodyne HDL-64E 64 beam LiDAR sensor is 10 Hz.</p><p>However, there are situations for which TORNADO-Net fails to predict the correct class in a scene. Case 4 presents one of the more common failures seen in our analysis. In this case, the truck on the middle-right part of the scene is predicted partially as a car and partially as a truck. This is mostly due to limited number of training samples and a fair amount of shared features between different types of vehicles such as overall structure, intensity reflection, etc. Nevertheless, the authors think these issues should be addressed even with limited labeled data and is part of the future research on this topic. For more thorough analysis of the results a video of the predictions on sequence 8 will be available publicly as supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>As shown in <ref type="table" target="#tab_3">Table 2</ref>, each of our contributions amounts to an improvement in the overall mIoU. We receive the biggest increases with the introduction of the TV Loss. Circular padding mostly had a positive effect (about 0.5%). The PPL block is also a key component introducing a 1.5% jump at the cost of extra parameters and reduction in speed. Using a hi-res version of the same model leads to a further improvement in the mIoU at the cost of speed. KNN post-processing is also a key component leading to a 2-3% jump. The high-res model benefits from a higher KNN search value due to the sparser projection of points onto the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Semantic segmentation has been a subject of interest in many fields, such as autonomous driving. While other deep learning techniques are promising on the LiDAR semantic segmentation task, they are complex systems to implement. However, other methods that are real-time systems lack accurate performance. The main objective of this work was to propose a novel deep neural network, TORNADO-Net, for 3D LiDAR point cloud semantic segmentation. We leveraged a multi-view (bird-eye and range) projection feature extraction with an encoder-decoder ResNet architecture with a novel diamond context block. Moreover, TV loss was introduced along with Lovász-Softmax, and WCE loss to efficiently train the network. We evaluated the proposed method on the SemanticKITTI benchmark and were able to achieve state-of-the-art results on its published leaderboard, outperforming all the previous methods.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>. 1 ,</head><label>1</label><figDesc>help the model pick up features that are otherwise difficult to extract. Results and ablation studies on the SemanticKITTI [1] dataset benchmark show the effectiveness of the proposed CNN model. The details of the new architecture are explained in the subsections below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>TORNADO Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Diamond contextual block. This block processes the range-image output from the PPL block and provides rich contextual features which are suitable for LiDAR semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>−5, 5],[−5, 5], and [−180, 180] degrees for x, y and z axes respectively. Similarly, for the translations, we sample in the range [−5, 5], [−3, 3], [−1, 1] for x, y, and z directions. Each augmentation is independently applied with a probability of 0.5.The model was trained using the Adam optimizer with a one-cycle learning rate scheduler for 50 epochs. The maximum learning rate was set to 0.004, division factor of 10 and cosine annealing phase split of 0.3. A weight decay of 10 −4 was also used. The models were trained on 4 Tesla V100 with per GPU batch sizes of 4 for the low-res and 3 for the high-res models respectively.The voxel size of the PPL block was set to [0.3125, 0.3125, 10] leading to a voxel grid of [512 × 512]. We used C = 64, C P = 7 , C D = 192 for filter sizes, and the height and width of the projected image were set to H = 64 , W = 2048, except for the high-res model where H = 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 4 :</head><label>44</label><figDesc>Ground truth (top) and prediction (bottom) on SemanticKITTI validation set projected onto the camera images. 4a, 4b, 4c show successful semantic predictions, while 4d highlights the challenge of distinguishing between 2 similar classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TORNADONet-HiRes [Ours] 63.1 94.2 55.7 48.1 40.0 38.2 63.6 60.1 34.9 89.7 66.3 74.5 28.7 91.3 65.6 85.6 67.0 71.5 58.0 65.9 4</figDesc><table><row><cell></cell><cell>TangentConv [30]</cell><cell>35.9 86.8 1.3 12.7 11.6 10.2 17.1 20.2 0.5 82.9 15.2 61.7 9.0 82.8 44.2 75.5 42.5 55.5 30.2 22.2 0.3</cell></row><row><cell></cell><cell>PointASNL [31]</cell><cell>46.8 87.9 0 25.1 39.0 29.2 34.2 57.6 0 87.4 24.3 74.3 1.8 83.1 43.9 84.1 52.2 70.6 57.8 36.9 −</cell></row><row><cell></cell><cell>RandLa-Net [15]</cell><cell>53.9 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7 22</cell></row><row><cell></cell><cell>S-BKI [32]</cell><cell>51.3 83.8 30.6 43.0 26.0 19.6 8.5 3.4 0.0 92.6 65.3 77.4 30.1 89.7 63.7 83.4 64.3 67.4 58.6 67.1 −</cell></row><row><cell></cell><cell>Kpconv [7]</cell><cell>58.8 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4 −</cell></row><row><cell></cell><cell>SqueezeSeg [3]</cell><cell>29.5 68.8 16.0 4.1 3.3 3.6 12.9 13.1 0.9 85.4 26.9 54.3 4.5 57.4 29.0 60.0 24.3 53.7 17.5 24.5 66</cell></row><row><cell></cell><cell>SqueezeSeg-CRF [3]</cell><cell>30.8 68.3 18.1 5.1 4.1 4.8 16.5 17.3 1.2 84.9 28.4 54.7 4.6 61.5 29.2 59.6 25.5 54.7 11.2 36.3 55</cell></row><row><cell></cell><cell>DeepTemporalSeg [6]</cell><cell>37.6 81.5 29.4 19.6 6.6 6.5 23.7 20.1 2.4 85.8 8.7 59.3 1.0 78.6 39.6 77.1 46.0 58.1 32.6 39.1 −</cell></row><row><cell>Projection-based</cell><cell>SqueezeSegV2-CRF [9] SqueezeSegV2 [9] SalsaNet [10] RangeNet21 [12] RangeNet53 [12] LatticeNet [11] RangeNet53++KNN [12]</cell><cell>39.6 82.7 21.0 22.6 14.5 15.9 20.2 24.3 2.9 88.5 42.4 65.5 18.7 73.8 41.0 68.5 36.9 58.9 12.9 41.0 40 39.7 81.8 18.5 17.9 13.4 14.0 20.1 25.1 3.9 88.6 45.8 67.6 17.7 73.7 41.1 71.8 35.8 60.2 20.2 36.3 50 45.4 87.5 26.2 24.6 24.0 17.5 33.2 31.1 8.4 89.7 51.7 70.7 19.7 82.8 48.0 73.0 40.0 61.7 31.3 41.9 26 47.4 85.4 26.2 26.5 18.6 15.6 31.8 33.6 4.0 91.4 57.0 74.0 26.4 81.9 52.3 77.6 48.4 63.6 36.0 50.0 20 49.9 86.4 24.5 32.7 25.5 22.6 36.2 33.6 4.7 91.8 64.8 74.6 27.9 84.1 55.0 78.3 50.1 64.0 38.9 52.2 13 52.9 92.9 16.6 22.2 26.6 21.4 35.6 43.0 46.0 90.0 59.4 74.1 22.0 88.2 58.8 81.7 63.6 63.1 51.9 48.4 7 52.2 91.4 25.7 34.4 25.7 23.0 38.3 38.8 4.8 91.8 65.0 75.2 27.8 87.4 58.6 80.5 55.1 64.6 47.9 55.9 12</cell></row><row><cell></cell><cell>PolarNet [13]</cell><cell>54.3 93.8 40.3 30.1 22.9 28.5 43.2 40.2 5.6 90.8 61.7 74.4 21.7 90.0 61.3 84.0 65.5 67.8 51.8 57.5 16</cell></row><row><cell></cell><cell>3D-MiniNet-KNN [14]</cell><cell>55.8 90.5 42.3 42.1 28.5 29.4 47.8 44.1 14.5 91.6 64.2 74.5 25.4 89.4 60.8 82.8 60.8 66.7 48.0 56.6 28</cell></row><row><cell></cell><cell>SqueezeSegV3-53 [33]</cell><cell>55.9 92.5 38.7 36.5 29.6 33.0 45.6 46.2 20.1 91.7 63.4 74.8 26.4 89.0 59.4 82.0 58.7 65.4 49.6 58.9 6</cell></row><row><cell></cell><cell>SalsaNext [5]</cell><cell>59.5 91.9 48.3 38.6 38.9 31.9 60.2 59.0 19.4 91.7 63.7 75.8 29.1 90.2 64.2 81.8 63.6 66.5 54.3 62.1 24</cell></row><row><cell></cell><cell>TORNADONet [Ours]</cell><cell>61.1 93.1 53.0 44.4 43.1 39.4 61.6 56.7 20.2 90.8 65.3 75.3 27.5 89.6 62.9 84.1 64.3 69.6 55.0 64.2 7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablative Analysis evaluated on SemanticKITTI dataset validation (seq 08).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="9296" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeptemporalseg: Temporally consistent semantic segmentation of 3d lidar scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saimir</forename><surname>Eren Erdal Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selcuk</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cavdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Radu Alexandru Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05905</idno>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14032</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10893</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11236</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scan-based semantic segmentation of lidar point clouds: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larissa</forename><forename type="middle">T</forename><surname>Triess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">B</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Marius</forename><surname>Zöllner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recent developments in total variation image restoration. Mathematical Models of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selim</forename><surname>Esedoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive total variation denoising based on difference curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quan Sen Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="306" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task learning and weighted cross-entropy for dnn-based keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sankaran Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Hoffmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="760" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tangent convolutions for dense prediction in 3d</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian spatial kernel smoothing for scalable dense semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><forename type="middle">W</forename><surname>Grizzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maani</forename><surname>Ghaffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="797" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
