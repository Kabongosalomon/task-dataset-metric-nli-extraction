<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Hyuk</forename><surname>Kim</surname></persName>
							<email>junhyuk.kim@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manri</forename><surname>Cheon</surname></persName>
							<email>manri.cheon@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
							<email>jong-seok.lee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Perceptual super-resolution</term>
					<term>deep learning</term>
					<term>aesthetics</term>
					<term>im- age quality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, it has been shown that in super-resolution, there exists a tradeoff relationship between the quantitative and perceptual quality of super-resolved images, which correspond to the similarity to the ground-truth images and the naturalness, respectively. In this paper, we propose a novel super-resolution method that can improve the perceptual quality of the upscaled images while preserving the conventional quantitative performance. The proposed method employs a deep network for multi-pass upscaling in company with a discriminator network and two quantitative score predictor networks. Experimental results demonstrate that the proposed method achieves a good balance of the quantitative and perceptual quality, showing more satisfactory results than existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single-image super-resolution, which is a task to increase the spatial resolution of low-resolution images, has been widely studied in recent decades. One of the simple solutions for the task is to employ interpolation methods such as nearest-neighbor and bicubic upsampling. However, their outputs are largely blurry because fine details of the images cannot be recovered. Therefore, many researchers have investigated how to effectively restore high-frequency details. Nevertheless, it is still highly challenging due to the lack of information in the low-resolution images, i.e., an ill-posed problem <ref type="bibr" target="#b16">[17]</ref>.</p><p>Until the mid-2010s, feature extraction-based methods have been proposed, including sparse coding <ref type="bibr" target="#b37">[38]</ref>, neighbor embedding <ref type="bibr" target="#b17">[18]</ref>, and Bayes forest <ref type="bibr" target="#b28">[29]</ref>. After that, the emergence of deep learning for visual representation <ref type="bibr" target="#b6">[7]</ref>, which is triggered by an image classification challenge (i.e., ImageNet) <ref type="bibr" target="#b15">[16]</ref>, has also flowed into the field of super-resolution <ref type="bibr" target="#b38">[39]</ref>. For instance, the super-resolution convolutional neural network (SRCNN) model proposed by Dong et al. <ref type="bibr" target="#b4">[5]</ref> introduced convolutional layers and showed better performance than the previous methods.</p><p>To build a deep learning-based super-resolution model, it is required to define loss functions that are the objectives of the model to be trained. Loss functions arXiv:1809.04789v2 [cs.CV] <ref type="bibr" target="#b18">19</ref> Apr 2019 <ref type="bibr">(a)</ref> (b) (c) (d) <ref type="figure">Fig. 1</ref>. Example results obtained for an image of the PIRM dataset <ref type="bibr" target="#b2">[3]</ref>. (a) Groundtruth (b) Upscaled by bicubic interpolation (c) Upscaled without perceptual consideration (d) Upscaled with perceptual consideration not related to the perceptual aspects. For instance, when the trained discriminator relies on just finding high-frequency components, the super-resolution model may add some unexpected textures in low-frequency regions such as ground and sky. Second, these approaches tend to sacrifice a large amount of the quantitative quality. For example, the SRGAN-based models achieve better perceptual performance than the other models in terms of BRISQUE and NIQE, but they record worse quantitative quality, showing larger RMSE values <ref type="bibr" target="#b3">[4]</ref>. Since the primary objective of the super-resolution task is to make the upscaled images identical to the ground-truth high-resolution images, it is necessary to properly regularize the upscaling modules to keep balance of the quantitative and qualitative quality.</p><p>In this paper, we propose a novel super-resolution method named "Fourpass perceptual super-resolution with enhanced upscaling (4PP-EUSR)," which is based on the recently proposed EUSR model <ref type="bibr" target="#b13">[14]</ref>. Our model aims at resolving the aforementioned issues via two innovative ways. First, our model employs so-called "multi-pass upscaling" during the training phase, where multiple upscaled images produced by passing the given low-resolution image through the multiple upscaling paths in our model are used in order to consider various possible characteristics of upscaled images. Second, we employ qualitative score predictors, which directly evaluate the aesthetic and subjective quality scores of the upscaled images. This architecture ensures high perceptual quality with preserving the quantitative performance of the upscaled images, as exemplified in <ref type="figure">Fig. 1 (d)</ref>.</p><p>The rest of the paper is organized as follows. First, we provide a brief review of the related work in Section 2. Then, an overview of the proposed method is given in Section 3, including the base deep learning model, multi-pass upscaling for training, structure of the discriminator, and structures of the qualitative score predictors. We explain training procedures of our model with the employed loss functions in Section 4. In-depth experimental analysis of our results is shown in Section 5. Finally, we conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we review the related work of deep learning-based super-resolution in two branches: super-resolution models without and with consideration of naturalness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning-based super-resolution</head><p>One of the earliest super-resolution models based on deep learning is SRCNN, which was proposed by Dong et al. <ref type="bibr" target="#b4">[5]</ref>. The model takes an image upscaled by the bicubic interpolation and enhances it via two convolutional layers. <ref type="bibr">Kim et al.</ref> proposed the very deep super-resolution (VDSR) model <ref type="bibr" target="#b12">[13]</ref>, which consists of 20 convolutional layers. In recent days, residual blocks having shortcut connections <ref type="bibr" target="#b8">[9]</ref> are commonly used in the super-resolution models. For example, Ledig et al. <ref type="bibr" target="#b16">[17]</ref> proposed a model named SRResNet, which contains 16 residual blocks with batch normalization <ref type="bibr" target="#b10">[11]</ref> and parametric ReLU activation <ref type="bibr" target="#b7">[8]</ref>. Lim et al. <ref type="bibr" target="#b18">[19]</ref> developed two super-resolution models for the NTIRE 2017 single-image superresolution challenge <ref type="bibr" target="#b33">[34]</ref>: the enhanced deep super-resolution (EDSR) model for single-scale super-resolution and the multi-scale deep super-resolution (MDSR) model for multi-scale super-resolution. They found that removing batch normalization and blending outputs generated from geometrically transformed inputs help improving the overall quantitative quality. Recently, Kim and Lee <ref type="bibr" target="#b13">[14]</ref> suggested a multi-scale super-resolution method named EUSR, which consists of so-called "enhanced upscaling modules" and performed well in the NTIRE 2018 single-image super-resolution challenge <ref type="bibr" target="#b34">[35]</ref>. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> proposed a superresolution model based on residual dense network (RDN), which extends the residual network to have densely-connected layers. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> proposed a residual channel attention networks (RCAN), which brings an attention mechanism into the super-resolution task and achieves better quantitative performance than EDSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Super-resolution considering naturalness</head><p>Along with ensuring high quantitative quality in terms of PSNR, RMSE, or SSIM, naturalness of the upscaled images, which can be measured by quality metrics such as BRISQUE and NIQE, has been also considered in some studies. There exist two common approaches: employing GANs <ref type="bibr" target="#b5">[6]</ref> and employing image classifiers. In the former approach, the discriminator network tries to distinguish the ground-truth images from the upscaled images and the super-resolution model is trained to fool the discriminator so that it cannot distinguish the upscaled images properly. When an image classifier is used, the super-resolution model is trained to minimize the difference of the features obtained at the intermediate layers of the classifier for the ground-truth and upscaled images. For example, Johnson et al. <ref type="bibr" target="#b11">[12]</ref> used the trained VGG16 network to extract the intermediate features and regarded the squared Euclidean distance between them as the loss function. Ledig et al. <ref type="bibr" target="#b16">[17]</ref> employed an adversarial network and differences of the features obtained from the trained VGG19 network for calculating losses of their super-resolution model (i.e., SRResNet), which is named as SRGAN. Mechrez et al. <ref type="bibr" target="#b21">[22]</ref> defined the so-called "contextual loss," which compares the statistical distribution of the intermediate features obtained from the trained VGG19 model, to train their super-resolution model. These models focus on ensuring naturalness of the upscaled images but tend to sacrifice a large amount of the quantitative quality <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of the proposed method</head><p>The architecture of the proposed method can be disassembled into four components ( <ref type="figure">Fig. 2)</ref>: a multi-scale upscaling model, employing the model in a multi-pass manner, a discriminator, and qualitative score predictors.  <ref type="figure">Fig. 2</ref>. Overview of the proposed method. First, our super-resolution model (Section 3.1) generates three upscaled images via multi-pass upscaling (Section 3.2). The discriminator tries to differentiate the upscaled images from the ground-truth (Section 3.3). The two qualitative score predictors measure the aesthetic and subjective quality scores, respectively (Section 3.4). The outputs of the discriminator and the score predictors are used to update the super-resolution model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enhanced upscaling super-resolution</head><p>The basic structure of our model is from the EUSR model <ref type="bibr" target="#b13">[14]</ref>, which is shown in <ref type="figure">Fig. 3</ref>. It mainly consists of three parts: scale-aware feature extraction, shared feature extraction, and enhanced upscaling. First, the scale-aware feature extraction part extracts low-level features from the input image by using so-called "local residual blocks." Then, a residual module in the shared feature extraction part, which consists of local residual blocks and a convolutional layer, extracts higher-level features regardless of the scale factor. Finally, the proceeded features are upscaled via "enhanced upscaling modules," where each module increases the spatial resolution of the input by a factor of 2. Thus, the ×2, ×4, and ×8 upscaling paths have one, two, and three enhanced upscaling modules, respectively. The configurable parameters of the EUSR model are the number of output channels of the first convolutional layer, the number of local residual blocks in the shared feature extraction part, and the number of local residual blocks in the enhanced upscaling modules. We consider EUSR as our base upscaling model because it is one of the state-of-the-art approaches supporting multi-scale super-resolution, which enables generating multiple upscaled images from a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-pass upscaling</head><p>The original EUSR model supports multi-scale super-resolution by factors of 2, 4, and 8. During the training phase, our model utilizes all these upscaling paths to produce three output images, where we make the output images have the same upscaling factor of 4 for a given image as follows <ref type="figure">(Fig. 4</ref>). The first one  <ref type="figure">Fig. 4</ref>. Multi-pass upscaling process, which produces three upscaled images by a factor of 4 from a shared pre-trained EUSR model.</p><p>is directly generated from the ×4 path. The second one is generated by passing the given image through the ×2 path two times. The third one is generated via bicubic downscaling of the image obtained from the ×8 path by a factor of 2. Thus, the model is employed four times for each input image.</p><p>The original purpose of multi-scale models such as MDSR <ref type="bibr" target="#b18">[19]</ref> and EUSR <ref type="bibr" target="#b13">[14]</ref> is to support variable scaling factors on a single model. On the other hand, our multi-pass upscaling extends it with a different objective, which is to improve the quality of the upscaled images for a fixed scaling factor. Since all three images obtained from different upscaling paths are used for training, the model has to learn reducing artifacts that may occur during direct upscaling via the ×4 path, two-pass upscaling via the ×2 path, and upscaling via the ×8 path and downscaling. This prevents the model to overfit towards specific patterns, thus it enables the model to handle various upscaling scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminator network</head><p>Our method employs a discriminator network during the training phase, which is designed to distinguish generated images from the ground-truth images. While the discriminator tries to do its best for identifying the upscaled images, the super-resolution model is trained to make the discriminator difficult to differentiate them from the ground-truth images. This helps our upscaling model generating more natural images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. Inspired by SRGAN <ref type="bibr" target="#b16">[17]</ref>, our discriminator network consists of several convolutional layers followed by LeakyReLU activations with α = 0.2 and two fully-connected layers, as shown in <ref type="figure">Fig. 5</ref>. The final sigmoid activation determines the probability that the input image is real or fake. Note that our discriminator network does not employ the batch normalization <ref type="bibr" target="#b10">[11]</ref>, because the batch size is too small to use that optimization. In addition, it contains two more convolutional layers than the original SRGAN model due to the different size of the input image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Qualitative score predictors</head><p>One of our main ideas for perceptually improved super-resolution is to utilize deep learning models classifying perceptual quality of images, instead of general image classifiers. For this, we employ two deep networks that predict aesthetic and subjective quality scores of images, respectively. To build the networks, we utilize the neural image assessment (NIMA) approach <ref type="bibr" target="#b32">[33]</ref>, which predicts the quality score of a given image. This approach replaces the last layer of a well-known image classifier such as VGG <ref type="bibr" target="#b30">[31]</ref> or Inception-v3 <ref type="bibr" target="#b31">[32]</ref> with a fullyconnected layer with the softmax activation, which produces probabilities of 10 score classes. In our implementation, MobileNetV2 <ref type="bibr" target="#b29">[30]</ref> is used as the base image classifier, because it is much faster than the other image classifiers and supports various sizes of input images. We build two score predictors: one for predicting aesthetic scores and the other for predicting subjective scores. For the aesthetic score predictor, we employ the AVA dataset <ref type="bibr" target="#b25">[26]</ref>, which contains aesthetic user ratings of the images shared in DPChallenge 1 . For the subjective score predictor, we use the TID2013 dataset <ref type="bibr" target="#b26">[27]</ref>, which consists of the subjective quality evaluation results for the test images degraded by various distortion types (e.g., compression, noise, and blurring). While the AVA dataset provides exact score distributions, the TID2013 dataset only provides the mean and standard deviation of the scores. Therefore, we approximate a Gaussian distribution with the mean and standard deviation to train the network based on TID2013. In addition, we adjust the score range of the TID2013 dataset from [0, 9] to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> to match the range of the AVA dataset (i.e., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>). After training the predictors, we use only the mean values of the predicted score distributions to enhance the perceptual quality of the upscaled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>The proposed model extends two existing networks: EUSR <ref type="bibr" target="#b13">[14]</ref> as an upscaling model and SRGAN <ref type="bibr" target="#b16">[17]</ref> as a discriminator. However, the two networks aim at different objectives: EUSR is for better quantitative quality and SRGAN is for better perceptual quality. Our proposed model combines them to ensure both quantitative and perceptual quality, with two newly proposed components: multi-pass upscaling and qualitative score predictors. In summary, our 4PP-EUSR model achieves the following benefits with the aforementioned components:</p><p>-Our model can upscale the input images with considering both the quantitative and perceptual quality. While the base EUSR model tries to make the upscaled images similar to the ground-truth ones, the discriminator reinforces it to focus on fine details. Therefore, our model can achieve better quantitative quality than the other methods concentrating on perceptual quality while keeping the perceptual quality similar to theirs. We will thoroughly investigate this in Section 5.1.</p><p>-Thanks to the multi-pass upscaling, the proposed model can learn various upscaling patterns, which will be further discussed in Sections 5.2 and 5.3.</p><p>-Employing the qualitative score predictors help our model generate perceptually improved images, since they are trained on the dataset that are obtained directly from human raters. We will discuss their benefits in Sections 5.4 and 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training details</head><p>We train our model in three phases: pre-training the EUSR model, building qualitative score predictors, and training the EUSR model in a perceptual manner. Our method is implemented on the TensorFlow framework <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-training multi-scale super-resolution model</head><p>In our method, we employ 32 and one local residual blocks in the residual module and the upscaling part of the EUSR model, respectively. The EUSR model is first pre-trained with the training set of the DIV2K dataset <ref type="bibr" target="#b34">[35]</ref> (i.e., 800 images)</p><p>using the L1 reconstruction loss as in <ref type="bibr" target="#b13">[14]</ref>. For each training step, 16 image patches having a size of 48×48 pixels are obtained by randomly cropping the training images. Then, one of the upscaling paths (i.e., ×2, ×4, and ×8) is randomly selected and trained at that step. For instance, when the ×2 path is selected, the parameters of the path of the model are trained to generate the upscaled images having a size of 96×96 pixels. The Adam optimization method <ref type="bibr" target="#b14">[15]</ref> with β 1 = 0.9, β 2 = 0.999, andˆ = 10 −8 is used to update the parameters. A total of 1,000,000 training steps are executed with an initial learning rate of 10 −4 and reducing the learning rate by a half for every 200,000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training qualitative score predictors</head><p>Along with pre-training EUSR, we also train the qualitative score predictors explained in Section 3.4. As the base image classifier, we employ MobileNetV2 <ref type="bibr" target="#b29">[30]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b27">[28]</ref> with a width multiplier of 1. In the original procedure of training NIMA <ref type="bibr" target="#b32">[33]</ref>, the input image is rescaled to 256×256 pixels without considering the aspect ratio and then randomly cropped to 224×224 pixels, which is the input size of VGG19 <ref type="bibr" target="#b30">[31]</ref> and Inception-v3 <ref type="bibr" target="#b31">[32]</ref>. However, these rescaling and cropping processes are not considered in our case because the MobileNetV2 model does not limit the size of an input image. Instead, we set the input resolution of MobileNetV2 to 192×192 pixels, which is the output size of the 4PP-EUSR model for input patches having a size of 48×48 pixels. In addition, we do not employ the rescaling step and only employ the cropping step to make the input image have a size of 192×192 pixels, because the objective of our score predictors is to evaluate the quality of patches, not the whole given image.</p><p>As the loss function for training the qualitative score predictors, we employ the squared Earth mover's distance defined in <ref type="bibr" target="#b9">[10]</ref> as</p><formula xml:id="formula_0">E(Q I , Q I ) = i F i (Q I ) − F i (Q I ) 2<label>(1)</label></formula><p>where I and I are the ground-truth and upscaled images, respectively, Q I and Q I are the probability distributions of the qualitative scores obtained from the predictor for the two images, respectively, and F i (·) is the i-th element of the cumulative distribution function of the input. The Adam optimization method <ref type="bibr" target="#b14">[15]</ref> with β 1 = 0.9, β 2 = 0.999, and = 10 −7 is used to train the parameters.</p><p>For the aesthetic score predictor, we use about 5,000 images of the AVA dataset <ref type="bibr" target="#b25">[26]</ref> for validation and the remaining 250,000 images for training. We first train the new last fully-connected layer for five epochs with a batch size of 128 and a learning rate of 10 −3 , while freezing all other layers. Then, all the layers are fine-tuned for five epochs with a batch size of 32 and a learning rate of 10 −5 . For the validation images cropped in the center parts, the predictor achieves an average squared Earth mover's distance of 0.079.</p><p>For the subjective score predictor, we consider the first three reference images and their degraded versions in the TID2013 dataset <ref type="bibr" target="#b26">[27]</ref> (corresponding to 360 score distributions) for validation and the remaining 22 reference images and their degraded versions (corresponding to 2,640 score distributions) for training. Similarly to the aesthetic score predictor, we first train the subjective score predictor with freezing all the layers except the new last fully-connected layer for 100 epochs with a batch size of 128 and a learning rate of 10 −3 . Then, the whole network is fine-tuned for 100 epochs with a batch size of 32 and a learning rate of 10 −5 . For the validation images cropped in the center parts, the predictor achieves a Spearman's rank correlation coefficient (SROCC) of 0.780.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training super-resolution model</head><p>Finally, we fine-tune the pre-trained EUSR model together with the discriminator network using the two trained qualitative score predictors. At each training step, the 4PP-EUSR model outputs three upscaled images by a factor of 4. Then, the discriminator is trained to differentiate the ground-truth and upscaled images based on the sigmoid cross entropy loss as in <ref type="bibr" target="#b16">[17]</ref>. After updating parameters of the discriminator, the 4PP-EUSR model is trained with six losses defined as follows.</p><p>-Reconstruction loss (l r ). The reconstruction loss represents the main objective of the super-resolution task: each pixel value of the super-resolved image must be as close as possible to that of the ground-truth image. In our model, this loss is measured by the pixel-by-pixel L1 loss between the ground-truth and generated images, i.e.,</p><formula xml:id="formula_1">l r = 1 W × H w h I w,h − I w,h<label>(2)</label></formula><p>where W and H are the width and height of the images, respectively, and I w,h and I w,h are the pixel values at (w, h) of the ground-truth and upscaled images, respectively.</p><p>-Adversarial loss (l g ). The output of the discriminator network is used to train the super-resolution model towards enhancing perceptual quality, which is denoted as the adversarial loss. It is calculated by the sigmoid cross entropy of the logits obtained from the discriminator for the upscaled images <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_2">l g = − log(D I )<label>(3)</label></formula><p>where D I is the output of the discriminator for the upscaled image I, which represents the probability that the given image is a real one.</p><p>-Aesthetic score loss (l as ). We obtain the aesthetic scores of both the ground-truth and upscaled images from the trained aesthetic score predictor. Then, we define the aesthetic score loss as the weighted difference between the scores, i.e.,</p><formula xml:id="formula_3">l as = max 0, (S a,max − S I a ) − α as (S a,max − S I a )<label>(4)</label></formula><p>where S I a and S I a are the predicted aesthetic scores of the ground-truth and upscaled images, respectively. S a,max is the maximum aesthetic score, which is 10 in our case. The term α as plays a role to control the expected level of aesthetic quality of the upscaled image. For example, α as &lt; 1.0 enforces the model to generate an image that is even perceptually better than the ground-truth image. In our experiments, we set α as to 0.8.</p><p>-Aesthetic representation loss (l ar ). Inspired by <ref type="bibr" target="#b16">[17]</ref>, we also define the aesthetic representation loss, which is the L2 loss between the intermediate outputs of the "global average pooling" layer in the aesthetic score predictor for both the ground-truth and upscaled images:</p><formula xml:id="formula_4">l ar = i P I a,i − P I a,i 2<label>(5)</label></formula><p>where P I a,i and P I a,i are the i-th values of the intermediate outputs for the ground-truth and upscaled images, respectively. The length of each intermediate output is 1,280 <ref type="bibr" target="#b29">[30]</ref>.</p><p>-Subjective score loss (l ss ). In the same manner as the aesthetic score loss, we calculate the subjective score loss using the trained subjective score predictor, i.e., </p><p>where S I s and S I s are the predicted subjective scores of the ground-truth and upscaled images, respectively. S s,max is the maximum subjective score, which is 10 in our case. Similarly to α as , the term α ss controls the contribution of S I s , which is set to 0.8 in our experiments.</p><p>-Subjective representation loss (l sr ). In the same manner as the aesthetic representation loss, we calculate the subjective representation loss using the subjective score predictor as</p><formula xml:id="formula_6">l sr = i P I s,i − P I s,i 2<label>(7)</label></formula><p>where P I s,i and P I s,i are the i-th values of the intermediate outputs at the "global average pooling" layer for the ground-truth and upscaled images, respectively.</p><p>The losses are calculated for all the three upscaled images and then averaged.</p><p>We use the 800 training images of the DIV2K dataset as in the pre-training phase. The Adam optimization method <ref type="bibr" target="#b14">[15]</ref> with β 1 = 0.9, β 2 = 0.999, and = 10 −8 is used to train both the 4PP-EUSR and discriminator. At every training step, two input image patches are selected, which results in generating six upscaled images. Thus, the effective batch sizes of the upscaling and discriminative models are six and eight (i.e., two ground-truth and six upscaled images), respectively. A total of 4 × 10 5 steps are executed with learning rates of 10 −5 and 2 × 10 −5 for the 4PP-EUSR and discriminator, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we report the results of five experiments: comparing the performance of our method and other state-of-the-art super-resolution models, comparing the outputs obtained from different upscaling paths, comparing the performance of our method trained with and without multi-pass upscaling, investigating the roles of loss functions, and comparing the results obtained from different combinations of the loss weights. For the first four experiments, we train our model with the following weighted sum of the six losses defined in Section 4.3: l = 0.05l r + 0.1l g + 0.01l as + 0.1l ar + 0.01l ss + 0.1l sr <ref type="bibr" target="#b7">(8)</ref> which is empirically determined to ensure high perceptual improvement with minimizing degradation of quantitative performance. We evaluate the super-resolution performance on the Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref>, and BSD100 <ref type="bibr" target="#b20">[21]</ref> datasets. Each dataset contains 4, 14, and 100 images, respectively. We employ five performance metrics that are widely used in the literature, including PSNR, SSIM <ref type="bibr" target="#b36">[37]</ref>, NIQE <ref type="bibr" target="#b24">[25]</ref>, a no-reference super-resolution (SR) score proposed by Ma et al. <ref type="bibr" target="#b19">[20]</ref>, and perceptual index (PI) <ref type="bibr" target="#b2">[3]</ref>. PSNR and SSIM are for measuring the quantitative quality, and higher values mean better quality. NIQE, the SR score, and PI are for measuring the perceptual quality, and PI is obtained from the combination of NIQE and the SR score, i.e., PI(Ĩ) = 1 2 (10 − SR(Ĩ)) + NIQE(Ĩ) <ref type="bibr" target="#b8">(9)</ref> whereĨ is a given upscaled image, and NIQE(·) and SR(·) are the measured NIQE value and the SR score, respectively. For NIQE and PI, lower values mean better quality. For the SR score, higher values mean better quality. All quality metrics are calculated on the Y channel of the YCbCr channels converted from the RGB channels with cropping 4 pixels of each border, as in many existing studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. In addition, we conduct a subjective test to assess the performance of our method in the perspective of real human observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with existing models</head><p>We first compare the result images obtained from the ×4 path of our model with those by the following existing super-resolution models.</p><p>-Bicubic interpolation. It is a traditional upscaling method, which interpolates pixel values based on values of their adjacent pixels.</p><p>-SRResNet <ref type="bibr" target="#b16">[17]</ref>. This is for single-scale super-resolution, which consists of several residual blocks. Its two variants are considered: The SRResNet-MSE model is trained with the mean-squared loss and the SRResNet-VGG22 model is trained with the Euclidean distance-based loss for the output of the second conv3-128 layer of VGG19. Their results are retrieved from the authors' supplementary material 2 .</p><p>-EDSR <ref type="bibr" target="#b18">[19]</ref>. This model also consists of residual blocks similarly to SRRes-Net, but does not employ batch normalization to improve the performance. In addition, the upscaled results are obtained by a so-called "geometric selfensemble" strategy, which obtains eight geometrically transformed versions of the input image via flipping and rotation and blends the model outputs for them. The compared results are obtained from a model trained on the DIV2K dataset, which is provided by the authors 3 .</p><p>-MDSR <ref type="bibr" target="#b18">[19]</ref>. It is an extended version of EDSR, which supports multiple factors of upscaling. We obtain the upscaled images from the ×4 path of the MDSR model trained on the DIV2K dataset <ref type="bibr" target="#b34">[35]</ref>. The trained model is provided by the authors 4 .</p><p>-EUSR <ref type="bibr" target="#b13">[14]</ref>. This is the base model of 4PP-EUSR, which supports multiscale super-resolution and consists of optimized residual modules as explained in Section 3.1. We consider the pre-trained EUSR model described in Section 4.1 as a baseline.</p><p>-RCAN <ref type="bibr" target="#b40">[41]</ref>. The RCAN model employs a channel attention mechanism along with the residual-in-residual structure, which contributes to output better super-resolved images than EDSR in terms of PSNR. In addition, RCAN employs the self-ensemble strategy to improve the performance as in the EDSR model. We obtain the output images from the trained model provided by the authors 5 .</p><p>-SRGAN <ref type="bibr" target="#b16">[17]</ref>. The SRGAN model is an extended version of the SRResNet model, where a discriminator network is added to improve the perceptual quality of the upscaled outputs. We consider three SRGAN models, which use different loss functions to train the discriminator: SRGAN-MSE (the mean-squared loss), SRGAN-VGG22 (the Euclidean distance-based loss for the output of the second conv3-128 layer of VGG19), and SRGAN-VGG54 (the Euclidean distance-based loss for the output of the fourth conv3-512 layer of VGG19). The compared results are retrieved from the authors' sup- -CX <ref type="bibr" target="#b21">[22]</ref>. This model is based on SRGAN but employs an additional loss function, the contextual loss <ref type="bibr" target="#b22">[23]</ref>, which measures the cosine distance between the VGG19 features for the ground-truth and upscaled images. The compared results are retrived from the authors' website 7 . <ref type="table" target="#tab_3">Table 1</ref> compares properties of the baselines and ours, including the number of model parameters, the existence of a multi-scale structure, whether to use the reconstruction loss, whether to employ the discriminator, whether to compare features obtained from well-known image classifiers (e.g., VGG19), and whether to use perceptual scores. First, the EDSR model consists of the largest number of parameters than the other models, while the SRResNet, SRGAN, and CX models have the smallest number of parameters. Our model contains a smaller number of parameters than the MDSR and RCAN models. In terms of the multi-scale structure, MDSR, EUSR, and our model utilize multiple scaling factors, while the other models are based on single-scale super-resolution. Although all the models except SRResNet-VGG22 employ the reconstruction loss, the SRGAN-VGG22 and SRGAN-VGG54 models use it only for pre-training. In addition, SRGANs, CX, and our model employ discriminator networks and use them for adversarial losses. SRResNet-MSE, SRGAN-VGG22, SRGAN-VGG54, and CX employ VGG19 as an additional network to use its intermediate outputs as feature-based losses. Our model employs the MobileNetV2-based networks instead of VGG19. Finally, ours estimates the aesthetic and subjective quality scores of the ground-truth and upscaled images for calculating perceptual losses. <ref type="table">Table 2</ref> shows the performance comparison of the baselines and ours evaluated on the three datasets. First of all, the bicubic interpolation introduces <ref type="table">Table 2</ref>. Performance comparison of the baselines and our model evaluated on the Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref>, and BSD100 <ref type="bibr" target="#b20">[21]</ref> datasets. The models are sorted by PSNR (dB) in an ascending order. a large amount of distortion, which results in low PSNR values, and the upscaled images have poor perceptual quality, according to the high PI values. The models that do not employ a discriminator network (i.e., SRResNet, EDSR, MDSR, EUSR, and RCAN) achieve better quantitative quality than the others, showing higher PSNR values, but their perceptual quality is worse except the bicubic interpolation, showing higher PI values. The models considering perceptual quality (i.e., SRGAN and CX) have similar or only slightly higher PSNR values in comparison to the bicubic interpolation, but their perceptual quality is far better than that of the bicubic interpolation, according to the much lower PI values. Our model (i.e., 4PP-EUSR) always records PSNR values higher than those of the other discriminator-based models, which means that ours generates quantitatively better upscaled outputs. At the same time, our model achieves perceptual quality similar to that of SRGAN-MSE in terms of the PI value. For instance, for the BSD100 dataset, the PI values of our model and SRGAN-MSE are 2.956 and 2.802, respectively. This appears more clearly in <ref type="figure" target="#fig_1">Fig. 6</ref>, which compares the baselines and our model with respect to PSNR and PI values measured for the BSD100 dataset. It confirms that our model achieves proper balances of the quantitative and qualitative quality of the upscaled images. <ref type="figure">Fig. 7</ref> shows example images upscaled by different methods. Enlarged images of the regions marked by red rectangles are also shown, where high-frequency textures are expected. First, the bicubic interpolation fails to resolve the textures, producing a highly blurred output. The SRResNet-based, EDSR, MDSR, EUSR, and RCAN models produce richer textures in that region, but still largely blurry. The output of SRResNet-VGG22 shows distinctive textures, which is due to the employment of a different loss function (i.e., differences of VGG19 features). Thanks to the adversarial loss, the other models, including SRGANs, CX, and 4PP-EUSR, generate much better outputs in terms of perceptual quality with sacrificing quantitative quality. Among them, SRGAN-VGG54 and CX recover the most detailed textures, while SRGAN-MSE produces blurry textures. Our model, 4PP-EUSR, restores the textures more clearly than SRGAN-VGG22 and less distinctly than SRGAN-VGG54. Nevertheless, ours achieves better quantitative quality than all the SRGANs in terms of PSNR in <ref type="table">Table 2</ref>.</p><p>Another comparison shown in <ref type="figure">Fig. 8</ref> further supports the importance of considering both the quantitative and perceptual quality. Similarly to <ref type="figure">Fig. 7</ref>, the bicubic interpolation shows the worst output than the others, the models employing only the reconstruction loss (i.e., SRResNets, EDSR, MDSR, EUSR, and RCAN) flatten most of the textured areas, and the rest (i.e., SRGANs, CX, and ours) produce outputs having detailed textures. However, the SRGAN and CX models tend to exaggerate the indistinct textures on the ground and airplane re-  <ref type="figure">Fig. 7</ref>. Images reconstructed by the baselines and our model. The input images are from the Set14 dataset <ref type="bibr" target="#b39">[40]</ref>.  <ref type="figure">Fig. 9</ref>. Subjective test results for 10 images of the BSD100 dataset <ref type="bibr" target="#b20">[21]</ref>.</p><p>gions, introducing sizzling artifacts. For example, the SRGAN-MSE model adds a considerable amount of undesirable noises over the whole image. On the other hand, thanks to the cooperation of the loss functions, our model successfully recovers much of the textures without any prominent artifacts.</p><p>In addition, we conduct a subjective test to examine the perceptual performance of the super-resolution methods. We compare the performance of the 12 super-resolution methods for selected ten images in the BSD100 dataset. We employ 15 participants, which meets the required number of participants for subjective tests in the recommendation ITU-R BT.500-13 <ref type="bibr" target="#b35">[36]</ref>. As for the evaluation method, we follow the same procedure used in <ref type="bibr" target="#b2">[3]</ref>: For a given test image, each participant is asked to rate each of the 120 images on a four-point scale raging among 1 (definitely fake), 2 (probably fake), 3 (probably real), and 4 (definitely real). <ref type="figure">Fig. 9</ref> summarizes the result of the subjective test. It demonstrates that our model outperforms the other methods in terms of the mean opinion score. Our model gets a mean opinion score of 3.07, which means that people regard the output images of ours as "probably real" ones. SRGAN-MSE and SRResNet-VGG22 get the lowest opinion scores among the compared methods. As shown in <ref type="figure">Fig. 8</ref>, it is due to the excessive amount of undesirable artifact introduced in the super-resolved images. The result supports that considering both quantitative and perceptual quality in our model is helpful to obtain visually pleasant upscaled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparing upscaling paths</head><p>As described in Section 3.2 and shown in <ref type="figure">Fig. 4</ref>, our model produces three upscaled images by utilizing all the upscaling paths: by passing through the ×4 path, by passing two times through the ×2 path, and by passing through the ×8 path and then downscaling via bicubic interpolation. Here, we compare the Ground-truth ×4 path ×2 path -×2 path ×8 path -downscale <ref type="figure">Fig. 10</ref>. Images reconstructed by different upscaling paths of our model. The input and ground-truth images are from the BSD100 dataset <ref type="bibr" target="#b20">[21]</ref>. results obtained from the different upscaling paths to examine what aspects our model considers to learn. <ref type="table" target="#tab_7">Table 3</ref> compares the performance of the three upscaling paths of our model. While the PSNR and SSIM values are very similar among the three cases, the ×4 path shows the best performance in terms of the NIQE and PI values. This implies that upscaling using the ×2 path or ×8 path is more difficult than the ×4 path. <ref type="figure">Fig. 10</ref> shows an example result showing large differences between the three cases. The appearances of the textures in the enlarged regions are different depending on the upscaling paths, although the overall patterns of the textures follow that of the ground-truth image. First, the output obtained by the twopass upscaling using the ×2 path contains grid-like textures. One possible reason is due to the uncertainty in the order of passing: the model does not know whether the current input image is firstly or secondly inputted between the two passes, thus the two-pass upscaling is not fully optimized. Second, the output <ref type="table">Table 4</ref>. Performance comparison of the 4PP-EUSR models trained with and without multi-pass upscaling for the Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref>, and BSD100 <ref type="bibr" target="#b20">[21]</ref>   <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of multi-pass upscaling</head><p>The 4PP-EUSR model employs multi-pass upscaling as aforementioned in Section 3.2. To investigate its effectiveness, we compare the performance of the models trained with and without multi-pass upscaling. <ref type="table">Table 4</ref> shows the performance measures of the models in terms of the PSNR, SSIM, NIQE, SR score, and PI values. It demonstrates that employing multipass upscaling is beneficial to enhance both the quantitative and perceptual quality. The model trained with multi-pass upscaling shows larger PSNR and SSIM values and smaller NIQE values for all the three datasets, and smaller PI values for the datasets except Set14. This confirms that the multi-pass upscaling can improve the overall quality of the upscaled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Roles of loss functions</head><p>Our model employs multiple types of loss functions as described in Section 4.3.</p><p>To analyze the role of each loss function, we conduct an experiment where our model is trained with excluding specific loss functions. In detail, we obtain the models trained without l r , without l g , without l as and l ar , and without l ss and l sr . <ref type="table" target="#tab_9">Table 5</ref> shows the PSNR, SSIM, NIQE, SR score, and PI values of the trained models. First, excluding l r deteriorates the quantitative quality of the upscaled images, showing smaller PSNR values, and improves the perceptual quality,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-truth With all losses</head><p>Without lr Without lg Without las, lar Without lss, lsr <ref type="figure">Fig. 11</ref>. Images reconstructed by our models trained with excluding specific loss functions. The input and ground-truth images are from the Set14 dataset <ref type="bibr" target="#b39">[40]</ref>. showing smaller PI values, in comparison to the model trained with all losses. Excluding l g results in the opposite outcomes: it increases the quantitative quality (i.e., larger PSNR values) and decreases the perceptual quality (i.e., larger PI values). Excluding the aesthetic losses (i.e., l as and l ar ) or subjective losses (i.e., l ss and l sr ) also affects to the performance in terms of PSNR. <ref type="figure">Fig. 11</ref> shows example output images, where more evident differences of the roles of the loss functions can be observed. First, the image obtained from the model trained without the reconstruction loss (i.e., l r ) contains the most distinct textures than the others, but the overall color distribution is slightly different from that of the ground-truth image. On the other hand, the result generated by the model trained without the adversarial loss (i.e., l g ) preserves the overall structure of the ground-truth image, while its details are more blurry than those of the others. The output of the model trained without the subjective loss func- <ref type="table">Table 6</ref>. Performance comparison of our models trained with different combinations of the loss weights. The models are evaluated on the Set5 <ref type="bibr" target="#b1">[2]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref>, and BSD100 <ref type="bibr" target="#b20">[21]</ref> datasets. tions contains more lattice-like textures than that of the model trained without the aesthetic loss functions. This implies that the aesthetic losses contribute to the restoration of highly structured textures, while the subjective losses are helpful to construct dispersed high-frequency textures. Finally, the image obtained by training with all the proposed loss functions is the most reliable and natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparing different loss weights</head><p>Finally, we train our model with different weights of the loss functions. Specifically, we alter the weight of the reconstruction loss in (8) as l = α r l r + 0.1l g + α p (0.01l as + 0.1l ar + 0.01l ss + 0.1l sr )</p><p>with α r ∈ {0.5, 0.05, 0.005} and α p ∈ {0, 1}. We can expect that a larger α r value leads the model to be trained towards producing outputs having better quantitative quality. The term α p determines whether to use the score predictors or not. <ref type="table">Table 6</ref> presents the performance of our model trained with different weight values. As expected, decreasing the level of contribution of the reconstruction loss with a smaller α r results in lower PSNR values. On the other hand, the PI values are also decreased, which indicates improved qualitative quality. These observations emerge as the visual differences of the upscaled images shown in <ref type="figure">Fig. 12</ref>. When we examine the enlarged regions where high-frequency textures are expected, a decreased α r value affects the clearness of the output images, due Ground-truth αp = 1, αr = 0.5 αp = 1, αr = 0.05 αp = 1, αr = 0.005 <ref type="figure">Fig. 12</ref>. Images reconstructed by our models trained with different combinations of the loss weights. The input and ground-truth images are from the BSD100 dataset <ref type="bibr" target="#b20">[21]</ref>.</p><p>to relatively larger contributions of the adversarial and perceptual losses. These confirm that there is a tradeoff between quantitative and perceptual quality as mentioned in <ref type="bibr" target="#b3">[4]</ref>, and our model has a capability to deal with the priorities of these quality measures by adjusting the weights of the loss functions. In addition, the result shows that employing the score predictors (i.e., with α p = 1) is helpful to improve the perceptual quality of the upscaled images, which can be observed as decreased PI values in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a perceptually improved super-resolution method, which employs multi-pass image restoration via a multi-scale super-resolution model and trains the model with a discriminator network and two qualitative score predictors. The results showed that our model successfully recovers the original textures in a perceptual manner while preventing quantitative quality degradation.</p><p>Interaction Technology Based on Context Awareness and Human Intention Understanding).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>l</head><label></label><figDesc>ss = max 0, (S s,max − S I s ) − α ss (S s,max − S I s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>PSNR and PI values of the baselines and our model for the BSD100 dataset<ref type="bibr" target="#b20">[21]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Properties of the baseline and our models with respect to the number of parameters, multi-scale structure, and loss functions.</figDesc><table><row><cell>Models</cell><cell># parameters</cell><cell>Multi-scale structure</cell><cell>Using reconstruc-tion loss</cell><cell>Using adversarial loss</cell><cell>Using feature-based loss</cell><cell>Using perceptual loss</cell></row><row><cell>SRResNet-MSE</cell><cell>1.5M</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>SRResNet-VGG22</cell><cell>1.5M</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>EDSR</cell><cell>43.1M</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>MDSR</cell><cell>8.0M</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>EUSR</cell><cell>6.3M</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>RCAN</cell><cell>15.6M</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>SRGAN-MSE</cell><cell>1.5M</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell></row><row><cell>SRGAN-VGG22</cell><cell>1.5M</cell><cell>No</cell><cell>Yes  †</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>SRGAN-VGG54</cell><cell>1.5M</cell><cell>No</cell><cell>Yes  †</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>CX</cell><cell>1.5M</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row><row><cell>4PP-EUSR (Ours)</cell><cell>6.3M</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>† For pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">plementary material 6 .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Images reconstructed by the baselines and our model. The input images are from the BSD100 dataset<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell>3.5</cell><cell></cell></row><row><cell>Bicubic MDSR SRGAN-VGG22 Fig. 8. 2.11 2.13 SRResNet-MSE EUSR SRGAN-VGG54 1.5 SRResNet-VGG22 SRGAN-MSE EUSR RCAN SRResNet-MSE 2.59 2.61 2.62 2.65 2.68 2.68 2.71 SRResNet-VGG22 RCAN CX MDSR SRGAN-VGG22 EDSR Bicubic SRGAN-VGG54 2.98 3.00 CX 2.0 2.5 3.0 Mean Opinion Score</cell><cell>EDSR SRGAN-MSE 4PP-EUSR (Ours) 4PP-EUSR (Ours) 3.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of the outputs obtained from different three upscaling paths of the 4PP-EUSR model. The results are for the Set5<ref type="bibr" target="#b1">[2]</ref>, Set14<ref type="bibr" target="#b39">[40]</ref>, and BSD100<ref type="bibr" target="#b20">[21]</ref> datasets.</figDesc><table><row><cell>Set5</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>×4</cell><cell>31.369</cell><cell>0.870 5.366</cell><cell>6.890</cell><cell>4.238</cell></row><row><cell>×2 -×2</cell><cell>31.491</cell><cell>0.875 6.500</cell><cell>6.887</cell><cell>4.806</cell></row><row><cell>×8 -downscale</cell><cell>31.255</cell><cell>0.867 6.044</cell><cell>7.008</cell><cell>4.518</cell></row><row><cell>Set14</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>×4</cell><cell>27.969</cell><cell>0.751 4.147</cell><cell>7.457</cell><cell>3.345</cell></row><row><cell>×2 -×2</cell><cell>28.096</cell><cell>0.759 4.858</cell><cell>7.429</cell><cell>3.714</cell></row><row><cell>×8 -downscale</cell><cell>27.906</cell><cell>0.750 4.631</cell><cell>7.684</cell><cell>3.474</cell></row><row><cell>BSD100</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>×4</cell><cell>26.904</cell><cell>0.701 3.820</cell><cell>7.907</cell><cell>2.956</cell></row><row><cell>×2 -×2</cell><cell>27.080</cell><cell>0.710 4.951</cell><cell>7.812</cell><cell>3.570</cell></row><row><cell>×8 -downscale</cell><cell>26.844</cell><cell>0.699 4.584</cell><cell>8.156</cell><cell>3.214</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>datasets. ×8 path with downscaling has unexpected white and black pixels, which are similar to the salt-and-pepper noise. It seems that since such noise tends to be removed by downscaling, inclusion of the noise in the output is not necessarily avoided during the training of the ×8 path. These results show that each upscaling path of our model learns a different strategy for superresolution and thus the model is trained to cope with various types of textures via the shared part of the upscaling paths (i.e., the intermediate residual module shown in</figDesc><table><row><cell>Set5</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With multi-pass</cell><cell>31.369</cell><cell>0.870 5.366</cell><cell>6.890</cell><cell>4.238</cell></row><row><cell>Without multi-pass</cell><cell>31.320</cell><cell>0.869 5.917</cell><cell>6.835</cell><cell>4.541</cell></row><row><cell>Set14</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With multi-pass</cell><cell>27.969</cell><cell>0.751 4.147</cell><cell>7.457</cell><cell>3.345</cell></row><row><cell>Without multi-pass</cell><cell>27.699</cell><cell>0.742 4.221</cell><cell>7.594</cell><cell>3.313</cell></row><row><cell>BSD100</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With multi-pass</cell><cell>26.904</cell><cell>0.701 3.820</cell><cell>7.907</cell><cell>2.956</cell></row><row><cell>Without multi-pass</cell><cell>26.614</cell><cell>0.688 4.327</cell><cell>8.140</cell><cell>3.093</cell></row><row><cell>obtained from the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison of the 4PP-EUSR models trained by excluding specific loss functions. The models are evaluated on the Set5<ref type="bibr" target="#b1">[2]</ref>, Set14<ref type="bibr" target="#b39">[40]</ref>, and BSD100<ref type="bibr" target="#b20">[21]</ref> datasets.</figDesc><table><row><cell>Set5</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With all losses</cell><cell>31.369</cell><cell>0.870 5.366</cell><cell>6.890</cell><cell>4.238</cell></row><row><cell>Without lr</cell><cell>29.252</cell><cell>0.834 5.121</cell><cell>8.434</cell><cell>3.344</cell></row><row><cell>Without lg</cell><cell>32.145</cell><cell>0.891 6.665</cell><cell>5.687</cell><cell>5.489</cell></row><row><cell>Without las, lar</cell><cell>30.974</cell><cell>0.862 5.503</cell><cell>7.432</cell><cell>4.035</cell></row><row><cell>Without lss, lsr</cell><cell>31.389</cell><cell>0.873 5.406</cell><cell>6.807</cell><cell>4.300</cell></row><row><cell>Set14</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With all losses</cell><cell>27.969</cell><cell>0.751 4.147</cell><cell>7.457</cell><cell>3.345</cell></row><row><cell>Without lr</cell><cell>26.137</cell><cell>0.705 4.187</cell><cell>8.132</cell><cell>3.028</cell></row><row><cell>Without lg</cell><cell>28.589</cell><cell>0.779 5.287</cell><cell>6.153</cell><cell>4.567</cell></row><row><cell>Without las, lar</cell><cell>27.601</cell><cell>0.738 3.976</cell><cell>7.804</cell><cell>3.086</cell></row><row><cell>Without lss, lsr</cell><cell>27.853</cell><cell>0.752 4.026</cell><cell>7.571</cell><cell>3.228</cell></row><row><cell>BSD100</cell><cell cols="3">PSNR (dB) SSIM NIQE SR score</cell><cell>PI</cell></row><row><cell>With all losses</cell><cell>26.904</cell><cell>0.701 3.820</cell><cell>7.907</cell><cell>2.956</cell></row><row><cell>Without lr</cell><cell>25.142</cell><cell>0.649 4.118</cell><cell>8.773</cell><cell>2.673</cell></row><row><cell>Without lg</cell><cell>27.546</cell><cell>0.734 5.362</cell><cell>6.403</cell><cell>4.480</cell></row><row><cell>Without las, lar</cell><cell>26.540</cell><cell>0.684 4.016</cell><cell>8.343</cell><cell>2.837</cell></row><row><cell>Without lss, lsr</cell><cell>26.870</cell><cell>0.703 3.780</cell><cell>7.989</cell><cell>2.895</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.dpchallenge.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://twitter.box.com/s/lcue6vlrd01ljkdtdkhmfvk7vtjhetog 3 https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar 4 https://cv.snu.ac.kr/research/EDSR/model_pytorch.tar 5 https://github.com/yulunzhang/RCAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://twitter.box.com/s/lcue6vlrd01ljkdtdkhmfvk7vtjhetog 7 http://cgm.technion.ac.il/people/Roey/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the "ICT Consilience Creative Program" (IITP-2018-2017-0-01015) supervised by the IITP (Institute for Information &amp; communications Technology Promotion). In addition, this work was also supported by the IITP grant funded by the Korea government (MSIT) (R7124-16-0004, Development of Intelligent</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The 2018 PIRM challenge on perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Squared Earth mover&apos;s distance-based loss for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05916</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual network with enhanced upscaling module for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="913" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single image super-resolution via subspace projection and neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>cedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04626</idno>
		<title level="m">Maintaining natural image statistics with the contextual loss</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02077</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image database TID2013: Peculiarities, results and perspectives</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="57" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Naive Bayes super-resolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>MobileNetV2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>NTIRE 2017 challenge on single image super-resolution: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="965" to="976" />
		</imprint>
	</monogr>
	<note>NTIRE 2018 challenge on single image super-resolution: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Union</surname></persName>
		</author>
		<title level="m">Recommendation ITU-R BT.500-13: Methodology for the subjective assessment of the quality of television pictures</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multitask dictionary learning and sparse representation based single-image super-resolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3193" to="3203" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03344</idno>
		<title level="m">Deep learning for single image super-resolution: A brief review</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Curves and Surfaces</title>
		<meeting>the International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
