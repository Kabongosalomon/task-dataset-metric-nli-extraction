<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">See and Read: Detecting Depression Symptoms in Higher Education Students Using Multimodal Social Media Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Mann</surname></persName>
							<email>paulomann@id</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">Universidade Federal Fluminense</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Paes</surname></persName>
							<email>alinepaes@ic.uff.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">Universidade Federal Fluminense</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elton</forename><forename type="middle">H</forename><surname>Matsushima</surname></persName>
							<email>eh.matsushima@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Universidade Federal Fluminense</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">See and Read: Detecting Depression Symptoms in Higher Education Students Using Multimodal Social Media Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mental disorders such as depression and anxiety have been increasing at alarming rates in the worldwide population. Notably, the major depressive disorder has become a common problem among higher education students, aggravated, and maybe even occasioned, by the academic pressures they must face. While the reasons for this alarming situation remain unclear (although widely investigated), the student already facing this problem must receive treatment. To that, it is first necessary to screen the symptoms. The traditional way for that is relying on clinical consultations or answering questionnaires. However, nowadays, the data shared at social media is a ubiquitous source that can be used to detect the depression symptoms even when the student is not able to afford or search for professional care. Previous works have already relied on social media data to detect depression on the general population, usually focusing on either posted images or texts or relying on metadata. In this work, we focus on detecting the severity of the depression symptoms in higher education students, by comparing deep learning to feature engineering models induced from both the pictures and their captions posted on Instagram. The experimental results show that students presenting a BDI score higher or equal than 20 can be detected with 0.92 of recall and 0.69 of precision in the best case, reached by a fusion model. Our findings show the potential of large-scale depression screening, which could shed light upon students at-risk.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Mental disorders have been alarmingly increasing in the worldwide population <ref type="bibr">(WHO 2017)</ref>. Individuals suffering from these problems may present a combination of abnormal thoughts, perceptions, emotions, and behavior. One of the most common mental disorder is depression, globally estimated as more than 300 million cases <ref type="bibr">(WHO 2017)</ref>. Particularly, Brazil has the highest prevalence of Major Depressive Disorder (MDD) 1 among South American countries, with nearly 5,8% (WHO 2017). These cases are not only valid to the general population but have also been increasingly observed in the academic environment, where students face many challenges and stressful events endorsed by academicrelated situations. Reports show that graduate students are 1 In this work, we use MDD and depression interchangeably. more than six times likely to experience depression and anxiety, compared to the general population <ref type="bibr" target="#b9">(Evans et al. 2018)</ref>. Furthermore, a previous study has shown a higher prevalence of MDD in undergraduate courses, with up to 28,2% of prevalence in one of the investigated courses <ref type="bibr" target="#b7">(de Melo Cavestro and Rocha 2006)</ref>. However, naturally, before an individual with depression receives treatment, this disorder must be detected. Many patients do not receive an earlier depression diagnosis in consultation with general practitioners, with roughly 50% of the cases detected <ref type="bibr" target="#b14">(Kessler et al. 2002;</ref><ref type="bibr" target="#b17">Mitchell, Vaze, and Rao 2009)</ref>; even worse, individuals might not have the money, knowledge, or they may have even fear of social stigma to look out for help <ref type="bibr" target="#b0">(Andrade et al. 2014;</ref><ref type="bibr" target="#b27">Roness, Mykletun, and Dahl 2005)</ref>. Because of that, the disorder may remain undiagnosed, unrecognized, and, therefore, untreated, which may further aggravate its symptoms. Thus, although the most reliable way to screen for depression is the clinical diagnosis with psychological and psychiatry doctors, it is crucial to enhance other detection options beyond the consultation-based ones that usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria.</p><p>Another common way of detecting MDD is relying on questionnaires, such as the Beck's Depression Inventory (BDI) and the Center for Epidemiological Studies Depression Scale (CES-D) <ref type="bibr" target="#b3">(Beck, Steer, and Brown 1996;</ref><ref type="bibr" target="#b24">Radloff 1977)</ref>. They evaluate the severity of depression through a final score obtained from the answers given to the questionnaire. There are at least two problems related to such methods. First, these questionnaires should also be handled by professionals, and the individual with MDD may not always have access to them. Second, these criteria have been defined years ago. As the world develops and evolves, the criteria to detect MDD should also change to go along with the new technologies that impact everyday routine and behavior.</p><p>Thus, the question that arises is if we could use regularly individual-generated data to detect depression. Notably, we want to investigate online environments such as social media, where the individual may express depression symptoms in a way different from the established DSM criteria. Several previous studies have already investigated social me-dia features that characterize a user with depressive behavior <ref type="bibr" target="#b31">(Shen et al. 2017;</ref><ref type="bibr" target="#b8">Ernala et al. 2018;</ref><ref type="bibr" target="#b20">Naslund et al. 2019;</ref><ref type="bibr" target="#b13">Jeri-Yabar et al. 2019)</ref>. Related to that, there is also a great interest in using machine learning to automatically distinguish between depressive and non-depressive users using their own generated data in the social media environment, or leveraging such sites to automatically gather features inspired by the DSM and questionnaires criteria <ref type="bibr" target="#b6">(De Choudhury et al. 2013;</ref><ref type="bibr" target="#b36">Tsugawa et al. 2015;</ref><ref type="bibr" target="#b25">Reece and Danforth 2017;</ref><ref type="bibr" target="#b31">Shen et al. 2017</ref>) (we expose some of them in Section ยง2). Screening depression symptoms from social media is related to the recently proposed concept of highperformance medicine <ref type="bibr" target="#b34">(Topol 2019)</ref>. In contrast with the traditional active diagnosis, when the individual seeks help after observing specific symptoms, the passive diagnosis systems inform individuals of possible disorders based on constant monitoring of their health, possibly through Machine Learning, for example.</p><p>The data shared by social media users, such as social networks, microblogs, and community networks consist mainly of texts and images. However, only a few recent works have focused on assessing depressive symptoms from multimodal sources of data <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018)</ref>. We believe that leveraging from both texts and images, which are the most common types of user-generated data, may help to distinguish different depressive groups, as depression symptoms may manifest through both verbal and nonverbal communication <ref type="bibr" target="#b18">(Morales, Scherer, and Levitan 2017)</ref>. We briefly explain multimodal learning techniques in section ยง3.</p><p>Thus, in this work, we gather data shared by higher education students from one of the largest Brazilian Universities in a broadly used picture-oriented with captions social media, namely Instagram. Next, we adopt such data and machine learning methods to classify the severity of depression symptoms directly from the verbal and nonverbal userprovided content. Choosing Instagram is based on the following reason: we are mainly motivated by the need of investigating the increasing number of mental disorders cases within the academic environment; accordingly, several previous works have pointed it out as one of the most trustful and used social platform by young adults <ref type="bibr" target="#b30">(Shane-Simpson et al. 2018;</ref><ref type="bibr" target="#b12">Huang and Su 2018)</ref>.</p><p>As ground truth, we use the results of the Portuguese translation of Beck's Depression Inventory (BDI) collected from an online, voluntarily answered, questionnaire 2 . Our primary research question is whether we can induce Machine Learning models from a set of Instagram posts that can distinguish students with moderate or severe depression symptoms from the others. Additionally, we would like to investigate if a model built from both images and texts performs better than using either only images or texts. We also want to assess whether we can achieve better results by learning the features and the classifier directly from the shared data with representation learning models, to avoid 2 We conducted the research under the approval of the ethical committee of the Universidade Federal Fluminense (UFF), CAAE: 89859418.1.0000.5243 the burden of inventing, engineering, and selecting specific metadata. Finally, to alleviate the negative black-box aspect of using representation learning methods, we also analyse the coefficients of a linear SVM over the induced features.</p><p>Our main contributions are as follows: (1) we create a methodology based on local search to generate a stratified oriented-to-the-individual dataset, with each example composed of a set of posts of a single individual (section Dataset Generation) so that our inferences do not consider only snapshots of posts but the target student instead; (2) we induce and compare the performance of several models that learn from representation learning <ref type="bibr" target="#b15">(LeCun, Bengio, and Hinton 2015)</ref> techniques (section Deep Learning Models) and compare them with classifiers based on metadata features (section Feature Engineering Models), both from textual and visual data; (3) we propose an early fusion neural network-based architecture to handle together the textual and visual features from posts (section Multimodal Classification). All code is available at our GitHub repository 3 .</p><p>The obtained results point out that the deep multimodal classifier reaches precision and recall values good enough to be useful in the task of screening depression using Instagram. The feature engineering models are competitive in terms of F1 score compared to the deep learning models. However, deep learning systems naturally lead to transfer the trained weights to other related domains or tasks. Furthermore, they avoid the effort of investigating and engineering the metadata to solve the task. Novel methods can provide further interpretability of black-box deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detecting Depression (Symptoms) from Social Media</head><p>Guntuku et al. survey the two main ways of assessing depression from social media, namely (1) using answers of psychological tests as attributes to fed a supervised machine learning task; (2) extracting public social media data shared by individuals that have declared themselves as suffering from depression <ref type="bibr" target="#b10">(Guntuku et al. 2017</ref>). In the present work, we follow a hybrid approach: we rely on the BDI psychological test to obtain the class attribute, but the features come from the user-provided content. In this way, we have a more reliable class than the auto-declaration and, at the same time, more intrinsic and general features than the ones observed in the tests, aiming at fulfilling our goal: to investigate if there are underlying patterns from the user-provided content that may point out some depression tendency. Previous works have also followed such a hybrid approach to investigate the predictive characteristics of depression reflected in the content of social media. In (De Choudhury et al. 2013), for example, tweets from individuals that answered the CES-D test were the content source. They created a binary supervised classification test according to a threshold of 22 in the value of the CES-D test. However, different from us that want to assess whether it is possible to avoid the effort of creating metadata by learning directly from the data, they rely only on feature engineering to extract attributes encompassing depressive language, linguistic style, emotion words, among others. In <ref type="bibr" target="#b36">(Tsugawa et al. 2015)</ref>, the methodology was the same as the previous work but targeting Japanese individuals recruited from an advertisement posted on Twitter. A surprising aspect observed from these both studies is that the former results have pointed out that the posting time and the numbers of followers and following are crucial attributes to distinguish between depressive individuals and the others. However, in the later, this difference was not observed, suggesting that cultural aspects, or merely the observed sample of individuals, may interfere in the detected patterns of depression.</p><p>In <ref type="bibr" target="#b31">(Shen et al. 2017)</ref>, the authors focus on classifying people from the general population as depressed or not based on their tweets. The positive examples were the ones satisfying the pattern "(I'm/ I was/ I am/ I've been) diagnosed depression", or the ones that loosely mention "depress". They build the machine learning models using features extracted from the tweets, computed from the users behavior in the social media and their profile. They create a multimodal dictionary to handle the features represented by different types <ref type="bibr">(numeric, vector, etc.)</ref>. That work was later extended in <ref type="bibr" target="#b32">(Shen et al. 2018</ref>) to transfer a model learned from one social site to another one, aiming at avoiding labeling new data. All those features are enlightening and grounded in psychological theories, but here we would like to mainly investigate how deep learning classifiers performs when trained directly from the data, avoiding the efforts invested in engineering metadata.</p><p>A similar motivation inspired the work presented in <ref type="bibr" target="#b35">(Trotzek, Koitka, and Friedrich 2018)</ref>, where convolutional neural networks are trained from linguistic metadata (gathered with Linguistic Inquiry and Word Count (LIWC) tool and others) and from embeddings of textual content. Several different embeddings techniques were also used in <ref type="bibr" target="#b22">(Orabi et al. 2018)</ref> to detect depression from tweets. Different from the two later and the two previously mentioned works, we investigate the data from Instagram, which is picture-oriented, making the users express their feelings and state-of-mind using both nonverbal and verbal communication <ref type="bibr" target="#b18">(Morales, Scherer, and Levitan 2017)</ref>. We build a fusion model to consider these types of data.</p><p>Regarding the nonverbal communication, in <ref type="bibr" target="#b25">(Reece and Danforth 2017)</ref>, the authors aim at distinguishing posts of individuals with depression from the rest of the users using metadata and measures related to the published images (for example, the number of likes, number of comments, number of faces in the images, etc.). They investigated the color patterns of the images, based on studies pointing out that individuals with depression tend to see the world more in tones of gray. We, on the other hand, also benefit from the captions of the pictures and from visual features learned directly from the pictures.</p><p>Previous works have also demonstrated that the pattern of social media usage is different among depressed and nondepressed users on both Twitter and Facebook <ref type="bibr">(Park, Mc-Donald, and Cha 2013;</ref><ref type="bibr" target="#b23">Park et al. 2013)</ref>. In this work, however, we assess whether this pattern exists -or not -by leveraging Machine Learning models capable of distinguishing depressed and non-depressed behavior automatically. Some of the previous works classify the posts in social media instead of the individuals. However, they are only short-content snapshots, due to the online communication nature, and probably do not have enough information to classify depression symptoms. For us, one example in the dataset is composed of a set of posts collected during a a certain period, in this way, we make the classification robust, and less error-prone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Brief on Multimodal (Fusion) Learning</head><p>Multimodal learning techniques induce a model by combining more than one modality of data, such as text, images, audio, video, etc., to solve applications ranging from the alignment of multiple data to classification from distinct sources <ref type="bibr" target="#b21">(Ngiam et al. 2011)</ref>. Recently, multimodal learning has increasingly gained attention due to the possibility of extracting latent features represented in a lowdimensional vector space with Deep-Representation learning (Ramachandram and Taylor 2017). Furthermore, this way of tackling data is particularly useful for the social media environment, where the users may express their feelings and thoughts using text, pictures, and even short videos <ref type="bibr" target="#b8">(Duong, Lebret, and Aberer 2017)</ref>.</p><p>To leverage those different data sources to induce a single, unified model, one can either fuse the data following a feature-based approach (early-fusion) or a decision-based approach (late-fusion) (Baltruลกaitis, Ahuja, and Morency 2018). In the first case, one may extract the features for each modality separately, followed by merging the features to feed a classifier. When using Deep Learning, commonly, the feature extraction process is to collect the weights matrix of a layer in the network (Ramachandram and Taylor 2017). The other possibility, still in the feature-based approach, is to extract the features in a shared space, by jointly creating them from the multiple sources of data. In the decisionbased approach, the final answer is based on the decisions taken from each modality by combining them using, for example, a voting process. The type of modality faced by Instagram data is particularly challenging as they are characterized by meaning multiplication <ref type="bibr" target="#b3">(Bateman 2014)</ref>: the caption and the pictures in the same post may refer to distinct contexts, but both modalities are essential to creating a new meaning that diverges from merely making a decision separately from the unimodal meanings. To tackle that, in this work, we contribute with a model that induces a classifier from concatenated textual and visual features.</p><p>Previous works have also focused on multimodal social media data sources to detect disorders, for example, the relationship between eating disorders and the removal of posts from Instagram (Chancellor, Lin, and De Choudhury 2016). Focusing on depression, the work presented in <ref type="bibr" target="#b38">(Victor et al. 2019</ref>) considers visual and verbal communication features in their dataset. The data was produced specifically to conduct the research, and not on a regular-basis data added in social media. Here, we are particularly interested in laying the foundations of a passive diagnosis from social media instead. Audiovisual features are also combined to detect depression symptoms in <ref type="bibr" target="#b29">(Scherer et al. 2014</ref>), using a dataset created from dyadic interactions between an interviewer and paid participants. In <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018)</ref>, several fusion approaches are built from features extracted from video, audio, and transcripts. The dataset is made through interviews conducted by an animated virtual interviewer controlled by a human in another room. In this work, we also investigate the benefits of a fusion architecture, but, different from there, from data extracted from a social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In order to induce the machine learning models, both the proposed models that learn directly from social media data and the ones based on metadata, it is first necessary to create the datasets. In the next subsections, we describe how we perform these major tasks, namely the data collection, the dataset generation, and the induction of ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection</head><p>To collect the Instagram data published by the students, we first created a Google forms questionnaire composed of (1) a number of demographic questions, such as the time spent on Facebook, Twitter, and Instagram; if they were diagnosed with depression; if they work; monthly pay income; Instagram username, etc., and (2) the already mentioned psychometric test, BDI. Then, we published a call for participation in various Facebook groups, and also asked the Universidade Federal Fluminense (UFF) to publish the call through the official email lists. The volunteers were presented with a written explanation of the overall goals of the project, the information that would be gathered, and how their information would be used. To answer the questionnaire, they needed to be regularly enrolled in any course of the University and be at least 18 years old; to ensure data integrity, we used the transparency portal that the University provides 4 to validate the students registration number and their enrollment status. We did not have any personal contact with the students as the whole process was performed online.</p><p>We relied on BDI as a primary tool to assess the severity of the depressive symptoms in a student and to annotate the examples. BDI is a questionnaire comprised of 21 self-reported questions about the mental and psychological state of the individual, wherein each question has a score from zero to three points to determine the level of that specific symptom severity, where higher scores mean higher levels of that symptom. The final score is the sum of all the 21 questions scores. It can be interpreted as follows: 0-13, minimal; <ref type="bibr">14-19, mild;</ref><ref type="bibr">20-28, moderate;</ref><ref type="bibr">and 29-63, severe (Gorenstein et al. 2011)</ref>. We first organize the data following these four intervals of depression intensity, yielding 37% of the sample marked as severe; 23% as moderate; 14% as mild; and 26% as minimal. However, as done in previous work <ref type="bibr" target="#b6">(De Choudhury et al. 2013;</ref><ref type="bibr" target="#b31">Shen et al. 2017)</ref> we separated the individuals into two classes: one comprising the students with non-intense depression symptoms (the ones scored in the minimal and mild classes) and the other one comprising the students with intense depression symptoms (the ones scored in the moderate and severe categories). In a real-world follow-up application of our method, the individuals classified in this last case would be the ones indicated to psychological treatment.</p><p>We gathered the Instagram data that were posted prior to the day the survey had been taken for each student, considering three different observation periods, namely 60 days, 212 days, and 365 days. For example, if a student answered the online questionnaire on October 15, considering the observation period of 60 days, we would collect all the student data ranging between August 16 to October 15. In this way, we prevent post introduced with the sole purpose of influencing the study. We choose 60 days because it was found to be the optimum period in <ref type="bibr" target="#b36">(Tsugawa et al. 2015)</ref>, whereas 365 was investigated in <ref type="bibr" target="#b6">(De Choudhury et al. 2013)</ref>, and 212 is the mean between these two values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Generation</head><p>Our target is the student classification, and not a single post, which is a snapshot of the student behavior in time. Thus, we formalized the problem as a Multiple Instance Learning task <ref type="bibr" target="#b5">(Carbonneau et al. 2018)</ref>, where the training instances are arranged in bags, and the label is provided to the entire bag. Here, the bag is the entire set of pictures or texts (or both) of each student, and the class (non-grave or grave depression symptoms) is given to the bag. In other words, the set of examples E is composed of a set of bags, i.e., E = {S 1 , S 2 , . . . , S m }, where S i = {post 1 , post 2 , . . . , post n } โ E is the bag related to a single student i, and post k โ S i is either (1) a tuple post k = (p k , c k ) where post k is an individual post of the student, p k is a picture and c k is its caption, or (2) post k = p k , when either the post contains only a picture or when we use the examples only for image classification, or, still, (3) post k = c k , when the post is used only for text classification. Note that the size of S i may vary from student to student since we do not oblige a maximum number of collected posts. As we still need a class for each element in the bag, we make each post k โ S i to have the same label y i of S i .</p><p>To acquire the training, validation, and test sets, we must require that a bag S i is not split into those different sets, as this would make the same student appearing in different phases of the learning and test process. It is also crucial to make the distribution of those sets to resemble the original distribution of the dataset. However, it is not trivial to attend all these conditions when considering both the number of bags and the size of each bag. In this way, to generate training, validation, and test sets, we implemented a local search method <ref type="bibr" target="#b10">(Gendreau, Potvin, and others 2010)</ref> to find the optimal solution in the space of candidate solutions. We start at an initial solution with three random sets V 1 , V 2 and V 3 , each one containing examples S i โ E selected at random. Next, we generate the space of candidate solutions by composing : (1) half of the solutions chosen at random; (2) for the other half, we select, at random, two bags from two distinct sets, namely, S j โ V w and S k โ V p , and switch them making S k โ V w and S j โ V p . The evaluation function of the local search checks if these newly generated solutions are better After this process, we end up with ten different datasets for each observation period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning Models</head><p>Our central hypothesis is that we can build the depression classifiers directly from the data shared in the social media, avoiding the effort of building and investigating metadata. Furthermore, we argue that the meaning multiplication of multimodal data has more to add than relying only on unimodal data. To assess these assumptions, we first focus on classifiers that take the students' pictures and written posts separately. Then, we investigate how these two types of data cope together to make the final decision.</p><p>The <ref type="figure">Figure 1</ref> illustrates the three types of models examined here: (a) models created from the individual images of the students (b) models created from the individual captions; (c) a fusion model that puts together the latent features extracted from the two previous types of models. As our target is the student, we combine the individual results for each post by calculating the average of all students' posts predictions to the positive class. Thus, given a student i set of posts S i = {post 1 , post 2 , ..., post n }, and their respective probabilities of being in the positive class determined by the softmax function probas i = {p 1 , p 2 , . . . , p n }, we take the average of probas i to compute the student probability of being in the positive class.</p><p>Image classification To create the pictures classifier, we selected the ResNet <ref type="bibr" target="#b11">(He et al. 2016</ref>) deep network as the representation learner, since it is widely used, easy to access in public frameworks, and won the ILSVRC 2015 5 competition with the ImageNet dataset. We also used the ResNeXt <ref type="bibr" target="#b39">(Xie et al. 2017</ref>) network, pretrained with Instagram images, and fine-tuned on ImageNet1k (Mahajan et al. 2018), available at PyTorch Hub 6 . We selected this network because it was pretrained on 940 million public Instagram images, and we hypothesize that it could further help the image-based predictions. The bag associated with a single student in this case is S i = {post 1 , post 2 , . . . , post n } and post k = p k .</p><p>We trained four distinct-size architectures with the Py-Torch framework (Paszke et al. 2017), namely ResNet-18, ResNet-34, ResNet-50, and ResNeXt-101 32x8d, all of them starting with the pretrained weights mentioned before. To extract the latent features, we partially freeze the pretrained weights (70%) and change the fully connected layer (FC) with the image FC block, which is a dropout layer (p = 0.5) followed by a linear layer. We induced a total of 12 image classifiers, considering the datasets created from the three observation periods (60, 212, 365), each ResNet (18, 34, 50) and ResNeXt architectures. We selected the model that reaches the best accuracy in the validation set.</p><p>We resized the pictures to 224 ร 224 of height and width since this is the input that both ResNet and ResNeXt implementations requires. We also standardize the pictures using the original ImageNet training mean and standard deviation.  . In all of these cases, the examples are the captions captured from the Instagram posts, such that S i = {post 1 , post 2 , . . . , post n } and post k = c k . If the post k has no caption, we use an empty string (c k = ""). After extracting the textual features with each technique, we use a text FC block, which is a linear layer, followed by a batch normalization layer, a ReLU nonlinearity, and a final linear classification layer. This architecture was chosen after achieving better convergence speed in the development set.</p><p>The Bag of Words (BOW) model works by computing a value for each distinct word in a corpus. Here, our final matrix of examples when using BOW has the dimension |E| i=1 |S i | ร |V |, where |V | is the vocabulary size. We used the Term frequency-inverse document frequency (tf-idf) metric to compute the value associated with each word within the example to balance the importance between frequent and uncommon terms.</p><p>Different from the BOW approach, word embeddings has a crucial role in deep learning techniques. To that end, Word2Vec <ref type="bibr" target="#b16">(Mikolov et al. 2013</ref>) was one of the pioneer techniques to achieve improvements in several NLP tasks by allowing words to capture multiples degrees of meaning through their low-dimensional latent representation. However, this technique has a few limitations that the other recent ones, used in this work, does not have. First, it can not represent polysemy because of the same vector representation for the word regardless of context. Second, all embeddings are trained to an entire corpus, which means that words not seen during training are not represented at test time. Third, it does not consider hierarchical representation for words, impairing the representation of syntax and semantics aspects.</p><p>The techniques used in this work, namely, FastText and ELMo, partially or integrally solve those limitations. Fast-Text is similar to Word2Vec, but it is robust to noisy data, as it considers subword information, which means that it can derive representations of words from morphemes, and retrieve good representations even for a small dataset <ref type="bibr" target="#b4">(Bojanowski et al. 2017</ref>). Furthermore, it is even capable of representing some of out-of-vocabulary (OOV) words -if their morphemes are available in training time.</p><p>ELMo, on the other hand, is a Language Model (LM), different from Word2Vec and FastText. ELMo can model polysemy, subword information with character convolutions in the first layer, and hierarchical representation with two bidirectional LSTM layers on the top. The first LSTM layer usually models aspects of syntax, while the second LSTM layer retrieves aspects of contextual meaning . The final ELMo representation layer (ELM o task k ) is generated by a linear combination of all these layers, which are softmax-normalized. By relying on ELMo, we allow for the implicit capture of syntax and context-dependence aspects, leaving to the model to decide which one is the most important to the task of screening depression. Given that we were only able to collect a small dataset, we used pretrained Portuguese weights for both models: Fast-Text weights as provided by Facebook 7 , and ELMo weights by AllenNLP 8 , both pretrained on a dump of the Portuguese Wikipedia. Moreover, since ELMo and FastText retrieve word embeddings, we take the arithmetic mean of the word embeddings for the caption representation.</p><p>We normalize all captions by removing punctuations, emojis and hashtags. We also changed irregular entities to a specific label: we convert numbers to "0", any URL to "url", @username to "username" (since it is not a Portuguese word), and email to "email" labels. The general architecture of the text classification model can be seen in <ref type="figure">Figure 1b</ref>.</p><p>Multimodal classification To classify the severity of depression symptoms using both the pictures and captions from users' posts, we define post k = (p k , c k ), and, as in the text classification, we use an empty string if the picture p k has no caption. To obtain the multimodal features, we first retrieve the textual and the visual features according to the previous explained models. Inspired by the concept of meaning multiplication, where both picture and caption can create a new complex meaning, we concatenate the features from both modalities, and then we perform the final classification with the fusion FC Block, which is a dropout layer (p = 0.5) followed by a final linear layer. We only optimize the fusion FC block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Engineering Models</head><p>To compare our findings with baseline classifiers based on metadata, we also performed a feature engineering task from both modalities. We trained the machine learning models with the same three observation periods, and text preprocessing as used in the deep learning methods.</p><p>For textual features, we use the Linguistic Inquiry Word Count (LIWC) <ref type="bibr" target="#b24">(Pennebaker, Francis, and Booth 2001)</ref> Portuguese translation (Balage Filho, Pardo, and Aluรญsio 2013), that was extensively investigated as useful to the task of detecting depression <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018;</ref><ref type="bibr" target="#b6">De Choudhury et al. 2013;</ref><ref type="bibr" target="#b26">Resnik et al. 2015)</ref>. LIWC is a text analysis program that counts words in psychologically meaningful categories <ref type="bibr" target="#b33">(Tausczik and Pennebaker 2010)</ref>. Its words categories range from, for example, linguistic style usage, as the number of used pronouns, verbs, and adverbs; and other emotional categories such as positive and negative affect words. To obtain the user-level features, we aggregate the features over all posts by taking the arithmetic mean, standard deviation, and total sum, resulting in 64 features.</p><p>As the color of images is one of the most notable features to the human eye, we extract HSV -hue, saturation and value (or brightness) -features by taking the average of the pixels in the image. Furthermore, other studies found the HSV values to be correlated with the severity of depression <ref type="bibr" target="#b25">(Reece and Danforth 2017)</ref>. We also capture the number of faces for each image using a deep-learning-based face detection model 9 . The user-level visual features are also aggregated in the same way as the textual features, resulting in 12 features.   To evaluate the hypothesis of meaning multiplication, we also investigate the multimodality vs. unimodality by simply concatenating the above features. Different from the deep learning models, here we already obtain user-level features by aggregating each post features values. For the classification, we used the same neural network architecture as in the text FC block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In this section, we present the experimental results obtained from the deep learning and feature engineering models, as explained before. We start by presenting the statistics related to the student sample we gathered, followed by the results considering the demographic data, and the engineered features. To that, we inspect the coefficients weights of a linear SVM model. Next, we evaluate the classifiers on the task of screening depressed individuals using text only, image only, and both types of media. The experiments were conducted on an NVIDIA DGX-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Statistics</head><p>We received a total of 416 answers between October 12 and December 2, 2018, and 2-9 April, 2019. We removed six answers that were not from currently enrolled students, and 221 students agreed to provide access to their Instagram data. Thus, we have collected these 221 students data using an Instagram scraper API 10 for Python.</p><p>Our final sample contains 136 females and 85 males with a median age of 23. For the education levels, we have 12 enrolled in Doctor's degree, 11 in Master's degree, and 198 in Bachelor's degree. For the BDI scores, we obtained a total of 82 students in the severe class, 50 in the moderate, 32 in the mild, and 57 in the minimal. We believe that the greater number of students in the severe group is because students with perceived depression might tend to participate more than their counterparts.</p><p>The <ref type="table" target="#tab_2">Table 2</ref> shows the distribution of posts according to each category in the BDI. As we can see, students in the severe category have almost half of the data (Instagram posts) collected for each observation period considered. We can also observe in the <ref type="table" target="#tab_3">Table 3</ref> the mean and standard deviations of posted pictures for each observation period.</p><p>We also investigated the most frequent hashtags that the sample of students use. As we can see in <ref type="table" target="#tab_1">Table 1</ref>, the mild group uses hashtags that refer to the university's city (Niteri), and state (Rio de Janeiro -RJ), where the University (UFF) is placed. On the other side, the students in the moderate group -who could be considered as depresseduse more hashtags related to traveling abroad. For example, Erasmus stands for European Community Action Scheme for the Mobility of University Student 11 and is a European Union student exchange program. In this group, we also have mentions to "#eurotrip", "#lisbon", and "#europe". We found intriguing the presence of so many references for traveling abroad or going to a foreign University. They might indicate a hope of a better life in another place, different from the one they are immersed. The severe group, however, was surprising as it frequently contains hashtags related to nature, summer, smile, and love. We hypothesize that the severe group might use such hashtags as a defense mechanism to alleviate depression symptoms, using a positive thinking perspective. The minimal BDI group, unlike the moderate and severe groups, focus on photography and art in general, more similar to the mild group. However, all those hypothe-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictive Results</head><p>We now focus on the predictive results obtained from the ML models, considering the students with the most severe symptoms as the positive class. When screening depression, it is particularly important to evaluate whether a person with high severity symptoms is incorrectly classified as possessing low severity symptoms (False Negative). Although the opposite is also important (False Positive), when screening individuals with depression, the false negative spectrum is alarming because a person with high severity symptoms, who should be detected for further treatment, is kept unknown. To that end, we choose precision, recall, and F1 metrics for model evaluation; in that way, we can have a precise measurement of how well our model is screening individuals at risk.</p><p>We perform a 10-fold cross-validation over all experiments, and report the average metrics across all the folds. We train all models with the SGD optimizer. <ref type="table" target="#tab_4">Table 4</ref> brings the other hyperparameters used for training. Next, we first show the most important features with the linear SVM coefficients; then, we show models' predictions results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis about the sample and elicited features</head><p>To gain insights about the classification, we employ an analysis based on linear SVM coefficients using the elicited features. We plot the top five most contributing features for the task of screening depression in <ref type="figure" target="#fig_2">Figure 2</ref>. The absolute size difference to each other can be used to determine the feature importance. As we can see from <ref type="figure" target="#fig_2">Figure 2c</ref>, among the most important features for classifying depression, the number of pronouns, social words -about family, and friends -, and bio (biological processes: eat, blood, pain) were amongst the top five correlated features for the depressed class. On the other hand, the least depressed group was correlated with the usage of personal pronouns (ppron). Although different from previous studies that found correlated signals between personal pronouns usage and depression <ref type="bibr" target="#b28">(Rude, Gortner, and Pennebaker 2004;</ref><ref type="bibr" target="#b19">Morales, Scherer, and Levitan 2018;</ref><ref type="bibr" target="#b6">De Choudhury et al. 2013</ref>  guage differently. Particularly because in Portuguese it is not mandatory to use personal pronouns (for example, it is correct, although colloquial, to say "going to somewhere" instead of "I'm going to somewhere" ). This simple example reinforces that the origin of our data may differ significantly from the previous studies, and the use of language can change across different domains. For the visual features <ref type="figure" target="#fig_2">(Figure 2b</ref>), we found that the standard deviation of the number of faces ("faces std"), and saturation were the most correlated features with the depressed class. We hypothesize that the standard deviation of the number of faces can be correlated with depression in the sense that more depressed people post pictures, sporadically, with a higher number of friends, but not frequently. For example, they might regularly post "selfies," or photographs of landscapes, and only a few pictures with a group of friends.</p><p>We also found that sex, and possessing a scholarship are correlated with the less depressed class <ref type="figure" target="#fig_2">(Figure 2d</ref>). On the other side, the time spent using facebook, total monthly income ("household income"), and whether the person was diagnosed with depression are all strongly correlated with the depressed class.</p><p>Surprisingly, when putting together both visual and textual features <ref type="figure" target="#fig_2">(Figure 2a</ref>), the results are almost the same as when using only textual features. This finding also supports previous research <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018;</ref><ref type="bibr" target="#b31">Shen et al. 2017</ref>) that merely concatenating the values of the features do not work very well when detecting depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Predictions</head><p>We exhibit the results with a scatter plot in the <ref type="figure" target="#fig_3">Figure 3</ref>. As one can see, results using an observation period of 60 days generally yields lower precision, along with higher recall scores. In this period, the model needs to give a "diagnosis" using data from 60 days only. For comparison, in a clinical setting, psychologists are encouraged to make a longitudinal evaluation, and a few sessions are not sufficient to make a final judgment, even in the presence of more evidence to support their hypotheses -like facial expressions, hand gestures, and general body language. Thus, when we train the model with an observation period of 60 days, higher recall scores suggests that the model has sufficient information to not classify a positive as a negative example comparable with higher observation periods. We expect this behavior since the BDI questionnaire asks how respondents have been feeling during the past two weeks onset of answering the questionnaire. By this means, the model supports finding individuals at higher risk as according to the BDI, even when using less data.</p><p>On the other hand, lower values of precision suggest that the model is more susceptible to classify negatives examples as positives, which might happen due to the small number of examples for training. When we feed more data to the model, it becomes clear that there is a tendency for achieving better precision scores -keeping, or even increasing the recall. However, there is one exception: visual-oriented deep learning models tend to have higher precision scores, even when facing only 60 days of data. This might happen because Instagram is a picture-oriented social media, and it can be easier to classify examples as true negatives using image embeddings. For the textual representations, Bag of Words performed poorly in all settings. We hypothesize that the frequency of words, although important, is not the single most relevant feature to the task of screening depression. Previous studies have pointed out the relationship between depression and syntax, or semantics <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018;</ref><ref type="bibr" target="#b6">De Choudhury et al. 2013)</ref>, where ELMo has been demonstrated to leverage these features . By regarding these aspects, ELMo achieves better results compared to all the textual techniques used in this study, with nearly 0.0256 of F1 improvement over the best FastText result. However, it is important to note that FastText is considerably more straightforward, and it is fast to train with few resources compared to ELMo. Textual models usually performed better than visual models in terms of F1 score. For example, the best textual and visual models are, respectively, ELMo with 0.75, and ResNeXt with 0.72 of F1 scores for 212 days, as can be seen in <ref type="table" target="#tab_6">Table 5</ref>. The best visual result from ResNeXt is not surprising as the pretrained weights were trained with 940 million Instagram images. However, visual models, as previously discussed, usually provided better precision scores, while textual models had higher recall scores.</p><p>For the feature engineering dataset, we had surprisingly good results. Isolated textual and visual features achieved, respectively, 0.75 and 0.73 of F1 score. This result is equivalent to their deep learning counterparts, but much more straightforward and naturally explain the classification, as we previously demonstrated with the linear SVM coefficients, which further supports the importance of syntax features for screening depression.</p><p>Considering the fused visual and textual features -for the deep learning models -, we achieve almost equivalent scores using ELMo concatenated with any ResNet, and ResNeXt architectures, where the best F1 score (0.79) was achieved with ELMo + ResNet-34. For the feature engineering dataset, however, the F1 score was not improved as expected when using fused features, resulting in a worse F1 score (0.73, for 212 days). This result can be related to the difference of features when concatenating both modalities, since we have 64 textual features and 12 visual features disposed into different representational spaces. It also indicates the necessity of more investigation on how to fuse modalities when using feature engineering, as previously explored in other studies <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018)</ref>. Finally, we also plot ROC and precision-recall curves in <ref type="figure" target="#fig_4">Figure 4</ref> for a single dataset for the best results in each modality, as in <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In general, using a deep multimodal classifier is beneficial for the task of screening depression. The feature engineering models (our baseline), on the other hand, yields competitive results when considering text or image separately; however, when using concatenated features, the results are worse. Previous studies have pointed the same direction for the screening depression task: simply concatenating engineered features makes the model focus on unimodal features instead of paying attention to both, that is why it is necessary to develop techniques for better multimodality representation, using, for example, informed fusion <ref type="bibr" target="#b19">(Morales, Scherer, and Levitan 2018)</ref>. Our results also support this finding, for the feature engineering models, that concatenating visual and textual features do not improve model accuracy, as previously demonstrated by the SVM coefficients in <ref type="figure" target="#fig_2">Figure 2a</ref>, relying only on textual features. One possible reason is the difference in the representational space, where we have 64 features for text, and only 12 features for images. Some alignment might be necessary in order to appropriately take advantage of both modalities in this scenario.</p><p>Instagram is a picture-oriented social media platform. Intuitively, as one might expect, detecting depression using image features should lead to improved results compared to textual features. However, our findings suggest that -with both deep learning and feature engineering -textual features perform better than using image features only. We hypothesize that this is because people express their feelings more explicitly through written texts, making the problem easier for the ML models. However, this argument needs further investigation from the psychological literature.</p><p>As we can see from the results, the feature engineering models yield competitive performance compared to the deep learning methods. However, we lose interpretability when using deep learning, which is important for trusting issues in AI-based systems. Nevertheless, deep learning naturally leads to transfer learning the trained weights, which in turn might be beneficial for detecting depression, as the acquired reliably-annotated datasets are usually quite small. Additionally, when doing feature engineering, one may find other features more relevant and change them across domains, which implicates on the need of retraining the entire model from scratch. Furthermore, social media usually implements the same paradigm: posts contain media, and media can be textual or visual. This paradigm simplifies the deployment of the same model across different social media platforms, leveraging previously acquired knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>The ability to distinguish between different levels of depressive symptoms from social media is a promising path for passive diagnosis of individuals at risk. To contribute in this direction, we leverage six different groups of ML architectures to distinguish students with intense depression symptoms from healthy students, relying on Instagram posts (containing both pictures and their captions). We create three deep learning models, and three feature engineering models, each based on the following media types: text-only, imageonly, and the fusion of text+image. Among all the classifiers, we obtain the best predictive results with the deep multimodal classifier using ELMo and ResNet-34 concatenated features with 0.69 of precision, and 0.92 of recall scores. This finding suggests that a deep multimodal classifier is helpful in the task of screening depression using Instagram. Feature engineering-based models also achieve competitive results, with the advantage of more easily providing insights about the model prediction. Deep learning, on the other hand, allows for natural transfer learning across different domains, which may help when the sample is small.</p><p>As future directions, we first envision to investigate the possibility of transfer our learned models to evaluate students in other universities. We intend to address explainable deep multimodal learning by employing novel methods such as attention <ref type="bibr" target="#b37">(Vaswani et al. 2017)</ref>. We also expect to refine our model by interviewing the individuals and obtaining a ground truth defined by the experts. Finally, we plan to include data from other social media sites, such as Twitter and further investigate the multimodal learning possibilities.</p><p>To conclude, we believe that our contributions show a potential of help on passive diagnosis of depression, to shed light upon students at-risk and guide them to receive adequate treatment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Text classification We use the classical Bag of Words (BOW), FastText (Bojanowski et al. 2017), and ELMo (Peters et al. 2018) techniques to extract the textual feature representations. BOW is computed with SciKit Learn (Pedregosa et al. 2011), FastText with the Gensim implemen-tation (ลehลฏลek and Sojka 2010), and ELMo with the Al-lenNLP platform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Linear SVM's coefficient weights for predicting the positive (red) and negative (blue) classes. ses require further investigation preferably conducted by a domain expert.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Predictive results of the positive class using various models with different observation periods. All results are for students predictions, not posts, over 10 different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Precision-Recall and ROC curves for the best image classifier (ResNeXt), text classifier (ELMo) and fusion classifier (ELMo + ResNet-34) for the observation period of 212 days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Deep learning architectures we have used to predict the intensity of depressive symptoms. Image, text, and fusion Fully Connected (FC) blocks are neural network classifiers designed especially for their particular modality.</figDesc><table><row><cell>Classification</cell><cell>Classification</cell><cell cols="2">Classification</cell></row><row><cell>Image FC Block</cell><cell>Text FC Block</cell><cell cols="2">Fusion FC Block</cell></row><row><cell>ResNet 18, 34, 50 or ResNext WSL</cell><cell>FastText, BoW or ELMo</cell><cell>ResNet 18, 34 50, or ResNext WSL</cell><cell>FastText, BoW or ELMo</cell></row><row><cell></cell><cell>"Antes tarde do que nunca!"</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>"Antes tarde do que nunca!"</cell></row><row><cell>(a) Image model</cell><cell>(b) Text model</cell><cell cols="2">(c) Early fusion model</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell></row></table><note>than the existing ones, according to the sum of the differ- ences between the distributions of the new solutions and the original data distribution; if the new solution has a better dis- tribution than the previous best one, then the new solution becomes the selected one. The stop criteria is either the run- time (5 minutes), or when the newly generated solution has a very similar distribution to the original distribution for the binary BDI (low intensity: 40.27%, high intensity: 59.73%) and to the defined dataset proportion: 60% of the examples for the training set, 20% for validation, and 20% for test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The ten most commonly used hashtags by different groups of BDI, from the most (top) to less frequent (bottom). *Nikiti is a nickname for the city of Niteri.</figDesc><table><row><cell>minimal</cell><cell>mild</cell><cell>moderate</cell><cell>severe</cell></row><row><cell>#art</cell><cell>#destinyrj</cell><cell>#rj</cell><cell>#love</cell></row><row><cell>#photooftheday</cell><cell>#womansolar</cell><cell cols="2">#erasmusstudent #rj</cell></row><row><cell>#photography</cell><cell>#inktober</cell><cell>#uffabroads</cell><cell>#tbt</cell></row><row><cell>#tbt</cell><cell>#inktober2018</cell><cell>#eurotrip</cell><cell>#smile</cell></row><row><cell>#artsy</cell><cell>#tbt</cell><cell>#instadesign</cell><cell>#summer</cell></row><row><cell>#drawing</cell><cell>#photooftheday</cell><cell>#erasmus</cell><cell>#nature</cell></row><row><cell>#vsco</cell><cell>#pictureoftheday</cell><cell>#europe</cell><cell>#friends</cell></row><row><cell>#painting</cell><cell cols="2">#homesweetocean #lisbon</cell><cell>#nikiti*</cell></row><row><cell cols="2">#artistoninstagram #guidetoniteri</cell><cell>#city</cell><cell>#photography</cell></row><row><cell>#blackandwhite</cell><cell cols="2">#proudtobeofniteri #life</cell><cell>#mumbling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Instagram data distribution (percentage of posts) for each observation period, and for each level of depression as obtained by the BDI.</figDesc><table><row><cell cols="3">Period\BDI Minimal Mild</cell><cell cols="2">Moderate Severe</cell></row><row><cell>60 days</cell><cell>26.62%</cell><cell cols="2">13.66% 18.02%</cell><cell>41.70%</cell></row><row><cell>212 days</cell><cell>25.43%</cell><cell cols="2">14.96% 16.44%</cell><cell>43.17%</cell></row><row><cell>365 days</cell><cell>26.05%</cell><cell cols="2">14.80% 15.35%</cell><cell>43.80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mean and standard deviation of posts for each observation period considered in the study.</figDesc><table><row><cell></cell><cell>Mean Std</cell></row><row><cell>Posts per person (60 days)</cell><cell>16.73 24.67</cell></row><row><cell cols="2">Posts per person (212 days) 26.27 34.85</cell></row><row><cell cols="2">Posts per person (365 days) 37.04 46.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters used in the learning process. *The number of MLP hidden units is always half of the input features when not used for classification.</figDesc><table><row><cell>Name</cell><cell>Value</cell><cell>Name</cell><cell>Value</cell></row><row><cell>Epochs Learning rate</cell><cell cols="3">30 0.001 Batch size # MLP h units size(input) 2 32</cell><cell>*</cell></row><row><cell>LR decay gamma</cell><cell>0.85</cell><cell cols="2">Nest. moment. 0.9</cell></row><row><cell>LR decay epochs</cell><cell>7</cell><cell>Optimizer</cell><cell>SGD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Best F1 results for each modality. All results are for the observation period of 212 days.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>Architecture</cell></row><row><cell>Multimodal</cell><cell>0.69</cell><cell>0.92</cell><cell cols="2">0.79 ELMo+RN34</cell></row><row><cell>Text</cell><cell>0.68</cell><cell>0.85</cell><cell cols="2">0.75 ELMo</cell></row><row><cell>Image</cell><cell>0.77</cell><cell>0.67</cell><cell cols="2">0.72 ResNeXt</cell></row><row><cell>Feature Eng.</cell><cell>0.65</cell><cell>0.90</cell><cell cols="2">0.75 Txt features</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/paulomann/ReadAndSee</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://app.uff.br/transparencia/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://image-net.org/challenges/LSVRC/2015/ 6 https://pytorch.org/hub</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://fasttext.cc/docs/en/crawl-vectors.html 8 https://allennlp.org/elmo 9 https://github.com/ageitgey/face recognition</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/rarcega/instagram-scraper 11 https://www.erasmusprogramme.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the Brazilian Research CNPq (Grant 421608/2018-8), FAPERJ (Grant E-26/202.914/2019) for the financial support, and Coordenao de Aperfeioamento de Pessoal de Nvel Superior -Brazil (CAPES) -Finance Code 001, for the scholarship granted to the first author. Additionaly, we would like to thank the reviewers for their valuable feed-back.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Barriers to mental health treatment: results from the who world mental health surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological medicine</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1303" to="1317" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An evaluation of the brazilian portuguese liwc dictionary for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pardo</forename><surname>[balage Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aluรญsio ; Balage</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A S</forename><surname>Aluรญsio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology</title>
		<meeting>the 9th Brazilian Symposium in Information and Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahuja</forename><surname>Baltruลกaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltruลกaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text and image: A critical introduction to the visual/verbal divide. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Steer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">San Antonio</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="498" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>Beck depression inventory-ii</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple instance learning: A survey of problem characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Carbonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">This post will just get taken down: characterizing removed pro-eating disorder social media content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhury ; Chancellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>of the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Predicting depression via social media</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prevalรชncia de depressรฃo entre estudantes universitรกrios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melo</forename><surname>Cavestro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rocha ; De Melo Cavestro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Bras Psiquiatr</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="264" to="267" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterizing audience engagement and assessing its impact on social media disclosures of mental illnesses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lebret</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ernala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Labetoulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. Conf. on Web and Social Media, ICWSM</title>
		<meeting>of the 12th Int. Conf. on Web and Social Media, ICWSM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
	<note>Multimodal classification for analysing social media</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop for NLP Open Source Software</title>
		<meeting>of Workshop for NLP Open Source Software</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
		<respStmt>
			<orgName>NLP-OSS</orgName>
		</respStmt>
	</monogr>
	<note>Evidence for a mental health crisis in graduate education</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Manual do inventรกrio de depressรฃo de beckbdi-ii. Sรฃo Paulo: Editora Casa do Psicรณlogo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potvin</forename><surname>Gendreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gendreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Potvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Detecting depression and mental illness on social media: an integrative review</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motives for instagram use and topics of interest among young adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Internet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and Su</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Association between social media use (twitter, instagram, facebook) and depressive symptoms: Are twitter users at higher risk?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeri-Yabar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Psychiatry</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of depression and anxiety in primary care: follow up study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmj</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="issue">7371</biblScope>
			<biblScope unit="page" from="1016" to="1017" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Hinton ; Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Deep learning</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clinical diagnosis of depression in primary care: a metaanalysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaze</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="issue">9690</biblScope>
			<biblScope unit="page" from="609" to="619" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A cross-modal review of indicators for depression detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scherer</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Workshop on Computational Linguistics and Clinical Psychology -From Linguistic Signal to Clinical Reality</title>
		<meeting>of the 4th Workshop on Computational Linguistics and Clinical Psychology -From Linguistic Signal to Clinical Reality</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A linguistically-informed fusion approach for multimodal depression detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scherer</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</title>
		<meeting>of the 5th Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring opportunities to support mental health care using social media: A survey of social media users with mental illness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Naslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Early intervention in psychiatry</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="413" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Int. Conf. on machine learning</title>
		<meeting>of the 28th Int. Conf. on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for depression detection of twitter users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Orabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Workshop on Comp. Linguistics and Clinical Psychology: From Keyboard to Clinic</title>
		<meeting>of the 5th Workshop on Comp. Linguistics and Clinical Psychology: From Keyboard to Clinic</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perception differences between the depressed and non-depressed users in twitter</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ICWSM</title>
		<meeting>of the 7th ICWSM</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ces-d scale: A self-report depression scale for research in the general population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Booth ; Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Radloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
	</analytic>
	<monogr>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="96" to="108" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Danforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>ลehลฏลek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>of the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
	<note>Instagram photos reveal predictive markers of depression</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond lda: exploring supervised topic modeling for depression-related language in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality</title>
		<meeting>of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Help-seeking behaviour in patients with anxiety disorder and depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykletun</forename><surname>Roness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dahl ; Roness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mykletun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychiatrica Scandinavica</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language use of depressed and depression-vulnerable college students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gortner</forename><surname>Pennebaker ; Rude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gortner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-M</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1121" to="1133" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic audiovisual behavior descriptors for psychological disorder analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="648" to="658" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why do college students prefer Facebook, Twitter, or Instagram? site affordances, tensions between privacy and self-expression, and implications for social capital</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shane-Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="276" to="288" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Depression detection via harvesting social media: A multimodal dictionary learning solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th Int. Joint Conf. on Artificial Intelligence (IJCAI-17)</title>
		<meeting>of the 26th Int. Joint Conf. on Artificial Intelligence (IJCAI-17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3838" to="3844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossdomain depression detection via harvesting social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th Int. Joint Conf. on Artificial Intelligence (IJCAI-2018</title>
		<meeting>of the 27th Int. Joint Conf. on Artificial Intelligence (IJCAI-2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1611" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The psychological meaning of words: Liwc and computerized text analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of language and social psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="54" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Tausczik and Pennebaker</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-performance medicine: the convergence of human and artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Utilizing neural networks and linguistic metadata for early detection of depression indications in text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koitka</forename><surname>Trotzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friedrich ; Trotzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing depression from twitter activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tsugawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>of the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3187" to="3196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Detecting depression using a framework combining deep multimodal neural networks with a purpose-built automated evaluation. Psychological assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Victor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Depression and other common mental disorders: global health estimates</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on computer vision and pattern recognition</title>
		<meeting>of the IEEE Conf. on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
