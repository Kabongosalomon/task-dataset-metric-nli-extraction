<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
							<email>yongcheng.liu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
							<email>bfan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
							<email>gfmeng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<email>smxiang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<email>chpan@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Department of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, the processing of point cloud, which comprises an irregular set of 3D points, has drawn a lot of attention, due to its wide range of applications such as robot manipulation <ref type="bibr" target="#b19">[20]</ref> and autonomous driving <ref type="bibr" target="#b30">[31]</ref>. However, modern applications usually demand for a high-level understanding of point cloud, i.e., identifying the implicit 3D shape pattern. This is quite challenging, since the diverse shapes, abstractly formed by these irregular points, are often hardly distinguishable. For this issue, it is essential to capture sufficiently contextual semantic information for a thorough grasp of the elusive shape (see <ref type="figure">Fig. 1</ref> for details).</p><p>Over the past few years, convolutional neural network (CNN) has demonstrated its powerful abstraction ability of semantic information in image recognition field <ref type="bibr" target="#b61">[61]</ref>. Ac- * Corresponding author: Bin Fan bottle vase PointNet aggregate bottle sufficient context <ref type="figure">Figure 1</ref>. Motivation: sufficiently contextual semantic information is essential for a thorough grasp of the elusive shape formed by point cloud. The "bottle" is misidentified as the "vase" by Point-Net <ref type="bibr" target="#b31">[32]</ref>, while with sufficient context aggregated, it can be accurately recognized. Here, we only illustrate the multi-level context around the blue point for visual clearness.</p><p>cordingly, much effort is focused on replicating its remarkable success on the analysis of image <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">41]</ref>, i.e., regular grid data, to irregular point cloud processing <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b26">27]</ref>. A straightforward strategy is to transform point cloud into regular voxels <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref> or multi-view images <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>, for easy application of CNN. These transformations, however, usually lead to much loss of rich 3D geometric information, as well as high complexity.</p><p>Another difficult yet attractive solution is to learn directly from irregular point cloud. PointNet <ref type="bibr" target="#b31">[32]</ref>, a pioneer in this direction, achieves the permutation invariance of points by learning over each point independently, then applying a symmetric function to accumulate features. Though impressive, it ignores local patterns that have been proven to be important for abstracting high-level visual semantics in image CNN <ref type="bibr" target="#b61">[61]</ref>. To remedy this defect, KCNet <ref type="bibr" target="#b39">[39]</ref> mines local patterns by creating a k-NN graph over each point in Point-Net. Nevertheless, it inherits another defect of PointNet, i.e., no pooling layer to explicitly raise the level of semantics. PointNet++ <ref type="bibr" target="#b33">[34]</ref> hierarchically groups point cloud into local subsets and learns on them by PointNet. This design indeed works like CNN, but the basic operator, PointNet, demands high complexity for enough effectiveness.</p><p>Besides high-level semantics, contextual information, which reflects the potential semantic dependencies between a target pattern and its surroundings <ref type="bibr" target="#b29">[30]</ref>, is also critical for shape pattern recognition. A typical approach in this view is multi-scale learning. Accordingly, PointNet++ <ref type="bibr" target="#b33">[34]</ref> directly applies multi-scale grouping in each layer, i.e., capturing context at the same semantic level. This way, however, is suboptimal as it ignores the inherent difference in semantic levels at different scales, and often causes huge computational cost, especially for lots of scales. Multiresolution grouping <ref type="bibr" target="#b33">[34]</ref> can partly alleviate the latter issue, yet actually, it also abandons crucial context acquisition. ShapeContextNet <ref type="bibr" target="#b55">[55]</ref> finds another strategy inspired by shape context <ref type="bibr" target="#b1">[2]</ref>. It applies self-attention <ref type="bibr" target="#b48">[48]</ref> in each layer of PointNet <ref type="bibr" target="#b31">[32]</ref> to dynamically learn the relation weight among all points, and regards this weight as global shape context. Though fully automatic, it lacks an explicit semantic abstraction like CNN from local to global, and the weight matrix N × N in self-attention can cause huge complexity when the number of points N increases.</p><p>In short, there are mainly two key requirements to exploit CNN for effective learning on point cloud: 1) A convolution operator on point cloud, which can be permutation invariant to unordered points, and can achieve efficient inductive learning of local patterns, is required; 2) A deep hierarchy, which can acquire sufficiently contextual semantics for accurate shape recognition, is also required.</p><p>Accordingly, we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Technically, DensePoint extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and respects the convolutional properties, i.e., local connectivity and weight sharing. Owing to its efficient inductive learning of local patterns, a deep hierarchy can be easily built in DensePoint for semantic abstraction. Architecturally, DensePoint finds inspiration from dense connection mode <ref type="bibr" target="#b12">[13]</ref>, to repeatedly aggregate multi-level and multi-scale semantics in the deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective.</p><p>The key contributions are highlighted as follows:</p><p>• A generalized convolution operator is formulated. It is permutation invariant to points, and respects the convolutional properties of local connectivity and weight sharing, thus extending regular grid CNN to irregular configuration for efficient point cloud processing. • A general architecture equipped with the generalized convolution operator to learn densely contextual representation of point cloud, i.e., DensePoint, is proposed. It can acquire sufficiently contextual semantic information for accurate recognition of the implicit shape. • Comprehensive experiments on challenging benchmarks across four tasks, i.e., shape classification, shape retrieval, part segmentation and normal estimation, as well as thorough model analysis, demonstrate that DensePoint achieves the state of the arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review existing deep learning methods for 3D shape learning. View-based and volumetric methods. View-based methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref> represent a 3D shape as a collection of 2D views, over which classic CNN used in image analysis field can be easily applied. However, 2D projections could cause much loss of 3D shape information due to many self-occlusions. Volumetric methods convert a 3D shape into a regular 3D grid <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref>, over which 3D CNN <ref type="bibr" target="#b47">[47]</ref> can be employed. The main limitation is the quantization loss of 3D shape information due to the low resolution enforced by 3D grid. Although this issue can be partly rescued by recent space partition methods like K-d trees <ref type="bibr" target="#b20">[21]</ref> or octrees <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">51]</ref>, they still rely on a subdivision of a bounding volume. By contrast, our work devotes to learn directly from irregular 3D point cloud.</p><p>Deep learning on point cloud. Much effort has been focused on learning directly on point cloud. PointNet <ref type="bibr" target="#b31">[32]</ref> pioneers this route by learning on each point independently and accumulating the final features. Yet it ignores local patterns, which limits its semantic learning ability. Accordingly, some works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">39]</ref> partition point cloud into local subsets and learn on them based on PointNet. Some other works introduce graph convolutional network to learn over a local graph <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b24">25]</ref> or geometric elements <ref type="bibr" target="#b22">[23]</ref>. However, these methods either lack an explicit semantic abstraction like CNN from local to global, or cause considerable complexity. By contrast, our work extends regular grid CNN to irregular point configuration, achieving efficient learning for point cloud processing.</p><p>In addition, there are some works mapping point cloud into a regular space to facilitate the application of classic CNN, e.g., a sparse lattice structure <ref type="bibr" target="#b43">[43]</ref> with bilateral convolution <ref type="bibr" target="#b17">[18]</ref> or a continuous volumetric function <ref type="bibr" target="#b0">[1]</ref> with 3D CNN. Nevertheless, in our case, we learn directly from irregular point cloud, which is much more challenging. Contextual learning on point cloud. Contextual information is important for identifying the implicit shape pattern. PointNet++ <ref type="bibr" target="#b33">[34]</ref> follows the traditional multi-scale learning by directly capturing context on the same layer, which often causes huge complexity. Hence an alternate called multi-resolution grouping <ref type="bibr" target="#b33">[34]</ref> is devised for efficiency. It forces each layer to learn from its previous layer and the raw input (on the same local region) simultaneously. However, this can be less effective as it actually abandons crucial context acquisition. ShapeContextNet <ref type="bibr" target="#b55">[55]</ref> finds another strategy inspired by shape context <ref type="bibr" target="#b1">[2]</ref>. Instead of the traditional handcrafted design, it applies self-attention <ref type="bibr" target="#b48">[48]</ref> to dynamically learn a weight for all point pairs. Though fully automatic, it lacks a local-to-global semantic learning like CNN. By contrast, we develop a deep hierarchy by an efficient generalized convolution operator, and organically aggregate multi-level contextual semantics in this hierarchy.   <ref type="formula" target="#formula_0">1)</ref>). Instead of classic CNN architecture with layer-by-layer connections, it finds inspiration from dense connection mode <ref type="bibr" target="#b12">[13]</ref>, to repeatedly aggregate multi-level along with multi-scale semantics in an organic manner. To avoid high complexity in deep layers, it forces the output of each layer to be equally narrow with a small constant k (e.g., <ref type="bibr" target="#b23">24)</ref>. As a result, densely contextual representation can be learned efficiently for point cloud processing. Here, N is the number of points while C, C * and k denote feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the generalized convolution operator and the pooling operator on point cloud. Then, we present DensePoint, and elaborate how it learns densely contextual representation for point cloud processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolution and Pooling on Point Cloud</head><p>PConv: convolution on point cloud. Classic convolution on the image operates on a local grid region (i.e., local connectivity), and the convolution filter weights of this grid region are shared along the spatial dimension (i.e., weight sharing). However, this operation is difficult to implement on point cloud due to the irregularity of points. To deal with this problem, we decompose the classic convolution into two core steps, i.e., feature transformation and feature aggregation. Accordingly, a generalized convolution on point cloud can be formulated as</p><formula xml:id="formula_0">f N (x) = ρ {φ(f xn ), ∀x n ∈ N (x)} ,<label>(1)</label></formula><p>where both x and x n denote a 3D point in R 3 , and f is feature vector. N (x), the neighborhood formed by a local point cloud to convolve, is sampled from the whole point cloud by taking a sampled point x as the centroid, and the nearby points as its neighbors x n . f N (x) , the convolutional result as the inductive representation of N (x), is obtained by: (i) performing a feature transformation with function φ on each point in N (x); (ii) applying a aggregation function ρ to aggregate these transformed features. Finally, as shown in the upper part of <ref type="figure" target="#fig_0">Fig. 2</ref> (PConv), similar to classic grid convolution, f N (x) is assigned to be the feature vector of the centroid point x in the next layer. Noticeably, some previous works such as <ref type="bibr" target="#b33">[34]</ref> also use this general formulation. In Eq. (1), f N (x) can be permutation invariant only when the inner function φ is shared over each point in N (x), and the outer function ρ is symmetric (e.g., sum). Accordingly, for high efficiency, we employ a shared single-layer perceptron (SLP, for short) following a nonlinear activator, as φ to implement feature transformation. Meanwhile, as done in classic convolution, φ is also shared over each local neighborhood, for achieving the weight sharing mechanism. As a result, with a symmetric ρ, the generalized PConv can achieve efficient inductive learning of local patterns, whilst be independent of the irregularity of points. Further, using PConv as the basic operator, a classic CNN architecture (no downsampling), as shown in the upper part of <ref type="figure" target="#fig_0">Fig. 2</ref>, can be easily built with layer-by-layer connections. PPool: pooling on point cloud. In classic CNN, pooling is usually performed to explicitly raise the semantic level of the representation and improve computational efficiency. Here, using PConv, this operation can be achieved on point cloud in a learnable way. Specifically, N o points are first uniformly sampled from the input N i points, where N o &lt; N i (e.g., N o = N i /2). Then, PConv can be applied to convolve all the local neighborhoods centered on those N o points, to generate a new downsampling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Densely Contextual Representation</head><p>Classic CNN architecture. In a classic CNN architecture with layer-by-layer connections (the upper part of <ref type="figure" target="#fig_0">Fig. 2</ref>), hierarchical representations can be learned with the lowlevel ones in early layers and the high-level ones in deep layers <ref type="bibr" target="#b61">[61]</ref>. However, a significant drawback is that each layer can only learn from single-level representation. As a consequence, all layers can capture only single-scale shape information from the input point cloud. Formally, assume a point cloud P 0 that is passed through this type of network. The network comprises L layers, in which the th layer performs a non-linear transformation H (·). Then, the output of the th layer can be learned from its previous layer as</p><formula xml:id="formula_1">P = H (P −1 ),<label>(2)</label></formula><p>where each point in P −1 is of single-scale receptive filed on the input point cloud P 0 , resulting that the learned P captures only single-scale shape information. Finally, this will lead to a weakly contextual representation, which is not effective enough for identifying the diverse implicit shapes. DensePoint architecture. To overcome the above issue, we present a general architecture, i.e., DensePoint shown in the lower part of <ref type="figure" target="#fig_0">Fig. 2</ref>, inspired by dense connection mode <ref type="bibr" target="#b12">[13]</ref>. Specifically, for each layer in DensePoint (no downsampling), the outputs of all preceding layers are used as its input, and its own output is used as the input to all subsequent layers. That is, P in Eq. <ref type="formula" target="#formula_1">(2)</ref> becomes</p><formula xml:id="formula_2">P = H [P 0 , P 1 , . . . , P −1 ] ,<label>(3)</label></formula><p>where [·] denotes the concatenation of the outputs of all preceding layers. Here, P is forced to learn from multi-level representations, which facilitates to aggregate multi-level shape semantics along with multi-scale shape information.</p><p>In this way, each layer in DensePoint can capture a certain level (scale) of context, and the level can be gradually increased as the network deepens. Moreover, the acquired dense context in deep layers can also improve the abstraction of high-level semantics in turn, making the whole learning process organic. Eventually, very rich local-to-global shape information in the input P 0 can be progressively aggregated together, resulting in a densely contextual representation for point cloud processing. Note that DensePoint is quite different from the traditional multi-scale strategy <ref type="bibr" target="#b33">[34]</ref>. The former progressively aggregates multi-level (multi-scale) semantics that is organically learned by each layer, while the latter artlessly gathers multi-scale information at the same level. It is also dissimilar to a simple concatenation of all layers as the final output, which results in each layer being less contextual. Narrow architecture. When the network deepens, Dense-Point will suffer from high complexity, since the convolutional overhead of deep layers will be huge with all preceding layers as the input. Thus, we narrow the output channels of each layer in DensePoint with a small constant k (e.g., <ref type="bibr" target="#b23">24</ref>), instead of the large ones (e.g., 512) in classic CNN. ePConv: enhanced PConv. Though lightweight, such narrow DensePoint will lack the expressive power, since with much narrow output k, the shared SLP in PConv, i.e., φ in Eq. (1), could be insufficient in terms of learning ability. To overcome this issue, we introduce the filter grouping <ref type="bibr" target="#b21">[22]</ref> to enhance PConv, which divides all the filters in a layer into several groups, and each group performs individual operation. Formally, the enhanced PConv (ePConv, for short) converts Eq. (1) to </p><formula xml:id="formula_3">f N (x) = ψ ρ { φ(f xn ), ∀x n ∈ N (x)} ,<label>(4)</label></formula><formula xml:id="formula_4">1 f 0 x ← f x, ∀x ∈ P; 2 for = 1...L do 3 for x ∈ P do 4 f N (x) ← ρ {σ( W φ · f −1 xn + b φ ), ∀xn ∈ N (x)} ; 5 f x ← σ(W ψ · f N (x) + b ψ ); 6 end 7 f x ← [f 0 x , ..., f x ], ∀x ∈ P; 8 end 9 return {rx ← f L x , ∀x ∈ P}</formula><p>where φ, the grouped version of SLP φ, can widen its output to enhance its learning ability and maintain the original efficiency, and ψ, a normal SLP (shared over each centroid point x), is added to integrate the detached information in all groups. Both φ and ψ include a nonlinear activator.</p><p>To elaborate ePConv with filter grouping, let SLP φ (resp. SLP ψ ) denote the SLP of φ (resp. ψ), and C i (resp. C o ) denote the input (resp. output) channels of SLP φ . N g is the number of groups. Then, the parameter number of SLP φ before and after filter grouping is,</p><formula xml:id="formula_5">C i × C o vs. (C i /N g ) × (C o /N g ) × N g = (C i × C o )/N g .</formula><p>Here C i and C o are divisible by N g and the few parameters in the bias term are ignored for clearness. In other words, using filter grouping, C o can be increased by N g times but with almost the same complexity. Besides, inspired by the bottleneck layer <ref type="bibr" target="#b8">[9]</ref>, we fix the output channels of SLP φ and SLP ψ as C o : k = 4 : 1 (i.e., C o = 4k), to hold the original narrowness for DensePoint. Hence, with a small k, SLP ψ actually leads to only a little complexity of 4k × k, which can be easily remedied by a suitable N g . The detailed forward pass procedure of DensePoint equipped with ePConv can be referred in Algorithm 1, where * indicates performing grouping operation. DensePoint for point cloud processing. DensePoint applied in point cloud classification and per-point analysis (e.g., segmentation) are illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. In both tasks, DensePoint with ePConv is applied in each stage of the network to learn densely contextual representation, while PPool with original PConv is used to explicitly raise the semantic level and improve efficiency. For classification, the final global representation is learned by three PPools and two DensePoints (11 layers in total, L = 11), followed by three fully connected (fc) layers as the classifier. For perpoint analysis, four levels of representations learned by four PPools and three DensePoints (17 layers in total, L = 17), are sequentially upsampled by feature propagation <ref type="bibr" target="#b33">[34]</ref> to generate per-point predictions. All the networks can be trained in an end-to-end manner. The configuration details are included in the supplementary material. Implementation details. PPool: the farthest points are picked from point cloud for uniform downsampling. Neighborhood: the spherical neighborhood is adopted; a fixed number of neighbors are randomly sampled in each neighborhood for batch processing (the centroid is reused if not enough), and they are normalized by subtracting the centroid. Group number N g in φ (Eq. (4)): N g = 2.</p><p>Nonlinear activator: ReLU <ref type="bibr" target="#b28">[29]</ref>. Dropout <ref type="bibr" target="#b42">[42]</ref>: for model regularization, we apply dropout with 20% ratio on f N (x) in Eq. <ref type="formula" target="#formula_3">(4)</ref> and dropout with 50% ratio on the first two fc layers in the classification network ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>). Narrowness k: k = 24. Aggregation function ρ: symmetric function max pooling is employed. Batch normalization (BN) <ref type="bibr" target="#b16">[17]</ref>: as done in image CNN, BN is used before each nonlinear activator for all layers. Note that only 3D coordinates (X, Y, Z) in R 3 are used as the initial input features. Code is available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We conduct comprehensive experiments to validate the effectiveness of DensePoint. We first evaluate Dense-Point for point cloud processing on challenging benchmarks across four tasks (Sec 4.1). We then provide detailed experiments to study DensePoint thoroughly (Sec 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DensePoint for Point Cloud Processing</head><p>Shape classification. We evaluate DensePoint on Model-Net40 and ModelNet10 classification benchmarks <ref type="bibr" target="#b53">[53]</ref>. The former comprises 9843 training models and 2468 test models in 40 classes, while the latter consists of 3991 training models and 908 test models in 10 classes. The point cloud data is sampled from these models by <ref type="bibr" target="#b31">[32]</ref>. For training, we uniformly sample 1024 points as the input. As in <ref type="bibr" target="#b20">[21]</ref>, we augment the input with random anisotropic scaling in range [-0.66, 1.5] and translation in range [-0.2, 0.2]. For testing, similar to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, we apply voting with 10 tests using random scaling and then average the predictions. The quantitative comparisons with the state-of-theart point-based methods are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Our DensePoint outperforms all the point-input methods. Specifically, it reduces the error rate of PointNet++ by 26.9% on ModelNet40, and also surpasses its advanced version that applies additional normal data with very dense points (5k). Furthermore, even using only point as the input, DensePoint can also surpass the best additional-input method SO-Net <ref type="bibr" target="#b23">[24]</ref> by 0.9% on ModelNet10. These results convincingly verify the effectiveness of DensePoint. Shape retrieval. To further explore the recognition ability of DensePoint for the implicit shapes, we apply the global features, i.e., the outputs of the penultimate fc layer in the classification network ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>), for shape retrieval. We sort the most relevant shapes for each query from the test set by cosine distance, and report mean Average Precision (mAP). Except for point-based methods, we also compare with some advanced 2D image-based ones. The results are summarized in <ref type="table">Table 2</ref>. As can be seen, DensePoint significantly outperforms PointNet by 18%. It is also com-  parable with those image-based methods (even the ensemble one <ref type="bibr" target="#b38">[38]</ref>), which greatly benefit from image CNN and pre-training with large-scale datasets (e.g., ImageNet <ref type="bibr" target="#b36">[37]</ref>). <ref type="figure" target="#fig_2">Fig. 4</ref> shows some retrieval examples. Shape part segmentation. Part segmentation is a challenging task for fine-grained shape recognition. Here we evaluate DensePoint on ShapeNet part benchmark <ref type="bibr" target="#b57">[57]</ref>. It contains 16881 shapes with 16 categories, and is labeled in 50 parts in total, where each shape has 2∼5 parts. We follow the data split in <ref type="bibr" target="#b31">[32]</ref>, and similarly, we also randomly pick 2048 points as the input and concatenate the one-hot encoding of the object label to the last feature layer of the segmentation network in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>. In testing, we also apply voting with ten tests using random scaling. Except for standard IoU (Inter-over-Union) score for each category, two types of mean IoU (mIoU) that are averaged across all classes and all instances respectively, are also reported. <ref type="table" target="#tab_3">Table 3</ref> summarizes the quantitative comparisons with the state-of-the-art methods, where DensePoint achieves the best performance. Furthermore, it significantly surpasses the second best point-input methods, i.e., DGCNN <ref type="bibr" target="#b52">[52]</ref>, with 1.9↑ in class mIoU and 1.3↑ in instance mIoU respectively. Noticeably, it also sets new state of the arts over the point-based methods in eight categories. These improvements demonstrate the robustness of DensePoint to diverse  shapes. Some segmentation examples are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Normal estimation. Normal estimation in point cloud is a crucial step for numerous applications, from surface reconstruction and scene understanding to rendering. Here, we regard normal estimation as a supervised regression task, and implement it by deploying DensePoint with the segmentation network in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>. The cosine-loss between the normalized output and the normal ground truth is employed for training. We evaluate DensePoint on Model-Net40 benchmark for this task, where 1024 points are uniformly sampled as the input. The quantitative comparisons of the estimation error are summarized in <ref type="table" target="#tab_4">Table 4</ref>, where DensePoint outperforms other advanced methods. Moreover, it significantly reduces the error of PointNet++ by 48.6%. <ref type="figure" target="#fig_5">Fig. 6</ref> shows some normal prediction examples. As can be seen, DensePoint with densely contextual semantics can obtain more decent normal predictions, while PointNet and PointNet++ present a lot of deviations above 90 • from the ground truth. However, in this task, DensePoint can not process some intricate shapes well, e.g., curtains and plants.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DensePoint Analysis</head><p>In this section, we first perform a detailed ablation study for DensePoint. Then, we discuss the group number N g in ePConv (Eq. (4)), the network narrowness k, the aggregation function ρ and the network stage to apply DensePoint, respectively. Finally, we analyze the robustness of Dense-Point on sampling density and random noise, and investigate the model complexity. All the experiments are conducted on ModelNet40 <ref type="bibr" target="#b53">[53]</ref> dataset.</p><p>Ablation study. The results of ablation study are summarized in <ref type="table" target="#tab_5">Table 5</ref>. We set two baselines: model A and model A. Model A is set as a classic hierarchical version (the upper part of <ref type="figure" target="#fig_0">Fig. 2</ref>, i.e., layer-by-layer connections without contextual learning by DensePoint) of the classification network with the same number of layers, and each layer is configured with the same width. Model A directly concatenates all layers in each stage of model A as the final output of that stage. Both of them are equipped with PConv in Eq. (1).</p><p>The baseline model A gets a low classification accuracy of 88.6%, and it increases by only 0.5 percent with direct concatenation (model A). However, with the densely contextual semantics of DensePoint, the accuracy raises significantly by 2.5 percent (91.1%, model B). This convincingly verifies its effectiveness. Then, when using ePConv to enhance the expressive power of each layer in DensePoint, the accuracy can be further improved to 92.5% (model C). Noticeably, the dropout on f N (x) in Eq. (4) can bring a boost of 0.3 percent (model D). The data augmentation technique can result in an accuracy variation of 0.7 percent (model E).  Finally, by voting strategy, our final model F can achieve an impressive accuracy of 93.2%. In addition, we also investigate the number of input points by increasing it to 2k, yet obtaining no gain (model G). Maybe the model needs to be modified to adapt for more input points.</p><p>Group number N g in ePConv <ref type="figure" target="#fig_2">(Eq. (4)</ref>). The filter grouping can greatly reduce the model complexity, whilst leading to a model regularization by rarefying the filter relationships <ref type="bibr" target="#b15">[16]</ref>. <ref type="table" target="#tab_6">Table 6</ref> summarizes the impact of N g on model parameters, model FLOPs (floating point operations/sample) and classification accuracy. As can be seen, the model parameters are very few (0.73M), even though the filter grouping is not performed. This is due to the narrow design (k = 24) of each layer in DensePoint and few parameters in the generalized convolution operator, ePConv. Eventually, with N g = 2, DensePoint can achieve the best result of 93.2% with acceptable model complexity.</p><p>Network narrowness k. <ref type="table" target="#tab_7">Table 7</ref> summarizes the comparisons of different k. One can see that a very small Dense-Point, i.e., k = 12, can even obtain an impressive accuracy of 92.1%. This further verifies the powerfulness of the densely contextual semantics acquired by DensePoint on shape identification. Note that a large k is usually unnecessary for DensePoint, as it will greatly raise the model complexity but not bring any gains.</p><p>Aggregation function ρ. We experiment with three symmetric functions, i.e., sum, average pooling and max pooling, whose results are 91.0%, 91.3% and 93.2%, respectively. The max pooling performs best, probably because it can select the biggest feature response to keep the most expressive representation and remove redundant information.</p><p>Network stage to apply DensePoint. To investigate the   <ref type="table">Table 8</ref>. The comparisons of DensePoint applied in different stages of the classification network ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref> impact of contextual semantics at different levels on shape recognition, we also apply DensePoint with ePConv in different stages of the classification network ( <ref type="figure" target="#fig_1">Fig. 3(a)</ref>). The results are summarized in <ref type="table">Table 8</ref>. The baseline (model A) is set as the same as the model A in <ref type="table" target="#tab_5">Table 5</ref> but equipped with ePConv for a fair comparison. One can see that Dense-Point applied in the 1 st stage (model B) and the 2 nd stage (model C) can both bring a considerable boost, while the latter performs better. This indicates the higher-level contextual semantics in the 2 nd stage can result in a more powerful representation for shape recognition. Finally, with DensePoint in each stage for sufficiently contextual semantic information, the best result of 93.2% can be reached.</p><p>Robustness analysis. The robustness of DensePoint on sampling density and random noise are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. For the former, we use sparser points of 1024, 512, 256, 128 and 64, as the input to a model trained with 1024 points. Random input dropout is applied during training, for fair comparisons with PointNet <ref type="bibr" target="#b31">[32]</ref>, PointNet++ <ref type="bibr" target="#b33">[34]</ref>, SO-Net <ref type="bibr" target="#b23">[24]</ref>, PCNN <ref type="bibr" target="#b0">[1]</ref> and DGCNN <ref type="bibr" target="#b52">[52]</ref>. <ref type="figure" target="#fig_6">Fig. 7(b)</ref> shows that our model and PointNet++ perform better in this testing. Nevertheless, our model can obtain higher accuracy than Point- <ref type="table">Table 9</ref>. The comparisons of model complexity ("-": unknown). method #params #FLOPs/sample PointNet <ref type="bibr" target="#b31">[32]</ref> 3.50M 440M PointNet++ <ref type="bibr" target="#b25">[26]</ref> 1.48M 1684M DGCNN <ref type="bibr" target="#b25">[26]</ref> 1.84M 2767M SpecGCN <ref type="bibr" target="#b25">[26]</ref> 2.05M 1112M KCNet <ref type="bibr" target="#b39">[39]</ref> 0.90M -PCNN <ref type="bibr" target="#b25">[26]</ref> 8.20M 294M PointCNN <ref type="bibr" target="#b25">[26]</ref> 0.60M 1581M Ours (k = 24, L = 11) 0.67M 651M Ours (k = 24, L = 6) 0.53M 148M Net++ at all densities. This indicates the densely contextual semantics of DensePoint, is much more effective than the traditional multi-scale information of PointNet++. For the latter, as in KCNet <ref type="bibr" target="#b39">[39]</ref>, we replace a certain number of randomly picked points with uniform noise ranging [-1.0, 1.0] during testing. The comparisons with Point-Net, PointNet++ and KCNet are shown in <ref type="figure" target="#fig_6">Fig. 7(d)</ref>. Note that for this testing, our model is trained without any data augmentations to avoid confusion. As can be seen, our model is quite robust on random noise, while the others are vulnerable. This demonstrates the powerfulness of densely contextual semantics in DensePoint.</p><p>Model complexity. The comparisons of model complexity with the state of the arts are summarized in <ref type="table">Table 9</ref>. As can be seen, our model is quite competitive and it can be the most efficient one with the network depth L = 6 (accuracy 92.1%). This shows its great potential for real-time applications, e.g., scene parsing in autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of limitations.</head><p>(1) The density of local point clouds is not considered, which could lead to less effectiveness in greatly non-uniform distribution; (2) The importance of each level of context is not evaluated, which could lead to the difficulty in identifying very alike shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, DensePoint, a general architecture to learn densely contextual representation for efficient point cloud processing, has been proposed. DensePoint extends regular grid CNN to irregular point configuration by an efficient generalized convolution operator. Based on this operator, DensePoint develops a deep hierarchy and progressively aggregate multi-level and multi-scale semantics from it. As a consequence, DensePoint can acquire sufficiently contextual information along with rich semantics in an organic manner, making it highly effective for implicit shape identification. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, have demonstrated that DensePoint achieves the state of the arts. In addition, DensePoint shows quite good robustness against noisy points, which could provide a promising direction for robust point cloud representation learning.</p><p>In this section, we provide further investigations of DensePoint on four aspects. Specifically, the discussion of neighborhood method is presented in Sec B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Neighborhood Method</head><p>In the main paper, the local convolutional neighborhood N (x) in Eq. (1) is set to be a spherical neighborhood, from which a fixed number of neighbors are randomly sampled for batch processing. We compare this strategy (Random-In-Sphere) with another typical one, i.e., k-nearest neighbor (k-NN). For a fair comparison, the models with these two strategies are configured with the same settings. <ref type="table" target="#tab_9">Table I  summarizes the results.</ref> As can be seen, the model with Random-In-Sphere performs better. We speculate that the model with k-NN will suffer from the distribution inhomogeneity of points. In this case, the contextual learning in DensePoint will be less effective, as the receptive fields will be confined to a local region with large density, which leads to ignoring those sparse points that are essential for recognizing the implicit shape. By contrast, Random-In-Sphere can have a better coverage of points even in the case of inhomogeneous distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Dropout on</head><formula xml:id="formula_6">f N (x) in Eq. (4)</formula><p>The dropout technique can force the whole network to behave as an ensemble of a lot of subsets and reduce the risk of model overfitting. To analyze its effect on DensePoint, we apply it with different ratios on f N (x) in Eq. (4). The results are summarized in <ref type="table" target="#tab_9">Table II</ref>. As can be seen, the best result of 93.2% can be achieved with a dropout ratio of 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Network Depth</head><p>We further explore the impact of the network depth (fully connected layers are not included) on classification performance. The results are summarized in <ref type="table" target="#tab_9">Table III</ref>. Surprisingly, a 6-layer network equipped with DensePoint can achieve an accuracy of 92.1% with only 0.53M params and 148M FLOPs/sample. This even outperforms PointNet++  <ref type="bibr" target="#b32">[33]</ref> (accuracy 90.7%, params 1.48M <ref type="bibr" target="#b25">[26]</ref>, FLOPs/sample 1684M <ref type="bibr" target="#b25">[26]</ref>) by 15% in error rate, whilst being one order of magnitude faster in terms of FLOPs/sample. We also observe that it is unnecessary to develop a very deep network (e.g., 23 layers) with DensePoint, as it increases complexity without bringing any gain. Eventually, the best result of 93.2% can be reached with acceptable complexity by an 11-layer network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Memory and runtime</head><p>The memory and runtime of the proposed DensePoint are summarized in <ref type="table" target="#tab_9">Table IV</ref>. As can be seen, the model (L=11) is competitive while another model (L=6) is the best one in terms of efficiency. Actually, the memory and training time issues in dense connection mode are greatly alleviated due to the shallow design of DensePoint and our highly-efficient implementation. Moreover, although extremely deep network could be unnecessary for 3D currently, in case of very deep DensePoint in the future, the technique of Shared Memory Allocations can be applied to achieve linear memory complexity.</p><p>In this section, we show more shape retrieval examples in <ref type="figure" target="#fig_8">Fig. 8</ref>. As can be seen, compared with PointNet <ref type="bibr" target="#b30">[31]</ref>, our DensePoint obtains superior shape identification results. Specifically, PointNet is confused between the query "bottle" and the sample "vase" due to their similar shapes. Nevertheless, DensePoint with densely contextual semantics acquired can identify them accurately. We notice that Dense-Point could also be confused for some very alike shapes, e.g., the query "bench" and the sample "tv stand". This could be improved by learning to weight multi-level contextual information instead of identically aggregating all levels of information. We leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Configuration Details</head><p>In this section, we present the configuration details of three networks on shape classification, shape part segmentation and normal estimation, respectively. For clearness, we describe the layer and corresponding setting format as follows:</p><p>PPool: [downsampling rate, neighborhood radius, #number of neighbors, SLP φ (#input channels, #output channels)]. The global pooling is achieved by directly applying PConv to convolve all points. ePConv: [neighborhood radius, #number of neighbors, SLP φ (#input channels, #output channels, #group number), SLP ψ (#input channels, #output channels), dropout ratio].</p><p>FP (feature propagation layer): MLP(#channels, · · · ). Feature propagation layer <ref type="bibr" target="#b32">[33]</ref> is used for transforming the features that are concatenated from current interpolated layer and long-range connected layer. We employ a multilayer perceptron (MLP) to implement this transformation.</p><p>FC (fully connected layer): [(#input channels, #output channels), dropout ratio]. Note that the dropout technique is applied for all FC layers except for the last FC layer (used for prediction).</p><p>In addition, except for the last prediction layer, all layers (including the inside perceptrons) are followed with batch normalization and ReLU activator. The output shape is in the format of (#feature dimension, #number of points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Shape Classification Network</head><p>The configuration details of shape classification network are presented in <ref type="table" target="#tab_9">Table VI</ref>. The network has 14 layers in total, which comprises 3 PPools (the last one is global pooling layer) and 2 DensePoints (the 1 st one has 3 layers while the 2 nd one has 5 layers), followed by 3 FC layers. <ref type="table">Table V</ref> summarizes the configuration details of shape part segmentation network. As it shows, the network has 23 layers in total, which comprises 4 PPools, 3 DensePoints (4 layers, 6 layers and 3 layers in 2 nd stage, 3 rd stage and 4 th stage respectively) and 4 FP layers, followed by 2 FC layers. As in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref>, we concatenate the one-hot encoding <ref type="bibr">(16-d)</ref> of the object label to the last feature layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Shape Part Segmentation Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Normal Estimation Network</head><p>The normal estimation network is presented in <ref type="table" target="#tab_9">Table VII</ref>. It is almost the same as the segmentation network, except for three aspects: (1) the input becomes 1024-d and the onehot encoding becomes 40-d for ModelNet40 dataset; (2) the settings of some layers are slightly changed to be consistent with the 1024-d input; (3) the final output becomes 3-d for normal prediction. As done in the segmentation network, we also concatenate the one-hot encoding (40-d) of the object label to the last feature layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Details</head><p>Our DensePoint is implemented using Pytorch. The Adam optimization algorithm is employed for training, with a mini-batch size of 32. The momentum for batch normalization starts with 0.9 and decays with a rate of 0.5 every 20 epochs. The learning rate begins with 0.001 and decays with a rate of 0.7 every 20 epochs. The weight is initialized using the techniques introduced by He et al. <ref type="bibr" target="#b7">[8]</ref>. <ref type="table">Table V</ref>. The configuration details of shape part segmentation network. "long-range" indicates the long-range connections (see <ref type="figure" target="#fig_1">Fig. 3(b</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The illustration of DensePoint. It extends regular grid CNN to irregular point configuration by an efficient generalized convolution operator (PConv in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>DensePoint applied in point cloud classification (a) and per-point analysis (b). PPool: pooling on point cloud (Sec 3.1). N is the number of points. The stage means several successive layers with the same number of points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Retrieval examples on ModelNet40. Top-10 matches are shown for each query, with the 1 st line for PointNet<ref type="bibr" target="#b31">[32]</ref> and the 2 nd line for our DensePoint. The mistakes are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Segmentation examples on ShapeNet part benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Normal estimation examples on ModelNet40 benchmark. For visual clearness, we only show the predictions with the angle less than 30 • in blue, and the angle greater than 90 • in red between the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>(a) Point cloud with different sampling densities. (b) Results of testing with sparser points. (c) Point cloud with some points being replaced with random noise (highlighted in red). (d) Results of testing with noisy points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1. The effect of dropout on f N (x) in Eq. (4) is analyzed in Sec B.2. The impact of network depth on classification performance is investigated in Sec B.3. The memory and runtime are summarized in Sec B.4. All the investigations are conducted on ModelNet40 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Retrieval examples on ModelNet40 dataset. Top-10 matches are shown for each query, with the 1 st line for PointNet<ref type="bibr" target="#b30">[31]</ref> and the 2 nd line for our DensePoint. The mistakes are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>DensePoint forward pass algorithm Input: point cloud P; input features {f x, ∀x ∈ P}; depth L; weight W φ , W ψ and bias b φ , b ψ for SLP φ and SLP ψ in Eq. (4), ∀ ∈ {1, ..., L}; non-linearity σ; aggregation function ρ; neighborhood method N Output: densely contextual representations {rx, ∀x ∈ P}</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2</head><label>12</label><figDesc>Shape classification results (overall accuracy, %) on ModelNet40 (M40) and ModelNet10 (M10) benchmarks (pnt: point coordinates, nor: normal, "-": unknown).</figDesc><table><row><cell>method</cell><cell></cell><cell>input</cell><cell cols="3">#points M40 M10</cell></row><row><cell cols="2">Pointwise-CNN [12]</cell><cell>pnt</cell><cell>1k</cell><cell>86.1</cell><cell>-</cell></row><row><cell cols="2">Deep Sets [60]</cell><cell>pnt</cell><cell>1k</cell><cell>87.1</cell><cell>-</cell></row><row><cell cols="2">ECC [40]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">87.4 90.8</cell></row><row><cell cols="2">PointNet [32]</cell><cell>pnt</cell><cell>1k</cell><cell>89.2</cell><cell>-</cell></row><row><cell cols="2">SCN [55]</cell><cell>pnt</cell><cell>1k</cell><cell>90.0</cell><cell>-</cell></row><row><cell cols="2">Kd-Net(depth=10) [21]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">90.6 93.3</cell></row><row><cell cols="2">PointNet++ [34]</cell><cell>pnt</cell><cell>1k</cell><cell>90.7</cell><cell>-</cell></row><row><cell cols="2">MC-Conv [11]</cell><cell>pnt</cell><cell>1k</cell><cell>90.9</cell><cell>-</cell></row><row><cell cols="2">KCNet [39]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">91.0 94.4</cell></row><row><cell cols="2">MRTNet [4]</cell><cell>pnt</cell><cell>1k</cell><cell>91.2</cell><cell>-</cell></row><row><cell cols="2">Spec-GCN [49]</cell><cell>pnt</cell><cell>1k</cell><cell>91.5</cell><cell>-</cell></row><row><cell cols="2">DGCNN [52]</cell><cell>pnt</cell><cell>1k</cell><cell>92.2</cell><cell>-</cell></row><row><cell cols="2">PointCNN [26]</cell><cell>pnt</cell><cell>1k</cell><cell>92.2</cell><cell>-</cell></row><row><cell cols="2">PCNN [1]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">92.3 94.9</cell></row><row><cell>Ours</cell><cell></cell><cell>pnt</cell><cell>1k</cell><cell cols="2">93.2 96.6</cell></row><row><cell cols="2">SO-Net [24]</cell><cell>pnt</cell><cell>2k</cell><cell cols="2">90.9 94.1</cell></row><row><cell cols="2">Kd-Net(depth=15) [21]</cell><cell>pnt</cell><cell>32k</cell><cell cols="2">91.8 94.0</cell></row><row><cell cols="2">O-CNN [50]</cell><cell>pnt, nor</cell><cell>-</cell><cell>90.6</cell><cell>-</cell></row><row><cell cols="2">Spec-GCN [49]</cell><cell>pnt, nor</cell><cell>1k</cell><cell>91.8</cell><cell>-</cell></row><row><cell cols="2">PointNet++ [34]</cell><cell>pnt, nor</cell><cell>5k</cell><cell>91.9</cell><cell>-</cell></row><row><cell cols="2">SpiderCNN [56]</cell><cell>pnt, nor</cell><cell>5k</cell><cell>92.4</cell><cell>-</cell></row><row><cell cols="2">SO-Net [24]</cell><cell>pnt, nor</cell><cell>5k</cell><cell cols="2">93.4 95.7</cell></row><row><cell>input</cell><cell>method</cell><cell></cell><cell cols="3">#points/views M40 M10</cell></row><row><cell>Points</cell><cell>PointNet [10] DGCNN [52]</cell><cell></cell><cell>1k 1k</cell><cell>70.5 85.3</cell><cell>--</cell></row><row><cell></cell><cell>PointCNN [26]</cell><cell></cell><cell>1k</cell><cell>83.8</cell><cell>-</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>1k</cell><cell>88.5</cell><cell>93.2</cell></row><row><cell></cell><cell>GVCNN [3]</cell><cell></cell><cell>12</cell><cell>85.7</cell><cell>-</cell></row><row><cell>Images</cell><cell cols="2">Triplet-center [10] PANORAMA-ENN [38]</cell><cell>12 -</cell><cell>88.0 86.3</cell><cell>-93.3</cell></row><row><cell></cell><cell>SeqViews [7]</cell><cell></cell><cell>12</cell><cell>89.1</cell><cell>89.5</cell></row></table><note>. Shape retrieval results (mAP, %) on ModelNet40 (M40) and ModelNet10 (M10) benchmarks ("-": unknown).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Shape part segmentation results (%) on ShapeNet part benchmark (nor: normal, "-": unknown). 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3 PointNet [32] 2k 80.4 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 SCN [55] 1k 81.8 84.6 83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.8 SPLATNet [43] -82.0 84.6 81.9 83.9 88.6 79.5 90.1 73.5 91.3 84.7 84.5 96.3 69.7 95.0 81.7 59.2 70.4 81.3</figDesc><table><row><cell>method</cell><cell>input</cell><cell>class</cell><cell>instance</cell><cell>air</cell><cell>bag cap car chair ear</cell><cell>guitar knife lamp laptop motor</cell><cell>mug pistol rocket skate</cell><cell>table</cell></row><row><cell></cell><cell></cell><cell>mIoU</cell><cell>mIoU</cell><cell>plane</cell><cell>phone</cell><cell>bike</cell><cell>board</cell><cell></cell></row><row><cell cols="9">Kd-Net [21] 80.1 KCNet [39] 4k 77.4 82.3 2k 82.2 84.7 82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.2 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3</cell></row><row><cell>RS-Net [15]</cell><cell>-</cell><cell>81.4</cell><cell>84.9</cell><cell cols="5">82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2</cell></row><row><cell>DGCNN [52]</cell><cell>2k</cell><cell>82.3</cell><cell>85.1</cell><cell cols="5">84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0</cell></row><row><cell>PCNN [1]</cell><cell>2k</cell><cell>81.8</cell><cell>85.1</cell><cell cols="5">82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8</cell></row><row><cell>Ours</cell><cell>2k</cell><cell>84.2</cell><cell>86.4</cell><cell cols="5">84.0 85.4 90.0 79.2 91.1 81.6 91.5 87.5 84.7 95.9 74.3 94.6 82.9 64.6 76.8 83.7</cell></row><row><cell>SO-Net [24]</cell><cell>-,nor</cell><cell>80.8</cell><cell>84.6</cell><cell cols="5">81.9 83.5 84.8 78.1 90.8 72.2 90.1 83.6 82.3 95.2 69.3 94.2 80.0 51.6 72.1 82.6</cell></row><row><cell>SyncCNN [58]</cell><cell>mesh</cell><cell>82.0</cell><cell>84.7</cell><cell cols="5">81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1</cell></row><row><cell>PointNet++ [34]</cell><cell cols="2">2k,nor 81.9</cell><cell>85.1</cell><cell cols="5">82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell>SpiderCNN [56]</cell><cell cols="2">2k,nor 82.4</cell><cell>85.3</cell><cell cols="5">83.5 81.0 87.2 77.5 90.7 76.8 91.1 87.3 83.3 95.8 70.2 93.5 82.7 59.7 75.8 82.8</cell></row><row><cell>Query</cell><cell></cell><cell cols="3">Top-10 retrieved CAD models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>piano</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>stool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Normal estimation error on ModelNet40 benchmark.</figDesc><table><row><cell>dataset</cell><cell>method</cell><cell cols="2">#points error</cell></row><row><cell cols="2">ModelNet40 PointNet [1]</cell><cell>1k</cell><cell>0.47</cell></row><row><cell></cell><cell>PointNet++ [1]</cell><cell>1k</cell><cell>0.29</cell></row><row><cell></cell><cell>PCNN [1]</cell><cell>1k</cell><cell>0.19</cell></row><row><cell></cell><cell>MC-Conv [11]</cell><cell>1k</cell><cell>0.16</cell></row><row><cell></cell><cell>Ours</cell><cell>1k</cell><cell>0.149</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of DensePoint (%) (DA: data augmentation, DP: DensePoint, DO: dropout on f N (x) in Eq.(4)).</figDesc><table><row><cell cols="2">model #points DA DP ePConv DO vote</cell><cell>acc.</cell></row><row><cell>A</cell><cell>1k</cell><cell>88.6</cell></row><row><cell>A</cell><cell>1k</cell><cell>89.1</cell></row><row><cell>B</cell><cell>1k</cell><cell>91.1</cell></row><row><cell>C</cell><cell>1k</cell><cell>92.5</cell></row><row><cell>D</cell><cell>1k</cell><cell>92.8</cell></row><row><cell>E</cell><cell>1k</cell><cell>92.1</cell></row><row><cell>F</cell><cell>1k</cell><cell>93.2</cell></row><row><cell>G</cell><cell>2k</cell><cell>93.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The impact of the group number Ng on network parameters, FLOPs and performance (k = 24).</figDesc><table><row><cell cols="4">group number Ng #params #FLOPs/sample acc. (%)</cell></row><row><cell>1</cell><cell>0.73M</cell><cell>1030M</cell><cell>92.7</cell></row><row><cell>2</cell><cell>0.67M</cell><cell>651M</cell><cell>93.2</cell></row><row><cell>4</cell><cell>0.62M</cell><cell>457M</cell><cell>92.2</cell></row><row><cell>6</cell><cell>0.61M</cell><cell>394M</cell><cell>92.3</cell></row><row><cell>12</cell><cell>0.60M</cell><cell>331M</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The comparisons of different narrowness k (Ng = 2).</figDesc><table><row><cell cols="4">narrowness k #params #FLOPs/sample acc. (%)</cell></row><row><cell>12</cell><cell>0.56M</cell><cell>294M</cell><cell>92.1</cell></row><row><cell>24</cell><cell>0.67M</cell><cell>651M</cell><cell>93.2</cell></row><row><cell>36</cell><cell>0.76M</cell><cell>957M</cell><cell>92.9</cell></row><row><cell>48</cell><cell>0.88M</cell><cell>1310M</cell><cell>92.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table I .</head><label>I</label><figDesc>The results (%) of two neighborhood strategies. The number of neighbors is equally set in each layer of the two models.Table IV. Time and memory of classification network, where k is network narrowness, L is network depth. The statistics of all the models are summarized with batch size 16 on NVIDIA TITAN Xp, and time is the mean time of 1000 tests. The compared models are tested using their available official codes.</figDesc><table><row><cell></cell><cell cols="5">neighborhood method acc.</cell><cell></cell></row><row><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell cols="2">91.3</cell><cell></cell></row><row><cell></cell><cell cols="3">Random-In-Sphere</cell><cell cols="2">93.2</cell><cell></cell></row><row><cell cols="7">Table II. The results (%) of dropout with different ratios applied</cell></row><row><cell cols="2">on f N (x) in Eq. (4).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ratio (%)</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>acc.</cell><cell cols="6">92.9 92.8 93.2 93.0 92.8 92.5</cell></row><row><cell cols="7">Table III. The results (%) of different network depths (fully con-</cell></row><row><cell cols="3">nected layers are not included).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">#layers #params #FLOPs/sample acc.</cell></row><row><cell>6</cell><cell cols="2">0.53M</cell><cell cols="2">148M</cell><cell cols="2">92.1</cell></row><row><cell>9</cell><cell cols="2">0.56M</cell><cell cols="2">510M</cell><cell cols="2">92.9</cell></row><row><cell>11</cell><cell cols="2">0.67M</cell><cell cols="2">651M</cell><cell cols="2">93.2</cell></row><row><cell>15</cell><cell cols="2">0.78M</cell><cell cols="2">779M</cell><cell cols="2">93.0</cell></row><row><cell>19</cell><cell cols="2">0.88M</cell><cell cols="2">1222M</cell><cell cols="2">92.7</cell></row><row><cell>23</cell><cell cols="2">1.03M</cell><cell cols="2">1416M</cell><cell cols="2">92.6</cell></row><row><cell>method</cell><cell></cell><cell>#points</cell><cell cols="2">Time (ms) training test</cell><cell cols="2">Memory (GB) training test</cell></row><row><cell>PointNet [31]</cell><cell></cell><cell>1024</cell><cell>55</cell><cell>22</cell><cell>1.318</cell><cell>0.469</cell></row><row><cell cols="2">PointNet++ [33]</cell><cell>1024</cell><cell>195</cell><cell>47</cell><cell>8.311</cell><cell>2.305</cell></row><row><cell>DGCNN [52]</cell><cell></cell><cell>1024</cell><cell>300</cell><cell>68</cell><cell>4.323</cell><cell>1.235</cell></row><row><cell>PointCNN [26]</cell><cell></cell><cell>1024</cell><cell>55</cell><cell>38</cell><cell>2.501</cell><cell>1.493</cell></row><row><cell cols="2">Ours (k=24, L=11)</cell><cell>1024</cell><cell>21</cell><cell>10</cell><cell>3.745</cell><cell>1.228</cell></row><row><cell cols="2">Ours (k=24, L=6)</cell><cell>1024</cell><cell>10</cell><cell>5</cell><cell>1.468</cell><cell>0.886</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>) in the main paper). K is the number of classes. This is the one-hot encoding of the object label on ShapeNet part dataset.</figDesc><table><row><cell cols="2">stage layer type</cell><cell>setting detail</cell><cell cols="2">output shape long-range</cell></row><row><cell>-</cell><cell>Input</cell><cell>-</cell><cell>(3, 2048)</cell><cell>FP 4</cell></row><row><cell>1</cell><cell>PPool</cell><cell>[1/2, 0.1, 32, (3, 64)]</cell><cell>(64, 1024)</cell><cell>FP 3</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.2, 64, (64, 128)]</cell><cell>(128, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (128, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell>2</cell><cell>ePConv</cell><cell>[0.3, 32, (152, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (176, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (200, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 2 nd stage</cell><cell>(224, 256)</cell><cell>FP 2</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.3, 32, (224, 192)]</cell><cell>(192, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (192, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (216, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell>3</cell><cell>ePConv</cell><cell>[0.5, 16, (240, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (264, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (288, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (312, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 3 rd stage</cell><cell>(336, 64)</cell><cell>FP 1</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.8, 32, (336, 360)]</cell><cell>(360, 16)</cell><cell></cell></row><row><cell>4</cell><cell>ePConv ePConv</cell><cell>[0.8, 8, (360, 96, 2), (96, 24), 20%] [0.8, 8, (384, 96, 2), (96, 24), 20%]</cell><cell>(24, 16) (24, 16)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.8, 8, (408, 96, 2), (96, 24), 20%]</cell><cell>(24, 16)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 4 th stage</cell><cell>(432, 16)</cell><cell></cell></row><row><cell></cell><cell>FP 1</cell><cell>(768, 512, 512)</cell><cell>(512, 64)</cell><cell></cell></row><row><cell></cell><cell>FP 2</cell><cell>(736, 384, 384)</cell><cell>(384, 256)</cell><cell></cell></row><row><cell></cell><cell>FP 3</cell><cell>(448, 256, 256)</cell><cell>(256, 1024)</cell><cell></cell></row><row><cell></cell><cell>FP 4</cell><cell>(259, 128, 128)</cell><cell>(128, 2048)</cell><cell></cell></row><row><cell></cell><cell>FC</cell><cell>[(128+16 1 , 128), 50%]</cell><cell>(128, 2048)</cell><cell></cell></row><row><cell></cell><cell>FC</cell><cell>[(128, K), -] → softmax</cell><cell>(K, 2048)</cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table VI .</head><label>VI</label><figDesc>The configuration details of shape classification network. K is the number of classes.</figDesc><table><row><cell cols="2">stage layer type</cell><cell>setting detail</cell><cell>output shape</cell></row><row><cell>-</cell><cell>Input</cell><cell>-</cell><cell>(3, 1024)</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/2, 0.25, 64, (3, 96)]</cell><cell>(96, 512)</cell></row><row><cell>1</cell><cell>ePConv ePConv</cell><cell>[0.2, 32, (96, 96, 2), (96, 24), 20%] [0.2, 32, (120, 96, 2), (96, 24), 20%]</cell><cell>(24, 512) (24, 512)</cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.2, 32, (144, 96, 2), (96, 24), 20%]</cell><cell>(24, 512)</cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 1 st stage</cell><cell>(168, 512)</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.3, 64, (168, 144)]</cell><cell>(144, 128)</cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.4, 16, (144, 96, 2), (96, 24), 20%]</cell><cell>(24, 128)</cell></row><row><cell>2</cell><cell>ePConv ePConv</cell><cell>[0.4, 16, (168, 96, 2), (96, 24), 20%] [0.4, 16, (192, 96, 2), (96, 24), 20%]</cell><cell>(24, 128) (24, 128)</cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.4, 16, (216, 96, 2), (96, 24), 20%]</cell><cell>(24, 128)</cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.4, 16, (240, 96, 2), (96, 24), 20%]</cell><cell>(24, 128)</cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 2 nd stage</cell><cell>(264, 128)</cell></row><row><cell></cell><cell>PPool</cell><cell>[-, -, 128, (264, 512)]</cell><cell>(512, )</cell></row><row><cell>3</cell><cell>FC FC</cell><cell>[(512, 512), 50%] [(512, 256), 50%]</cell><cell>(512, ) (256, )</cell></row><row><cell></cell><cell>FC</cell><cell>[(256, K), -] → softmax</cell><cell>(K, )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table VII .</head><label>VII</label><figDesc>The configuration details of normal estimation network. "long-range" indicates the long-range connections (seeFig. 3(b) in the main paper).</figDesc><table><row><cell cols="2">stage layer type</cell><cell>setting detail</cell><cell cols="2">output shape long-range</cell></row><row><cell>-</cell><cell>Input</cell><cell>-</cell><cell>(3, 1024)</cell><cell>FP 4</cell></row><row><cell>1</cell><cell>PPool</cell><cell>[1, 0.2, 32, (3, 64)]</cell><cell>(64, 1024)</cell><cell>FP 3</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.2, 32, (64, 128)]</cell><cell>(128, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (128, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell>2</cell><cell>ePConv</cell><cell>[0.3, 32, (152, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (176, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.3, 32, (200, 96, 2), (96, 24), 20%]</cell><cell>(24, 256)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 2 nd stage</cell><cell>(224, 256)</cell><cell>FP 2</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.3, 32, (224, 192)]</cell><cell>(192, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (192, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (216, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell>3</cell><cell>ePConv</cell><cell>[0.5, 16, (240, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (264, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (288, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.5, 16, (312, 96, 2), (96, 24), 20%]</cell><cell>(24, 64)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 3 rd stage</cell><cell>(336, 64)</cell><cell>FP 1</cell></row><row><cell></cell><cell>PPool</cell><cell>[1/4, 0.8, 32, (336, 360)]</cell><cell>(360, 16)</cell><cell></cell></row><row><cell>4</cell><cell>ePConv ePConv</cell><cell>[0.8, 8, (360, 96, 2), (96, 24), 20%] [0.8, 8, (384, 96, 2), (96, 24), 20%]</cell><cell>(24, 16) (24, 16)</cell><cell></cell></row><row><cell></cell><cell>ePConv</cell><cell>[0.8, 8, (408, 96, 2), (96, 24), 20%]</cell><cell>(24, 16)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The output of DensePoint in 4 th stage</cell><cell>(432, 16)</cell><cell></cell></row><row><cell></cell><cell>FP 1</cell><cell>(768, 512, 512)</cell><cell>(512, 64)</cell><cell></cell></row><row><cell></cell><cell>FP 2</cell><cell>(736, 384, 384)</cell><cell>(384, 256)</cell><cell></cell></row><row><cell></cell><cell>FP 3</cell><cell>(448, 256, 256)</cell><cell>(256, 1024)</cell><cell></cell></row><row><cell></cell><cell>FP 4</cell><cell>(259, 128, 128)</cell><cell>(128, 1024)</cell><cell></cell></row><row><cell></cell><cell>FC</cell><cell>[(128+40 2 , 128), 50%]</cell><cell>(128, 1024)</cell><cell></cell></row><row><cell></cell><cell>FC</cell><cell>[(128, 3), -]</cell><cell>(3, 1024)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Yochengliu/DensePoint</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is the one-hot encoding of the object label on ModelNet40 dataset.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline</head><p>This supplementary material provides: (1) further investigations of the proposed DensePoint (Sec B); (2) more shape retrieval examples of DensePoint and some analysis (Sec C); (3) network configuration details (Sec D). (4) training details (Sec E)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GVCNN: Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3D point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PCPNet: Learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3D object retrieval with deep embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SeqViews2SeqLabels: Learning 3D global features via aggregating sequential views by RNN with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Man</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Philip Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet-Center loss for multi-view 3D object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Pau</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvar</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ropinski</surname></persName>
		</author>
		<idno>235:1-235:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning local shape descriptors from part correspondences with multiview convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<idno>6:1-6:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep roots: Improving CNN efficiency with hierarchical filter groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yani</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><forename type="middle">P</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3D point clouds with object affordance for robot manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Inkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><forename type="middle">S</forename><surname>Sukhatme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5578" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep Kd-Networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SO-Net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bin Fan, Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Hao Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6620" to="6629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">Konstantinos Sfikas, Ioannis Pratikakis, and Theoharis Theoharis. Ensemble of PANORAMA-based convolutional neural networks for 3D model classification and retrieval. Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RGCNN: Regularized graph CNN for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gusi</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">O-CNN: octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno>72:1-72:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive O-CNN: a patch-based deep representation of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno>217:1-217:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DeepShape: Deep-learned shape descriptor for 3D shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1335" to="1345" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attentional ShapeContextNet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SpiderCNN: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>210:1-210:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sync-SpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">P2P-NET: bidirectional point displacement net for shape transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao (richard)</forename><surname>Zhang</surname></persName>
		</author>
		<idno>152:1-152:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
