<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Neural Architecture Search Image Classifiers via Ensemble Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Macko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Weill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Mazzawi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gonzalvo</surname></persName>
						</author>
						<title level="a" type="main">Improving Neural Architecture Search Image Classifiers via Ensemble Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding the best neural network architecture requires significant time, resources, and human expertise. These challenges are partially addressed by neural architecture search (NAS) which is able to find the best convolutional layer or cell that is then used as a building block for the network. However, once a good building block is found, manual design is still required to assemble the final architecture as a combination of multiple blocks under a predefined parameter budget constraint. A common solution is to stack these blocks into a single tower and adjust the width and depth to fill the parameter budget. However, these single tower architectures may not be optimal. Instead, in this paper we present the AdaNAS algorithm, that uses ensemble techniques to compose a neural network as an ensemble of smaller networks automatically. Additionally, we introduce a novel technique based on knowledge distillation to iteratively train the smaller networks using the previous ensemble as a teacher. Our experiments demonstrate that ensembles of networks improve accuracy upon a single neural network while keeping the same number of parameters. Our models achieve comparable results with the state-of-theart on CIFAR-10 and sets a new state-of-the-art on CIFAR-100.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing neural network (NN) architectures is often a demanding process. It often requires significant time, resources, and human expertise. These challenges are partially addressed by neural architecture search (NAS), which is able to find the best convolutional layer or cell that is then used as a building block for the network <ref type="bibr" target="#b12">(Real et al., 2018;</ref> Equal contribution 1 Google Research, New York, NY, USA 2 Work done as a member of the Google AI Residency program (g.co/brainresidency). Correspondence to: Vladimir Macko &lt;vlejd@google.com&gt;, <ref type="bibr">Charles Weill &lt;weill@google.com&gt;. et al., 2017)</ref>. However, once a good building block is found, it is still required to manually design the final architecture as a combination of multiple blocks. Moreover, there is usually a need to design multiple architectures with different parameter budgets, as different applications might pose different hardware constraints on memory and computation.</p><p>The critical question is how to upscale a small building block into a large architecture? A common solution is to stack those blocks into a single tower and adjust the width and depth to fill the parameter budget. The solution of having one tower is common also for architectures that are not the result of neural architecture search <ref type="bibr" target="#b13">(Springenberg et al., 2014;</ref><ref type="bibr" target="#b14">Szegedy et al., 2015;</ref><ref type="bibr" target="#b8">He et al., 2016)</ref>.</p><p>Recently, it was proposed to construct the network as an ensemble of smaller networks trained in a special way <ref type="bibr" target="#b5">(Dutt et al., 2018)</ref>. Ensembles of neural networks are known to be much more robust and accurate than individual subnetworks. Ensembles perform well on a wide variety of tasks <ref type="bibr" target="#b1">(Caruana et al., 2004)</ref> and are frequently used in the winning solutions of machine learning competitions (e.g. Kaggle) often consist of ensembles of multiple models. Unlike a single large neural network, an ensemble's size is not bounded by training, since each of the component subnetworks can be trained independently, and their outputs computed in parallel. Ultimately the final ensemble's size is bounded by its ability to fit on a serving hardware, and latency constraints.</p><p>The main questions we tackle in this paper are the following: Can ensembles perform better than a single tower model with the same number of parameters? Can we benefit from sequentially training the component subnetworks, and leverage information acquired from previously trained networks to improve the final ensemble performance? Is it possible to construct ensemble architectures automatically or with minimal human expertise?</p><p>In this work, we present a new paradigm to automatically generate ensembles of subnetworks that achieve high accuracy given a fixed parameter budget. Our AdaNAS algorithm works in an iterative manner, and it increases the size of each new subnetwork at each iteration until the ensemble hits the budget limit.</p><p>As we iteratively learn the composition of the ensemble, we arXiv:1903.06236v1 <ref type="bibr">[cs.</ref>LG] 14 Mar 2019 leverage information learned from subnetworks trained in previous iterations. We explore the effects of using ideas from Born Again Networks (BAN) <ref type="bibr" target="#b6">(Furlanello et al., 2018)</ref> to the final ensemble performance. In addition, we introduce a novel technique called Adaptive Knowledge Distillation (AKD) that extends Born Again Networks to use the previous iteration's ensemble as a teacher to assist in training the current iteration's subnetworks.</p><p>Resulting models are comprised of multiple separate towers that can be easily parallelized at inference time. Our presented technique requires minimal hyperparameter tuning to achieve these results. Our experiments demonstrate that ensembles of subnetworks improve accuracy upon a single neural network with the same number of parameters. On CIFAR-10 our algorithm achieves error 2.26 and on CIFAR-100 it achieves error 14.58. To our knowledge, and as we will show in Section 5, our technique achieves a new state-of-the-art on CIFAR-100 compared to methods that do not use additional regularization or data augmentation (e.g., ShakeDrop <ref type="bibr" target="#b16">(Yamada et al., 2018)</ref> or AutoAugment <ref type="bibr" target="#b3">(Cubuk et al., 2018)</ref>).</p><p>This paper has been implemented as an extension of a framework for the construction and search of boosted ensembles, AdaNet <ref type="bibr" target="#b2">(Cortes et al., 2016;</ref><ref type="bibr" target="#b15">Weill et al., 2018)</ref>. The code to reproduce our results is available in the AdaNet project repository 1 . Our implementation uses open-sourced code provided by <ref type="bibr" target="#b18">(Zoph et al., 2017)</ref> in the TensorFlow Models repository 2 . This paper is organized as follows. We review previous work in Section 2. In Section 3 we describe the ensembling algorithm. Experiment settings are outlined in Section 4 and Section 5 shows the final results. Finally, Section 6 discusses our proposed technique and our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The work by <ref type="bibr" target="#b5">(Dutt et al., 2018)</ref> showed a split into parallel branches. The main difference with previous ensembles was a tighter coupling of these branches by placing an averaging layer before the softmax layer. These coupled ensembles were shown to outperform simple ensembles. In the best configuration of the coupled ensemble setting two models are trained without sharing any parameters.</p><p>The main differences with our work are as follows. First, we train subnetworks sequentially. This allows us to use knowledge distillation techniques between iterations. Second, as we show in the following sections, one of our best configurations extends the average layer to become a weighted average of the different branches. Due to this extension, each subnetwork is trained sequentially and a new weight is applied to each of them. Third, we demonstrate that we can achieve better accuracy with more than two subnetworks.</p><p>Regarding the neural network design part of our work, the work presented by <ref type="bibr" target="#b17">(Zoph &amp; Le, 2016;</ref><ref type="bibr" target="#b18">Zoph et al., 2017)</ref> are the closest references. They search for an architectural building block on a small dataset (i.e., CIFAR-10) and then transfer the block to a larger dataset (e.g., Imagenet). Their main contribution is a new search space (i.e., NAS-Net) which produces a transferable cell architecture that improved upon the previous state-of-the-art. We use the cell found in the NASNet search space as our main building block in our experiments.</p><p>Model Compression <ref type="bibr" target="#b0">(Bucilu et al., 2006)</ref> and Knowledge Distillation <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> are techniques for transferring the predictive power from a large "teacher" model or ensemble into a smaller "student" model. The goal of the student model is to learn the predictions of the teacher network, optionally in addition to the ground truth labels. Recent work by <ref type="bibr" target="#b6">(Furlanello et al., 2018)</ref> has extended its application to be iterative, turning the student into the teacher at each iteration, and at the very end combining all the students's outputs to form an ensemble. Our work applies this technique, and extends it to use the ensemble of previous students as the teacher each iteration, instead of just the last student. Moreover, as mentioned earlier, we also explore ensembles with more than three students.</p><p>A related work that uses ensemble-learning for neural network design was presented by <ref type="bibr" target="#b2">(Cortes et al., 2016)</ref>. Similarly, in this paper we use multiple subnetworks whose outputs are combined via a learned weighted average. However, unlike AdaNet we use subnetworks composed of stacked NASNet blocks. Furthermore, we improve candidate subnetworks training by using knowledge distillation techniques. Finally, the combination of the subnetworks is unconstrained and unregularized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AdaNAS algorithm</head><p>In this section we describe our ensembling algorithm. First, we describe it at a high level, and later, we present concrete realizations of each of its parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General algorithm</head><p>We consider the standard supervised learning scenario and assume that training and test examples are drawn i.i.d. according to some distribution and denote by S = {(x 1 , y 1 ) . . . (x m , y m )} ⊆ X × Y a training set of size m. Additionally, for I ∈ Z, we denote by [I] the set {1, 2, . . . , I}. <ref type="figure">Figure 1</ref>. Illustration of the search process over four iterations. At each iteration, the search space is composed of three possible architectures to be explored (h</p><formula xml:id="formula_0">w (1) 4 w (2) 4 w (3) 4 w (4) 4 f 4 h 1 (1) h 1 (2) h 1 (3) h 2 (1) h 2 (2) h 2 (3) h 3 (1) h 3 (2) h 3 (3) h 4 (1) h 4 (2) h 4 (3) G( f ) 0 G( f ) 1 G( f ) 2 G( f ) 3</formula><formula xml:id="formula_1">(j) i , for the i-th iteration and the j-th candidate, where j ∈ [3]). The final ensemble is composed of the best subnetwork (h (2) 1 , h (1) 2 , h (1) 3 , h (3) 4 ) from each iteration. w 2 f (x) 4 h 1 h 2 h 3 h 4 x w 3 w 1 w 4 Figure 2.</formula><p>Illustration of the final ensemble. The output of each subnetwork hi is scaled by mixture weight wi and summed to form the final ensemble.</p><p>Our algorithm is iterative. It starts with an empty ensemble f 0 . In I iterations it generates I trained subnetworks h 1 , h 2 , . . . , h I that will form an ensemble f I . In each iteration i ∈ [I] it proposes a set of candidate subnetworks called candidate subnetworks</p><formula xml:id="formula_2">H i = {h (1) i , h<label>(2)</label></formula><p>i , . . .} (note that at this point, the candidate subnetworks are not trained and solely represent an architecture). By h (j) i we denote the j-th candidate subnetwork in generated in iteration i. Generator G is responsible for proposes candidate subnetworks based on the ensemble from the previous iteration, that is,</p><formula xml:id="formula_3">H i = G(f i−1 ).</formula><p>We denote by p i,j the set of parameters of the candidate subnetwork h (j) i . Given a set of real numbers θ ∈ R |pi,j | . We denote by h (j) i (θ) the resulting function when fixing the candidate subnetwork's parameters to θ.</p><p>The algorithm trains the parameters p i,j of each candidate subnetwork h (j) i independently. When training, the algorithm tries to find a θ ∈ R |pi,j | that minimizes a classifi-</p><formula xml:id="formula_4">cation loss L h (S, h (j) i (θ)) as well as a knowledge distilla- tion loss L KD (S, f i−1 , h (j) i (θ)) against the previous ensem- ble f i−1 .</formula><p>The previous ensemble and its component subnetworks are kept frozen, and are not affected by training the candidate subnetworks. By sharing a common computation subgraph, we can more efficiently train several candidate subnetworks within a single GPU.</p><p>Once all candidate subnetworks are trained, the algorithm tries to add each one to the previously constructed ensemble f i−1 ; It chooses the one that minimizes the loss of the whole ensemble. The best candidates subnetwork h ( * ) i then becomes a part of the ensemble as a subnetwork h i . The logits of the ensemble f i are a weighted sum of the trained subnetworks' logits, that is,</p><formula xml:id="formula_5">f i = i k=1 w k · h k ,<label>(1)</label></formula><p>where w k ∈ R for k ∈ [i], denotes the mixture weight and h k is the subnetwork in the k-th iteration, respectively. Without loss of generality, we denote by h k the logits of the trained subnetwork selected in the k-th iteration. Note that the mixture weights are applied to each subnetwork's outputs before the last non-linearity, such as a softmax activation in the case of multi-class classification.</p><p>In the experiments section we also explore a simpler more restricted version of the algorithm where the logits of the ensemble is just an average of previously selected subnetworks' logits, that is,</p><formula xml:id="formula_6">f i = 1 i Σ i k=1 h k .</formula><p>The vectors of mixture weights w (k) is trained to minimize the classification loss of the final ensemble. During training, the algorithm alternates between solving two optimization problems: training the mixture weights w (k) and training the candidate subnetworks h After presenting the general algorithm, we describe concrete possible realizations by discussing possible generators G, knowledge distillation losses L KD , and mixture weight learning procedures we have used to train the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 AdaNAS algorithm</head><p>Input: S, I, G f 0 ← 0</p><formula xml:id="formula_7">for i = 1 to I do H i ← G(f i−1 ) for h (j) i in H i do θ (j) ← argmin θ L h (S, h (j) i (θ))+ L KD (S, f i−1 , h (j) i (θ)) w (j) ← argmin w L f (S, Σ i−1 k=1 w (j) k ·h k +w (j) i ·h (j) i ) end for j * ← argmin j L f (S, Σ i−1 k=1 w (j) k ·h k +w (j) i ·h (j) i (θ (j) )) h i ← h (j * ) i (θ (j * ) ), w ← w (j * ) f i ← Σ i k=1 w k h k end for return f I</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Algorithm variants</head><p>For classification loss L h we use cross entropy H. For knowledge distillation loss L KD we propose three options. We can use previous subnetwork as the teacher (BAN)</p><formula xml:id="formula_8">L BAN (S, f i−1 , h (j) i (θ)) = H(h i−1 , h (j) i (θ))<label>(2)</label></formula><p>We also propose a novel loss based on knowledge distillation <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref> that instead uses the previous iteration's entire ensemble as the teacher, hence adaptive knowledge distillation (AKD):</p><formula xml:id="formula_9">L AKD (S, f i−1 , h (j) i (θ)) = H(f i−1 , h (j) i (θ))<label>(3)</label></formula><p>Finally, we can turn knowledge distillation off completely (NOKD) L N OKD = 0.</p><p>We train mixtures weights w concurrently yet independently from the candidate subnetwork parameters. The gradients from the mixture weights do not propagate through the subnetworks, and therefore do not affect subnetwork training. We do this efficiently by using different train ops in Tensor-Flow for each subnetwork. We also explore a case where mixture weights are not trained and just set to uniform values w (k) i = 1 i . For generator G we discuss two main variants. G C , the "constant" generator, always generates a single candidate with the same architecture. This subnetwork is always added to the ensemble at the end of the iteration.</p><p>In general, generator G can decide what architectures to propose based on the previously selected candidates (based on the architectures of subnetworks already present in the ensemble). We propose a dynamic generator G D . G D at iteration i looks at the previously selected subnetwork h i−1 and generate one candidate that is deeper (more layers/blocks/cells), and one that is wider (more convolution channels).</p><p>Even though generators can propose very different architectures, we train them with the same hyperparameters regardless. This could be addressed by generator proposing hyperparameters as well as architectures. However this approach risks a combinatorial explosion of candidates.</p><p>As an illustration, if we use the constant generator (G C ), no knowledge transfer (L N OKD = 0), do not train w i (set uniformly to 1 I ), this algorithm is equivalent to a uniformaverage ensemble of I independently trained subnetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setting</head><p>In this section we describe the experimental conditions used to obtain the results shown in Section 5. First, we fix our parameter budget to 33M parameters similar to the larger NASNet-A(7@2304) as this is the main baseline we try to improve upon.</p><p>In notation NASNet-A(X@Y), X describes the depth of the model (in terms of cells) and Y describes the number of convolution channels.</p><p>We explore two sets of experiments. The first uses a constant generator combined with different knowledge distillation methods to learn the ensemble. The second, explores a restricted search space of candidate subnetwork architectures, greedily growing the ensemble with the subnetwork that most improves its performance.</p><p>For all our experiments we use the same hyperparameters as described in <ref type="bibr" target="#b18">(Zoph et al., 2017)</ref> with no extra tuning. We use batch size 32, learning rate 0.025, cosine learning rate schedule, momentum optimizer with momentum 0.9 and we clip gradients to 5. We also use 1M training steps per iteration.</p><p>All our experiments are run on 11 NVIDIA Tesla V100 GPUs with asynchronous gradient descent employing data parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>We ran all experiments on two well calibrated datasets, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b10">(Krizhevsky &amp; Hinton, 2009</ref>). Both datasets consists of 60, 000 RGB images uniformly spread across 10 and 100 classes, respectively. Images are preprocessed in the following way. All images are upscaled to size 40 × 40, randomly croped back to 32 × 32 and randomly horizontally flipped. Finally the images are whitened. This procedure follows <ref type="bibr" target="#b18">(Zoph et al., 2017)</ref> as other works. We additionally used Cutout as the augmentation technique <ref type="bibr" target="#b4">(DeVries &amp; Taylor, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Constant generator</head><p>The first set of experiments focuses on the case where a constant generator G C is used. This generator always proposes a single candidate subnetwork. In our experiments, the architecture of this candidate will be NASNet-</p><formula xml:id="formula_10">A(6@768) h 6@768 , hence G C (f i ) = H i = {h 6@768 }.</formula><p>Once the candidate subnetwork is trained it is added directly to the ensemble as it is the only candidate subnetwork.</p><p>Three types of experiments are analyzed depending on the knowledge distillation technique used: Adaptive Knowledge Distillation (AKD), Born Again Networks (BAN) and no knowledge distillation (NOKD). Mixture weights are learned for AKD and NOKD. Note that we have intentionally left BAN out of this option as we observed that BAN performs worse without mixture weights.</p><p>The algorithm in these experiments has a maximum of 10 iterations, which generates ensembles of 10 NASNet-A(6@768) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Growing architecture</head><p>The objective of the second set of experiments is to explore two dynamic generators. The first generator G D6@768 proposes two types of candidates, a deeper one, (NASNet-A(7@768) in the first iteration), and a wider one, (NASNet-A(6@1008) in the first iteration). During the first iteration it uses NASNet-A(6@768) as a starting point. During the following iterations it uses the previously chosen subnetwork to generates one candidate that is deeper by one cell and one that is wider by 240 convolution filters. The number 240 was chosen arbitrarily as a reasonable small constant.</p><p>The second generator G D1@240 which is identical to the previous generator but the starting point is a minimal architecture NASNet-A(1@240). This generator has less human bias regarding the model architecture.</p><p>Despite training multiple candidate subnetworks at each iteration, we only select the best subnetwork to add to the ensemble. We qualify as "best" the candidate subnetwork that most improves the ensemble's performance on the full training set, thereby eliminating the need for a hold-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head><p>For baselines we do not consider regularization techniques such as Shake-shake <ref type="bibr" target="#b7">(Gastaldi, 2017)</ref> or Shake-drop <ref type="bibr" target="#b16">(Yamada et al., 2018)</ref> nor novel data augmentation techniques like AutoAugment <ref type="bibr" target="#b3">(Cubuk et al., 2018)</ref>.</p><p>The main baseline we try to improve upon is NASNet-A(7@2304) which reaches an error of 2.40 on CIFAR-10. We managed to reproduce these results to 2.41 ± 0.04 (2.47 ± 0.06 without early stopping).</p><p>Throughout the experiments we will use NASNet-A(6@768) that should achieve error 2.65. With hyperparameters provided in the paper we were able to achieve error 2.78 ± 0.07 on a single GPU (2.82 ± 0.07 without early stopping). This error slightly increased for training on 11 GPUs to 2.81 ± 0.06. This is the starting point for most of our algorithms, as this is the error achieved at the end of the first iteration. See supplement for thorough breakdown of hyperparameters that we used.</p><p>As most of our experiments are run for 10M steps with 11GPUs, we benchmark NASNet-A(7@2304) in the same conditions: This substantially increased the error to 3.54 ± 0.11 (3.69 ± 0.17 without early stopping).</p><p>On CIFAR-100 NASNet-A(6@768) reportedly achieves error 16.58 and NASNet-A(7@2304) 16.03 <ref type="bibr" target="#b11">(Luo et al., 2018;</ref><ref type="bibr" target="#b17">Zoph &amp; Le, 2016)</ref>.</p><p>We used the same hyperparameters and achieved error 17.70 ± 0.28 (17.87 ± 0.29) for NASNet-A(7@2304) and 17.58 ± 0.19 (17.71 ± 0.13) for NASNet-A(6@768).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ensembling</head><p>As seen in <ref type="table" target="#tab_2">Table 1</ref>, we were able to improve upon our NASNet-A(7@2304) baseline. By ensembling 10 small models we achieved relative 5% reduction in error (absolute 0.11) on CIFAR-10 and relative 9% reduction in error (absolute 1.45) on CIFAR-100. We observe that with uniform mixture weights it is best to not use any knowledge distillation (NOKD). This is a positive result for practical applications, as it means that different subnetworks do not need to be trained sequentially, and instead could be trained in parallel.</p><p>Note, that the NASNet baseline on CIFAR-100 was probably trained with different parameters than the ones used on CIFAR-10. As our reproduction suggests, hyperparameters from CIFAR-10 do not translate well to the CIFAR-100. However, our algorithm achieved state-ofthe-art results even though the hyperparameters were not fine tuned on this dataset.</p><p>We observe that by training mixture weights (w+) we were able to improve upon these results even further. Training mixture weights increases the error for NOKD and decreases the error for AKD. On CIFAR-10 we achieve error 2.26 ± 0.05 which is a 6% relative improvement (0.14 absolute) upon NASNet baseline, which was a previous state-of-the art result. To our knowledge, this result is surpassed only by <ref type="bibr">128)</ref> 2.13 ± 0.04 <ref type="bibr" target="#b12">(Real et al., 2018)</ref>, but uses a different architecture search space, and NAONet(f=128) 2.11 that has 4 times more parameters (128M ) <ref type="bibr" target="#b11">(Luo et al., 2018)</ref>. We do not consider results from <ref type="table" target="#tab_2">Table 1</ref>. Accuracy error for different types of knowledge distillation applied to the candidates in the ensemble. All models were trained with cutout for a fixed number of steps without early stopping. Different types of knowledge distillation are denoted as NOKD (no knowledge distillation), AKD (adaptive knowledge distillation) and BAN (born again knowledge distillation). Experiments with mixture weight training are denoted with w. methods that use additional regularization or data augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPE OF KD</head><p>Using early stopping, these results can be further improved by approximately 0.1. Our best run achieved error 2.09.</p><p>On CIFAR-10, the performance of NOKD is on par with the large NASNet-A(6@768) baseline. However, on CIFAR-100 NOKD significantly outperforms the baseline. Learning mixture weights can improve performance in some cases. In particular, AKD benefits from learning them more than NOKD. We conclude that on CIFAR-10 the best combination is to use Adaptive Knowledge Distillation and learn mixture weights. On CIFAR-100 it is best to use a uniform average ensemble of independently-trained models.</p><p>We see, that even experiments with an adaptive search G D1@240 can get similar results to a hand-crafted configuration on CIFAR-100. For CIFAR-10, these results are substantially worse the NASNet baseline. We observe, that tweaking the generator G D1@240 to also reconsider the previous iteration's best architecture can achieve 2.38 ± 0.06 error on CIFAR-10, which is competitive with the NASNet baseline but requires less human expertise.</p><p>G D6@768 performs worse than or similar to G D1@240 . One possible reason is that G D6@768 only trains for 4 iterations. At the 5-th iteration the model cannot fit the two proposed candidates into the V100 GPU's memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with other results</head><p>To the best to our knowledge, <ref type="table" target="#tab_1">Table 2</ref> contains current state-of-the-art results for CIFAR-10 and <ref type="table">Table 3</ref> for CIFAR-100.</p><p>On CIFAR-10 our results is surpassed only by NAONET with 4 times more parameters and AmoebaNet, which used  <ref type="bibr" target="#b18">(Zoph et al., 2017;</ref><ref type="bibr" target="#b11">Luo et al., 2018;</ref><ref type="bibr" target="#b12">Real et al., 2018)</ref>. Our approach (AdaNAS) uses adaptive knowledge distillation and mixture weights learning. This corresponds to w + AKD from a different search space to discover its cell.</p><p>On CIFAR-100 our method's results surpasses the previous state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Search space</head><p>As the dynamic generator explores different configurations of the NASNet architecture space, we observe an interesting behaviour between CIFAR-10 and CIFAR-100. We observe (see <ref type="figure">Figure 3</ref>) that our algorithm consistently chose much wider networks for CIFAR-100 than it did for CIFAR-10. The last architecture selected on CIFAR-100 was NASNet-A(1@2640) (8M parameters). The last architecture selected on CIFAR-10 was NASNet-A(6@1440) (11M parameters).</p><p>This is a valuable insight that even similar dataset may require very different architectures to perform well. This also means that eliminating the human bias even after NAS finds the building block is an important contribution. Width CIFAR-10 CIFAR-100 <ref type="figure">Figure 3</ref>. Difference between explored architectures on CIFAR-10 and CIFAR-100. Each point represents one subnetwork that was chosen across 3 experiments. Depth is the first NASNet parameter (number of stacked cells) and width is the second (number of convolution filters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We have shown that our AdaNAS algorithm consistently improves the performance of NASNet models by learning an ensemble of smaller models with minimal hyperparameter tuning. We explored two methodologies for generating candidate subnetworks for the ensemble, and observed positive results in both cases. Interestingly, a simple ensemble of identical architectures trained independently with a uniform averaged output already performs better than the baseline single large model. The benefit is that these models can be trained in parallel and ensembled together in a single step, allowing for a minimal wall-clock time from training start to serving.</p><p>Conversely, our adaptive methods show performance gains for applications where we can afford to train ensemble sequentially. For one, we were able to achieve near state-ofthe-art results by using a combination of learning mixture weights and applying Adaptive Knowledge Distillation. We show that it is possible to make the ensembling process even more automatic by using a dynamic generator, that can produce models with minimal human knowledge and still outperform architectures designed by experts.</p><p>Finally, as specialized multi-core machine learning hardware such as GPUs and TPUs become more ubiquitous, we envision model-parallelism at serving time to become increasingly important. Ensembles of neural networks provide a simple form of model-parallelism, while producing high-quality models given a fixed parameter budget.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the i-th subnetwork is selected by the algorithm, it advances to the next iteration. After I iterations, the algorithm outputs the final ensemble f I of the last iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy errors on CIFAR-10. Baseline results in this table are gathered from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table 1</head><label>1</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/tensorflow/adanet/tree/master/research/improve nas 2 github.com/tensorflow/models/tree/master/research/ slim/nets/nasnet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Scott Yak, Eugen Hotaj, Vitaly Kuznetsov, Mehryar Mohri, and Corinna Cortes for their insights and advice during this project. We also extend a special thanks to our collaborators, residents and interns Gus Kristiansen, Galen Chuang, Ghassen Jerfel, Ben Adlam and the many others at Google and beyond who helped us shape AdaNet <ref type="bibr" target="#b15">(Weill et al., 2018)</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01097</idno>
		<title level="m">Adaptive structural learning of artificial neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coupled ensembles of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quénot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Content-Based Multimedia Indexing (CBMI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anandkumar</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m">A. Born again neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7827" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adanet: Fast and flexible automl with learning guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mazzawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hotaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Macko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/adanet" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02375</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Shakedrop regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
