<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Compose Hypercolumns for Visual Correspondence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<address>
									<postBox>POSTECH *</postBox>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NPRC †</orgName>
								<address>
									<addrLine>3 Inria 4 ENS</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<address>
									<postBox>POSTECH *</postBox>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NPRC †</orgName>
								<address>
									<addrLine>3 Inria 4 ENS</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<address>
									<postBox>POSTECH *</postBox>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NPRC †</orgName>
								<address>
									<addrLine>3 Inria 4 ENS</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Compose Hypercolumns for Visual Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual correspondence</term>
					<term>multi-layer features</term>
					<term>dynamic feature composition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature representation plays a crucial role in visual correspondence, and recent methods for image matching resort to deeply stacked convolutional layers. These models, however, are both monolithic and static in the sense that they typically use a specific level of features, e.g., the output of the last layer, and adhere to it regardless of the images to match. In this work, we introduce a novel approach to visual correspondence that dynamically composes effective features by leveraging relevant layers conditioned on the images to match. Inspired by both multi-layer feature composition in object detection and adaptive inference architectures in classification, the proposed method, dubbed Dynamic Hyperpixel Flow, learns to compose hypercolumn features on the fly by selecting a small number of relevant layers from a deep convolutional neural network. We demonstrate the effectiveness on the task of semantic correspondence, i.e., establishing correspondences between images depicting different instances of the same object or scene category. Experiments on standard benchmarks show that the proposed method greatly improves matching performance over the state of the art in an adaptive and efficient manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual correspondence is at the heart of image understanding with numerous applications such as object recognition, image retrieval, and 3D reconstruction <ref type="bibr" target="#b11">[12]</ref>. With recent advances in neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>, there has been a significant progress in learning robust feature representation for establishing correspondences between images under illumination and viewpoint changes. Currently, the de facto standard is to use as feature representation the output of deeply stacked convolutional layers in a trainable architecture. Unlike in object classification and detection, however, such learned features have often achieved only modest performance gains over hand-crafted ones <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref> in the task of visual correspondence <ref type="bibr" target="#b47">[48]</ref>. In particular, correspondence between images under large intra-class variations still remains an extremely challenging problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref> while modern neural networks are known to excel at classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. What do we miss in using deep neural features for correspondence?</p><p>Most current approaches for correspondence build on monolithic and static feature representations in the sense that they use a specific feature layer, e.g., the last convolutional layer, and adhere to it regardless of the images to match. Correspondence, however, is all about precise localization of corresponding positions, which requires visual features at different levels, from local patterns to semantics and context; in order to disambiguate a match on similar patterns, it is necessary to analyze finer details and larger context in the image. Furthermore, relevant feature levels may vary with the images to match; the more we already know about images, the better we can decide which levels to use. In this aspect, conventional feature representations have fundamental limitations.</p><p>In this work, we introduce a novel approach to visual correspondence that dynamically composes effective features by leveraging relevant layers conditioned on the images to match. Inspired by both multi-layer feature composition, i.e., hypercolumn, in object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> and adaptive inference architectures in classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref>, we combine the best of both worlds for visual correspondence. The proposed method learns to compose hypercolumn features on the fly by selecting a small number of relevant layers in a deep convolutional neural network. At inference time, this dynamic architecture greatly improves matching performance in an adaptive and efficient manner. We demonstrate the effectiveness of the proposed method on several benchmarks for semantic correspondence, i.e., establishing visual correspondences between images depicting different instances of the same object or scene categories, where due to large variations it may be crucial to use features at different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Feature representation for semantic correspondence. Early approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref> tackle the problem of visual correspondence using hand-crafted descriptors such as HOG <ref type="bibr" target="#b5">[6]</ref> and SIFT <ref type="bibr" target="#b39">[40]</ref>. Since these lack high-level image semantics, the corresponding methods have difficulties with significant changes in background, view point, deformations, and instance-specific patterns. The advent of convolutional neural networks (CNN) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref> has led to a paradigm shift from this hand-crafted representations to deep features and boosted performance in visual correspondence <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">57]</ref>. Most approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref> learn to predict correlation scores between local regions in an input image pair, and some recent methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> cast this task as an image alignment problem in which a model learns to regress global geometric transformation parameters. All typically adopt a CNN pretrained on image classification as their backbone, and make predictions based on features from its final convolutional layer. While some methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b55">56]</ref> have demonstrated the advantage of using different CNN layers in capturing low-level to high-level patterns, leveraging multiple layers of deeply stacked layers has remained largely unexplored in correspondence problems. Multi-layer neural features. To capture different levels of information distributed over all intermediate layers, <ref type="bibr">Hariharan et al.</ref> propose the hypercolumn <ref type="bibr" target="#b17">[18]</ref>, a vector of multiple intermediate convolutional activations lying above a pixel for fine-grained localization. Attempts at integrating multi-level neural features have addressed object detection and segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. In the area of visual correspondence, only a few methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref> attempt to use multi-layer features. Unlike ours, however, these models use static features extracted from CNN layers that are chosen manually <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53]</ref> or by greedy search <ref type="bibr" target="#b41">[42]</ref>. While the use of hypercolumn features on the task of semantic visual correspondence has recently been explored by Min et al. <ref type="bibr" target="#b41">[42]</ref>, the method predefines hypercolumn layers by a greedy selection procedure, i.e., beam search, using a validation dataset. In this work, we clearly demonstrate the benefit of a dynamic and learnable architecture both in strongly-supervised and weakly-supervised regimes and also outperform the work of <ref type="bibr" target="#b41">[42]</ref> with a significant margin. Dynamic neural architectures. Recently, dynamic neural architectures have been explored in different domains. In visual question answering, neural module networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> compose different answering networks conditioned on an input sentence. In image classification, adaptive inference networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref> learn to decide whether to execute or bypass intermediate layers given an input image. Dynamic channel pruning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> skip unimportant channels at run-time to accelerate inference. All these methods reveal the benefit of dynamic neural architectures in terms of either accuracy or speed, or both. To the best of our knowledge, our work is the first that explores a dynamic neural architecture for visual correspondence.</p><p>Our main contribution is threefold: (1) We introduce a novel dynamic feature composition approach to visual correspondence that composes features on the fly by selecting relevant layers conditioned on images to match. (2) We propose a trainable layer selection architecture for hypercolumn composition using Gumbelsoftmax feature gating. (3) The proposed method outperforms recent state-ofthe-art methods on standard benchmarks of semantic correspondence in terms of both accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic hyperpixel flow</head><p>Given two input images to match, a pretrained convolutional network extracts a series of intermediate feature blocks for each image. The architecture we propose in this section, dynamic hyperpixel flow, learns to select a small number of layers (feature blocks) on the fly and composes effective features for reliable matching of the images. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the overall architecture. In this section, we describe the proposed method in four steps: (i) multi-layer feature extraction, (ii) dynamic layer gating, (iii) correlation computation and matching, and (iv) training objective.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-layer feature extraction</head><p>We adopt as a feature extractor a convolutional neural network pretrained on a large-scale classification dataset, e.g., ImageNet <ref type="bibr" target="#b6">[7]</ref>, which is commonly used in most related methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b22">23]</ref>. Following the work on hypercolumns <ref type="bibr" target="#b17">[18]</ref>, however, we view the layers of the convolutional network as a non-linear counterpart of image pyramids and extract a series of multiple features along intermediate layers <ref type="bibr" target="#b41">[42]</ref>.</p><p>Let us assume the backbone network contains L feature extracting layers. Given two images I and I , source and target, the network generates two sets of L intermediate feature blocks. We denote the two sets of feature blocks by B = {b l } L−1 l=0 and B = {b l } L−1 l=0 , respectively, and call the earliest blocks, b 0 and b 0 , base feature blocks. As in <ref type="figure" target="#fig_1">Fig. 1</ref>, each pair of source and target feature blocks at layer l is passed to the l-th layer gating module as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic layer gating</head><p>Given L feature block pairs {(b l , b l )} L−1 l=0 , L layer gating modules learn to select relevant feature block pairs and transform them for establishing robust correspondences. As shown in the top of <ref type="figure" target="#fig_1">Fig. 1</ref>, the module has two branches, one for layer gating and the other for feature transformation.</p><p>Gumbel layer gating. The first branch of the l-th layer gating module takes the l-th pair of feature blocks (b l , b l ) as an input and performs global average pooling on two feature blocks to capture their channel-wise statistics. Two average pooled features of size 1 × 1 × c l from b l and b l are then added together to form a vector of size c l . A multi-layer perceptron (MLP) composed of two fully-connected layers with ReLU non-linearity takes the vector and predicts a relevance vector r l of size 2 for gating, whose entries indicate the scores for selecting or skipping ('on' or 'off') the l-th layer, respectively. We can simply obtain a gating decision using argmax over the entries, but this naïve gating precludes backpropagation since argmax is not differentiable.</p><p>To make the layer gating trainable and effective, we adopt the Gumbelmax trick <ref type="bibr" target="#b13">[14]</ref> and its continuous relaxation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>. Let z be a sequence of i.i.d. Gumbel random noise and let Y be a discrete random variable with Kclass categorical distribution u, i.e., p(Y = y) ∝ u y and y ∈ {0, ..., K − 1}. Using the Gumbel-max trick <ref type="bibr" target="#b13">[14]</ref>, we can reparamaterize sampling Y to y = arg max k∈{0,...,K−1} (log u k + z k ). To approximate the argmax in a differentiable manner, the continuous relaxation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref> of the Gumbel-max trick replaces the argmax operation with a softmax operation. By expressing a discrete random sample y as a one-hot vector y, a sample from the Gumbel-softmax can be represented byŷ = softmax((log u + z)/τ ), where τ denotes the temperature of the softmax. In our context, the discrete random variable obeys a Bernoulli distribution, i.e., y ∈ {0, 1}, and the predicted relevance scores represent the log probability distribution for 'on' and 'off', i.e., log u = r l . Our Gumbel-softmax gate thus has a form ofŷ</p><formula xml:id="formula_0">l = softmax(r l + z l ),<label>(1)</label></formula><p>where z l is a pair of i.i.d. Gumbel random samples and the softmax temperature τ is set to 1.</p><p>Convolutional feature transformation. The second branch of the l-th layer gating module takes the l-th pair of feature blocks (b l , b l ) as an input and transforms each feature vector over all spatial positions while reducing its dimension by 1 ρ ; we implement it using 1 × 1 convolutions, i.e., position-wise linear transformations, followed by ReLU non-linearity. This branch is designed to transform the original feature block of size h l × w l × c l into a more compact and effective representation of size h l × w l × c l ρ for our training objective. We denote the pair of transformed feature blocks by (b l ,b l ). Note that if l-th Gumbel gate chooses to skip the layer, then the feature transformation of the layer can be also ignored thus reducing the computational cost.</p><p>Forward and backward propagations. During training, we use the straightthrough version of the Gumbel-softmax estimator <ref type="bibr" target="#b23">[24]</ref>: forward passes proceed with discrete samples by argmax whereas backward passes compute gradients of the softmax relaxation of Eq.(1). In the forward pass, the transformed feature pair (b l ,b l ) is simply multiplied by 1 ('on') or 0 ('off') according to the gate's discrete decision y. While the Gumbel gate always makes discrete decision y in the forward pass, the continuous relaxation in the backward pass allows gradients to propagate through softmax outputŷ, effectively updating both branches, the feature transformation and the relevance estimation, regardless of the gate's decision. Note that this stochastic gate with random noise increases the diversity of samples and is thus crucial in preventing mode collapse in training. At test time, we simply use deterministic gating by argmax without Gumbel noise <ref type="bibr" target="#b23">[24]</ref>. As discussed in Sec. 4.2, we found that the proposed hard gating trained with Gumbel softmax is superior to conventional soft gating with sigmoid in terms of both accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correlation computation and matching</head><p>The output of gating is a set of selected layer indices, S = {s 1 , s 2 , ..., s N }. We construct a hyperimage H for each image by concatenating transformed feature blocks of the selected layers along channels with upsampling:</p><formula xml:id="formula_1">H = ζ(b s1 ), ζ(b s2 ), ..., ζ(b s N ) ,</formula><p>where ζ denotes a function that spatially upsamples the input feature block to the size of b 0 , the base block. Note that the number of selected layers N is fully determined by the gating modules. If all layers are off, then we use the base feature block by setting S = {0}. We associate with each spatial position p of the hyperimage the corresponding image coordinates and hyperpixel feature <ref type="bibr" target="#b41">[42]</ref>. Let us denote by x p the image coordinate of position p, and by f p the corresponding feature, i.e., f p = H(x p ). The hyperpixel at position p in the hyperimage is defined as h p = (x p , f p ). Given source and target images, we obtain two sets of hyperpixels, H and H . In order to reflect geometric consistency in matching, we adapt probablistic Hough matching (PHM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> to hyperpixels, similar to <ref type="bibr" target="#b41">[42]</ref>. The key idea of PHM is to re-weight appearance similarity by Hough space voting to enforce geometric consistency. In our context, let D = (H, H ) be two sets of hyperpixels, and m = (h, h ) be a match where h and h are respectively elements of H and H . Given a Hough space X of possible offsets (image transformations) between the two hyperpixels, the confidence for match m, p(m|D), is computed as p(m|D) ∝ p(m a ) x∈X p(m g |x) m∈H×H p(m a )p(m g |x) where p(m a ) represents the confidence for appearance matching and p(m g |x) is the confidence for geometric matching with an offset x, measuring how close the offset induced by m is to x. By sharing the Hough space X for all matches, PHM efficiently computes match confidence with good empirical performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref>. In this work, we compute appearance matching confidence using hyperpixel features by p(m a ) ∝ ReLU</p><formula xml:id="formula_2">fp·f p fp f p 2 ,</formula><p>where the squaring has the effect of suppressing smaller matching confidences. On the output |H| × |H | correlation matrix of PHM, we perform soft mutual nearest neighbor filtering <ref type="bibr" target="#b46">[47]</ref> to suppress noisy correlation values and denote the filtered matrix by C. Dense matching and keypoint transfer. From the correlation matrix C, we establish hyperpixel correspondences by assigning to each source hyperpixel h i the target hyperpixelĥ j with the highest correlation. Since the spatial resolutions of the hyperimages are the same as those of base feature blocks, which are relatively high in most cases (e.g., 1/4 of input image with ResNet-101 as the backbone), such hyperpixel correspondences produce quasi-dense matches.</p><p>Furthermore, given a keypoint p m in the source image, we can easily predict its corresponding positionp m in the target image by transferring the keypoint using its nearest hyperpixel correspondence. In our experiments, we collect all correspondences of neighbor hyperpixels of keypoint p m and use the geometric average of their individual transfers as the final predictionp m <ref type="bibr" target="#b41">[42]</ref>. This consensus  <ref type="figure" target="#fig_1">1 .1 .1 .1 .1   .1 .1 .1 .3 .2 .2   .1 .1 .1 .5 .1 .1   .1 .1 .1 .2 .2 .3   .1 .1 .1 .1 .1 .5   .3 .1 .2 .1 .1 .1   .1 .3 .1 .1 .1 .3   .1 .1 .1 .5 .1</ref>   <ref type="figure" target="#fig_1">1 .1 .1 .1 .5   .3 .1 .2 .1 .1 .1   .1 .3 .1 .1 .1</ref>   keypoint transfer method improves accuracy by refining mis-localized predictions of individual transfers.</p><formula xml:id="formula_3">( −) ∈ ℝ |ℋ|×|ℋ′| Row-wise L1 normalization + ( −) ( +) − (b) Weakly-supervised loss.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training objective</head><p>We propose two objectives to train our model using different degrees of supervision: strongly-supervised and weakly-supervised regimes. Learning with strong supervision. In this setup, we assume that keypoint match annotations are given for each training image pair, as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref>; each image pair is annotated with a set of coordinate pairs M = {(p m , p m )} M m=1 , where M is the number of match annotations.</p><p>To compare the output of our network with ground-truth annotations, we convert the annotations into a form of discrete correlation matrix. First of all, for each coordinate pair (p m , p m ), we identify their nearest position indices (k m , k m ) in hyperimages. On the one hand, given the set of identified match index pairs {(k m , k m )} M m=1 , we construct a ground-truth matrix G ∈ {0, 1} M ×|H | by assigning one-hot vector representation of k m to the m-th row of G. On the other hand, we constructĈ ∈ R M ×|H | by assigning the k m -th row of C to the m-th row ofĈ. We apply softmax to each row of the matrixĈ after normalizing it to have zero mean and unit variance. <ref type="figure" target="#fig_3">Figure 2a</ref> illustrates the construction ofĈ and G. Corresponding rows betweenĈ and G can now be compared as categorical probability distributions. We thus define the strongly-supervised matching loss as the sum of cross-entropy values between them:</p><formula xml:id="formula_4">L match = − 1 M M m=1 ω m |H | j=1 G mj logĈ mj ,<label>(2)</label></formula><p>where ω m is an importance weight for the m-th keypoint. The keypoint weight ω m helps training by reducing the effect of the corresponding cross-entropy term if the Eucliean distance between predicted keypointp m and target keypoint p m is smaller than some threshold distance δ thres :</p><formula xml:id="formula_5">ω m = ( p m − p m /δ thres ) 2 if p m − p m &lt; δ thres , 1 otherwise.<label>(3)</label></formula><p>The proposed objective for strongly-supervised learning can also be used for self-supervised learning with synthetic pairs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref> * , which typically results in trading off the cost of supervision against the generalization performance.</p><p>Learning with weak supervision. In this setup, we assume that only imagelevel labels are given for each image pair as either positive (the same class) or negative (different class), as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref>. Let us denote the correlation matrix of a positive pair by C + and that of a negative pair by C − . For C ∈ R |H|×|H | , we define its correlation entropy as s(</p><formula xml:id="formula_6">C) = − 1 |H| |H| i=1 |H | j=1 φ(C) ij log φ(C) ij</formula><p>where φ(·) denotes row-wise L1-normalization. Higher correlation entropy indicates less distinctive correspondences between the two images. As illustrated in <ref type="figure" target="#fig_3">Fig. 2b</ref>, assuming that the positive images are likely to contain more distinctive correspondences, we encourage low entropy for positive pairs and high entropy for negative pairs. The weakly-supervised matching loss is formulated as</p><formula xml:id="formula_7">L match = s(C + ) + s(C + ) s(C − ) + s(C − ) .<label>(4)</label></formula><p>Layer selection loss. Following the work of <ref type="bibr" target="#b53">[54]</ref>, we add a soft constraint in our training objective to encourage the network to select each layer at a certain rate: <ref type="bibr" target="#b1">2</ref> wherez l is a fraction of image pairs within a mini-batch for which the l-th layer is selected and µ is a hyperparameter for the selection rate. This improves training by increasing diversity in layer selection and, as will be seen in our experiments, allows us to trade off between accuracy and speed in testing.</p><formula xml:id="formula_8">L sel = L−1 l=0 (z l − µ)</formula><p>Finally, the training objective of our model is defined as the combination of the matching loss (either strong or weak) and the layer selection loss: L = L match + L sel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we compare our method to the state of the art and discuss the results. The code and the trained model are available online at our project page.</p><p>Feature extractor networks. As the backbone networks for feature extraction, we use ResNet-50 and ResNet-101 <ref type="bibr" target="#b18">[19]</ref>, which contains 49 and 100 conv layers in total (excluding the last FC), respectively. Since features from adjacent layers are strongly correlated, we extract the base block from conv1 maxpool and intermediate blocks from layers with residual connections (before ReLU). They amounts to 17 and 34 feature blocks (layers) in total, respectively, for ResNet-50 and ResNet-101. Following related work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b22">23]</ref>, we freeze the backbone network parameters during training for fair comparison. * For example, we can obtain keypoint annotations for free by forming a synthetic pair by applying random geometric transformation (e.g., affine or TPS <ref type="bibr" target="#b7">[8]</ref>) on an image and then sampling some corresponding points between the original image and the warped image using the transformation applied. <ref type="table">Table 1</ref>: Performance on standard benchmarks in accuracy and speed (avg. time per pair). The subscript of each method name denotes its feature extractor. Some results are from <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. Numbers in bold indicate the best performance and underlined ones are the second best. The average inference time (the last column) is measured on test split of PF-PASCAL <ref type="bibr" target="#b15">[16]</ref> and includes all the pipelines of the models: from feature extraction to keypoint prediction. <ref type="bibr">Sup</ref> Datasets. Experiments are done on four benchmarks for semantic correspondence: PF-PASCAL <ref type="bibr" target="#b15">[16]</ref>, PF-WILLOW <ref type="bibr" target="#b14">[15]</ref>, Caltech-101 <ref type="bibr" target="#b33">[34]</ref>, and SPair-71k <ref type="bibr" target="#b42">[43]</ref>. PF-PASCAL and PF-WILLOW consist of keypoint-annotated image pairs, 1,351 pairs from 20 categories, and 900 pairs from 4 categories, respectively. Caltech-101 <ref type="bibr" target="#b33">[34]</ref> contains segmentation-annotated 1,515 pairs from 101 categories. SPair-71k <ref type="bibr" target="#b42">[43]</ref> is a more challenging large-scale dataset recently introduced in <ref type="bibr" target="#b41">[42]</ref>, consisting of keypoint-annotated 70,958 image pairs from 18 categories with diverse view-point and scale variations.</p><p>Evaluation metrics. As an evaluation metric for PF-PASCAL, PF-WILLOW, and SPair-71k, the probability of correct keypoints (PCK) is used. The PCK value given a set of predicted and ground-truth keypoint pairs</p><formula xml:id="formula_9">P = {(p m , p m )} M m=1 is measured by PCK(P) = 1 M M m=1 1[ p m − p m ≤ α τ max (w τ , h τ )].</formula><p>As an evaluation metric for the Caltech-101 benchmark, the label transfer accuracy (LT-ACC) <ref type="bibr" target="#b35">[36]</ref> and the intersection-over-union (IoU) <ref type="bibr" target="#b8">[9]</ref> are used. Running time (average time per pair) for each method is measured using its authors' code on a machine with an Intel i7-7820X CPU and an NVIDIA Titan-XP GPU.</p><p>Hyperparameters. The layer selection rate µ and the channel reduction factor ρ are determined by grid search using the validation split of PF-PASCAL. As a result, we set µ = 0.5 and ρ = 8 in our experiments if not specified otherwise. The threshold δ thres in Eq.(3) is set to be max(w τ , h τ )/10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and comparisons</head><p>First, we train both of our strongly and weakly-supervised models on the PF-PASCAL <ref type="bibr" target="#b15">[16]</ref> dataset and test on three standard benchmarks of PF-PASCAL (test split), PF-WILLOW and Caltech-101. The evaluations on PF-WILLOW and Caltech-101 are to verify transferability. In training, we use the same splits of PF-PASCAL proposed in <ref type="bibr" target="#b16">[17]</ref> where training, validation, and test sets respectively contain 700, 300, and 300 image pairs. Following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, we augment the training pairs by horizontal flipping and swapping. <ref type="table">Table 1</ref> summarizes our result and those of recent methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. Second, we train our model on the SPair-71k dataset <ref type="bibr" target="#b42">[43]</ref> and compare it to other recent methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. <ref type="table" target="#tab_3">Table 2</ref> summarizes the results.</p><p>Strongly-supervised regime. As shown in the bottom sections of <ref type="table">Table 1</ref> and 2, our strongly-supervised model clearly outperforms the previous state of the art by a significant margin. It achieves 5.9%, 3.2%, and 9.1% points of PCK (α img = 0.1) improvement over the current state of the art <ref type="bibr" target="#b41">[42]</ref> on PF-PASCAL, PF-WILLOW, and SPair-71k, respectively, and the improvement increases further with a more strict evaluation threshold, e.g., more than 15% points of PCK with α img = 0.05 on PF-PASCAL. Even with a smaller backbone network (ResNet-50) and smaller selection rate (µ = 0.4), our method achieves competitive performance with the smallest running time on the standard benchmarks of PF-PASCAL, PF-WILLOW, and Caltech-101.</p><p>Weakly-supervised regime. As shown in the middle sections of <ref type="table">Table 1</ref> and 2, our weakly-supervised model also achieves the state of the art in the weaklysupervised regime. In particular, our model shows more reliable transferablility compared to strongly-supervised models, outperforming both weakly <ref type="bibr" target="#b22">[23]</ref> and strongly-supervised <ref type="bibr" target="#b41">[42]</ref> state of the arts by 6.4% and 5.8% points of PCK respectively on PF-WILLOW. On the Caltech-101 benchmark, our method is comparable to the best among the recent methods. Note that unlike other benchmarks, the evaluation metric of Caltech-101 is indirect (i.e., accuracy of mask transfer). On the SPair-71k dataset, where image pairs have large view point and scale differences, the methods of <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> as well as ours do not successfully learn in the weakly-supervised regime; they (FT) all underperform transferred models (TR) trained on PF-PASCAL. This result reveals current weakly-supervised objectives are all prone to large variations, which requires further research in the future.</p><p>Effect of layer selection rate µ <ref type="bibr" target="#b53">[54]</ref>. The plot in <ref type="figure" target="#fig_4">Fig. 3a</ref> shows PCK and running time of our models trained with different layer selection rates µ. It shows that smaller selection rates in training lead to faster running time in testing, at the cost of some accuracy, by encouraging the model to select a smaller number of layers. The selection rate µ can thus be used for speed-accuracy trade-off.</p><p>Analysis of layer selection patterns. Category-wise layer selection patterns in <ref type="figure" target="#fig_4">Fig. 3b</ref> show that each group of animal, vehicle, and man-made object categories shares its own distinct selection patterns. The model with a small rate (µ = 0.3) tends to select the most relevant layers only while the model with larger rates = 0.2 = 0.5 = 0.7 <ref type="figure">Fig. 4</ref>: Frequencies over the numbers of selected layers with different selection rates µ (x-axis: the number of selected layers, y-axis: frequency). Best viewed in electronics.   <ref type="bibr" target="#b45">[46]</ref>, (e) A2Net <ref type="bibr" target="#b48">[49]</ref>, (f) NC-Net <ref type="bibr" target="#b46">[47]</ref>, and (g) HPF <ref type="bibr" target="#b41">[42]</ref>.</p><p>(µ &gt; 0.3) tends to select more complementary layers as seen in <ref type="figure" target="#fig_4">Fig.3c</ref>. For each µ ∈ {0.3, 0.4, 0.5} in <ref type="figure" target="#fig_4">Fig.3c</ref>, the network tends to select low-level features for vehicle and man-made object categories while it selects mostly high-level features for animal category. We conjecture that it is because low-level (geometric) features such as lines, corners and circles appear more often in the vehicle and man-made classes compared to the animal classes. <ref type="figure">Figure 4</ref> plots the frequencies over the numbers of selected layers with different selection rate µ, where vehicles tend to require more layers than animals and man-made objects.</p><p>Qualitative results. Some challenging examples on SPair-71k <ref type="bibr" target="#b42">[43]</ref> and PF-PASCAL <ref type="bibr" target="#b15">[16]</ref> are shown in <ref type="figure" target="#fig_5">Fig.5 and 6</ref> respectively: Using the keypoint correspondences, TPS transformation <ref type="bibr" target="#b7">[8]</ref> is applied to source image to align target image. The object categories of the pairs in <ref type="figure" target="#fig_6">Fig.6</ref> are in order of table, potted plant, and tv. Alignment results of each pair demonstrate the robustness of our model against major challenges in semantic correspondences such as large changes in view-point and scale, occlusion, background clutters, and intra-class variation.</p><p>Ablation study. We also conduct an ablation study to see the impacts of major components: Gumbel layer gating (GLG), conv feature transformation (CFT), probabilistic Hough matching (PHM), keypoint importance weight ω m , and layer selection loss L sel . All the models are trained with strong supervision   and evaluated on PF-PASCAL. Since the models with a PHM component have no training parameters, they are directly evaluated on the test split. <ref type="table" target="#tab_4">Table 3</ref> summarizes the results. It reveals that among others CFT in the dynamic gating module is the most significant component in boosting performance and speed; without the feature transformation along with channel reduction, our models do not successfully learn in our experiments and even fail to achieve faster per- With all the components jointly used, our model achieves the highest PCK measure of 90.7%. Even with the smaller backbone network, ResNet-50, the model still outperforms previous state of the art and achieves real-time matching as well as described in <ref type="figure" target="#fig_4">Fig.3</ref> and <ref type="table">Table 1</ref>.</p><p>Computational complexity. The average feature dimensions of our model before correlation computation are 2089, 3080, and 3962 for each µ ∈ {0.3, 0.4, 0.5} while those of recent methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23]</ref> are respectively 6400, 3072, 1024, 1024. The dimension of hyperimage is relatively small as GLG efficiently prunes irrelevant features and CFT effectively maps features onto smaller subspace, thus being more practical in terms of speed and accuracy as demonstrated in <ref type="table">Table 1</ref> and 3. Although <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b22">23]</ref> use lighter feature maps compared to ours, a series of 4D convolutions heavily increases time and memory complexity of the network, making them expensive for practical use (31ms (ours) vs. 261ms <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b22">23]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to soft layer gating</head><p>The Gumbel gating function in our dynamic layer gating can be replaced with conventional soft gating using sigmoid. We have investigated different types of soft gating as follows: (1) 'sigmoid': The MLP of dynamic gating at each layer predicts a scalar input for sigmoid and the transformed feature block pairs are weighted by the sigmoid output. (2) 'sigmoid µ=0.5 ': In training the 'sigmoid' gating, the layer selection loss L sel with µ = 0.5 is used to encourage the model to increase diversity in layer selection. (3) 'sigmoid + 1': In training the 'sigmoid' gating, the 1 regularization on the sigmoid output is used to encourage the soft selection result to be sparse. <ref type="table" target="#tab_5">Table 4</ref> summarizes the results and <ref type="figure" target="#fig_7">Fig. 7</ref> compares their layer selection frequencies.</p><p>While the soft gating modules provide decent results, all of them perform worse than the proposed Gumbel layer gating in both accuracy and speed. The slower per-pair inference time of 'sigmoid' and 'sigmoid µ=0.5 ' indicates that soft gating is not effective in skipping layers due to its non-zero gating values. We find that the sparse regularization of 'sigmoid + 1' recovers the speed but only at the cost of significant accuracy points. Performance drop of soft gating in accuracy may result from the deterministic behavior of the soft gating during training that prohibits exploring diverse combinations of features at different levels. In contrast, the Gumbel gating during training enables the network to perform more comprehensive trials of a large number of different combinations of multi-level features, which help to learn better gating. Our experiments also show that discrete layer selection along with stochastic learning in searching the best combination is highly effective for learning to establish robust correspondences in terms of both accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a dynamic matching network that predicts dense correspondences by composing hypercolumn features using a small set of relevant layers from a CNN. The state-of-the-art performance of the proposed method indicates that the use of dynamic multi-layer features in a trainable architecture is crucial for robust visual correspondence. We believe that our approach may prove useful for other domains involving correspondence such as image retrieval, object tracking, and action recognition. We leave this to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The overall architecture of Dynamic Hyperpixel Flow (DHPF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Matching loss computation using (a) keypoint annotations (strong supervision) and (b) image pairs only (weak supervision). Best viewed in electronic form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>PCK vs. running time (b) Category-wise layer selection frequency (c) ResNet-101 layer selection frequencies at different Analysis of layer selection on PF-PASCAL dataset (a) PCK vs. running time with varying selection rate µ (b) Category-wise layer selection frequencies (x-axis: candidate layer index, y-axis: category) of the strongly-supervised model with different backbones: ResNet-101 (left) and ResNet-50 (right) (c) ResNet-101 layer selection frequencies of strongly (left) and weakly (right) supervised models at different layer selection rates µ. Best viewed in electronic form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Example results on SPair-71k dataset. The source images are warped to the target ones using resultant correspondences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Example results on PF-PASCAL [16]: (a) source image, (b) target image and (c) DHPF (ours), (d) WeakAlign</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>ResNet-101 layer selection frequencies for 'sigmoid' (left), 'sigmoidµ=0.5' (middle), and 'sigmoid + 1' (right) gating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>pair inference time. The result of 'w/o ω m ' reveals the effect of the keypoint weight ω m in Eq.(2) by replacing it with uniform weights for all m, i.e., ω m = 1; putting less weights on easy examples helps in training the model by focusing on hard examples. The result of 'w/o L sel ' shows the performance of the model using L match only in training; performance drops with slower running time, demonstrating the effectiveness of the layer selection constraint in terms of both speed and accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance on SPair-71k dataset in accuracy (per-class PCK with α bbox = 0.1). TR represents transferred models trained on PF-PASCAL while FT denotes fine-tuned (trained) models on SPair-71k.<ref type="bibr" target="#b20">21</ref>.8 57.2 13.9 34.3 23.1 17.3 50.4 17.4 34.8 36.2 19.7 24.3 32.5 22.2 17.6 30.9 36.5 28.5 FT DHPFres101 (ours) 17.5 19.0 52.5 15.4 35.0 19.4 15.7 51.9 17.3 37.3 35.7 19.7 25.5 31.6 20.9 18.5 24.2 41.1 27.7 ours) 22.6 23.0 57.7 15.1 34.1 20.5 14.7 48.6 19.5 31.9 34.5 19.6 23.0 30.0 22.9 15.5 28.2 30.2 27.4 FT DHPFres101 (ours) 38.4 23.8 68.3 18.9 42.6 27.9 20.1 61.6 22.0 46.9 46.1 33.5 27.6 40.1 27.6 28.1 49.5 46.5 37.3</figDesc><table><row><cell>Sup.</cell><cell>Methods</cell><cell>aero bike bird boat bottle bus car cat chair cow dog horse mbike person plant sheep train tv all</cell></row><row><cell></cell><cell cols="2">TR CNNGeores101 [45] 21.3 15.1 34.6 12.8 31.2 26.3 24.0 30.6 11.6 24.3 20.4 12.2 19.7 15.6 14.3 9.6 28.5 28.8 18.1</cell></row><row><cell>self</cell><cell cols="2">FT CNNGeores101 [45] 23.4 16.7 40.2 14.3 36.4 27.7 26.0 32.7 12.7 27.4 22.8 13.7 20.9 21.0 17.5 10.2 30.8 34.1 20.6 TR A2Netres101 [49] 20.8 17.1 37.4 13.9 33.6 29.4 26.5 34.9 12.0 26.5 22.5 13.3 21.3 20.0 16.9 11.5 28.9 31.6 20.1</cell></row><row><cell></cell><cell>FT A2Netres101 [49]</cell><cell>22.6 18.5 42.0 16.4 37.9 30.8 26.5 35.6 13.3 29.6 24.3 16.0 21.6 22.8 20.5 13.5 31.4 36.5 22.3</cell></row><row><cell></cell><cell cols="2">TR WeakAlignres101 [46] 23.4 17.0 41.6 14.6 37.6 28.1 26.6 32.6 12.6 27.9 23.0 13.6 21.3 22.2 17.9 10.9 31.5 34.8 21.1</cell></row><row><cell></cell><cell cols="2">FT WeakAlignres101 [46] 22.2 17.6 41.9 15.1 38.1 27.4 27.2 31.8 12.8 26.8 22.6 14.2 20.0 22.2 17.9 10.4 32.2 35.1 20.9</cell></row><row><cell>weak</cell><cell>TR NC-Netres101 [47] FT NC-Netres101 [47]</cell><cell>24.0 16.0 45.0 13.7 35.7 25.9 19.0 50.4 14.3 32.6 27.4 19.2 21.7 20.3 20.4 13.6 33.6 40.4 26.4 17.9 12.2 32.1 11.7 29.0 19.9 16.1 39.2 9.9 23.9 18.8 15.7 17.4 15.9 14.8 9.6 24.2 31.1 20.1</cell></row><row><cell cols="3">FT HPFres101 [42] TR DHPFres101 (ours) 21.5 strong 25.2 18.9 52.1 15.7 38.0 22.8 19.1 52.9 17.9 33.0 32.8 20.6 24.4 27.9 21.1 15.9 31.5 35.6 28.2 TR DHPFres101 (</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on PF-PASCAL.</figDesc><table><row><cell cols="2">(GLG: Gumbel layer gating with selection</cell></row><row><cell cols="2">rates µ, CFT: conv feature transformation)</cell></row><row><cell>Module</cell><cell>PCK (αimg) time</cell></row><row><cell cols="2">GLG CFT PHM 0.05 0.1 0.15 (ms)</cell></row><row><cell>0.5</cell><cell>75.7 90.7 95.0 58</cell></row><row><cell>0.4</cell><cell>73.6 90.4 95.3 51</cell></row><row><cell>0.3</cell><cell>73.1 88.7 94.4 47</cell></row><row><cell></cell><cell>70.4 88.1 94.1 64</cell></row><row><cell>0.5</cell><cell>43.6 74.7 87.5 176</cell></row><row><cell>0.5</cell><cell>68.3 86.9 91.6 57</cell></row><row><cell></cell><cell>37.6 68.7 84.6 124</cell></row><row><cell></cell><cell>68.1 85.5 91.6 61</cell></row><row><cell>0.5</cell><cell>35.0 54.8 63.4 173</cell></row><row><cell>w/o ωm</cell><cell>69.8 86.1 91.9 57</cell></row><row><cell>w/o L sel</cell><cell>68.1 89.2 93.5 56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison to soft layer gating on PF-PASCAL.</figDesc><table><row><cell>Gating function</cell><cell>PCK (αimg) time 0.05 0.1 0.15 (ms)</cell></row><row><cell cols="2">Gumbelµ=0.5 75.7 90.7 95.0 58</cell></row><row><cell>sigmoid</cell><cell>71.1 88.2 92.8 74</cell></row><row><cell cols="2">sigmoidµ=0.5 72.1 87.8 93.3 75</cell></row><row><cell cols="2">sigmoid + 1 65.9 87.2 91.0 60</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by Samsung Advanced Institute of Technology (SAIT) and also by Basic Science Research Program (NRF-2017R1E1A1A01077999) and Next-Generation Information Computing Development Program (NRF-2017M3C4A7069369) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT, Korea. Jean Ponce was supported in part by the Louis Vuitton/ENS chair in artificial intelligence and the Inria/NYU collaboration and also by the French government under management of Agence Nationale de la Recherche as part of the "Investissements dâavenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS) (2016) 2, 4</title>
		<meeting>Neural Information essing Systems (NeurIPS) (2016) 2, 4</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximate thin plate spline mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV</title>
		<meeting>European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical metric learning and matching for 2d and 3d geometric correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeeshan Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computer Vision: A Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic channel pruning: Feature boosting and suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications: a series of lectures. Applied mathematics series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gumbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U. S. Govt. Print. Office</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 2, 6</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 2, 6</meeting>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 1</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12549</idno>
		<title level="m">Channel gating neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV) (2019) 4, 8, 9</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV) (2019) 4, 8, 9</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parn: Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dctm: Discrete-continuous transformation matching for semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 2, 4</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 2, 4</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonparametric scene parsing: Label transfer via dense scene alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Whye Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<title level="m">SPair-71k: A large-scale benchmark for semantic correspondence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv prepreint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anchornet: A weakly supervised network to learn geometry-sensitive features for semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 2, 4</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 2, 4</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>2, 4, 8, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)<address><addrLine>2, 4, 6, 8, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Comparative evaluation of hand-crafted and learned local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep semantic feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object-aware dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
