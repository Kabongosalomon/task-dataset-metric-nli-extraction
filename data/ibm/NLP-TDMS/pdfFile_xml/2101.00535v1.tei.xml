<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RV-GAN : Retinal Vessel Segmentation from Fundus Images using Multi-scale Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<email>skamran@nevada.unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariha</forename><surname>Khondker</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hossain</surname></persName>
							<email>khossain@deakin.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
							<email>tavakkol@unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuckerbrod</forename><surname>Houston</surname></persName>
							<email>szuckerbrod@houstoneye.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eye</forename><surname>Associates</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
							<email>ksanders@med.unr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
							<email>sabubaker@med.unr.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Deakin University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RV-GAN : Retinal Vessel Segmentation from Fundus Images using Multi-scale Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinal vessel segmentation contributes significantly to the domain of retinal image analysis for the diagnosis of vision-threatening diseases. With existing techniques the generated segmentation result deteriorates when thresholded with higher confidence value. To alleviate from this, we propose RVGAN, a new multi-scale generative architecture for accurate retinal vessel segmentation. Our architecture uses two generators and two multi-scale autoencoder based discriminators, for better microvessel localization and segmentation. By combining reconstruction and weighted feature matching loss, our adversarial training scheme generates highly accurate pixel-wise segmentation of retinal vessels with threshold ≥ 0.5. The architecture achieves AUC of 0.9887, 0.9814, and 0.9887 on three publicly available datasets, namely DRIVE, CHASE-DB1, and STARE, respectively. Additionally, RV-GAN outperforms other architectures in two additional relevant metrics, Mean-IOU and SSIM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The fundoscopic exam is a procedure to diagnose different retinal degenerative diseases such as Diabetic Retinopathy, Macular Edema, Cytomegalovirus Retinitis, etc. This examination provides necessary information that can be utilized by Ophthalmologists to detect symptoms for visionthreatening diseases <ref type="bibr" target="#b35">[36]</ref>. To diagnosis these vascular diseases, an automated retinal vessel segmentation system is required that can find abnormalities in blood vessels of the retinal subspace. In addition to that, this system can be utilized for extracting complex morphological attributes, performing temporal or multimodal image registration <ref type="bibr" target="#b41">[42]</ref>, retinal image mosaic synthesis, optic disk identification, and fovea localization <ref type="bibr" target="#b22">[23]</ref>. So far, many image processing and machine learning-based approaches have been proposed for retinal vessel segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref>. However, such methods fail to precisely pixel-wise segment blood vessels, due to bad illumination, periodic noises, etc. This becomes more problematic when extracting microvessels with the optic-disc present in the background. Moreover, attributes like retinal boundary, bright and dark lesions present in the subspace can create false-positive segmentation <ref type="bibr" target="#b10">[11]</ref>.</p><p>In recent times, UNet based deep learning architectures have become very popular for retinal vessel segmentation. UNet consists of an encoder to capture context information and a decoder for enabling precise localization <ref type="bibr" target="#b32">[33]</ref>. Many derivative works based on UNet have been proposed, such as Dense-UNet, Deformable UNet <ref type="bibr" target="#b14">[15]</ref>, IterNet <ref type="bibr" target="#b23">[24]</ref> etc. These models were able to achieve remarkable results for vessel segmentation. Especially, IterNet achieving stateof-the-art results and precisely detecting microvessel in the retinal subspace. But with the current deep learning architectures, the generated segmentation map deteriorates if the prediction values are thresholded to a higher confidence score. This is clearly illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the threshold value is set to t = 0.5 and for three different data-set, we see that the UNet derived architectures performing poorly. To alleviate this, we need an architecture that can differentiate and discard irrelevant global and local information in the retinal subspace. Additionally, discriminative features need to be extracted and utilized for prediction with a high confidence score. By taking all of these into account, we propose Retinal-Vessel GAN or RV-GAN, consisting of coarse and fine generators and multi-scale autoencoder-based discriminators for producing highly accurate segmentation of blood vessel with strong confidence scores. Additonally, we utilize a new weighted feature matching loss with inner and outer weights and combine it with reconstruction and hinge loss for adversarial training of our architecture. From <ref type="figure" target="#fig_0">Fig. 1</ref>, it is apparent that our architecture produces segmentation map with high confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>For a long time, Fully Convolutional Network (FCN) and its derivative architectures have been the state-of-the-art model for any image segmentation task. FCN is trained endto-end for pixel-wise segmentation and spatially dense prediction tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18]</ref>. The concept is to extract the raw image into a feature vector and transform it into a segmented image. On the other hand, UNet <ref type="bibr" target="#b32">[33]</ref>, an encoder-decoder architecture with skip connections, also utilizes this trait of extracting features from the images. The combination of FCN and UNet has been adopted for many retinal vessel segmentation tasks, such as IterNet <ref type="bibr" target="#b23">[24]</ref>, DUNet <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_0">R2UNet [1] etc.</formula><p>In recent times Generative Architectures have become popular for retinal image synthesis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, denoising <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>, and retinal vessel segmentation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b40">41]</ref>. While most architectures ensure high-quality image generation and image-to-image translation, they all rely on Patch-GAN <ref type="bibr" target="#b13">[14]</ref> as a discriminator. The discriminator uses image patches to find discriminative features and dictate if the image is real or synthesized. Despite using auto-encoders in generators, pixel-wise segmentation result underperforms when coupled with Patch-GAN-based discriminators. The performance of such GAN architectures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>, even fails to exceed some of the single auto-encoder-based UNet architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. Another important factor is, the confidence score is not taken into account while generating segmentation maps. As the outputs activation function can be sigmoid [0 − 1] or TanH [−1, 1] depending on the archi- tecture, the resulting pixel value in grayscale can range between [0−255]. The same predicted segmentation map with low confidence might look highly accurate with a threshold value between 0.1 − 0.2 (the range is between [0 − 1]). Whereas the segmentation starts to deteriorate when the threshold is increased to 0.5 − 0.7. To address all of the aforementioned challenges, an architecture is needed that can generate an accurate segmentation map with high confidence scores. We propose a new multi-scale generative adversarial network, with the following improvements: i) We employ coarse and fine generators with spatial aggregation block and multi-scale autoencoders as a discriminator to extract better discriminative features and synthesize accurate segmentation maps, ii) We incorporate a weighted feature matching loss for discriminators along with a reconstruction loss for generators. The adversarial loss combines all the above along with hinge loss for training the GAN in an end-to-end manner. iii) Lastly, we benchmark our architecture on two new relevant metrics for evaluating accurate segmentation map and structural similarity, namely Mean-IOU and SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methodology</head><p>In the next section, we give a detailed outline of our multi-scale generators in subsection 3.1. Next, we elaborate our Encoder-Decoder, Identity Residual Block and SFA block in subsection 3.2, 3.3, 3.4. After that, we delve into the auto-encoder based discriminators and their connectivity with the multi-scale generators to outline the end-to-end system for the generative network in subsections 3.5. Conclusively, in subsection 3.6, we explain the associated losses with each of the architectures and weight distributions of these losses that form the proposed GAN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale Generators</head><p>Pairing multi-scale coarse and fine generators produces high-quality domain-specific retinal image synthesis, as observed in recent generative networks, such as Fun-dus2Angio <ref type="bibr" target="#b16">[17]</ref>, Attention2Angio <ref type="bibr" target="#b15">[16]</ref> and V-GAN <ref type="bibr" target="#b36">[37]</ref> etc. Inspired by this, we also adopt this feature in our architecture by using two generators (G f ine and G coarse ), as visualized in <ref type="figure" target="#fig_1">Fig. 2</ref>. The generator G f ine synthesizes fine-  In contrast, the generator G coarse tries to learn and conserve global information, such as the structures of the maco branches, while producing less detailed microvessel segmentation. The generators consist of multiple encoders, decoder, spatial-aggregation blocks, residual identity blocks, and a feature appending block between the fine and coarse generators. The input and output dimension of G f ine is 128 × 128. Similarly, G coarse takes an image with half the size of G f ine (64 × 64) and generates a segmentation map of the same size as input. Additionally, the G coarse outputs a feature vector of the size 64 × 64 × 64, which is elementwise summed with one of the intermediate layers of G f ine using the feature appending operation. Both G coarse and G f ine takes real fundus image and a mask for the fovea as input. The detailed structure of these generators is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. In the concurrent sections, we elaborate on each of these atomic blocks in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Encoder and Decoder Blocks</head><p>As both our generators and discriminators are autoencoders, they consist of both encoder blocks for downsampling the feature maps. On the contrary, only the generators use decoder blocks for upsampling to get the desired feature maps and output. The encoder block comprises of a convolution layer, a batch-norm layer <ref type="bibr" target="#b12">[13]</ref> and a Leaky-ReLU activation function consecutively and is illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Contrarily, the decoder block consists of a transposed convolution layer, batch-norm <ref type="bibr" target="#b12">[13]</ref>, and Leaky-ReLU activation layer successively and can be visualized in <ref type="figure" target="#fig_2">Fig.3(b)</ref>. We use the encoder twice for 2× downsampling in G coarse . This is followed by nine successive residual identity blocks. Finally, the decoder blocks are used to upsample 2× again to make the spatial dimension the same as the input. For G f ine , we utilize the encoder once, and after placing three residual identity blocks, a single decoder is employed to get the same spatial dimension as the input. Kernel size, k = 4, and stride, s = 2 is applied for both of our convolutions and transposed convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distinct Identity Blocks for Generator &amp; Discriminator</head><p>For spatial and depth feature propagation, residual identity blocks have become go-to building blocks for image style transfer, image inpainting, and image segmentation tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>. The original design consisted of a residual block with two consecutive convolution layers and a skip connection that adds feature tensor of the input with the output. Vanilla convolution layers are both computationally inefficient and fail to retain accurate spatial and depth information, as opposed to separable convolution <ref type="bibr" target="#b5">[6]</ref>. Separable convolution comprises of a depth-wise convolution and a point-wise convolution successively. As a result, it extracts and preserves depth and spatial features while forward propagating the network. Recent advancement in retinal image classification has shown that combining separable convolutional layers with dilation allows for more robust feature extraction <ref type="bibr" target="#b18">[19]</ref>. We design two unique residual identity blocks, for our generators and discriminators, as illustrated in <ref type="figure" target="#fig_2">Fig. 3(d</ref> <ref type="figure" target="#fig_2">Fig. 3</ref>(e). The identity block for our generator consists of Reflection padding, Separable Convolution, Batch-Norm, and Leaky-ReLU layers followed by two branches of the same repetition of layers. In contrast, one branch consists of a separable convolution with a dilation rate of, d = 1 and the other with a dilation rate, d = 2. All the separable convolution layers have, kernel size, k = 3 and stride, s = 1.. The skip connection and output of the two branches are all added together to produce the final output. In contrast, the residual block for the discriminator consists of two branches, one with a convolution layer, followed by Batch-Normalization and Leaky-ReLU activation function. The other one with the Separable convolution layer, followed by Batch-Norm and Leaky-ReLU layers. Both the convolution and separable convolution have a kernel size of k = 2, dilation rate of d = 2 and stride, s = 1.</p><formula xml:id="formula_1">) &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spatial Feature Aggregation</head><p>In this section, we discuss our proposed spatial feature aggregation (SFA) block, as illustrated in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. The block consists of two residual units. One with Searable Convolution, Batch-Norm, and Leaky-ReLU layer and one with Convolution, BatchNorm, and Leaky-ReLU layers. Both the separable convolution and convolution layer have kernel, k = 3, dilation rate, d = 1 and stride, s = 1. Additionally, there are three skip connections, one going from the input and being element-wise summed to the output of the first residual unit. The next one is coming from the input and being added with the output of the last residual unit. Lastly, the third one is coming out of the output of the first residual unit and being element-wise summed with the output of the last residual unit. We use spatial feature aggregation block for combining spatial and depth features from the bottom layers of the network with the top layers of the network, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. G coarse comprises of two SFA blocks, that connects each of the two encoders with the two decoders successively. Perversely, the G f ine has only one SFA block between the sole encoder and decoder. The rationale behind employing the SFA block is to extract and retain spatial and depth information, that is otherwise lost in deep networks. Consequently, these features can be combined with the learned features of the deeper layers of the network to get an accurate approximation, as observed in similar GAN architectures <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Auto-encoder as Discriminators</head><p>For better pixel-wise segmentation, we need an architecture that can extract both global and local features from the image. To mitigate this underlying problem, we need a deep and dense architecture with lots of computable parameters. It, in turn, might lead to overfitting or vanishing gradient while training the model. To address this issue, rather than having a single dense segmentation architecture, we adopt light-weight discriminators as autoencoders, as proposed in Energy-based GANs <ref type="bibr" target="#b43">[44]</ref>. Additionally, we use multi-scale discriminators for both our coarse and fine generators, as previously proposed in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. The arrangement consists of two discriminators with variable sized input and can help with the overall adversarial training.We define two discriminators as D f and D c ] as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. We resize each of the coarse and fine mask and fundus with size 128 × 128 and 64 × 64 by a factor of 2 using the Lanczos filter <ref type="bibr" target="#b8">[9]</ref>. D f and D c have identical blocks consisting of three repetitive encoder and residual block pairs (in <ref type="figure" target="#fig_2">Fig. 3(a)</ref> and <ref type="figure" target="#fig_2">Fig. 3(e)</ref>). This is followed by three decoders as illustrated in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. Ultimately, a convolution layer is used for getting spatial di-mension of 128×128 for D f and 64×64 for D c as outputs. By doing adversarial training, the discriminators try to distinguish real and synthesize retinal vessel segmentation output. D c learns feature at a lower resolution and convinces the coarse generator, G c , to retain more global features such as the macro branches and connections. Contrarily, the D f dictates the fine generator, G f , to produce more detailed local features such as microvessels and arteries. By doing this we combine features from both G f and G c while training them autonomously with their paired multi-scale autoencoder based discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Weighted Objective and Adversarial Loss</head><p>With the given multi-scale discriminators, D f , D c , and generators, G f , G c ), the objective function can be formed as Eq. 1. It's a multi-objective problem of maximizing the loss of the discriminators while minimizing the loss of the generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>min</head><formula xml:id="formula_2">G f ,Gc max D f ,Dc L adv (G f , G c , D f , D c )<label>(1)</label></formula><p>For adversarial training, we use Hinge-Loss <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26]</ref> as given in Eq. 2 and Eq. 3. Conclusively, all the fundus images and their corresponding segmentation map pairs are normalized, to [−1, 1]. As a result, it broadens the difference between the pixel intensities of the real and synthesized segmentation maps. In Eq. 4, we multiply L adv (G) with λ adv as weight multiplier. Next, we add L adv (D) with the output of the multiplication. </p><p>In Eq. 2 and Eq. 3, we first train the discriminators on the real fundus, x and real segmentation map, y. After that, we train with the real fundus, x, and synthesized segmentation map, G(x). We begin by batch-wise training the discriminators D f , and D c for a couple of iterations on the training data. Following that, we train the G c while keeping the weights of the discriminators frozen. In a similar fashion, we train G f on a batch training image while keeping weights of all the discriminators frozen. The generators also incorporate the reconstruction loss (Mean Squared Error) as shown in Eq. 5. By utilizing the loss we ensure the synthesized images contain more realistic microvessel, arteries, and vascular structure. We also incorporate a weighted feature matching loss with all our discriminators and is given in Eq. 6. </p><formula xml:id="formula_4">L rec (G) = E x,y G(x) − y 2<label>(5)</label></formula><formula xml:id="formula_5">L f m (G, D n ) = E x,y k i=1 λ i w N D i n (x, y) − D i n (x, G(x))</formula><p>(6) For Eq. 5, L rec is the reconstruction loss for humanannotated segmentation map, y, given a generated segmentation map, G(x). We use this loss for both G f and G c so that the model can generate highly accurate segmentation maps of different scales. Eq. 6 is calculated by taking the features from each of the output of the encoder and decoder blocks of the discriminator by first inserting the real and synthesize segmentation maps consecutively. Here, λ w is the inner weight multiplier for each of the extracted feature maps. The weight values are between [0, 1], and we use a higher weight value for the decoder feature maps than the encoder feature maps. By incorporating Eq. 4, 5 and 6, we can formulate our final objective function as given in Eq. 7.</p><formula xml:id="formula_6">min G f ,Gc max D f ,Dc (L adv (G f , G c , D f , D c ))+ λ rec L rec (G f , G c ) + λ f m L f m (G f , G c , D f , D c )<label>(7)</label></formula><p>Here, λ adv , λ rec , and λ f m implies different weights, that is multiplied with their respective losses. The loss weighting decides which architecture to prioritize while training. For our system, more weights are given to the L adv (G), L rec , L f m , and thus we select bigger λ values for those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the next section, we detail our model experiments and evaluate our architecture based on qualitative and quantitative metrics. First, we elaborate on the structuring and pre-processing of our dataset in Sec. 4.1. Then detail our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 RV-GAN training</head><p>Input: for d iter = 0 to max d iter do 5:</p><formula xml:id="formula_7">x i X, m i M , y i Y Output: G f , G C 1: Initialize hyper-parameters: max epoch, b, max d iter, ω f D , ω c D , ω f G , ω c G α f D , α c D , α f G , α c G , β f D , β c D , β f G , β c G , λ rec , λ f m , λ w , λ</formula><formula xml:id="formula_8">L adv (D) ← D c (x c , y c ), D f (x f , y f ) 6: L adv (D) ← D c (x c , G c (x c , m c )), D f (x f , G F (x f , m f )) 7: L adv (G) ← G c (x c , m c ), G F (x f , m f ) 8: L adv (G, D) ← L adv (D) + λ adv (L adv (G)) 9: ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D )</formula><p>10:</p><formula xml:id="formula_9">ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D ) 11: end for Freeze ω c D , ω f D 12</formula><p>:</p><formula xml:id="formula_10">Sample x f , x c , y f , y c , using batch-size b 13: L rec (G c ) ← G(x c , m c ), y c 14: L rec (G f ) ← G(x f , m f ), y f 15: ω c G ← ω c G + Adam(G c , ω c G , α c G , β c G ) 16: ω f G ← ω f G + Adam(G f , ω f G , α f G , β f G ) Unfreeze ω c D , ω f D 17: L f m (D c , λ w ) ← D n c (x c , y c ), D n f (x c , G(x c , m c )), λ n w 18: L f m (D f , λ w ) ← D n f (x f , y f ), D n f (x f , G(x f , m f )), λ n w 19</formula><p>:</p><formula xml:id="formula_11">ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D )</formula><p>20:</p><formula xml:id="formula_12">ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D ) Freeze ω c D , ω f D 21: L adv (D) ← D c (x c , y c ), D f (x f , y f ) 22: L adv (D) ← D c (x c , G c (x c , m c )), D f (x f , G F (x f , m f )) 23: L adv (G) ← G c (x c , m c ), G f (x f , m f ) 24:</formula><p>L adv (G, D) ← L adv (D) + λ adv (L adv (G)) <ref type="bibr" target="#b24">25</ref>:</p><formula xml:id="formula_13">ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D )</formula><p>26:</p><formula xml:id="formula_14">ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>Save weights and snapshot of G f , G c 28: </p><formula xml:id="formula_15">L ← L adv + λ rec (L rec ) + λ f m (L f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>For benchmarking our model's performance, we use three retinal segmentation datasets, namely, DRIVE [38],</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hyper-parameter Initialization</head><p>For adversarial training, we used hinge loss <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26]</ref>. We picked λ adv = 10 (Eq. 4), λ rec = 10 (Eq. 5) and λ w = 10 ,λ f m = 10 (Eq. 6). We used Adam optimizer <ref type="bibr" target="#b20">[21]</ref>, with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We train with mini-batches with batch size, b = 24 for 100 epochs in three stages. It took between 24-48 hours to train our model on NVIDIA P100 GPU depending on which data-set the model was trained on. Because DRIVE and STARE have lower number of image patches compared to CHASE-DB1 dataset, it takes less amount to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Steps</head><p>In this section, we elaborate on our detailed algorithm provided in Algorithm 1. To train our RV-GAN, we start by initializing all the hyper-parameters. Next, we sample a batch of the real fundus, mask, and segmentation images, x, m, y. We train the real fundus and segmentation map pairs with D f , D c . After that, we use G f , G c to synthesize segmentation map and use the real fundus and synthesized segmentation map, x, G(x, m) to train discriminators D f , D c . Following that, we calculate the adversarial loss, L adv (D, G), and update the weights. We freeze the weights of the discriminators. Next, we train the generators and calculate the L rec (G) losses and update both generator's weights. Subsequently, we unfreeze both discriminators weights, calculate the feature matching loss L f m (D) and update the discriminator weights. In the final stage, we freeze both discriminator's weights and jointly fine-tune all the discriminator and generators. We calculate the total loss by adding and multiplying with their relative weights. For testing, we save the snapshot of the model and its weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative Bench-marking</head><p>We compared our architecture with some best performing ones, including UNet <ref type="bibr" target="#b14">[15]</ref>, DenseBlock-UNet <ref type="bibr" target="#b24">[25]</ref>, Deform-UNet <ref type="bibr" target="#b14">[15]</ref> and IterNet <ref type="bibr" target="#b23">[24]</ref> as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We trained and evaluated the first three architectures using their publicly available source code by ourselves on the three datasets. For IterNet, the pre-trained weight was provided, so we used that to get the inference result. Receiveroperating-characteristics (ROC) curve, illustrated in <ref type="figure" target="#fig_7">Fig. 5</ref>, was generated using the same weights and with a stride of 3 and image patches size of 128 × 128. We visualize ROC for DRIVE in <ref type="figure" target="#fig_7">Fig. 5(a)</ref>, STARE in <ref type="figure" target="#fig_7">Fig. 5(b)</ref>, and CHASE-DB1 in <ref type="figure" target="#fig_7">Fig. 5(c)</ref>, where our produced ROC shows better overall performance over other architectures. Next, we do a comparative analysis with existing retinal vessel segmentation architectures, which includes both UNet and GAN based models, such as Residual UNet <ref type="bibr" target="#b0">[1]</ref>, Recurrent UNet <ref type="bibr" target="#b0">[1]</ref>, R2UNet <ref type="bibr" target="#b0">[1]</ref>, and SUD-GAN <ref type="bibr" target="#b40">[41]</ref>, M-GAN <ref type="bibr" target="#b29">[30]</ref>. The prediction results for DRIVE are provided in <ref type="table">Table.</ref> 1, CHASE-DB1 in <ref type="table">Table.</ref> 2 and STARE in <ref type="table">Table.</ref> 3. We report traditional metrics such as F1-score, Sensitivity, Specificity, Accuracy, and AUC-ROC. Additionally, we use two other metrics for predicting accurate segmentation and structural similarity of the retinal vessels, namely Mean-IOU (Jaccard Similarity Coefficient) and Structural Similarity Index <ref type="bibr" target="#b39">[40]</ref>. We chose Mean-IOU because its the gold standard for measuring segmentation results for many Semantic Segmentation Challenges such as Pascal-VOC2012 <ref type="bibr" target="#b9">[10]</ref>, MS-COCO <ref type="bibr" target="#b26">[27]</ref>, Cityscapes <ref type="bibr" target="#b6">[7]</ref>. Contrarily, SSIM is a standard metric for evaluating GANs for image-to-image translation tasks. As illustrated in all the tables, our model outperforms both UNet derived architectures and recent GAN based models in terms of AUC-ROC, Mean-IOU, and SSIM, the three main metrics for this task. In <ref type="table" target="#tab_2">Table. 2 and Table.</ref> 3, M-GAN achieves better Specificity and Accuracy. However, higher Specificity means better background pixel segmentation (True Negative), which is less essential than having better retinal vessel segmentation (True Positive). We want both, better Sensitivity and AUC-ROC, which equates to having a higher confidence score with threshold, t ≥ 0.5. In <ref type="figure" target="#fig_7">Fig. 5</ref> we can see that our True positive Rate (TPR) is always better than other architectures for all three data-set. As source codes and pre-trained weights weren't provided for some of the architectures, we couldn't report SSIM and Mean-IOU for those in <ref type="table" target="#tab_1">Table. 1, Table. 2, and Table.</ref> 3.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new multi-scale generative architecture called RV-GAN. The architecture synthesizes precise venular structure segmentation with high confidence scores. Additionally ,we illustrated its robust segmentation generation and retaining similarity with two new metrics. As a result, the proposed network can be efficiently employed to generate precise retinal blood vessel segmentation for various applications of ophthalmology. This is best suited for finding analyzing retinal degenerative diseases and monitoring for future prognosis. We hope to extend this work for other areas of ophthalmological data modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RV-GAN segments vessel with better precision than other architectures. The 1st row is the whole image, while 2nd row is specific zoomed-in area of the image. The Red bounded box signifies the zoomed-in region. The figure illustrates retinal vessel segmentation with threshold, t = 0.5. RV-GAN generates better segmentation map with high confidence scores. The row contains DRIVE, CHASE-DB1 and STARE data-set. Whereas the column contains fundus images, humanannotations and segmentation maps for RV-GAN, DFUNet, IterNet and UNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>RV-GAN consisting of Coarse and Fine generators G f , G c and discriminators D f , D c . The generators incorporates reconstruction loss, Loss rec and Hinge loss Loss G . Whereas the discriminators uses weighted feature matching loss, Loss f m and Hinge loss Loss D . All of these losses are multiplied by weight multiplier and then added in the final adversarial loss, Loss adv . The generators consists of Encoder, Decoder, SFA and its distinct residual blocks. On the other hand, the discriminators consists of Encoder, Decoder and counterpart residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Proposed Encoder, Decoder, Spatial Feature Aggregation block, Generator and Discriminator Residual blocks. Here, K = Kernel size, S = Stride, D = Dilation Rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>grained vessel segmentation images by extracting local features such as micro branches, connections, blockages, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>L</head><label></label><figDesc>adv (D) = −E x,y min(0, −1 + D(x, y)) − E x min(0, −1 − D(x, G(x))) (2) L adv (G) = −E x,y (D(G(x), y))(3)L adv (G, D) = L adv (D) + λ adv (L adv (G))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Raw images, masks and annotation from the dataset (a) DRIVE (b) CHASE-DB1 (c) STARE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>adv 2 : 3 :</head><label>23</label><figDesc>for e = 0 to max epoch do Sample x f , x c , m f , m c , y f , y c , using batch-size b 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>ROC Curves on (a) DRIVE (b) STARE (c) CHASE-DB1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>m ) 29: end for hyper-parameter selection and tuning in Sec. 4.2. Next, we describe our adversarial training scheme in Sec. 4.3. Lastly, we compare our architecture with existing state-ofthe-art generative models based on some quantitative evaluation metrics in Sec. 4.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on the DRIVE<ref type="bibr" target="#b37">[38]</ref> dataset.</figDesc><table><row><cell>Method</cell><cell cols="8">Year F1 Score Sensitivity Specificity Accuracy AUC-ROC Mean-IOU SSIM</cell></row><row><cell>UNet [15]</cell><cell>2018</cell><cell>0.8174</cell><cell>0.7822</cell><cell>0.9808</cell><cell>0.9555</cell><cell>0.9752</cell><cell>0.9635</cell><cell>0.8868</cell></row><row><cell cols="2">Residual UNet [1] 2018</cell><cell>0.8149</cell><cell>0.7726</cell><cell>0.9820</cell><cell>0.9553</cell><cell>0.9779</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Recurrent UNet [1] 2018</cell><cell>0.8155</cell><cell>0.7751</cell><cell>0.9816</cell><cell>0.9556</cell><cell>0.9782</cell><cell>-</cell><cell>-</cell></row><row><cell>R2UNet [1]</cell><cell>2018</cell><cell>0.8171</cell><cell>0.7792</cell><cell>0.9813</cell><cell>0.9556</cell><cell>0.9784</cell><cell>-</cell><cell>-</cell></row><row><cell>DFUNet [15]</cell><cell>2019</cell><cell>0.8190</cell><cell>0.7863</cell><cell>0.9805</cell><cell>0.9558</cell><cell>0.9778</cell><cell>0.9605</cell><cell>0.8789</cell></row><row><cell>IterNet [24]</cell><cell>2019</cell><cell>0.8205</cell><cell>0.7735</cell><cell>0.9838</cell><cell>0.9573</cell><cell>0.9816</cell><cell>0.9692</cell><cell>0.9008</cell></row><row><cell>SUD-GAN [41]</cell><cell>2020</cell><cell>-</cell><cell>0.8340</cell><cell>0.9820</cell><cell>0.9560</cell><cell>0.9786</cell><cell>-</cell><cell>-</cell></row><row><cell>M-GAN [30]</cell><cell>2020</cell><cell>0.8324</cell><cell>0.8346</cell><cell>0.9836</cell><cell>0.9706</cell><cell>0.9868</cell><cell>-</cell><cell>-</cell></row><row><cell>RV-GAN (Ours)</cell><cell>2020</cell><cell>0.8690</cell><cell>0.7927</cell><cell>0.9969</cell><cell>0.9790</cell><cell>0.9887</cell><cell>0.9762</cell><cell>0.9237</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the CHASE-DB1<ref type="bibr" target="#b28">[29]</ref> dataset.</figDesc><table><row><cell>Method</cell><cell cols="8">Year F1 Score Sensitivity Specificity Accuracy AUC-ROC Mean-IOU SSIM</cell></row><row><cell>UNet [15]</cell><cell>2018</cell><cell>0.7993</cell><cell>0.7841</cell><cell>0.9823</cell><cell>0.9643</cell><cell>0.9812</cell><cell>0.9536</cell><cell>0.9029</cell></row><row><cell cols="2">DenseBlock-UNet [25] 2018</cell><cell>0.8006</cell><cell>0.8178</cell><cell>0.9775</cell><cell>0.9631</cell><cell>0.9826</cell><cell>0.9454</cell><cell>0.8867</cell></row><row><cell>DFUNet [15]</cell><cell>2019</cell><cell>0.8001</cell><cell>0.7859</cell><cell>0.9822</cell><cell>0.9644</cell><cell>0.9834</cell><cell>0.9609</cell><cell>0.9175</cell></row><row><cell>IterNet [24]</cell><cell>2019</cell><cell>0.8073</cell><cell>0.7970</cell><cell>0.9823</cell><cell>0.9655</cell><cell>0.9851</cell><cell>0.9584</cell><cell>0.9123</cell></row><row><cell>M-GAN [30]</cell><cell>2020</cell><cell>0.8110</cell><cell>0.8234</cell><cell>0.9938</cell><cell>0.9736</cell><cell>0.9859</cell><cell>-</cell><cell>-</cell></row><row><cell>RV-GAN (Ours)</cell><cell>2020</cell><cell>0.8957</cell><cell>0.8199</cell><cell>0.9806</cell><cell>0.9697</cell><cell>0.9914</cell><cell>0.9705</cell><cell>0.9266</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on the STARE<ref type="bibr" target="#b11">[12]</ref> dataset.</figDesc><table><row><cell>Method</cell><cell cols="8">Year F1 Score Sensitivity Specificity Accuracy AUC-ROC Mean-IOU SSIM</cell></row><row><cell>UNet [15]</cell><cell>2018</cell><cell>0.7595</cell><cell>0.6681</cell><cell>0.9915</cell><cell>0.9639</cell><cell>0.9710</cell><cell>0.9744</cell><cell>0.9271</cell></row><row><cell cols="2">DenseBlock-UNet [25] 2018</cell><cell>0.7691</cell><cell>0.6807</cell><cell>0.9916</cell><cell>0.9651</cell><cell>0.9755</cell><cell>0.9604</cell><cell>0.9034</cell></row><row><cell>DFUNet [15]</cell><cell>2019</cell><cell>0.7629</cell><cell>0.6810</cell><cell>0.9903</cell><cell>0.9639</cell><cell>0.9758</cell><cell>0.9701</cell><cell>0.9169</cell></row><row><cell>IterNet [24]</cell><cell>2019</cell><cell>0.8146</cell><cell>0.7715</cell><cell>0.9886</cell><cell>0.9701</cell><cell>0.9881</cell><cell>0.9752</cell><cell>0.9219</cell></row><row><cell>SUD-GAN [41]</cell><cell>2020</cell><cell>-</cell><cell>0.8334</cell><cell>0.9897</cell><cell>0.9663</cell><cell>0.9734</cell><cell>-</cell><cell>-</cell></row><row><cell>M-GAN [30]</cell><cell>2020</cell><cell>0.8370</cell><cell>0.8234</cell><cell>0.9938</cell><cell>0.9876</cell><cell>0.9873</cell><cell>-</cell><cell>-</cell></row><row><cell>RV-GAN (Ours)</cell><cell>2020</cell><cell>0.8323</cell><cell>0.8356</cell><cell>0.9864</cell><cell>0.9754</cell><cell>0.9887</cell><cell>0.9754</cell><cell>0.9292</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Md Zahangir Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-gan for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="164" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dn-gan: Denoising generative adversarial networks for speckle noise reduction in optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zailiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peishan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">101632</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optical coherence tomography image denoising using a generative adversarial network with speckle modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Jerwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biophotonics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">201960135</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Blood vessel segmentation methodologies in retinal images-a survey. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Muhammad Moazam Fraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bunyarit</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alicja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="407" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kouznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dunet: A deformable network for retinal vessel segmentation. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangguo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fariha Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09191</idno>
		<title level="m">Atten-tion2angiogan: Synthesizing fluorescein angiography from retinal fundus images using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fariha Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05267</idno>
		<title level="m">Fundus2angio: A novel conditional gan architecture for generating fluorescein angiography images from retinal fundus photography</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient yet deep convolutional neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Sabbir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Symposium on Advanced Intelligent Informatics (SAIN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optic-net: A novel convolutional neural network for diagnosis of retinal diseases from optical tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving robustness using joint attention network for detecting retinal degeneration from optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><forename type="middle">Lee</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automated feature extraction in color retinal images by a model based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Opas</forename><surname>Chutatape</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="254" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iternet: Retinal image segmentation utilizing structural redundancy in vessel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3656" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computerassisted image analysis of the retina (caiar) program. Investigative ophthalmology &amp; visual science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher G Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alicja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Rudnicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothy</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Whincup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyeong-Beom</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Yeol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mgan</surname></persName>
		</author>
		<title level="m">Retinal blood vessel segmentation by balancing losses through stacked deep fully convolutional networks. IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retinal blood vessel segmentation using line operators and support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzo</forename><surname>Perfetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">Jg</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Jun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu-Hwan</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09318</idno>
		<title level="m">Retinal vessel segmentation in fundoscopic images with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards accurate segmentation of retinal vessels and the optic disc in fundoscopic images with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Jun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu-Hwan</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="499" to="512" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joes</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meindert</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">A</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sudgan: Deep convolution generative adversarial network combined with short connection and dense block for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multimodal registration algorithm of eye fundus images using vessels detection and hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Zana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Claude</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
