<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Densely Connected Search Space for More Flexible Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhu</forename><surname>Sun</surname></persName>
							<email>yzsun@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<email>qian01.zhang@horizon.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
							<email>yuan.li@horizon.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Densely Connected Search Space for More Flexible Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has dramatically advanced the development of neural network design. We revisit the search space design in most previous NAS methods and find the number and widths of blocks are set manually. However, block counts and block widths determine the network scale (depth and width) and make a great influence on both the accuracy and the model cost (FLOPs/latency). In this paper, we propose to search block counts and block widths by designing a densely connected search space, i.e., DenseNAS. The new search space is represented as a dense super network, which is built upon our designed routing blocks. In the super network, routing blocks are densely connected and we search for the best path between them to derive the final architecture. We further propose a chained cost estimation algorithm to approximate the model cost during the search. Both the accuracy and model cost are optimized in DenseNAS. For experiments on the MobileNetV2based search space, DenseNAS achieves 75.3% top-1 accuracy on ImageNet with only 361MB FLOPs and 17.9ms latency on a single TITAN-XP. The larger model searched by DenseNAS achieves 76.1% accuracy with only 479M FLOPs. DenseNAS further promotes the ImageNet classification accuracies of ResNet-18, -34 and -50-B by 1.5%, 0.5% and 0.3% with 200M, 600M and 680M FLOPs reduction respectively. The related code is available at https: //github.com/JaminFong/DenseNAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, neural architecture search (NAS) <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> has demonstrated great successes in designing neural architectures automatically and achieved remarkable performance gains in various tasks such as image classification <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b36">37]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref> and object detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47]</ref>. NAS has been a critically important topic † The work is performed during the internship at Horizon Robotics. ‡ Corresponding author. The block widths are set manually as well. Bottom: The search space in DenseNAS allows more blocks with various widths in each stage. Each block is densely connected to its subsequent ones. We search for the best path (the red line) to derive the final architecture, in which the number of blocks in each stage and the widths of blocks are allocated automatically.</p><p>for architecture designing. In NAS research, the search space plays a crucial role that constrains the architectures in a prior-based set. The performance of architectures produced by NAS methods is strongly associated with the search space definition. A more flexible search space has the potential to bring in architectures with more novel structures and promoted performance. We revisit and analyze the search space design in most previous works <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>. For a clear illustration, we review the following definitions. Block denotes a set of layers/operations in the network which output feature maps with the same spatial resolution and the same width (number of channels). Stage denotes a set of sequential blocks whose outputs are under the same spatial resolution settings. Different blocks in the same stage are allowed to have various widths. Many recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b7">8]</ref> stack the inverted residual convolution modules (MBConv) defined in MobileNetV2 <ref type="bibr" target="#b39">[40]</ref> to construct the search space. They search for different kernel sizes and expansion ratios in each MBConv. The depth is searched in terms of layer numbers in each block. The searched networks with MBConvs show high performance with low latency or few FLOPs.</p><p>In this paper, we aim to perform NAS in a more flexible search space. Our motivation and core idea are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. As the upper part of <ref type="figure" target="#fig_0">Fig. 1</ref> shows, the number of blocks in each stage and the width of each block are set manually and fixed during the search process. It means that the depth search is constrained within the block and the width search cannot be performed. It is worth noting that the scale (depth and width) setting is closely related to the performance of a network, which has been demonstrated in many previous theoretical studies <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34]</ref> and empirical results <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref>. Inappropriate width or depth choices usually cause drastic accuracy degradation, significant computation cost, or unsatisfactory model latency. Moreover, we find that recent works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b7">8]</ref> manually tune width settings to obtain better performance, which indicates the design of network width demands much prior-based knowledge and trial-and-error.</p><p>We propose a densely connected search space to tackle the above obstacles and name our method as DenseNAS. We show our novelly designed search space schematically in the bottom part of <ref type="figure" target="#fig_0">Fig. 1</ref>. Different from the search space design principles in the previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref>, we allow more blocks with various widths in one stage. Specifically, we design the routing blocks to construct the densely connected super network which is the representation of the search space. From the beginning to the end of the search space, the width of the routing block increases gradually to cover more width options. Every routing block is connected to several subsequent ones. This formulation brings in various paths in the search space and we search for the best path to derive the final architecture. As a consequence, the block widths and counts in each stage are allocated automatically. Our method extends the depth search into a more flexible space. Not only the number of layers within one block but also the number of blocks within one stage can be searched. The block width search is enabled as well. Moreover, the positions to conduct spatial down-sampling operations are determined along with the block counts search.</p><p>We integrate our search space into the differentiable NAS framework by relaxing the search space. We assign a probability parameter to each output path of the routing block. During the search process, the distribution of probabilities is optimized. The final block connection paths in the super network are derived based on the probability distribution. To optimize the cost (FLOPs/latency) of the network, we design a chained estimation algorithm targeted at approximating the cost of the model during the search.</p><p>Our contributions can be summarized as follows.</p><p>• We propose a densely connected search space that en-ables network/block widths search and block counts search. It provides more room for searching better networks and further reduces expert designing efforts.</p><p>• We propose a chained cost estimation algorithm to precisely approximate the computation cost of the model during search, which makes the DenseNAS networks achieve high performance with low computation cost.</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Search Space Design NASNet <ref type="bibr" target="#b52">[53]</ref> is the first work to propose a cell-based search space, where the cell is represented as a directed acyclic graph with several nodes inside. NASNet searches for the operation types and the topological connections in the cell and repeat the searched cell to form the whole network architecture. The depth of the architecture (i.e., the number of repetitions of the cell), the widths and the occurrences of down-sampling operations are all manually set. Afterwards, many works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31</ref>] adopt a similar cell-based search space. However, architectures generated by cell-based search spaces are not friendly in terms of latency or FLOPs. Then MnasNet <ref type="bibr" target="#b42">[43]</ref> stacks MBConvs defined in MobileNetV2 <ref type="bibr" target="#b39">[40]</ref> to construct a search space for searching efficient architectures. Some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9]</ref> simplify the search space by searching for the expansion ratios and kernel sizes of MBConv layers. Some works study more about the search space. Liu et al. <ref type="bibr" target="#b29">[30]</ref> proposes a hierarchical search space that allows flexible network topologies (directed acyclic graphs) at each level of the hierarchies. Auto-DeepLab <ref type="bibr" target="#b27">[28]</ref> creatively designs a two-level hierarchical search space for semantic segmentation networks. CAS <ref type="bibr" target="#b49">[50]</ref> customizes the search space design for real-time segmentation networks. RandWire <ref type="bibr" target="#b45">[46]</ref> explores randomly wired architectures by designing network generators that produce new families of models for searching. Our proposed method designs a densely connected search space beyond conventional search constrains to generate the architecture with a better trade-off between accuracy and model cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>···</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing Block</head><p>Basic Layer NAS Method Some early works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51]</ref> propose to search architectures based on reinforcement learning (RL) methods. Then evolutionary algorithm (EA) based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> achieve great performance. However, RL and EA based methods bear huge computation cost. As a result, ENAS <ref type="bibr" target="#b36">[37]</ref> proposes to use weight sharing for reducing the search cost.</p><p>Recently, the emergence of differentiable NAS methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref> and one-shot methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> greatly reduces the search cost and achieves superior results. DARTS <ref type="bibr" target="#b30">[31]</ref> is the first work to utilize the gradient-based method to search neural architectures. They relax the architecture representation as a super network by assigning continuous weights to the candidate operations. They first search on a small dataset, e.g., CIFAR-10 <ref type="bibr" target="#b23">[24]</ref>, and then apply the architecture to a large dataset, e.g., ImageNet <ref type="bibr" target="#b10">[11]</ref>, with some manual adjustments. ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> reduces the memory consumption by adopting a dropping path strategy and conducts search directly on the large scale dataset, i.e., Ima-geNet. FBNet <ref type="bibr" target="#b44">[45]</ref> searches on the subset of ImageNet and uses the Gumbel Softmax function <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref> to better optimize the distribution of architecture probabilities. TAS <ref type="bibr" target="#b12">[13]</ref> utilizes a differentiable NAS scheme to search and prune the width and depth of the network and uses knowledge distillation (KD) <ref type="bibr" target="#b19">[20]</ref> to promote the performance of the pruned network. FNA <ref type="bibr" target="#b14">[15]</ref> proposes to adapt the neural network to new tasks with low cost by a parameter remap-ping mechanism and differentiable NAS. It is challenging for differentiable/one-shot NAS methods to search for more flexible architectures as they need to integrate all subarchitectures into the super network. The proposed Dense-NAS tends to solve this problem by integrating a densely connected search space into the differentiable paradigm and explores more flexible search schemes in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first introduce how to design the search space targeted at a more flexible search. A routing block is proposed to construct the densely connected super network. Secondly, we describe the method of relaxing the search space into a continuous representation. Then, we propose a chained cost estimation algorithm to approximate the model cost during the search. Finally, we describe the whole search procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Densely Connected Search Space</head><p>As shown in <ref type="figure">Fig. 2</ref>, we define our search space using the following three terms, i.e., (basic layer, routing block and dense super Network). Firstly, a basic layer is defined as a set of all the candidate operations. Then we propose a novel routing block which can aggregate tensors from different routing blocks and transmit tensors to multiple other routing blocks. Finally, the search space is constructed as a dense super network with many routing blocks where there are various paths to transmit tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Basic Layer</head><p>We define the basic layer to be the elementary structure in our search space. One basic layer represents a set of candidate operations which include MBConvs and the skip connection. MBConvs are with kernel sizes of {3, 5, 7} and expansion ratios of {3, 6}. The skip connection is for the depth search. If the skip connection is chosen, the corresponding layer is removed from the resulting architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Routing Block</head><p>For the purpose of establishing various paths in the super network, we propose the routing block with the ability of aggregating tensors from preceding routing blocks and transmit tensors to subsequent ones. We divide the routing block into two parts, shape-alignment layers and basic layers.</p><p>Shape-alignment layers exist in the form of several parallel branches, while every branch is a set of candidate operations. They take input tensors with different shapes (including widths and spatial resolutions) which come from multiple preceding routing blocks and transform them into tensors with the same shape. As shape-alignment layers are required for all routing blocks, we exclude the skip connection in candidate operations of them. Then tensors processed by shape-alignment layers are aggregated and sent to several basic layers. The subsequent basic layers are used for feature extraction whose depth can also be searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Dense Super Network</head><p>Many previous works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref> manually set a fixed number of blocks, and retain all the blocks for the final architecture. Benefiting from the aforementioned structures of routing blocks, we introduce more routing blocks with various widths to construct the dense super network which is the representation of the search space. The final searched architecture is allowed to select a subset of the routing blocks and discard the others, giving the search algorithm more room.</p><p>We define the super network as N sup and assume it to consist of N routing blocks,</p><formula xml:id="formula_0">N sup = {B 1 , B 2 , ..., B N }.</formula><p>The network structure is shown in <ref type="figure">Fig. 2</ref>. We partition the entire network into several stages. As Sec. 1 defines, each stage contains routing blocks with various widths and the same spatial resolution. From the beginning to the end of the super network, the widths of routing blocks grow gradually. In the early stage of the network, we set a small growing stride for the width because large width settings in the early network stage will cause huge computational cost. The growing stride becomes larger in the later stages. This design principle of the super network allows more possibilities of block counts and block widths.</p><p>We assume that each routing block in the super network connects to M subsequent ones. We define the connection between the routing block B i and its subsequent routing block B j (j &gt; i) as C ij . The spatial resolutions of B i and B j are H i × W i and H j × W j respectively (normally H i = W i and H j = W j ). We set some constraints on the connections to avoid the stride of the spatial down-sampling exceeding 2. Specifically, C ij only exists when j − i ≤ M and H i /H j ≤ 2. Following the above paradigms, the search space is constructed as a dense super network based on the connected routing blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relaxation of Search Space</head><p>We integrate our search space by relaxing the architectures into continuous representations. The relaxation is implemented on both the basic layer and the routing block. We can search for architectures via back-propagation in the relaxed search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Relaxation in the Basic Layer</head><p>Let O be the set of candidate operations described in Sec. 3.1.1. We assign an architecture parameter α o to the candidate operation o ∈ O in basic layer . We relax the basic layer by defining it as a weighted sum of outputs from all candidate operations. The architecture weight of the operation is computed as a softmax of architecture parameters over all operations in the basic layer:</p><formula xml:id="formula_1">w o = exp(α o ) o ∈O exp(α o )</formula><p>.</p><p>(1)</p><p>The output of basic layer can be expressed as</p><formula xml:id="formula_2">x +1 = o∈O w o · o(x ),<label>(2)</label></formula><p>where x denotes the input tensor of basic layer .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relaxation in the Routing Block</head><p>We assume that the routing block B i outputs the tensor b i and connects to m subsequent blocks. To relax the block connections as a continuous representation, we assign each output path of the block an architecture parameter. Namely the path from B i to B j has a parameter β ij . Similar to how we compute the architecture weight of each operation above, we compute the probability of each path using a softmax function over all paths between the two routing blocks:</p><formula xml:id="formula_3">p ij = exp(β ij ) m k=1 exp(β ik ) .<label>(3)</label></formula><p>For routing block B i , we assume it takes input tensors from its m preceding routing blocks</p><formula xml:id="formula_4">(B i−m , B i−m +1 , B i−m +2 ... B i−1 )</formula><p>. As shown in <ref type="figure">Fig. 2</ref>, the input tensors from these routing blocks differ in terms of width and spatial resolution. Each input tensor is transformed to a same size by the corresponding branch of shape-alignment layers in B i . Let H ik denotes the kth transformation branch in B i which is applied to the input tensor from B i−k , where k = 1 . . . m . Then the input tensors processed by shapealignment layers are aggregated by a weighted-sum using the path probabilities,</p><formula xml:id="formula_5">x i = m k=1 p i−k,i · H ik (x i−k ).<label>(4)</label></formula><p>It is worth noting that the path probabilities are normalized on the output dimension but applied on the input dimension (more specifically on the branches of shape-alignment layers). One of the shape-alignment layers is essentially a weighted-sum mixture of the candidate operations. The layer-level parameters α control which operation to be selected, while the outer block-level parameters β determine how blocks connect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Chained Cost Estimation Algorithm</head><p>We propose to optimize both the accuracy and the cost (latency/FLOPs) of the model. To this end, the model cost needs to be estimated during the search. In conventional cascaded search spaces, the total cost of the whole network can be computed as a sum of all the blocks. Instead, the global effects of connections on the predicted cost need to be taken into consideration in our densely connected search space. We propose a chained cost estimation algorithm to better approximate the model cost.</p><p>We create a lookup table which records the cost of each operation in the search space. The cost of every operation is measured separately. During the search, the cost of one basic layer is estimated as follows,</p><formula xml:id="formula_6">cost = o∈O w o · cost o ,<label>(5)</label></formula><p>where cost o refers to the pre-measured cost of operation o ∈ O in layer . We assume there are N routing blocks in total (B 1 , . . . , B N ). To estimate the total cost of the whole network in the densely connected search space, we define the chained cost estimation algorithm as follows.</p><formula xml:id="formula_7">cost N = cost N b cost i = cost i b + i+m j=i+1 p ij · (cost ij align + cost j b ),<label>(6)</label></formula><p>where cost i b denotes the total cost of all the basic layers of B i which can be computed as a sum cost i b = cost i, b , m denotes the number of subsequent routing blocks to which B i connects, p ij denotes the path probability between B i and B j , and cost ij align denotes the cost of the shape-alignment layer in block B j which processes the data from block B i .</p><p>The cost of the whole architecture can thus be obtained by computingcost 1 with a recursion mechanism, cost =cost 1 .</p><p>We design a loss function with the cost-based regularization to achieve the multi-objective optimization:</p><formula xml:id="formula_9">L(w, α, β) = L CE + λ log τ cost,<label>(8)</label></formula><p>where λ and τ are the hyper-parameters to control the magnitude of the model cost term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Search Procedure</head><p>Benefiting from the continuously relaxed representation of the search space, we can search for the architecture by updating the architecture parameters (introduced in Sec. 3.2) using stochastic gradient descent. We find that at the beginning of the search process, all the weights of the operations are under-trained. The operations or architectures which converge faster are more likely to be strengthened, which leads to shallow architectures. To tackle this, we split our search procedure into two stages. In the first stage, we only optimize the weights for enough epochs to get operations sufficiently trained until the accuracy of the model is not too low. In the second stage, we activate the architecture optimization. We alternatively optimize the operation weights by descending ∇ w L train (w, α, β) on the training set, and optimize the architecture parameters by descending ∇ α,β L val (w, α, β) on the validation set. Moreover, a dropping-path training strategy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> is adopted to decrease memory consumption and decouple different architectures in the super network.</p><p>When the search procedure terminates, we derive the final architecture based on the architecture parameters α, β. At the layer level, we select the candidate operation with the maximum architecture weight, i.e., arg max o∈O α o . At the network level, we use the Viterbi algorithm <ref type="bibr" target="#b15">[16]</ref> to derive the paths connecting the blocks with the highest total transition probability based on the output path probabilities. Every block in the final architecture only connects to the next one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first show the performance with the MobileNetV2 [40]-based search space on ImageNet <ref type="bibr" target="#b9">[10]</ref> classification. Then we apply the architectures searched on ImageNet to object detection on COCO <ref type="bibr" target="#b25">[26]</ref>. We further extend our DenseNAS to the ResNet [19]-based search space. Finally, we conduct some ablation studies and analysis. The implementation details are provided in the appendix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance on MobileNetV2-based Search Space</head><p>We implement DenseNAS on the MobileNetV2 [40]based search space, set the GPU latency as our secondary optimization objective, and search models with different sizes under multiple latency optimization magnitudes (defined in Eq. 8). The ImageNet results are shown in Tab. 1. We divide Tab. 1 into several parts and compare Dense-NAS models with both manually designed models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref> and NAS models. DenseNAS achieves higher accuracies with both fewer FLOPs and lower latencies. Note that for FBNet-A, the group convolution in the 1×1 conv and the channel shuffle operation are used, which do not exist in FBNet-B, -C and Proxyless. In the compared NAS methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45]</ref>, the block counts and block widths in the search space are set and adjusted manually. DenseNAS allocates block counts and block widths automatically. We further visualize the results in <ref type="figure" target="#fig_1">Fig. 3</ref>, which clearly demonstrates that DenseNAS achieves a better trade-off between accuracy and latency. The searched architectures are shown in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalization Ability on COCO Object Detection</head><p>We apply the searched DenseNAS networks on the COCO <ref type="bibr" target="#b25">[26]</ref> object detection task to evaluate the generalization ability of DenseNAS networks and show the results    <ref type="figure">Figure 4</ref>: Visualization of the searched architectures. We use rectangles with different colors and widths to denote the layer operations. KxEy denotes the MBConv with kerner size x × x and expansion ratio y. We label the width in each layer. Layers in the same block are contained in the dashed box. We separate the stages with the green lines.</p><p>in Tab. 2. We choose two commonly used object detection frameworks RetinaNet <ref type="bibr" target="#b26">[27]</ref> and SSDLite <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref> to conduct our experiments. All the architectures shown in Tab. 2 are utilized as the backbone networks in the detection frameworks. The experiments are performed based on the MMDetection <ref type="bibr" target="#b4">[5]</ref> framework. We compare our results with both manually designed and NAS models. Results of MobileNetV2 <ref type="bibr" target="#b39">[40]</ref>, FB-Net <ref type="bibr" target="#b44">[45]</ref> and ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> are obtained by our reimplementation and all models are trained under the same settings and hyper-parameters for fair comparisons. Det-NAS <ref type="bibr" target="#b6">[7]</ref> is a recent work that aims at searching the backbone architectures directly on object detection. Though DenseNAS searches on the ImageNet classification task and applies the searched architectures on detection tasks, our DenseNAS models still obtain superior detection performance in terms of both accuracy and FLOPs. The superiority over the compared methods demonstrates the great generalization ability of DenseNAS networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance on ResNet-based Search Space</head><p>We apply our DenseNAS framework on the ResNet [19]based search space to further evaluate the generalization ability of our method. It is convenient to implement Dense-NAS on ResNet <ref type="bibr" target="#b18">[19]</ref> as we set the candidate operations in the basic layer as the basic block defined in ResNet <ref type="bibr" target="#b18">[19]</ref> and the skip connection. The ResNet-based search space is also constructed as a densely connected super network.</p><p>We search for several architectures with different FLOPs and compare them with the original ResNet models on the ImageNet [10] classification task in Tab. 3. We further replace all the basic blocks in DenseNAS-R2 with the bottleneck blocks and obtain DenseNAS-R3 to compare with ResNet-50-B and the NAS model RandWire-WS, C=109 <ref type="bibr" target="#b45">[46]</ref> (WS, C=109). Though WS, C=109 achieves a higher accuracy, the FLOPs increases 600M, which is a great number, 17.6% of DenseNAS-R3. Besides, WS C=109 uses separable convolutions which greatly decrease the FLOPs while DenseNAS-R3 only contains plain convolutions. Moreover, RandWire networks are unfriendly to inference on existing hardware for the complicated connection patterns. Our proposed DenseNAS promotes the accuracy of ResNet-18, -34 and -50-B by 1.5%, 0.5% and 0.3% with 200M, 600M, 680M fewer FLOPs and 1.5ms, 2.4ms, 6.1ms lower latency respectively. We visualize the comparison results in <ref type="figure" target="#fig_3">Fig. 5</ref> and the performance on the ResNet- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study and Analysis</head><p>Comparison with Other Search Spaces To further demonstrate the effectiveness of our proposed densely connected search space, we conduct the same search algorithm used in DenseNAS on the search spaces of FBNet and Prox-ylessNAS as well as a new search space which is constructed following the settings of block counts and block widths in MobileNetV2. The three search spaces are denoted as FBNet-SS, Proxyless-SS and MBV2-SS respectively. All the search/training settings and hyper-parameters are the same as that we use for DenseNAS. The results are shown in Tab. 4 and DenseNAS achieves the highest accuracy with the lowest latency. Comparison with Random Search As random search <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41]</ref> is treated as an important baseline to validate NAS methods. We conduct random search experiments and show the results in Tab. 1. We randomly sample 15 models in our search space whose FLOPs are similar to DenseNAS-C. Then we train every model for 5 epochs on ImageNet. Finally, we select the one with the highest validation accuracy and train it under the same settings as DenseNAS. The total search cost of the random search is the same as DenseNAS. We observe that DenseNAS-C is 1% accuracy higher compared with the randomly searched model, which proves the effectiveness of DenseNAS. The Number of Block Connections We explore the effect of the maximum number of connections between routing blocks in the search space. We set the maximum connection number as 4 in DenseNAS. Then we try more options and show the results in Tab. 5. When we set the connection number to 3, the searched model gets worse performance. We attribute this to the search space shrinkage which causes the loss of many possible architectures with good performance. As we set the number to 5 and the search process takes the same number of epochs as DenseNAS, i.e. 150 epochs. The performance of the searched model is not good, even worse than that of the connection number 3. Then we increase the search epochs to 200 and the search process achieves a comparable result with DenseNAS. This phenomenon indicates that larger search spaces need more search cost to achieve comparable/better results with/than smaller search spaces with some added constraints.</p><p>Cost Estimation Method As the super network is densely connected and the final architecture is derived based on the total transition probability, the model cost estimation needs to take the effects of all the path probabilities on the whole network into consideration. We try a local cost estimation strategy that does not involve the global connection effects on the whole super network. Specifically, we compute the cost of the whole network by summing the cost of every routing block during the search as follows, while the transition probability p ji is only used for computing the cost of each individual block rather than the whole network.</p><formula xml:id="formula_10">cost = B i ( j=i−1 j=i−m p ji · cost ji align + cost i b ),<label>(9)</label></formula><p>where all definitions in the equation are the same as that in Eq. 6. We randomly generate the architecture parameters (α and β) to derive the architectures. Then we draw the approximated cost values computed by local cost estimation and our proposed chained cost estimation respectively, and compare with the real cost values in <ref type="figure" target="#fig_4">Fig. 6</ref>. In this experiment, we take FLOPs as the model cost because the FLOPs is easier to measure than latency. 1,500 models are sampled in total. The results show that the predicted cost values computed by our chained cost estimation algorithm has a much stronger correlation with the real values and approximate more to the real ones. As the predicted values are computed based on the randomly generated architecture parameters which are not binary parameters, there are still differences between the predicted and real values.</p><p>Architecture Analysis We visualize the searched architectures in <ref type="figure">Fig. 4</ref>. It shows that DenseNAS-B and -C have one more block in the last stage than other architectures, which indicates enlarging the depth in the last stage of the network tends to obtain a better accuracy. Moreover, the smallest architecture DenseNAS-A whose FLOPs is only 251M has one fewer block than DenseNAS-B and -C to decrease the model cost. The structures of the final searched architectures show the great flexibility of DenseNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a densely connected search space for more flexible architecture search, DenseNAS. We tackle the limitations in previous search space design in terms of the block counts and widths. The novelly designed routing blocks are utilized to construct the search space. The proposed chained cost estimation algorithm aims at optimizing both accuracy and model cost. The effectiveness of DenseNAS is demonstrated on both MobileNetV2-and ResNet-based search spaces. We leave more applications, e.g. semantic segmentation, face detection, pose estimation, and more network-based search space implementations, e.g. MobileNetV3 <ref type="bibr" target="#b20">[21]</ref>, ShuffleNet <ref type="bibr" target="#b48">[49]</ref> and VarGNet <ref type="bibr" target="#b47">[48]</ref>, for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation Details</head><p>Before the search process, we build a lookup table for every operation latency of the super network as described in Sec. 3.3. We set the input shape as <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">224,</ref><ref type="bibr">224)</ref> with the batch size of 32 and measure each operation latency on one TITAN-XP GPU. All models and experiments are implemented using PyTorch <ref type="bibr" target="#b35">[36]</ref>.</p><p>For the search process, we randomly choose 100 classes from the original 1K-class ImageNet training set. We sample 20% data of each class from the above subset as the validation set. The original validation set of ImageNet is only used for evaluating our final searched architecture. The search process takes 150 epochs in total. We first train the operation weights for 50 epochs on the divided training set. For the last 100 epochs, the updating of architecture parameters (α, β) and operation weights (w) alternates in each epoch. We use the standard GoogleNet <ref type="bibr" target="#b41">[42]</ref> data augmentation for the training data preprocessing. We set the batch size to 352 on 4 Tesla V100 GPUs. The SGD optimizer is used with 0.9 momentum and 4 × 10 −5 weight decay to update the operation weights. The learning rate decays from 0.2 to 1 × 10 −4 with the cosine annealing schedule <ref type="bibr" target="#b32">[33]</ref>. We use the Adam optimizer <ref type="bibr" target="#b1">[2]</ref> with 10 −3 weight decay, β = (0.5, 0.999) and a fixed learning rate of 3 × 10 −4 to update the architecture parameters.</p><p>For retraining the final derived architecture, we use the same data augmentation strategy as the search process on the whole ImageNet dataset. We train the model for 240 epochs with a batch size of 1024 on 8 TITAN-XP GPUs. The optimizer is SGD with 0.9 momentum and 4 × 10 −5 weight decay. The learning rate decays from 0.5 to 1×10 −4 with the cosine annealing schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Viterbi Algorithm for Block Deriving</head><p>The Viterbi Algorithm <ref type="bibr" target="#b15">[16]</ref> is widely used in dynamic programming which targets at finding the most likely path between hidden states. In DenseNAS, only a part of routing blocks in the super network are retained to construct the final architecture. As described in Sec. 3.4, we implement the Viterbi algorithm to derive the final sequence of blocks. We treat the routing block in the super network as each hidden state in the Viterbi algorithm. The path probability p ij serves as the transition probability from routing block B i to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Implementation Details of ResNet Search</head><p>We design the ResNet-based search space as follows. As enlarging the kernel size of the ResNet block causes a huge computation cost increase, the candidate operations in the basic layer only include the basic block <ref type="bibr" target="#b18">[19]</ref> and the skip connection. That means we aim at width and depth search for ResNet networks. During the search, the batch size is set as 512 on 4 Tesla V100 GPUs. The search process takes 70 epochs in total and we start to update the architecture parameters from epoch 10. We set all the other search settings and hyper-parameters the same as that in the Mo-bileNetV2 <ref type="bibr" target="#b39">[40]</ref> search. For the architecture retraining, the same training settings and hyper-parameters are used as that for architectures searched in the MobileNetV2-based search space. The architectures searched by DenseNAS are shown in Tab. 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Experimental Comparison of Cost Estimation Method</head><p>We study the design of the model cost estimation algorithm in Sec. 4.5. 1, 500 models are derived based on the randomly generated architecture parameters. Cost values predicted by our proposed chained cost estimation algorithm demonstrate a stronger correlation with the real values and more accurate prediction results than the compared local cost estimation strategy. We further perform the same search process as DenseNAS on the MobileNetV2 [40]based search space with the local estimation strategy and show the searched results in Tab. 7. DenseNAS with the chained cost estimation algorithm shows a higher accuracy with lower latency and fewer FLOPs. It proves the effectiveness of the chained cost estimation algorithm on achieving a good trade-off between accuracy and model cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Search space comparison between conventional methods and DenseNAS. Upper: Conventional search spaces manually set a fixed number of blocks in each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The comparison of model performance on Ima-geNet under the MobileNetV2-based search spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>112×112</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Graphical comparisons between ResNets and DenseNAS networks on the ResNet-based search space. based search space further demonstrates the great generalization ability and effectiveness of DenseNAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Predicted values of FLOPs computed by chained cost estimation and local cost estimation algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The proposed routing block which contains shape-alignment layers, an element-wise sum operation and some basic layers. It takes multiple input tensors and outputs one tensor. Bottom: The proposed dense super network which is constructed with densely connected routing blocks. Routing blocks in the same stage hold the same color. Architectures are searched within various path options in the super network.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Cn-1×Hn-1×Wn-1</cell><cell cols="5">Shape-Alignment Layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Cn-2×Hn-2×Wn-2</cell><cell></cell><cell cols="2">C n ×H n ×W n</cell><cell></cell><cell></cell><cell></cell><cell>C n ×H n ×W n</cell><cell></cell><cell>OP1</cell><cell>OP2</cell><cell>OP3</cell><cell>···</cell><cell>OPn</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Basic Layers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Cn-m×Hn-m×Wn-m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3×224×224</cell><cell>16</cell><cell>24</cell><cell>32</cell><cell>40</cell><cell>48</cell><cell>56</cell><cell>64</cell><cell>72</cell><cell>96</cell><cell>112</cell><cell>128</cell><cell>160</cell><cell cols="2">176</cell><cell>192</cell><cell>320</cell><cell>352</cell><cell>384</cell><cell>Pooling FC</cell></row><row><cell></cell><cell cols="2">112×112 56×56 Stage1 Stage2</cell><cell cols="2">28×28 Stage3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">14×14 Stage4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">7×7 Stage5</cell><cell></cell><cell></cell></row><row><cell cols="20">Figure 2: We define our search space on three levels. Upper right: A basic layer that contains a set of candidate operations.</cell></row><row><cell cols="2">Upper left:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Our results on the ImageNet classification with the MobileNetV2-based search space compared with other methods. Our models achieve higher accuracies with lower latencies. For GPU latency, we measure all the models with the same setup (on one TITAN-XP with a batch size of 32).</figDesc><table><row><cell>Model</cell><cell>FLOPs</cell><cell>GPU Latency</cell><cell>Top-1 Acc(%)</cell><cell>Search Time (GPU hours)</cell></row><row><cell cols="3">1.4-MobileNetV2 [40] 585M 28.0ms</cell><cell>74.7</cell><cell>-</cell></row><row><cell>NASNet-A [53]</cell><cell cols="2">564M -</cell><cell>74.0</cell><cell>48K</cell></row><row><cell>AmoebaNet-A [39]</cell><cell cols="2">555M -</cell><cell>74.5</cell><cell>76K</cell></row><row><cell>DARTS [31]</cell><cell cols="2">574M 36.0ms</cell><cell>73.3</cell><cell>96</cell></row><row><cell>RandWire-WS [46]</cell><cell cols="2">583M -</cell><cell>74.7</cell><cell>-</cell></row><row><cell>DenseNAS-Large</cell><cell cols="2">479M 28.9ms</cell><cell>76.1</cell><cell>64</cell></row><row><cell cols="3">1.0-MobileNetV1 [22] 575M 16.8ms</cell><cell>70.6</cell><cell>-</cell></row><row><cell cols="3">1.0-MobileNetV2 [40] 300M 19.5ms</cell><cell>72.0</cell><cell>-</cell></row><row><cell>FBNet-A [45]</cell><cell cols="2">249M 15.8ms</cell><cell>73.0</cell><cell>216</cell></row><row><cell>DenseNAS-A</cell><cell cols="2">251M 13.6ms</cell><cell>73.1</cell><cell>64</cell></row><row><cell>MnasNet [43]</cell><cell cols="2">317M 19.7ms</cell><cell>74.0</cell><cell>91K</cell></row><row><cell>FBNet-B [45]</cell><cell cols="2">295M 18.9ms</cell><cell>74.1</cell><cell>216</cell></row><row><cell>Proxyless(mobile) [4]</cell><cell cols="2">320M 21.3ms</cell><cell>74.6</cell><cell>200</cell></row><row><cell>DenseNAS-B</cell><cell cols="2">314M 15.4ms</cell><cell>74.6</cell><cell>64</cell></row><row><cell>MnasNet-92 [43]</cell><cell cols="2">388M -</cell><cell>74.8</cell><cell>91K</cell></row><row><cell>FBNet-C [45]</cell><cell cols="2">375M 22.1ms</cell><cell>74.9</cell><cell>216</cell></row><row><cell>Proxyless(GPU) [4]</cell><cell cols="2">465M 22.1ms</cell><cell>75.1</cell><cell>200</cell></row><row><cell>DenseNAS-C</cell><cell cols="2">361M 17.9ms</cell><cell>75.3</cell><cell>64</cell></row><row><cell>Random Search</cell><cell cols="2">360M 26.9ms</cell><cell>74.3</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Object detection results on COCO. The FLOPs are calculated with 1088 × 800 input.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Params FLOPs mAP(%)</cell></row><row><cell>MobileNetV2 [40]</cell><cell></cell><cell cols="2">11.49M 133.05B</cell><cell>32.8</cell></row><row><cell>DenseNAS-B</cell><cell></cell><cell cols="2">12.69M 133.09B</cell><cell>34.3</cell></row><row><cell>DetNAS [7] FBNet-C [45]</cell><cell>RetinaNet</cell><cell cols="2">13.41M 133.26B 12.65M 134.17B</cell><cell>33.3 34.9</cell></row><row><cell>Proxyless(GPU) [4]</cell><cell></cell><cell cols="2">14.62M 135.81B</cell><cell>35</cell></row><row><cell>DenseNAS-C</cell><cell></cell><cell cols="2">13.24M 133.91B</cell><cell>35.1</cell></row><row><cell>MobileNetV2 [40]</cell><cell></cell><cell>4.3M</cell><cell>0.8B</cell><cell>22.1</cell></row><row><cell>Mnasnet-92 [43]</cell><cell></cell><cell>5.3M</cell><cell>1.0B</cell><cell>22.9</cell></row><row><cell>FBNet-C [45]</cell><cell>SSDLite</cell><cell>6.27M</cell><cell>1.06B</cell><cell>22.9</cell></row><row><cell>Proxyless(GPU) [4]</cell><cell></cell><cell>7.91M</cell><cell>1.24B</cell><cell>22.8</cell></row><row><cell>DenseNAS-C</cell><cell></cell><cell>6.87M</cell><cell>1.05B</cell><cell>23.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>ImageNet classification results of ResNets and DenseNAS networks searched on the ResNet-based search spaces.</figDesc><table><row><cell>Model</cell><cell cols="2">Params FLOPs</cell><cell>GPU Latency</cell><cell>Top-1 Acc(%)</cell></row><row><cell>ResNet-18 [19]</cell><cell>11.7M</cell><cell>1.81B</cell><cell>13.5ms</cell><cell>72.0</cell></row><row><cell>DenseNAS-R1</cell><cell>11.1M</cell><cell>1.61B</cell><cell>12.0ms</cell><cell>73.5</cell></row><row><cell>ResNet-34 [19]</cell><cell>21.8M</cell><cell>3.66B</cell><cell>24.6ms</cell><cell>75.3</cell></row><row><cell>DenseNAS-R2</cell><cell>19.5M</cell><cell>3.06B</cell><cell>22.2ms</cell><cell>75.8</cell></row><row><cell>ResNet-50-B [19]</cell><cell>25.6M</cell><cell>4.09B</cell><cell>47.8ms</cell><cell>77.7</cell></row><row><cell cols="2">RandWire-WS, C=109 [46] 31.9M</cell><cell>4.0B</cell><cell>-</cell><cell>79.0</cell></row><row><cell>DenseNAS-R3</cell><cell>24.7M</cell><cell>3.41B</cell><cell>41.7ms</cell><cell>78.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with other search spaces on Ima-geNet. SS: Search Space. MBV2: MobileNetV2.</figDesc><table><row><cell>Search Space</cell><cell cols="3">FLOPs GPU Latency Top-1 Acc(%)</cell></row><row><cell>FBNet-SS [45]</cell><cell>369M</cell><cell>25.6ms</cell><cell>74.9</cell></row><row><cell>Proxyless-SS [4]</cell><cell>398M</cell><cell>18.9ms</cell><cell>74.8</cell></row><row><cell>MBV2-SS [40]</cell><cell>383M</cell><cell>32.1ms</cell><cell>74.7</cell></row><row><cell>DenseNAS</cell><cell>361M</cell><cell>17.9ms</cell><cell>75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with different connection number settings on ImageNet.</figDesc><table><row><cell cols="4">Connect Num GPU Latency Top-1 Acc(%) Search Epochs</cell></row><row><cell>3</cell><cell>18.9ms</cell><cell>74.8</cell><cell>150</cell></row><row><cell>4</cell><cell>17.9ms</cell><cell>75.3</cell><cell>150</cell></row><row><cell>5</cell><cell>19.2ms</cell><cell>74.6</cell><cell>150</cell></row><row><cell>5</cell><cell>16.7ms</cell><cell>74.9</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Architectures searched by DenseNAS in the ResNet-based search space.</figDesc><table><row><cell cols="2">Stage Output Size</cell><cell>DenseNAS-R1</cell><cell>DenseNAS-R2</cell><cell>DenseNAS-R3</cell></row><row><cell>1</cell><cell>112 × 112</cell><cell></cell><cell>3×3, 32, stride 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of different cost estimation methods.</figDesc><table><row><cell>Estimation Method</cell><cell cols="3">FLOPs GPU Latency Top-1 Acc(%)</cell></row><row><cell>local cost estimation</cell><cell>396M</cell><cell>27.5ms</cell><cell>74.8</cell></row><row><cell>chained cost estimation</cell><cell>361M</cell><cell>17.9ms</cell><cell>75.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Dropping-path Search Strategy</head><p>The super network includes all the possible architectures defined in the search space. To decrease the memory consumption and accelerate the search process, we adopt the dropping-path search strategy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> (which is mentioned in Sec. <ref type="bibr">3.4)</ref>. When training the weights of operations, we sample one path of the candidate operations according to the architecture weight distribution {w o |o ∈ O} in every basic layer. The dropping-path strategy not only accelerates the search but also weakens the coupling effect between operation weights shared by different sub-architectures in the search space. To update the architecture parameters, we sample two operations in each basic layer according to the architecture weight distribution. To keep the architecture weights of the unsampled operations unchanged, we compute a re-balancing bias to adjust the sampled and newly updated parameters.</p><p>where O s refers to the set of sampled operations, α o denotes the original value of the sampled architecture parameter in layer and α o denotes the updated value of the architecture parameter. The computed bias is finally added to the updated architecture parameters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detnas: Neural architecture search on object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08934</idno>
		<title level="m">Matt Uyttendaele, and Niraj K. Jha. Chamnet: Towards efficient network design through platform-aware model adaptation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dpp-net: Device-aware progressive search for pareto-optimal neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-Chieh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09717</idno>
		<title level="m">Network pruning via transformable architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05884</idno>
		<title level="m">EAT-NAS: elastic architecture transfer for accelerating large-scale neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast neural network adaptation via parameter remapping and architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangjian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The viterbi algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv:abs/1802.01548</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05653</idno>
		<title level="m">Vargnet: Variable group convolutional neural network for efficient embedded computing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
