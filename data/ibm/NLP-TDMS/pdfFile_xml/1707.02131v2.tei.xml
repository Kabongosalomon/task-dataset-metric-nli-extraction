<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters SigNet: Convolutional Siamese Network for Writer Independent Offline Signature Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sounak</forename><surname>Dey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Ignacio</forename><surname>Toledo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Lladós</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<addrLine>Edifici O, Campus UAB</addrLine>
									<postCode>08193</postCode>
									<settlement>Bellaterra</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision and Pattern Recognition Unit</orgName>
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<addrLine>203, B. T. Road</addrLine>
									<settlement>Kolkata-700108</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Recognition Letters SigNet: Convolutional Siamese Network for Writer Independent Offline Signature Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Offline signature verification is one of the most challenging tasks in biometrics and document forensics. Unlike other verification problems, it needs to model minute but critical details between genuine and forged signatures, because a skilled falsification might only differ from a real signature by some specific kinds of deformation. This verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases. In this paper, we model an offline writer independent signature verification task with a convolutional Siamese network. Siamese networks are twin networks with shared weights, which can be trained to learn a feature space where similar observations are placed in proximity. This is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs. Experiments conducted on cross-domain datasets emphasize the capability of our network to handle forgery in different languages (scripts) and handwriting styles. Moreover, our designed Siamese network, named SigNet, provided better results than the state-of-the-art results on most of the benchmark signature datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Signature is one of the most popular and commonly accepted biometric hallmarks that has been used since the ancient times for verifying different entities related to human beings, viz. documents, forms, bank checks, individuals, etc. Therefore, signature verification is a critical task and many efforts have been made to remove the uncertainty involved in the manual authentication procedure, which makes signature verification an important research line in the field of machine learning and pattern recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Depending on the input format, signature verification can be of two types: (1) online and (2) offline. Capturing online signature needs an electronic writing pad together with a stylus, which can mainly record a sequence of coordinates of the electronic pen tip while signing. Apart from the writing coordinates of the signature, these devices are also capable of fetching the writing speed, pressure, etc., as additional information, which are used in the online verification process.</p><p>On the other hand, the offline signature is usually captured by a scanner or any other type of imaging devices, which basically produces two dimensional signature images. As signature verification has been a popular research topic through decades and substantial efforts are made both on offline as well as on online signature verification purpose.</p><p>Online verification systems generally perform better than their offline counter parts <ref type="bibr" target="#b2">[3]</ref> due to the availability of complementary information such as stroke order, writing speed, pressure,etc. However, this improvement in performances comes at the cost of requiring a special hardware for recording the pen-tip trajectory, rising its system cost and reducing the real application scenarios. There are many cases where authenticating offline signature is the only option such as check transaction and document verification. Because of its broader application area, in this paper, we focus on the more challenging task-automatic offline signature verification. Our objective is to propose a convolutional Siamese neural network model to discriminate the genuine signatures and skilled forgeries.</p><p>Offline signature verification can be addressed with (1) writer dependent and (2) writer independent approaches <ref type="bibr" target="#b3">[4]</ref>. The writer independent scenario is preferable over writer dependent approaches, as for a functioning system, a writer dependent system needs to be updated (retrained) with every new writer (signer). For a consumer based system, such as bank, where every day new consumers can open their account this incurs huge cost. Whereas, in writer independent case, a generic system is built to model the discrepancy among the genuine and forged signatures. Training a signature verification system under a writer independent scenario, divides the available signers into train and test sets. For a particular signer, signatures are coupled as similar (genuine, genuine) or dissimilar (genuine, forged) pairs. From all the tuples of a single signer, equal number of tuples similar and dissimilar pairs are stochastically selected for balancing the number of instances. This procedure is applied to all the signers in the train and test sets to construct the training and test examples for the classifier.</p><p>In this regard a signature verifier can be efficiently modelled by a Siamese network which consists of twin convolutional networks accepting two distinct signature images coming from the tuples that are either similar or dissimilar. The constituting convolutional neural networks (CNN) are then joined by a cost function at the top, which computes a distance metric between the highest level feature representation on each side of the network. The parameters between this twin networks are shared, which in turns guarantees that two extremely similar images could not possibly be mapped by their respective networks to very different locations in feature space because each network computes the same function.</p><p>Different hand crafted features have been proposed for offline signature verification tasks. Many of them take into account the global signature image for feature extraction, such as, block codes, wavelet and Fourier series etc <ref type="bibr" target="#b4">[5]</ref>. Some other methods consider the geometrical and topological characteristics of local attributes, such as position, tangent direction, blob structure, connected component and curvature <ref type="bibr" target="#b2">[3]</ref>. Projection and contour based methods <ref type="bibr" target="#b5">[6]</ref> are also quite popular for offline signature verification. Apart from the above mentioned methods, approaches fabricated on direction profile <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, surroundedness features <ref type="bibr" target="#b7">[8]</ref>, grid based methods <ref type="bibr" target="#b8">[9]</ref>, methods based on geometrical moments <ref type="bibr" target="#b9">[10]</ref>, and texture based features <ref type="bibr" target="#b10">[11]</ref> have also become famous in signature verification task. Few structural methods that consider the relations among local features are also explored for the same task. Examples include graph matching <ref type="bibr" target="#b11">[12]</ref> and recently proposed compact correlated features <ref type="bibr" target="#b12">[13]</ref>. On the other hand, Siamese like networks are very popular for different verification tasks, such as, online signature verification <ref type="bibr" target="#b13">[14]</ref>, face verification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> etc. Furthermore, it has also been used for one-shot image recognition <ref type="bibr" target="#b16">[17]</ref>, as well as for sketch-based image retrieval task <ref type="bibr" target="#b17">[18]</ref>. Nevertheless, to the best of our knowledge, till date, convolutional Siamese network has never been used to model an offline signature verifier, which provides our main motivation.</p><p>The main contribution of this paper is the proposal of a convolutional Siamese network, named SigNet, for offline signature verification problem. This, in contrast to other methods based on hand crafted features, has the ability to model generic signature forgery techniques and many other related properties that envelops minute inconsistency in signatures from the training data. In contrary to other one-shot image verification tasks, the problem with signature is far more complex because of subtle variations in writing styles independent of scripts, which could also encapsulate some degrees of forgery. Here we mine this ultra fine anamorphosis and create a generic model using SigNet.</p><p>The rest of the paper is organized as follows: In Section 2 we describe the SigNet and its architechture. Section 3 presents our experimental validation and compares the proposed method with available state-of-the-art algorithms. Finally, in Section 4, we conclude the paper with a defined future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SigNet: Siamese Network for Signature Verification</head><p>In this section, at first, the preprocessing performed on signature images is explained in Section 2.1. This is followed by a detailed description of the proposed Siamese architecture in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>Since batch training a neural network typically needs images of same sizes but the signature images we consider have different sizes ranges from 153 × 258 to 819 × 1137. We resize all the images to a fixed size 155 × 220 using bilinear interpolation. Afterwards, we invert the images so that the background pixels have 0 values. Furthermore, we normalize each image by dividing the pixel values with the standard deviation of the pixel values of the images in a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CNN and Siamese Network</head><p>Deep Convolutuional Neural Networks (CNN) are multilayer neural networks consists of several convolutional layers with different kernel sizes interleaved by pooling layers, which summarizes and downsamples the output of its convolutions before feeding to next layers. To get nonlinearity rectified linear units are also used. In this work, we used different convolutional kernels with sizes starting with 11 × 11 to 3 × 3. Generally a differentiable loss function is chosen so that Gradient descent can be applied and the network weights can be optimized. Given a differentiable loss function, the weights of different layers are updated using back propagation. As the optimization can not be applied to all training data where training size is large batch optimizations gives a fair alternative to optimize the network.</p><p>Siamese neural network is a class of network architectures that usually contains two identical subnetworks. The twin CNNs have the same configuration with the same parameters and shared weights. The parameter updating is mirrored across both the subnetworks. This framework has been successfully used for dimensionality reduction in weakly supervised metric learning <ref type="bibr" target="#b14">[15]</ref> and for face verification in <ref type="bibr" target="#b15">[16]</ref>. These subnetworks are joined by a loss function at the top, which computes a similarity metric involving the Euclidean distance between the feature representation on each side of the Siamese network. One such loss function that is mostly used in Siamese network is the contrastive loss <ref type="bibr" target="#b14">[15]</ref> defined as follows: </p><formula xml:id="formula_0">L(s 1 , s 2 , y) = α(1 − y)D 2 w + βy max(0, m − D w ) 2<label>(1)</label></formula><p>where s 1 and s 2 are two samples (here signature images), y is a binary indicator function denoting whether the two samples belong to the same class or not, α and β are two constants and m is the margin equal to 1 in our case. D w = f (s 1 ; w 1 ) − f (s 2 ; w 2 ) 2 is the Euclidean distance computed in the embedded feature space, f is an embedding function that maps a signature image to real vector space through CNN, and w 1 , w 2 are the learned weights for a particular layer of the underlying network. Unlike conventional approaches that assign binary similarity labels to pairs, Siamese network aims to bring the output feature vectors closer for input pairs that are labelled as similar, and push the feature vectors away if the input pairs are dissimilar. Each of the branches of the Siamese network can be seen as a function that embeds the input image into a space. Due to the loss function selected (Eqn. 1), this space will have the property that images of the same class (genuine signature for a given writer) will be closer to each other than images of different classes (forgeries or signatures of different writers). Both branches are joined together by a layer that computes the Euclidean distance between the two points in the embedded space. Then, in order to decide if two images belong to the similar class (genuine, genuine) or a dissimilar class (genuine, forged) one needs to determine a threshold value on the distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Architecture</head><p>We have used a CNN architecture that is inspired by Krizhevsky et al. <ref type="bibr" target="#b18">[19]</ref> for an image recognition problem. For the easy reproducibility of our results, we present a full list of parameters used to design the CNN layers in <ref type="table" target="#tab_0">Table 1</ref>. For convolution and pooling layers, we list the size of the filters as N × H × W, where N is the number of filters, H is the height and W is the width of the corresponding filter. Here, stride signifies the distance between the application of filters for the convolution and pooling operations, and pad indicates the width of added borders to the input. Here it is to be mentioned that padding is necessary in order to convolve the filter from the very first pixel in the input image. Throughout the network, we use Rectified Linear Units (ReLU) as the activation function to the output of all the convolutional and fully connected layers. For generalizing the learned features, Local Response Normalization is applied according to <ref type="bibr" target="#b18">[19]</ref>, with the parameters shown in the corresponding row in <ref type="table" target="#tab_0">Table 1</ref>. With the last two pooling layers and the first fully connected layer, we use a Dropout with a rate equal to 0.3 and 0.5, respectively.</p><p>The first convolutional layers filter the 155 × 220 input signature image with 96 kernels of size 11 × 11 with a stride of 1 pixels. The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5×5. The third and fourth convolutional layers are connected to one another without any intervention of pooling or normalization of layers. The third layer has 384 kernels of size 3 × 3 connected to the  (normalized, pooled, and dropout) output of the second convolutional layer. The fourth convolutional layer has 256 kernels of size 3×3. This leads to the neural network learning fewer lower level features for smaller receptive fields and more features for higher level or more abstract features. The first fully connected layer has 1024 neurons, whereas the second fully connected layer has 128 neurons. This indicates that the highest learned feature vector from each side of SigNet has a dimension equal to 128. We initialize the weights of the model according to the work of Glorot and Bengio <ref type="bibr" target="#b19">[20]</ref>, and the biases equal to 0. We trained the model using RMSprop for 20 epochs, using momentum rate equal to 0.9, and mini batch size equal to 128. We started with an intial learning rate (LR) equal to 1e − 4 with hyper parameters ρ = 0.9 and = 1e − 8. All these values are shown in <ref type="table" target="#tab_1">Table 2</ref>. Our entire framework is implemented using Keras library with the TensorFlow as backend. The training was done using a GeForce GTX 1070 and a TITAN X Pascal GPU, and it took 2 to 9 hours to run approximately, depending on different databases. <ref type="figure" target="#fig_1">Figure 2</ref> shows five different filter activations in the last convolutional layer on a pair of (genuine, forged) signatures, which have received comparatively higher discrimination. The first row corresponds to the genuine signature image, whereas, the second row corresponds to the forged one and these two sigan- tures are correctly classified as dissimilar by SigNet. Each column starting from the second one shows the activations under the convolution of the same filter. The responses in the respective zones show the areas or signature features that are learned by the network for distinguishing these two signatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In order to evaluate our signature verification algorithm, we have considered four widely used benchmark databases, viz., (1) CEDAR, (2) GPDS300, (3) GPDS Synthetic Signature Database, and (4) BHSig260 signature corpus. The source code of SigNet will be available once the paper gets accepted for publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">CEDAR</head><p>CEDAR signature database 1 contains signatures of 55 signers belonging to various cultural and professional backgrounds. Each of these signers signed 24 genuine signatures 20 minutes apart. Each of the forgers tried to emulate the signatures of 3 persons, 8 times each, to produce 24 forged signatures for each of the genuine signers. Hence the dataset comprise 55 × 24 = 1, 320 genuine signatures as well as 1, 320 forged signatures. The signature images in this dataset are available in gray scale mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">GPDS300</head><p>GPDS300 signature corpus 2 comprises 24 genuine and 30 forged signatures for 300 persons. This sums up to 300 × 24 = 7, 200 genuine signatures and 300 × 30 = 9, 000 forged signatures. The 24 genuine signatures of each of the signers were collected in a single day. The genuine signatures are shown to each forger and are chosen randomly from the 24 genuine ones to be imitated. All the signatures in this database are available in binary form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">GPDS Synthetic</head><p>GPDS synthetic signature database 3 is built based on the synthetic individuals protocol <ref type="bibr" target="#b20">[21]</ref>. This dataset is comprised of 4000 signers, where each individual has 24 genuine and 30 forged signatures resulting in 4000 × 24 = 96, 000 genuine and 4000 × 30 = 120, 000 forged signatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">BHSig260</head><p>The BHSig260 signature dataset 4 contains the signatures of 260 persons, among them 100 were signed in Bengali and 160 are signed in Hindi <ref type="bibr" target="#b10">[11]</ref>. The authors have followed the same protocol as in GPDS300 to generate these signatures. Here also, for each of the signers, 24 genuine and 30 forged signatures are available. This results in 100 × 24 = 2, 400 genuine and 100 × 30 = 3, 000 forged signatures in Bengali, and 160 × 24 = 3, 840 genuine and 160×30 = 4, 800 forged signatures in Hindi. Even though this dataset is available together, we experimented with our method separately on the Bengali and Hindi dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance Evaluation</head><p>A threshold d is used on the distance measure D(x i , x j ) output by the SigNet to decide whether the signature pair (i, j) belongs to the similar or dissimilar class. We denote the signature pairs (i, j) with the same identity as P similar , whereas all pairs of different identities as P dissimilar . Then, we can define the set of all true positives (TP) at d as</p><formula xml:id="formula_1">T P(d) = {(i, j) ∈ P similar , with D(x i , x j ) ≤ d}</formula><p>Similarly the set of all true negatives (TN) at d can be defined as</p><formula xml:id="formula_2">T N(d) = {(i, j) ∈ P dissimilar , with D(x i , x j ) &gt; d}</formula><p>Then the true positive rate T PR(d) and the true negative rate T NR(d) for a given signature, distance d are then defined as</p><formula xml:id="formula_3">T PR(d) = |T P(d)| |P similar | , T NR(d) = |T N(d)| |P dissimilar |</formula><p>where P similar is the number of similar signature pairs. The final accuracy is computed as</p><formula xml:id="formula_4">Accuracy = max d∈D 1 2 (T PR(d) + T NR(d))<label>(2)</label></formula><p>which is the maximum accuracy obtained by varying d ∈ D from the minimum distance value to the maximum distance value of D with step equal to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Protocol</head><p>Since our method is designed for writer independent signature verification, we divide each of the datasets as follows. We randomly select M signers from the K (where K &gt; M) available signers of each of the datasets. We keep all the original and forged signatures of these M signers for training and the rest of the K − M signers for testing. Since all the above mentioned datasets contain 24 genuine signatures for each of the authors, there are only 24 C 2 = 276 (genuine, genuine) signature pairs available for each author. Similarly, since most of the datasets contain 30 (for CEDAR 24) forged signatures for each signer, there are only 24 × 30 = 720 (for CEDAR 24 × 24 = 576) (genuine, forged) signature pairs can be obtained for each author. For balancing the similar and dissimilar classes, we randomly choose only 276 (genuine, forged) signature pairs from each of the writers. This protocol results in M × 276 (genuine, genuine) as well as (genuine, forged) signature pairs for training and (K − M) × 276 for testing. <ref type="table" target="#tab_2">Table 3</ref> shows the values of K and M for different datasets, that are considered for our experiments. Although most of the existing datasets contain forged signatures, in real life scenarios, there can be cases where getting training samples from forgers might be difficult. Thus, a system trained with genuine-forged signature pairs will be inadequate to deal with such set up. One way to deal with this type of situations is to use only the genuine signatures of other signer as forged signatures (called as unskilled forged signatures). To be applicable in such scenarios, we have performed an experiment only on the GPDS-300 dataset, where the genuine signatures of other writers are used as unskilled forged signatures. However, during testing, we have used genuine-forged pairs of the same signers, i.e., we tested our system for it's ability to distinguish between genuine and forged signatures of the same person. <ref type="table" target="#tab_3">Table 4</ref> shows the accuracies of our proposed SigNet together with other state-of-the-art methods on different datasets discussed in Section 3.1. It is to be noted that SigNet outperformed the state-of-the-art methods on three datasets, viz. GPDS Synthetic, Bengali, and CEDAR dataset. A possible reason for the lower performance on the GPDS300 is the less number of signature samples for learning with many different signature styles. However, on GPDS Synthetic, our proposed network outperformed the same method proposed by Dutta et al. <ref type="bibr" target="#b12">[13]</ref> possibly because there were plenty of training samples for learning the available signature styles. Moreover, it can be To get some ideas on the generalization of the proposed network and the strength of the models learned on different datasets, we performed a second experiment with cross dataset settings. To do this, at a time, we have trained a model on one of the above mentioned datasets and tested it on all the other corpus. We have repeated this same process over all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results and Discussions</head><p>The accuracies obtained by SigNet on the cross dataset settings are shown in <ref type="figure">Figure 3</ref>, where the datasets used for training are indicated in rows and the datasets used for testing are exhibited along columns. It is to be observed that for all the datasets, the highest accuracy is obtained with a model trained on the same dataset. This implies all the datasets have some distinctive features, despite the fact that, CEDAR, GPDS300 and GPDS Synthetic datasets contain signatures with nearly same style (some handwritten letters with initials etc.). However, this fact is justifiable in case of BHSig260 dataset, because it contains signatures in Indic script and the signatures generally look like normal text containing full names of persons. Therefore, it is probable that the network models some script based features in this case. Furthermore, it is usually noted that the system trained on a comparatively bigger and diverse dataset is more robust than the others, which is the reason why better average accuracies are obtained by the model trained on GPDS Synthetic and GPDS300. These experiments strongly show the possibility of creating signature verification system in those cases where training is not possible due to the dearth of sufficient data. In those situation, a robust pretrained model could be used with a lightweight fine tuning on the available specific data. We also thought of the situation where the forger not knowing the real identity of the person, he or she introduces his signature or scribbling which has more variations than the skilled forged ones. To evaluate this, we used the trained model on GPDS300 (trained with skilled forgery) and tested it on signatures placed against a random forgery (i.e.genuine signature of another person) giving an expected increase in performance with 79.19% accuracy rate in GPDS300 dataset (keeping rest of the experimental setup same). This also proves that the model trained to find subtle differences in signature, also performs well when the variations in signatures are large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we have presented a framework based on Siamese network for offline signature verification, which uses writer independent feature learning. This method does not rely on hand-crafted features unlike its predecessors, instead it learns them from data in a writer independent scenario. Experiments conducted on GPDS Syntehtic dataset demonstrate that this is a step towards modelling a generic prototype for real forgeries based on synthetically generated data. Also, our experiments made on cross domain datasets emphasize how well our architecture models the fraudulence of different handwriting style of different signers and forgers with diverse background and scripts. Furthermore, the SigNet designed by us has surpassed the state-of-the-art results on most of the benchmark Signature datasets, which is encouraging for further research in this direction. Our future work in this line will focus on the development of more enriched network model. Furthermore, other different frameworks for verification task will also be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>LFig. 1 .</head><label>1</label><figDesc>(s 1 , s 2 , y) Architecture of SigNet: the input layer, i.e. the 11×11 convolution layer with ReLU, is shown in blue, whereas all the 3×3 and 5×5 convolution layers are depicted in cyan and green respectively. All the local response normalization layers are shown in magenta, all the max pooling layers are depicted in brick color and the dropout layers are exhibited in gray. The last orange block represents the high level feature output from the constituting CNNs, which are joined by the loss function in Eqn. 1. (Best viewed in pdf)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A pair of genuine (top left) and forged (bottom left) signatures, and corresponding response maps with five different filters that have produced higher energy activations in the last convolution layer of SigNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the constituting CNNs</figDesc><table><row><cell>Layer</cell><cell>Size</cell><cell>Parameters</cell></row><row><cell>Convolution</cell><cell>96 × 11 × 11</cell><cell>stride = 1</cell></row><row><cell>Local Response Norm.</cell><cell>-</cell><cell>α = 10 −4 , β = 0.75 k = 2, n = 5</cell></row><row><cell>Pooling</cell><cell>96 × 3 × 3</cell><cell>stride = 2</cell></row><row><cell>Convolution</cell><cell cols="2">256 × 5 × 5 stride = 1, pad = 2</cell></row><row><cell>Local Response Norm.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3</cell></row><row><cell>Fully Connected + Dropout</cell><cell>1024</cell><cell>p = 0.5</cell></row><row><cell>Fully Connected</cell><cell>128</cell><cell></cell></row></table><note>- α = 10 −4 , β = 0.75 k = 2, n = 5 Pooling + Dropout 256 × 3 × 3 stride = 2, p = 0.3 Convolution 384 × 3 × 3 stride = 1, pad = 1 Convolution 256 × 3 × 3 stride = 1, pad = 1 Pooling + Dropout 256 × 3 × 3 stride = 2, p = 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Training Hyper-parameters</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Initial Learning Rate (LR)</cell><cell>1e-4</cell></row><row><cell>Learning Rate Schedule</cell><cell>LR ← LR × 0.1</cell></row><row><cell>Weight Decay</cell><cell>0.0005</cell></row><row><cell>Momentum (ρ)</cell><cell>0.9</cell></row><row><cell>Fuzz factor ( )</cell><cell>1e-8</cell></row><row><cell>Batch Size</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>K and M values of different datasets</figDesc><table><row><cell>Datasets</cell><cell>K</cell><cell>M</cell></row><row><cell>CEDAR</cell><cell>55</cell><cell>50</cell></row><row><cell>GPDS300</cell><cell>300</cell><cell>150</cell></row><row><cell cols="3">GPDS Synthetic 4000 3200</cell></row><row><cell>Bengali</cell><cell>100</cell><cell>50</cell></row><row><cell>Hindi</cell><cell>160</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the proposed method with the state-of-the-art methods on various signature databases. This is quite justified and very intuitive as identifying forgeries of a signature needs attention to minute details of one's signature, which can not be captured when unskilled forged signatures (i.e.genuine signatures of other signers) are used as training examples.Fig. 3. Accuracies obtained by SigNet with cross dataset settings.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Databases</cell><cell></cell><cell></cell><cell>State-of-the-art Methods</cell><cell>#Signers Accuracy FAR</cell><cell>FRR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Word Shape (GSC) (Kalera et al. [5])</cell><cell>55</cell><cell>78.50</cell><cell>19.50 22.45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Zernike moments (Chen and Srihari [22])</cell><cell>55</cell><cell>83.60</cell><cell>16.30 16.60</cell></row><row><cell></cell><cell></cell><cell cols="4">CEDAR Signature Database</cell><cell cols="2">Graph matching (Chen and Srihari [12]) Surroundedness features (Kumar et al. [8])</cell><cell>55 55</cell><cell>92.10 91.67</cell><cell>8.20 8.33</cell><cell>7.70 8.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dutta et al. [13]</cell><cell>55</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SigNet</cell><cell>55</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ferrer et al. [7]</cell><cell>160</cell><cell>86.65</cell><cell>12.60 14.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Vargas et al. [23]</cell><cell>160</cell><cell>87.67</cell><cell>14.66 10.01</cell></row><row><cell></cell><cell></cell><cell cols="4">GPDS 300 Signature Corpus</cell><cell></cell><cell>Solar et al. [24] Kumar et al. [8]</cell><cell>160 300</cell><cell>84.70 86.24</cell><cell>14.20 16.40 13.76 13.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dutta et al. [13]</cell><cell>300</cell><cell>88.79</cell><cell>11.21 11.21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SigNet</cell><cell>300</cell><cell>76.83</cell><cell>23.17 23.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SigNet (unskilled forged)</cell><cell>300</cell><cell>65.36</cell><cell>34.64 34.64</cell></row><row><cell></cell><cell></cell><cell cols="4">GPDS Synthetic Signature Corpus</cell><cell></cell><cell>Dutta et al. [13] SigNet</cell><cell>4000 4000</cell><cell>73.67 77.76</cell><cell>28.34 27.62 22.24 22.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pal et al. [11]</cell><cell>100</cell><cell>66.18</cell><cell>33.82 33.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bengali</cell><cell></cell><cell></cell><cell>Dutta et al. [13]</cell><cell>100</cell><cell>84.90</cell><cell>15.78 14.43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SigNet</cell><cell>100</cell><cell>86.11</cell><cell>13.89 13.89</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pal et al. [11]</cell><cell>100</cell><cell>75.53</cell><cell>24.47 24.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hindi</cell><cell></cell><cell></cell><cell>Dutta et al. [13]</cell><cell>100</cell><cell>85.90</cell><cell>13.10 15.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SigNet</cell><cell>100</cell><cell>84.64</cell><cell>15.36 15.36</cell></row><row><cell cols="8">observed that our system trained on genuine-unskilled forged</cell></row><row><cell cols="8">pairs is outperformed by the system trained on genuine-forged</cell></row><row><cell cols="3">GPDS Synthetic examples. 77.76 GPDS Synthetic</cell><cell cols="3">62.65 Test Datasets 63.77 66.65 GPDS300 Hindi Bengali</cell><cell>79.13 CEDAR</cell><cell>100</cell></row><row><cell>Train Datasets</cell><cell>GPDS300 Hindi Bengali</cell><cell>52.61 52.78 52.66</cell><cell>76.83 55.78 52.98</cell><cell>63.01 84.64 64.57</cell><cell>69.00 60.65 86.81</cell><cell>94.82 59.57 50.00</cell><cell>75</cell></row><row><cell></cell><cell>CEDAR</cell><cell>54.26</cell><cell>55.79</cell><cell>55.61</cell><cell>64.15</cell><cell>100.00</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at http://www.cedar.buffalo.edu/NIJ/data/ signatures.rar</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Available at http://www.gpds.ulpgc.es/download 3 Available at http://www.gpds.ulpgc.es/download 4 The dataset is available at https://goo.gl/9QfByd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been partially supported by the European Union's research and innovation program under the Marie Skłodowska-Curie grant agreement No. 665919. The TITAN X Pascal GPU used for this research was donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online and off-line handwriting recognition: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic signature verification: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Impedovo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pirlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSMC</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="635" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual identification by signature tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Munich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing forgeries in writer-independent off-line signature verification through ensemble of classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Justino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="387" to="396" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Offline signature verification and identification using distance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPRAI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1339" to="1360" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-expert signature verification system for bankcheck processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dimauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Impedovo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPRAI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="827" to="844" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline geometric parameters for automatic signature verification using fixed-point arithmetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Travieso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="993" to="997" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Writer-independent off-line signature verification using surroundedness feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Off-line signature verification based on geometric feature extraction and neural network classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Off-line signature verification using genetically optimized weighted features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Performance of an off-line signature verification method based on texture features on a large indic-script signature dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blumenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>DAS</publisher>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A new off-line signature verification method based on graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srihari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ICPR</publisher>
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Compact correlated features for writer independent signature verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICPR</publisher>
			<biblScope unit="page" from="3411" to="3416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>NIPS</publisher>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Siamese neural networks for oneshot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sketch-based image retrieval via siamese convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICIP</publisher>
			<biblScope unit="page" from="2460" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks, in: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AISTATS</publisher>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Synthetic off-line signature image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diaz-Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICB</publisher>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Use of exterior contours and shape features in off-line signature verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srihari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ICDAR</publisher>
			<biblScope unit="page" from="1280" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Off-line handwritten signature gpds-960 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Travieso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alonso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ICDAR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="764" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Offline signature verification using local interest points and descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz Del Solar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loncomilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Concha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CIARP</publisher>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
