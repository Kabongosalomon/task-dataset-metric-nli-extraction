<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Energy-Based Models for Deep Probabilistic Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><forename type="middle">K</forename><surname>Gustafsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goutam Bhat</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Schön</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Energy-Based Models for Deep Probabilistic Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/fregu856/ebms_regression.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x, y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x, y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a 2.2% AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms_regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised regression entails learning a model capable of predicting a continuous target value y from an input x, given a set of paired training examples. It is a fundamental machine learning problem with many important applications within computer vision and other domains. Common regression tasks within computer vision include object detection <ref type="bibr" target="#b22">[54,</ref><ref type="bibr">28,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b38">70]</ref>, head-and body-pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">64,</ref><ref type="bibr" target="#b27">59,</ref><ref type="bibr" target="#b34">66]</ref>, age estimation <ref type="bibr" target="#b23">[55,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b3">4]</ref>, visual tracking [45, <ref type="bibr" target="#b39">71,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b8">9]</ref> and medical image registration <ref type="bibr">[46,</ref><ref type="bibr" target="#b5">6]</ref>, just to mention a few. Today, such regression problems are commonly tackled using Deep Neural Networks (DNNs), due to their ability to learn powerful feature representations directly from data. While classification is generally addressed using standardized losses and output representations, a wide variety of different techniques are employed for regression. The most conventional strategy is to train a DNN to directly predict a target y given an input x <ref type="bibr">[33]</ref>. In such direct regression approaches, the model parameters of the DNN are learned by minimizing a loss function, for example the L 2 or L 1 loss, penalizing discrepancy between the predicted and ground truth target values. From a probabilistic perspective, this approach corresponds to creating a simple parametric model of the conditional target density p(y|x), and minimizing the associated negative log-likelihood. The L 2 loss, for example, corresponds to a fixed-variance Gaussian model. More recent work <ref type="bibr">[29,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">61,</ref><ref type="bibr" target="#b20">52]</ref> has also explored learning more expressive models of p(y|x), by letting a DNN instead output the full set of parameters of a certain family of probability distributions. To allow for straightforward implementation and training, many of these probabilistic regression approaches however restrict the parametric model to unimodal distributions such as Gaussian <ref type="bibr">[32,</ref><ref type="bibr" target="#b6">7]</ref> or Laplace <ref type="bibr">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">27]</ref>, still severely limiting the expressiveness of the learned conditional target density. While these methods benefit from a clear probabilistic interpretation, they thus fail to fully exploit the predictive power of the DNN.</p><p>The quest for improved regression accuracy has also led to the development of more specialized methods, designed for a specific set of tasks. In computer vision, one particularly popular approach is that of confidence-based regression. Here,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Gaussian Ours is generated by the ground truth conditional target density p(y|x). Our energy-based model p(y|x; θ) ∝ e f θ (x,y) of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo importance sampling. In contrast to the Gaussian model p(y|x; θ) = N (y; µ θ (x), σ 2 θ (x)), our energy-based model can learn multimodal and complex conditional target densities directly from data. a DNN instead predicts a scalar confidence value for input-target pairs (x, y). The confidence can then be maximized w.r.t. y to obtain a target prediction for a given input x. This approach is commonly employed for image-coordinate regression tasks within e.g. human pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">64,</ref><ref type="bibr" target="#b27">59]</ref> and object detection <ref type="bibr">[34,</ref><ref type="bibr" target="#b38">70]</ref>, where a 2D heatmap over image pixel coordinates y is predicted. Recently, the approach was also applied to the problem of bounding box regression by Jiang et al. <ref type="bibr">[28]</ref>. Their proposed method, IoU-Net, obtained state-of-the-art accuracy on object detection, and was later also successfully applied to the task of visual tracking <ref type="bibr" target="#b8">[9]</ref>. The training of such confidence-based regression methods does however entail generating additional pseudo ground truth labels, e.g. by employing a Gaussian kernel <ref type="bibr" target="#b30">[62,</ref><ref type="bibr" target="#b32">64]</ref>, and selecting an appropriate loss function. This both requires numerous design choices to be made, and limits the general applicability of the methods. Moreover, confidence-based regression methods do not allow for a natural probabilistic interpretation in terms of the conditional target density p(y|x). In this work, we therefore set out to develop a method combining the general applicability and the clear interpretation of probabilistic regression with the predictive power of the confidence-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>We propose a general and conceptually simple regression method with a clear probabilistic interpretation. Our method employs an energybased model <ref type="bibr">[36]</ref> to predict the un-normalized conditional target density p(y|x) from the input-target pair (x, y). It is trained by directly minimizing the associated negative log-likelihood, exploiting tailored Monte Carlo approximations. At test time, targets are predicted by maximizing the conditional target density p(y|x) through gradient-based refinement. Our energy-based model is straightforward both to implement and train. Unlike commonly used probabilistic models, it can however still learn highly flexible target densities directly from data, as visualized in <ref type="figure" target="#fig_1">Figure 2</ref>. Compared to confidence-based approaches, our method requires no pseudo labels, benefits from a clear probabilistic interpretation, and is directly applicable to a variety of computer vision applications. We evaluate the proposed method on four diverse computer vision regression tasks: object detection, visual tracking, age estimation and head-pose estimation. Our method is found to significantly outperform both direct regression baselines, and popular probabilistic and confidence-based alternatives, including the state-of-the-art IoU-Net <ref type="bibr">[28]</ref>. Notably, our method achieves a 2.2% AP improvement over FPN Faster-RCNN [38] when applied for object detection on COCO [39], and sets a new state-of-the-art on standard benchmarks <ref type="bibr">[44,</ref><ref type="bibr">43]</ref> when applied for bounding box estimation in the recent ATOM <ref type="bibr" target="#b8">[9]</ref> visual tracker. Our method is also shown to be directly applicable to the more general tasks of age and head-pose estimation, consistently improving performance of a variety of baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Related Work</head><p>In supervised regression, the task is to learn to predict a target value y ∈ Y from a corresponding input x ∈ X , given a training set of i.</p><formula xml:id="formula_0">i.d. input-target examples, D = {(x i , y i )} N i=1 , (x i , y i ) ∼ p(x, y)</formula><p>. As opposed to classification, the target space Y is a continuous set, e.g. Y = R K . In computer vision, the input space X often corresponds to the space of images, whereas the output space Y depends on the task at hand. Common examples include Y = R 2 in imagecoordinate regression <ref type="bibr" target="#b32">[64,</ref><ref type="bibr">34]</ref>, Y = R + in age estimation <ref type="bibr" target="#b23">[55,</ref><ref type="bibr">49]</ref>, and Y = R 4 in object bounding box regression <ref type="bibr" target="#b22">[54,</ref><ref type="bibr">28]</ref>. A variety of techniques have previously been applied to supervised regression tasks. In order to motivate and provide intuition for our proposed method, we here describe a few popular approaches. Direct Regression Over the last decade, DNNs have been shown to excel at a wide variety of regression problems. Here, a DNN is viewed as a function f θ : U → O, parameterized by a set of learnable weights θ ∈ R P . The most conventional regression approach is to train a DNN to directly predict the targets, y = f θ (x ), called direct regression. The model parameters θ are learned by minimizing a loss (f θ (x i ), y i ) that penalizes discrepancy between the prediction f θ (x i ) and the ground truth target value y i on training examples (x i , y i ). Common choices include the L 2 loss, (ŷ, y) = ŷ − y 2 2 , the L 1 loss, (ŷ, y) = ŷ − y 1 , and their close relatives <ref type="bibr">[26,</ref><ref type="bibr">33]</ref>. From a probabilistic perspective, the choice of loss corresponds to minimizing the negative log-likelihood − log p(y|x; θ) for a specific model p(y|x; θ) of the conditional target density. For example, the L 2 loss is derived from a fixed-variance Gaussian model, p(y|x; θ) = N (y; f θ (x), σ 2 ). Probabilistic Regression More recent work <ref type="bibr">[29,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">27,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b29">61]</ref> has explicitly taken advantage of this probabilistic perspective to achieve more flexible parametric models p(y|x; θ) = p(y; φ θ (x)), by letting the DNN output the parameters φ of a family of probability distributions p(y; φ). For example, a general 1D Gaussian model can be realized as p(y|x; θ) = N y; µ θ (x), σ 2 θ (x) , where the DNN outputs the mean and log-variance as</p><formula xml:id="formula_1">f θ (x) = φ θ (x) = [ µ θ (x) log σ 2 θ (x) ] T ∈ R 2 .</formula><p>The model parameters θ are learned by minimizing the negative log-likelihood − N i=1 log p(y i |x i ; θ) over the training set D. At test time, a target estimate y is obtained by first predicting the density parameter values φ θ (x ) and then, for instance, taking the expected value of p(y; φ θ (x)). Previous work has applied simple Gaussian and Laplace models on computer vision tasks such as object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">23]</ref> and optical flow estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">27]</ref>, usually aiming to not only achieve accurate predictions, but also to provide an estimate of aleatoric uncertainty <ref type="bibr">[29,</ref><ref type="bibr">20]</ref>. To allow for multimodal models p(y; φ θ (x)), mixture density networks (MDNs) <ref type="bibr" target="#b2">[3]</ref> have also been applied <ref type="bibr">[40,</ref><ref type="bibr" target="#b29">61]</ref>. The DNN then outputs weights for K mixture components along with K sets of parameters, e.g. K sets of means and log-variances for a mixture of Gaussians. Previous work has also applied infinite mixture models by utilizing the conditional VAE (cVAE) framework <ref type="bibr" target="#b26">[58,</ref><ref type="bibr" target="#b20">52]</ref>. A latent variable model p(y|x; θ) = p(y; φ θ (x, z))p(z; φ θ (x))dz is then employed, where p(y; φ θ (x, z)) and p(z; φ θ (x) typically are Gaussian distributions. Our proposed method also entails predicting a conditional target density p(y|x; θ) and minimizing the associated negative log-likelihood. However, our energy-based model p(y|x; θ) is not limited to the functional form of any specific probability density (e.g. Gaussian or Laplace), but is instead directly defined via a learned scalar function of (x, y). In contrast to MDNs and cVAEs, our model p(y|x; θ) is not even limited to densities which are simple to generate samples from. This puts minimal restricting assumptions on the true p(y|x), allowing it to be efficiently learned directly from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence-Based Regression</head><p>Another category of approaches reformulates the regression problem as y = arg max y f θ (x, y), where f θ (x, y) ∈ R is a scalar confidence value predicted by the DNN. The idea is thus to predict a quantity f θ (x, y), depending on both input x and target y, that can be maximized over y to obtain the final prediction y . This maximization-based formulation is inherent in Structural SVMs <ref type="bibr" target="#b28">[60]</ref>, but has also been adopted for DNNs. We term this family of approaches confidence-based regression. Compared to direct regression, the predicted confidence f θ (x, y) can encapsulate multiple hypotheses and other ambiguities. Confidence-based regression has been shown particularly suitable for image-coordinate regression tasks, such as hand keypoint localization <ref type="bibr" target="#b25">[57]</ref> and body-part detection <ref type="bibr" target="#b30">[62,</ref><ref type="bibr" target="#b19">51,</ref><ref type="bibr" target="#b32">64]</ref>. In these cases, a CNN is trained to output a 2D heatmap over the image pixel coordinates y, thus taking full advantage of the translational invariance of the problem. In computer vision, confidence prediction has also been successfully employed for tasks other than pure image-coordinate regression. Jiang et al. <ref type="bibr">[28]</ref> proposed the IoU-Net for bounding box regression in object detection, where a bounding box y ∈ R 4 and image x are both input to the DNN to predict a confidence f θ (x, y). It employs a pooling-based architecture that is differentiable w.r.t. the bounding box y, allowing efficient gradient-based maximization to obtain the final estimate y = arg max y f θ (x, y). IoU-Net was later also successfully applied to target object estimation in visual tracking <ref type="bibr" target="#b8">[9]</ref>.</p><p>In general, confidence-based approaches are trained using a set of pseudo label confidences c(x i , y i , y) generated for each training example (x i , y i ), and by employing a loss f θ (x i , y), c(x i , y i , y) . One strategy <ref type="bibr" target="#b19">[51,</ref><ref type="bibr">34]</ref> is to treat the confidence prediction as a binary classification problem, where c(x i , y i , y) represents either the class, c ∈ {0, 1}, or its probability, c ∈ [0, 1], and employ cross-entropy based losses . The other approach is to treat the confidence prediction as a direct regression problem itself by applying standard regression losses, such as L 2 <ref type="bibr" target="#b25">[57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">62]</ref> or the Huber loss [28]. In these cases, the pseudo label confi-dences c can be constructed using a similarity measure S in the target space, c(x i , y i , y) = S(y, y i ), for example defined as the Intersection over Union (IoU) between two bounding boxes <ref type="bibr">[28]</ref> or simply by a Gaussian kernel <ref type="bibr" target="#b30">[62,</ref><ref type="bibr" target="#b32">64,</ref><ref type="bibr" target="#b27">59]</ref>.</p><p>While these methods have demonstrated impressive results, confidence-based approaches thus require important design choices. In particular, the strategy for constructing the pseudo labels c and the choice of loss are often crucial for performance and highly task-dependent, limiting general applicability. Moreover, the predicted confidence f θ (x, y) can be difficult to interpret, since it has no natural connection to the conditional target density p(y|x). In contrast, our approach is directly trained to predict p(y|x) itself, and importantly it does not require generation of pseudo label confidences or choosing a specific loss. Regression-by-Classification A regression problem can also be treated as a classification problem by first discretizing the target space Y into a finite set of C classes. Standard techniques from classification, such as softmax and the cross-entropy loss, can then be employed. This approach has previously been applied to both age estimation <ref type="bibr" target="#b23">[55,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b35">67]</ref> and head-pose estimation <ref type="bibr" target="#b24">[56,</ref><ref type="bibr" target="#b34">66]</ref>. The discretization of the target space Y however complicates exploiting its inherent neighborhood structure, an issue that has been addressed by exploring ordinal regression methods for 1D problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. While our energy-based approach can be seen as a generalization of the softmax model for classification to the continuous target space Y, it does not suffer from the aforementioned drawbacks of regression-by-classification. On the contrary, our model naturally allows the network to exploit the full structure of the continuous target space Y. Energy-Based Models Our approach is of course also related to the theoretical framework of energy-based models, which often has been employed for machine learning problems in the past <ref type="bibr">[42,</ref><ref type="bibr">24,</ref><ref type="bibr">36]</ref>. It involves learning an energy function E θ (x) ∈ R that assigns low energy to observed data x i and high energy to other values of x. Recently, energy-based models have been used primarily for unsupervised learning problems within computer vision <ref type="bibr" target="#b33">[65,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">35,</ref><ref type="bibr">47]</ref>, where DNNs are directly used to predict E θ (x). These models are commonly trained by minimizing the negative log-likelihood, stemming from the probabilistic model p(x; θ) = e −E θ (x) / e −E θ (x) dx, for example by generating approximate image samples from p(x; θ) using Markov Chain Monte Carlo <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">47</ref>]. In contrast, we study the application of energy-based models for p(y|x) in supervised regression, a mostly overlooked research direction in recent years, and obtain state-of-the-art performance on four diverse computer vision regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Regression Method</head><p>We propose a general and conceptually simple regression method with a clear probabilistic interpretation. Our method employs an energy-based model within a probabilistic regression formulation, defined in Section 3.1. In Section 3.2, we introduce our training strategy which is designed to be simple, yet highly effective and applicable to a wide variety of regression tasks within computer vision. Lastly, we describe our prediction strategy for high accuracy in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>We take the probabilistic view of regression by creating a model p(y|x; θ) of the conditional target density p(y|x), in which θ is learned by minimizing the associated negative log-likelihood. Instead of defining p(y|x; θ) by letting a DNN predict the parameters of a certain family of probability distributions (e.g. Gaussian or Laplace), we construct a versatile energy-based model that can better leverage the predictive power of DNNs. To that end, we take inspiration from confidence-based regression approaches and let a DNN directly predict a scalar value for any input-target pair (x, y). Unlike confidence-based methods however, this prediction has a clear probabilistic interpretation. Specifically, we view a DNN as a function f θ : X × Y → R, parameterized by θ ∈ R P , that maps an input-target pair (x, y) ∈ X × Y to a scalar value f θ (x, y) ∈ R. Our model p(y|x; θ) of the conditional target density p(y|x) is then defined according to,</p><formula xml:id="formula_2">p(y|x; θ) = e f θ (x,y) Z(x, θ) , Z(x, θ) = e f θ (x,ỹ) dỹ ,<label>(1)</label></formula><p>where Z(x, θ) is the input-dependent normalizing partition function. We train this energy-based model (1) by directly minimizing the negative log-likelihood</p><formula xml:id="formula_3">− log p({y i } i |{x i } i ; θ) = N i=1 − log p(y i |x i ; θ),</formula><p>where each term is given by,</p><formula xml:id="formula_4">− log p(y i |x i ; θ) = log e f θ (xi,y) dy − f θ (x i , y i ).<label>(2)</label></formula><p>This direct and straightforward training approach thus requires the evaluation of the generally intractable Z(x, θ) = e f θ (x,y) dy. Many fundamental computer vision tasks, such as object detection, keypoint estimation and pose estimation, however rely on regression problems with a low-dimensional target space Y. In such cases, effective finite approximations of Z(x, θ) can be applied. In some tasks, such as image-coordinate regression, this is naturally performed by a grid approximation, utilizing the dense prediction obtained by fully-convolutional networks. In this work, we however investigate a more generally applicable technique, namely Monte Carlo approximations with importance sampling. This procedure, when employed for training the network, is detailed in Section 3.2. At test time, given an input x , our model in (1) allows evaluating the conditional target density p(y|x ; θ) for any target y by first approximating Z(x , θ), and then predicting the scalar f θ (x , y) using the DNN. This enables the computation of, e.g., the mean and variance of the target value y. In this work, we take inspiration from confidence-based regression and focus on finding the most likely prediction, y = arg max y p(y|x ; θ) = arg max y f θ (x , y), which does not require the evaluation of Z(x , θ) during inference. Thanks to the auto-differentiation capabilities of modern deep learning frameworks, we can apply gradient-based techniques to find the final prediction by simply maximizing the network output f θ (x , y) w.r.t. y. We elaborate on this procedure for prediction in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Our model p(y|x; θ) = e f θ (x,y) /Z(x, θ) of the conditional target density is trained by directly minimizing the negative log-likelihood N i=1 − log p(y i |x i ; θ). To evaluate the integral in (2), we employ Monte Carlo importance sampling. Each term − log p(y i |x i ; θ) is therefore approximated by sampling values {y (k) } M k=1 from a proposal distribution q(y|y i ) that depends on the ground truth target value y i ,</p><formula xml:id="formula_5">− log p(y i |x i ; θ) ≈ log 1 M M k=1 e f θ (xi,y (k) ) q(y (k) |y i ) − f θ (x i , y i ).<label>(3)</label></formula><p>The final loss J(θ) used to train the DNN f θ is then obtained by averaging over</p><formula xml:id="formula_6">all training examples {(x i , y i )} n i=1 in the current mini-batch, J(θ) = 1 n n i=1 log 1 M M m=1 e f θ (xi,y (i,m) ) q(y (i,m) |y i ) − f θ (x i , y i ),<label>(4)</label></formula><p>where {y (i,m) } M m=1 are M samples drawn from q(y|y i ). Qualitatively, minimizing J(θ) encourages the DNN to output large values f θ (x i , y i ) for the ground truth target y i , while minimizing the predicted value f θ (x i , y) at all other targets y.</p><p>In ambiguous or uncertain cases, the DNN can output small values everywhere or large values at multiple hypotheses, but at the cost of a higher loss.</p><p>As can be seen in <ref type="formula" target="#formula_6">(4)</ref>, the DNN f θ is applied both to the input-target pair (x i , y i ), and all input-sample pairs {(x i , y (i,m) )} M m=1 during training. While this can seem inefficient, most applications in computer vision employ network architectures that first extract a deep feature representation for the input x i . The DNN f θ can thus be designed to combine this input feature with the target y at a late stage, as visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. The input feature extraction process, which becomes the main computational bottleneck, therefore needs to be performed only once for each x i . In practice, we found our training strategy to not add any significant overhead compared to the direct regression baselines, and the computational cost to be identical to that of the confidence-based methods.</p><p>Compared to confidence-based regression, a significant advantage of our approach is however that there is no need for generating task-dependent pseudo label confidences or choosing between different losses. The only design choice of our training method is the proposal distribution q(y|y i ). Note however that since the loss J(θ) in (4) explicitly adapts to q(y|y i ), this choice has no effect on the overall behaviour of the loss, only on the quality of the sampled approximation. We found a mixture of a few equally weighted Gaussian components, all centered at the target label y i , to consistently perform well in our experiments across all four diverse computer vision applications. Specifically, q(y|y i ) is set to,</p><formula xml:id="formula_7">q(y|y i ) = 1 L L l=1 N (y; y i , σ 2 l I),<label>(5)</label></formula><p>where the standard deviations {σ l } L l=1 are hyperparameters selected based on a validation set for each experiment. We only considered the simple Gaussian proposal in <ref type="bibr" target="#b4">(5)</ref>, as this was found sufficient to obtain state-of-the-art experimental results. Full ablation studies for the number of components L and {σ l } L l=1 are provided in the supplementary material. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates that our model p(y|x; θ) can learn complex conditional target densities, containing both multimodalities and asymmetry, directly from data using the described training procedure. In this illustrative example, we use (5) with L = 2 and σ 1 = 0.1, σ 2 = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction</head><p>Given an input x at test time, the trained DNN f θ can be used to evaluate the full conditional target density p(y|x ; θ) = e f θ (x ,y) /Z(x , θ), by employing the aforementioned techniques to approximate the constant Z(x , θ). In many applications, the most likely prediction y = arg max y p(y|x ; θ) is however the single desired output. For our energy-based model, this is obtained by directly maximizing the DNN output, y = arg max y f θ (x , y), thus not requiring Z(x , θ) to be evaluated. By taking inspiration from IoU-Net [28] and designing the DNN f θ to be differentiable w.r.t. the target y, the gradient ∇ y f θ (x , y) can be efficiently evaluated using the auto-differentiation tools implemented in modern deep learning frameworks. An estimate of y = arg max y f θ (x , y) can therefore be obtained by performing gradient ascent to find a local maximum of f θ (x , y).</p><p>The gradient ascent refinement is performed either on a single initial estimateŷ, or on a set of random initializations {ŷ k } K k=1 to obtain a final accurate prediction y . Starting at y =ŷ k , we thus run T gradient ascent iterations, y ← y + λ∇ y f θ (x , y), with step-length λ. In our experiments, we fix T (typically, T = 10) and select λ using grid search on a validation set. As noted in Section 3.2, this prediction procedure can be made highly efficient by extracting the feature representation for x only once. Back-propagation is then performed only through a few final layers of the DNN to evaluate the gradient ∇ y f θ (x , y). The gradient computation for a set of candidates {ŷ k } K k=1 can also be parallelized on the GPU by simple batching, requiring no significant overhead. Overall, the inference speed is somewhat decreased compared to direct regression baselines, but is identical to confidence-based methods such as IoU-Net <ref type="bibr">[28]</ref>. An algorithm detailing this prediction procedure is found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform comprehensive experiments on four different computer vision regression tasks: object detection, visual tracking, age estimation and head-pose estimation. Our proposed approach is compared both to baseline regression methods and to state-of-the-art models. Notably, our method significantly outperforms the confidence-based IoU-Net [28] method for bounding box regression in direct comparisons, both when applied for object detection on the COCO dataset <ref type="bibr">[39]</ref> and for target object estimation in the recent ATOM <ref type="bibr" target="#b8">[9]</ref> visual tracker. On age and head-pose estimation, our approach is shown to consistently improve performance of a variety of baselines. All experiments are implemented in PyTorch <ref type="bibr" target="#b18">[50]</ref>. For all tasks, further details are also provided in the supplementary material. <ref type="table">Table 1</ref>. Impact of L and {σ l } L l=1 in the proposal distribution q(y|yi) (5), for the object detection task on the 2017 val split of the COCO [39] dataset. For L = 2, σ1 = σ2/4. For L = 3, σ1 = σ3/4 and σ2 = σ3/2. L = 3 with σL = 0.15 is selected.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Detection</head><p>We first perform experiments on object detection, the task of classifying and estimating a bounding box for each object in a given image. Specifically, we compare our regression method to other techniques for the task of bounding box regression, by integrating them into an existing object detection pipeline.</p><p>To this end, we use the Faster-RCNN <ref type="bibr" target="#b22">[54]</ref> framework, which serves as a popular baseline in the object detection field due to its strong state-of-the-art performance. It employs one network head for classification and one head for regressing the bounding box using the direct regression approach. We also include various probabilistic regression baselines and compare with simple Gaussian and Laplace models, by modifying the Faster-RCNN regression head to predict both the mean and log-variance of the distribution, and adopting the associated negative loglikelihood loss. Similarly, we compare with mixtures of K = {2, 4, 8} Gaussians by duplicating the modified regression head K times and adding a network head for predicting K component weights. Moreover, we compare with an infinite mixture of Gaussians by training a cVAE. Finally, we also compare our approach to the state-of-the-art confidence-based IoU-Net <ref type="bibr">[28]</ref>. It extends Faster-RCNN with an additional branch that predicts the IoU overlap between a target bounding box y and the ground truth. The IoU prediction branch uses differentiable region pooling [28], allowing the initial bounding box predicted by the Faster-RCNN to be refined using gradient-based maximization of the predicted IoU confidence. For our approach, we employ an identical architecture as used in IoU-Net for a fair comparison. Instead of training the network to output the IoU, we predict the exponent f θ (x, y) in (1), trained by minimizing the negative log-likelihood in <ref type="bibr" target="#b3">(4)</ref>. We parametrize the bounding box as y = (c x /w 0 , c y /h 0 , log w, log h) ∈ R 4 , where (c x , c y ) and (w, h) denote the center coordinate and size, respectively. The reference size (w 0 , h 0 ) is set to that of the ground truth during training and the initial box during prediction. Based on the ablation study found in <ref type="table">Table 1</ref>, we employ L = 3 isotropic Gaussians with standard deviation σ l = 0.0375 · 2 l−1 for the proposal distribution <ref type="bibr" target="#b4">(5)</ref>. In addition to the standard IoU-Net, we compare with a version (denoted IoU-Net * ) employing the same proposal distribution and inference settings as in our approach. For both our method and IoU-Net * , we set the refinement step-length λ using grid search on a separate validation set. Our experiments are performed on the large-scale COCO benchmark [39]. We use the 2017 train split (≈ 118 000 images) for training and the 2017 val split (≈ 5 000 images) for setting our hyperparameters. The results are reported on the 2017 test-dev split (≈ 20 000 images), in terms of the standard COCO metrics AP, AP 50 and AP 75 . We also report the inference speed in terms of frames-per-second (FPS) on a single NVIDIA TITAN Xp GPU. We initialize all networks in our comparison with the pre-trained Faster-RCNN weights, using the ResNet50-FPN [38] backbone, and re-train only the newly added layers for a fair comparison. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. Our proposed method obtains the best results, significantly outperforming Faster-RCNN and IoU-Net by 2.2% and 1.1% in AP, respectively. The Gaussian model is outperformed by the mixture of 2 Gaussians, but note that adding more components does not further improve the performance. In comparison, the cVAE achieves somewhat improved performance, but is still clearly outperformed by our method. Compared to the probabilistic baselines, we believe that our energy-based model offers a more direct and effective representation of the underlying density via the scalar DNN output f θ (x, y). The inference speed of our method is somewhat lower than that of Faster-RCNN, but identical to IoU-Net. How the number of iterations T in the gradient-based refinement affects inference speed and performance is analyzed in <ref type="figure" target="#fig_3">Figure 3a</ref>, showing that our choice T = 10 provides a reasonable trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Tracking</head><p>Next, we evaluate our approach on the problem of generic visual object tracking. The task is to estimate the bounding box of a target object in every frame of a video. The target object is defined by a given box in the first video frame. We employ the recently introduced ATOM [9] tracker as our baseline. Given the first-frame annotation, ATOM trains a classifier to first roughly localize the target in a new frame. The target bounding box is then determined using   an IoU-Net-based module, which is also conditioned on the first-frame target appearance using a modulation-based architecture. We train our network to predict the conditional target density through f θ (x, y) in (1), using a network architecture identical to the baseline ATOM tracker. In particular, we employ the same bounding box parameterization as for object detection (Section 4.1) and sample M = 128 boxes during training from a proposal distribution (5) generated by L = 2 Gaussians with standard deviations σ 1 = 0.05, σ 2 = 0.5. During tracking, we follow the same procedure as in ATOM, sampling 10 boxes in each frame followed by gradient ascent to refine the estimate generated by the classification module. The inference speed of our approach is thus identical to ATOM, running at over 30 FPS on a single NVIDIA GT-1080 GPU.</p><p>We demonstrate results on two standard tracking benchmarks: TrackingNet [44] and UAV123 <ref type="bibr">[43]</ref>. TrackingNet contains challenging videos sampled from YouTube, with a test set of 511 videos. The main metric is the Success, defined as the average IoU overlap with the ground truth. UAV123 contains 123 videos captured from a UAV, and includes small and fast-moving objects. We report the overlap precision metric (OP H ), defined as the percentage of frames having bounding box IoU overlap larger than a threshold H. The final AUC score is computed as the average OP over all thresholds H ∈ [0, 1]. Hyperparameters are set on the OTB <ref type="bibr" target="#b31">[63]</ref> and NFS [30] datasets, containing 100 videos each. Due to the significant challenges imposed by the limited supervision and generic nature of the tracking problem, there are no competitive baselines employing direct bounding box regression. Current state-of-the-art employ either confidence-based regression, as in ATOM, or anchor-based bounding box regression techniques <ref type="bibr" target="#b39">[71,</ref><ref type="bibr">37]</ref>. We therefore only compare with the ATOM baseline and include other recent state-of-the-art methods in the comparison. As in Section 4.1, we also <ref type="table">Table 4</ref>. Results for the age estimation task on the UTKFace <ref type="bibr" target="#b37">[69]</ref> dataset. Gradientbased refinement using our proposed method consistently improves MAE (lower is better) for the age predictions produced by a variety of different baselines. compare with a version (denoted ATOM * ) of the IoU-Net-based ATOM employing the same training and inference settings as our final approach. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>, and the success plot on UAV123 is shown in <ref type="figure" target="#fig_3">Figure 3b</ref>. Our approach achieves a significant 2.5% and 2.2% absolute improvement over ATOM on the overall metric on TrackingNet and UAV123, respectively. Note that the improvements are most prominent for high-accuracy boxes, as indicated by OP 0.75 . Our approach also outperforms the recent SiamRPN++ [37], which employs anchor-based bounding box regression <ref type="bibr" target="#b22">[54,</ref><ref type="bibr" target="#b21">53]</ref> and a much deeper backbone network (ResNet50) compared to ours (ResNet18). <ref type="figure" target="#fig_0">Figure 1</ref> (bottom) visualizes an illustrative example of the target density p(y|x; θ) ∝ e f θ (x,y) predicted by our approach during tracking. As illustrated, it predicts flexible densities which qualitatively capture meaningful uncertainty in challenging cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Age Estimation</head><p>To demonstrate the general applicability of our proposed method, we also perform experiments on regression tasks not involving bounding boxes. In age estimation, we are given a cropped image x ∈ R h×w×3 of a person's face, and the task is to predict his/her age y ∈ R + . We utilize the UTKFace <ref type="bibr" target="#b37">[69]</ref> dataset, specifically the subset of 16 434 images used by Cao et al. <ref type="bibr" target="#b3">[4]</ref>. We also utilize the dataset split employed in <ref type="bibr" target="#b3">[4]</ref>, with 3 287 test images and 11 503 images for training. Additionally, we use 1 644 of the training images for validation. Methods are evaluated in terms of the Mean Absolute Error (MAE). The DNN architecture f θ (x, y) of our model first extracts ResNet50 <ref type="bibr">[22]</ref> features g x ∈ R 2048 from the input image x. The age y is processed by four fully-connected layers, generating g y ∈ R 128 . The two feature vectors are then concatenated and processed by two fully-connected layers, outputting f θ (x, y) ∈ R. We apply our proposed method to refine the age predicted by baseline models, using the gradient ascent maximization of f θ (x, y) detailed in Section 3.3. All baseline DNN models employ a similar architecture, including an identical ResNet50 for feature extraction and the same number of fully-connected layers to output either the age y ∈ R (Direct), mean and variance parameters for Gaussian and Laplace distributions, or to output logits for C discretized classes <ref type="bibr">(Softmax )</ref>. The results are found in <ref type="table">Table 4</ref>. We observe that age refinement provided by our method consistently improves the accuracy of the predictions generated by the baselines. For Direct, e.g., this refinement marginally decreases inference speed from 49 to 36 FPS. <ref type="table">Table 5</ref>. Results for the head-pose estimation task on the BIWI <ref type="bibr" target="#b13">[14]</ref> dataset. Gradientbased refinement using our proposed method consistently improves the average MAE (lower is better) for yaw, pitch and roll for the predicted pose produced by our baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Head-Pose Estimation</head><p>Lastly, we evaluate our method on the task of head-pose estimation. In this case, we are given an image x ∈ R h×w×3 of a person, and the aim is to predict the orientation y ∈ R 3 of his/her head, where y is the yaw, pitch and roll angles.</p><p>We utilize the BIWI <ref type="bibr" target="#b13">[14]</ref> dataset, specifically the processed dataset provided by Yang et al. <ref type="bibr" target="#b34">[66]</ref>, in which the images have been cropped to faces detected using MTCNN <ref type="bibr" target="#b36">[68]</ref>. We also employ protocol 2 as defined in <ref type="bibr" target="#b34">[66]</ref>, with 10 613 images for training and 5 065 images for testing. Additionally, we use 1 010 training images for validation. The methods are evaluated in terms of the average MAE for yaw, pitch and roll. The network architecture of the DNN f θ (x, y) defining our model takes the image x ∈ R h×w×3 and orientation y ∈ R 3 as inputs, but is otherwise identical to the age estimation case (Section 4.3). Our model is again evaluated by applying the gradient-based refinement to the predicted orientation y ∈ R 3 produced by a number of baseline models. We use the same baselines as for age estimation, and apart from minor changes required to increase the output dimension from 1 to 3, identical network architectures are also used. The results are found in <ref type="table">Table 5</ref>, and also in this case we observe that refinement using our proposed method consistently improves upon the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a general and conceptually simple regression method with a clear probabilistic interpretation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide additional details and results. It consists of Appendix A -Appendix F. Appendix A contains a detailed algorithm for our employed prediction procedure. Appendix B contains details on the illustrative 1D regression problem in <ref type="figure" target="#fig_1">Figure 2</ref> in the main paper. Further details on the employed training and inference procedures are provided in Appendix C for the object detection experiments, and in Appendix D for the visual tracking experiments. Lastly, Appendix E and Appendix F contain details and full results for the experiments on age estimation and head-pose estimation, respectively. Note that equations, tables, figures and algorithms in this supplementary document are numbered with the prefix "S". Numbers without this prefix refer to the main paper.</p><p>The Gaussian model is defined using a DNN f θ (x) according to,</p><formula xml:id="formula_8">p(y|x; θ) = N y; µ θ (x), σ 2 θ (x) , f θ (x) = [ µ θ (x) log σ 2 θ (x) ] T ∈ R 2 . (S1)</formula><p>It is trained by minimizing the negative log-likelihood, corresponding to the loss,</p><formula xml:id="formula_9">J(θ) = 1 n n i=1 (y i − µ θ (x i )) 2 σ 2 θ (x i ) + log σ 2 θ (x i ).<label>(S2)</label></formula><p>The DNN f θ is a simple feed-forward neural network, containing two shared fully-connected layers (dimensions: 1 → 10, 10 → 10) and two identical heads for µ and log σ 2 of three fully-connected layers (10 → 10, 10 → 10, 10 → 1). Our proposed model p(y|x; θ) = e f θ (x,y) /Z(x, θ) (Eq. 1 in the paper) is defined using a feed-forward neural network f θ (x, y) containing two fully-connected layers (1 → 10, 10 → 10) for both x and y, and three fully-connected layers (20 → 10, 10 → 10, 10 → 1) processing the concatenated (x, y) feature vector. It is trained using M = 1024 samples from a proposal distribution q(y|y i ) (Eq. 5 in the paper) with L = 2 and variances σ 2 1 = 0.1 2 , σ 2 2 = 0.8 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Object Detection</head><p>Here, we provide further details about the network architectures, training procedure, and hyperparameters used for our experiments on object detection (Section 4.1 in the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Network Architecture</head><p>We use the Faster-RCNN <ref type="bibr" target="#b22">[54]</ref> detector with ResNet50-FPN [38] as our baseline. As visualized in <ref type="figure" target="#fig_0">Figure S1a</ref>, Faster-RCNN generates object proposals using a region proposal network (RPN). The features from the proposal regions are then pooled to a fixed-sized feature map using the RoiPool layer <ref type="bibr" target="#b17">[18]</ref>. The pooled features are then passed through a feature extractor (denoted Feat-Box) consisting of two fully-connected (FC) layers. The output feature vector is then passed through two parallel FC layers, one which predicts the class label (denoted FC-Cls), and another which regresses the offsets between the proposal and the ground truth box (denoted FC-BB). We use the PyTorch implementation for Faster-RCNN from <ref type="bibr">[41]</ref>. Note that we use the RoiAlign [21] layer instead of RoiPool in our experiments as it has been shown to achieve better performance <ref type="bibr">[21]</ref>. For the Gaussian and Laplace probabilistic models (Gaussian and Laplace in <ref type="table" target="#tab_1">Table 2</ref> in the paper), we replace the FC-BB layer in Faster-RCNN with two parallel FC layers, denoted FC-BBMean and FC-BBVar, which predict the mean and the log-variance of the distribution modeling the offset between the proposal and the ground truth box for each coordinate. This architecture is  shown in <ref type="figure" target="#fig_0">Figure S1b</ref>. For the mixtures of K = {2, 4, 8} Gaussians, we duplicate FC-BBMean and FC-BBVar K times, and add an FC layer for predicting the K component weights. For the cVAE, FC-BBMean and FC-BBVar instead outputs the mean and log-variance of a Gaussian distribution for the latent variable z ∈ R 10 . Duplicates of FC-BBMean and FC-BBVar, modified to also take sampled z values as input, then predicts the mean and log-variance of the distribution modeling the bounding box offset. For our confidence-based IoU-Net [28] models (IoU-Net and IoU-Net * in Table 2), we use the same network architecture as employed in the original paper, shown in <ref type="figure" target="#fig_0">Figure S1c</ref>. That is, we add an additional branch to predict the IoU overlap between the proposal box and the ground truth. This branch uses the PrRoiPool [28] layer to pool the features from the proposal regions. The pooled features are passed through a feature extractor (denoted Feat-Conf) consisting of two FC layers. The output feature vector is passed through another FC layer, FC-Conf, which predicts the IoU. We use an identical architecture for our approach, but train it to output f θ (x, y) in p(y|x; θ) = e f θ (x,y) /Z(x, θ) instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Training</head><p>We use the pre-trained weights for Faster-RCNN from <ref type="bibr">[41]</ref>. Note that the bounding box regression in Faster-RCNN is trained using a direct method, with an Huber loss <ref type="bibr">[26]</ref>. We trained the other networks in <ref type="table" target="#tab_1">Table 2</ref> in the paper (Gaussian, Gaussian Mixt. 2, Gaussian Mixt. 4, Gaussian Mixt. 8, Gaussian cVAE, Laplace, IoU-Net, IoU-Net * and Ours) on the MS-COCO [39] training split (2017 train) using stochastic gradient descent (SGD) with a batch size of 16 for 60k iterations. The base learning rate lr base is reduced by a factor of 10 after 40k and 50k iterations, for all the networks. We also warm up the training by linearly increasing the learning rate from <ref type="bibr" target="#b0">1</ref> 3 lr base to lr base during the first 500 iterations. We use a weight decay of 0.0001 and a momentum of 0.9. For all the networks, we only trained the newly added layers, while keeping the backbone and the region proposal network fixed.</p><p>For the Gaussian, mixture of Gaussians, cVAE and Laplace models, we only train the final predictors (FC-BBMean and FC-BBVar), while keeping the class predictor (FC-Cls) and the box feature extractor (Feat-Box) fixed. We also tried fine-tuning the FC-Cls and Feat-Box weights for the Gaussian model, with different learning rate settings, but obtained worse performance on the validation set. The weights for both FC-BBMean and FC-BBVar were initialized with zero mean Gaussian with standard deviation of 0.001. All these models were trained with a base learning rate lr base = 0.005 by minimizing the negative log-likelihood, except for the cVAE which is trained by maximizing the ELBO (using 128 sampled z values to approximate the expectation).</p><p>For the IoU-Net, IoU-Net * and our proposed model, we only trained the newly added confidence branch. We found it beneficial to initialize the feature extractor block (Feat-Conf) with the corresponding weights from Faster-RCNN, i.e. the Feat-Box block. The weights for the predictor FC-Conf were initialized with zero mean Gaussian with standard deviation of 0.001. As in the original paper [28], we used a base learning rate lr base = 0.01 for the IoU-Net and IoU-Net * networks. For our proposed model, we used lr base = 0.001 due to the different scaling of the loss. Note that we did not perform any parameter tuning for setting the learning rates. We generate 128 proposals for each ground truth box during training. For the IoU-Net, we use the proposal generation strategy mentioned in the original paper <ref type="bibr">[28]</ref>. That is, for each ground truth box, we generate a large set of candidate boxes which have an IoU overlap of at least 0.5 with the ground truth, and uniformly sample 128 proposals from this candidate set w.r.t. the IoU. For IoU-Net * and our model, we sample boxes from a proposal distribution (Eq. 5 in the paper) generated by L = 3 Gaussians with standard deviations of 0.0375, 0.075 and 0.15. The IoU-Net and IoU-Net * are trained by minimizing the Huber loss between the predicted IoU and the ground truth, while our model is trained by minimizing the negative log likelihood of the training data (Eq. 4 in the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Analysis of Proposal Distribution</head><p>An extensive ablation study for the number of components L and standard deviations {σ l } L l=1 in the proposal distribution q(y|y i ) = 1 L L l=1 N (y; y i , σ 2 l ) (Eq. 5 in the paper) is provided in <ref type="table">Table S1</ref>, which is an extended version of <ref type="table">Table 1</ref> in the paper. We find that L = 1 downgrades performance, while there is no significant difference between L = 2 and L = 3. For L ∈ {2, 3}, the results are not particularly sensitive to the specific choice of {σ l } L l=1 , but benefits from including both small and relatively large values in {σ l } L l=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Inference</head><p>The inference in both the Gaussian and Laplace models is identical to the one employed by Faster-RCNN, except the output mean is taken as the prediction. Thus, we do not utilize the output variances during inference. For the mixture of K = {2, 4, 8} Gaussians, we compute the mean of the distribution and take that <ref type="table">Table S1</ref>. Impact of L and {σ l } L l=1 in the proposal distribution q(y|yi) (Eq. 5 in the paper), for the object detection task on the 2017 val split of COCO as our prediction. Instead picking the component with the largest weight and taking its mean as the prediction resulted in somewhat worse validation performance. For cVAE, we approximately compute the mean (using 128 samples) of the distribution and take that as our prediction.</p><p>For IoU-Net and IoU-Net * , we perform IoU-Guided NMS as described in [28], followed by gradient-based refinement (Algorithm S1). For our proposed approach we adopt the same NMS technique, but guide it with the values f θ (x, y) predicted by our network instead. We use a step-length λ = 0.5 and step-length decay η = 0.1 for IoU-Net. For IoU-Net * and our approach we perform the gradient-based refinement in the relative bounding box parametrization y = (c x /w 0 , c y /h 0 , log w, log h) (see Section 4.1 in the paper). Here, we employ different step-lengths for position and size. For IoU-Net * , we use λ = 0.002 and λ = 0.008 respectively, with a decay of η = 0.2. For our proposed approach, we use λ = 0.0001 and λ = 0.0004 with η = 0.5. For all methods, these hyperparameters (λ and η) were set using a grid search on the COCO validation split (2017 val ). We used T = 10 refinement iterations for each of the three models. Note that since a given image x can have multiple ground truth instances, multiple bounding boxes are usually refined. The gradient-based refinement then moves each individual box y towards the maximum of a local mode in f θ (x, y). Thus, they will not converge to a single solution. Also note that f θ (x, y) is class-conditional (as in the IoU-Net baseline), eliminating the risk of confusing neighboring objects of different classes. <ref type="table" target="#tab_1">Table S2</ref>. Impact of L and {σ l } L l=1 in the proposal distribution q(y|yi) (Eq. 5 in the paper), for the visual tracking task on the combined OTB <ref type="bibr" target="#b31">[63]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Visual Tracking</head><p>Here, we provide further details about the training procedure and hyperparameters used for our experiments on visual object tracking (Section 4.2 in the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Training</head><p>We adopt the ATOM <ref type="bibr" target="#b8">[9]</ref> tracker as our baseline, and use the PyTorch implementation and pre-trained weights from <ref type="bibr" target="#b7">[8]</ref>. ATOM trains an IoU-Net-based module to predict the IoU overlap between a candidate box and the ground truth, conditioned on the first-frame target appearance. The IoU predictor is trained by generating 16 candidates for each ground truth box. The candidates are generated by adding a Gaussian noise for each ground truth box coordinate, while ensuring a minimum IoU overlap of 0.1 between the candidate box and the ground truth. The network is trained by minimizing the squared error (L 2 loss) between the predicted and ground truth IoU. Our proposed model is instead trained by sampling 128 candidate boxes from a proposal distribution (Eq. 5 in the paper) generated by L = 2 Gaussians with standard deviations of 0.05 and 0.5, and minimizing the negative log likelihood of the training data. An ablation study for the proposal distribution is found in <ref type="table" target="#tab_1">Table S2</ref>. We use the training splits of the TrackingNet [44], LaSOT <ref type="bibr" target="#b12">[13]</ref>, GOT10k [25], and COCO datasets for our training. Our network is trained for 50 epochs, using the ADAM optimizer with a base learning rate of 0.001 which is reduced by a factor of 5 after every 15 epochs. The rest of the training parameters are exactly the same is in ATOM. The ATOM * model is trained by using the exact same proposal distribution, datasets and settings. It only differs by the loss, which is the same squared error between the predicted and ground truth IoU as in the original ATOM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Inference</head><p>During tracking, the ATOM tracker first applies the classification head network, which is trained online, to coarsely localize the target object. 10 random boxes are then sampled around this prediction, to be refined by the IoU prediction network. We only alter the final bounding box refinement step of the 10 given random initial boxes, and preserve all other settings as in the original ATOM tracker. The original version performs T = 5 gradient ascent iterations with a step length of λ = 1.0. For our proposed model and the ATOM * version, we use T = 10 iterations, employing the bounding box parameterization described in Section 4.1. For our approach, we set the step length to λ = 2 · 10 −4 for position and λ = 10 −3 for size dimensions. For ATOM * , we use λ = 10 −2 for position and λ = 5 · 10 −2 for size dimensions. These parameters were set on the separate validation set. For simplicity, we adopt the vanilla gradient ascent strategy employed in ATOM for the two other methods as well. That is, we have no decay (η = 1) and do not perform checks whether the confidence score is increasing in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Qualitative Results</head><p>Illustrative examples of the target density p(y|x; θ) ∝ e f θ (x,y) predicted by our approach during tracking are visualized in <ref type="figure" target="#fig_1">Figure S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E Age Estimation</head><p>In this appendix, further details on the age estimation experiments (Section 4.3 in the paper) are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Network Architecture</head><p>The DNN architecture f θ (x, y) of our proposed model first extracts ResNet50 features g x ∈ R 2048 from the input image x. The age y is processed by four fullyconnected layers (dimensions: 1 → 16, 16 → 32, 32 → 64, 64 → 128), generating g y ∈ R 128 . The two feature vectors g x , g y are then concatenated to form g x,y ∈ R 2048+128 , which is processed by two fully-connected layers (2048 + 128 → 2048, 2048 → 1), outputting f θ (x, y) ∈ R. <ref type="table" target="#tab_2">Table S3</ref>. Impact of {σ l } 2 l=1 in the proposal distribution q(y|yi) (Eq. 5 in the paper), for the age estimation task on our validation split of the UTKFace <ref type="bibr" target="#b37">[69]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Training</head><p>Our model is trained using M = 1024 samples from a proposal distribution q(y|y i ) (Eq. 5 in the paper) with L = 2 and variances σ 2 1 = 0.1 2 , σ 2 2 = 20 2 . An ablation study for the variances is found in <ref type="table" target="#tab_2">Table S3</ref>. The model is trained for 75 epochs with a batch size of 32, using the ADAM optimizer with weight decay of 0.001. The images x are of size 200×200. For data augmentation, we use random flipping along the vertical axis and random scaling in the range [0.7, 1.4]. After random flipping and scaling, a random image crop of size 200 × 200 is also selected. The ResNet50 is imported from torchvision.models in PyTorch with the pretrained option set to true, all other network parameters are randomly initialized using the default initializer in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Prediction</head><p>For this experiment, we use a slight variation of Algorithm S1, which is found in Algorithm S2. There, T is the number of gradient ascent iterations, λ is the stepsize, Ω 1 is an early-stopping threshold and Ω 2 is a degeneration tolerance. Following IoU-Net, we set T = 5, Ω 1 = 0.001 and Ω 2 = −0.01. Based on the validation set, we select λ = 3. We refine a single estimateŷ, predicted by each baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm S2 Prediction via gradient-based refinement (variation).</head><p>Input: x ,ŷ, T , λ, Ω1, Ω2.</p><p>1: y ←ŷ. 2: for t = 1, . . . , T do 3:</p><p>PrevValue ← f θ (x , y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>y ← y + λ∇yf θ (x , y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>NewValue ← f θ (x , y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if |PrevValue − NewValue| &lt; Ω1 or (NewValue − PrevValue) &lt; Ω2 then 7:</p><p>Return y. 8: Return y. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Baselines</head><p>All baselines are trained for 75 epochs with a batch size of 32, using the ADAM optimizer with weight decay of 0.001. Identical data augmentation and parameter initialization as for our proposed model is used. Direct The DNN architecture of Direct first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two fully-connected layers (2048 → 2048, 2048 → 1), outputting the predictionŷ ∈ R. It is trained by minimizing either the Huber or L 2 loss. Gaussian The Gaussian model is defined using a DNN f θ (x) according to, p(y|x; θ) = N y; µ θ (x), σ 2</p><formula xml:id="formula_10">θ (x) , f θ (x) = [ µ θ (x) log σ 2 θ (x) ] T ∈ R 2 .<label>(S3)</label></formula><p>It is trained by minimizing the negative log-likelihood, corresponding to the loss,</p><formula xml:id="formula_11">J(θ) = 1 n n i=1 (y i − µ θ (x i )) 2 σ 2 θ (x i ) + log σ 2 θ (x i ).<label>(S4)</label></formula><p>The DNN architecture of f θ (x) first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two heads of two fully-connected layers (2048 → 2048, 2048 → 1) to output µ θ (x) and log σ 2 θ (x). The mean µ θ (x) is taken as the predictionŷ. Laplace The Laplace model is defined using a DNN f θ (x) according to,</p><formula xml:id="formula_12">p(y|x; θ) = 1 2β θ (x) exp − |y − µ θ (x)| β θ (x) , f θ (x) = [ µ θ (x) log β θ (x) ] T ∈ R 2 .<label>(S5)</label></formula><p>It is trained by minimizing the negative log-likelihood, corresponding to the loss,</p><formula xml:id="formula_13">J(θ) = 1 n n i=1 |y i − µ θ (x i )| β θ (x i ) + log β θ (x i ). (S6)</formula><p>The DNN architecture of f θ (x) first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two heads of two fully-connected layers (2048 → 2048, 2048 → 1) to output µ θ (x) and log β θ (x). The mean µ θ (x) is taken as the predictionŷ. Softmax The DNN architecture of Softmax first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two fully-connected layers (2048 → 2048, 2048 → C), outputting logits for C = 101 discretized classes {0, 1, . . . , 100}. It is trained by minimizing either the cross-entropy (CE) and L 2 losses, J = J CE + 0.1J L 2 , or the CE, L 2 and variance [49] losses, J = J CE + 0.1J L 2 + 0.05J V ar . The predictionŷ is computed as the softmax expected value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Full Results</head><p>Full experiment results, extending the results found in <ref type="table">Table 4</ref> (Section 4.3 in the paper), are provided in <ref type="table" target="#tab_10">Table S4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F Head-Pose Estimation</head><p>In this appendix, further details on the head-pose estimation experiments (Section 4.4 in the paper) are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Network Architecture</head><p>The DNN architecture f θ (x, y) of our proposed model first extracts ResNet50 features g x ∈ R 2048 from the input image x. The pose y ∈ R 3 is processed by four fully-connected layers (dimensions: 3 → 16, 16 → 32, 32 → 64, 64 → 128), generating g y ∈ R 128 . The two feature vectors g x , g y are then concatenated to form g x,y ∈ R 2048+128 , which is processed by two fully-connected layers (2048 + 128 → 2048, 2048 → 1), outputting f θ (x, y) ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Training</head><p>Our model is trained using M = 1024 samples from a proposal distribution q(y|y i ) (Eq. 5 in the paper) with L = 2 and variances σ 2 1 = 1 2 , σ 2 2 = 20 2 for yaw, pitch and roll. An ablation study for the variances is found in <ref type="table">Table S5</ref>. The model is trained for 75 epochs with a batch size of 32, using the ADAM optimizer with weight decay of 0.001. The images x are of size 64 × 64. For data augmentation, we use random flipping along the vertical axis and random scaling in the range [0.7, 1.4]. After random flipping and scaling, a random image crop of size 64×64 is also selected. The ResNet50 is imported from torchvision.models in PyTorch with the pretrained option set to true, all other network parameters are randomly initialized using the default initializer in PyTorch. <ref type="table">Table S5</ref>. Impact of {σ l } 2 l=1 in the proposal distribution q(y|yi) (Eq. 5 in the paper), for the head-pose estimation task on our validation split of the BIWI <ref type="bibr" target="#b13">[14]</ref>   <ref type="table">Table S6</ref>. Full results for the head-pose estimation experiments. Gradient-based refinement using our proposed method consistently improves the average MAE for yaw, pitch, roll (lower is better) for the predicted poses outputted by a number of baselines.</p><p>roll angles (in degrees). It is trained by minimizing either the cross-entropy (CE) and L 2 losses, J = J CE + 0.1J L 2 , or the CE, L 2 and variance [49] losses, J = J CE + 0.1J L 2 + 0.05J V ar . The predictionŷ is obtained by computing the softmax expected value for yaw, pitch and roll.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Full Results</head><p>Full experiment results, extending the results found in <ref type="table">Table 5</ref> (Section 4.4 in the paper), are provided in <ref type="table">Table S6</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:1909.12297v4 [cs.LG] 19 Jul 2020 An overview of the proposed regression method (top). We train an energybased model p(y|x; θ) ∝ e f θ (x,y) of the conditional target density p(y|x), using a DNN f θ to predict the un-normalized density directly from the input-target pair (x, y). Our approach is capable of predicting highly flexible densities and produce highly accurate estimates. This is demonstrated for the problem of bounding box regression (bottom), visualizing the marginal density for the top right box corner as a heatmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An illustrative 1D regression problem. The training data D = {(xi, yi)} 2000 i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Impact of the number of gradient ascent iterations T on performance (AP) and inference speed (FPS), for the object detection task on the 2017 val split of the COCO [39] dataset. (b) Success plot on the UAV123 [43] visual tracking dataset, showing the overlap precision OPH as a function of the overlap threshold H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. S1 .</head><label>S1</label><figDesc>Network architectures for the different object detection networks used in our experiments (Section 4.1 in the paper). The backbone feature extractor (ResNet50-FPN), and the region proposal network (RPN) is not shown for clarity. We do not train the blocks in blue color, using the pre-trained Faster-RCNN weights from[41]   instead. The blocks in red are initialized with the pre-trained Faster-RCNN weights and fine-tuned. The blocks in green on the other hand are trained from scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. S2 .</head><label>S2</label><figDesc>Visualization of the conditional target density p(y|x; θ) ∝ e f θ (x,y) predicted by our network for the task of bounding box estimation in visual tracking. Since the target space y ∈ R 4 is 4-dimensional, we visualize the density for different locations of the top-right corner as a heatmap, while the bottom-left is kept fixed at the tracker output (red box). Our network predicts flexible densities which qualitatively capture meaningful uncertainty in challenging cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>38.5 37.5 39.0 39.1 39.0 39.0 39.1 38.8</figDesc><table><row><cell>Number of components L</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>Base proposal st. dev. σL</cell><cell>0.02 0.04 0.08</cell><cell>0.1 0.15 0.2</cell><cell>0.1 0.15 0.2</cell></row><row><cell>AP (%)</cell><cell>38.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results for the object detection task on the 2017 test-dev split of the COCO [39] dataset. Our proposed method significantly outperforms the baseline FPN Faster-RCNN [38] and the state-of-the-art confidence-based IoU-Net [28].</figDesc><table><row><cell>Formulation</cell><cell>Direct</cell><cell cols="8">Gaussian Gaussian Gaussian Gaussian Gaussian Laplace Confidence Confidence</cell><cell></cell></row><row><cell>Approach</cell><cell>Faster-RCNN</cell><cell></cell><cell cols="4">Mixt. 2 Mixt. 4 Mixt. 8 cVAE</cell><cell></cell><cell cols="3">IoU-Net IoU-Net  *  Ours</cell></row><row><cell>AP (%)</cell><cell>37.2</cell><cell>36.7</cell><cell>37.1</cell><cell>37.0</cell><cell>36.8</cell><cell>37.2</cell><cell>37.1</cell><cell>38.3</cell><cell>38.2</cell><cell>39.4</cell></row><row><cell>AP50(%)</cell><cell>59.2</cell><cell>58.7</cell><cell>59.1</cell><cell>59.1</cell><cell>59.1</cell><cell>59.2</cell><cell>59.1</cell><cell>58.3</cell><cell>58.4</cell><cell>58.6</cell></row><row><cell>AP75(%)</cell><cell>40.3</cell><cell>39.6</cell><cell>40.0</cell><cell>39.9</cell><cell>39.7</cell><cell>40.0</cell><cell>40.2</cell><cell>41.4</cell><cell>41.4</cell><cell>42.1</cell></row><row><cell>FPS</cell><cell>12.2</cell><cell>12.2</cell><cell>12.2</cell><cell>12.1</cell><cell>12.1</cell><cell>9.6</cell><cell>12.2</cell><cell>5.3</cell><cell>5.3</cell><cell>5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results for the visual tracking task on the two common datasets Track-ingNet [44] and UAV123[43]. The symbol † indicates an approximate value (±1), taken from the plot in the corresponding paper. Our proposed method significantly outperforms the baseline ATOM and other recent state-of-the-art trackers.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell cols="9">ECO SiamFC MDNet UPDT DaSiamRPN SiamRPN++ ATOM ATOM  *  Ours [10] [1] [45] [2] [71] [37] [9]</cell></row><row><cell></cell><cell>Precision (%)</cell><cell>49.2</cell><cell>53.3</cell><cell>56.5</cell><cell>55.7</cell><cell>59.1</cell><cell>69.4</cell><cell>64.8</cell><cell>66.6</cell><cell>69.7</cell></row><row><cell>TrackingNet</cell><cell cols="2">Norm. Prec. (%) 61.8</cell><cell>66.6</cell><cell>70.5</cell><cell>70.2</cell><cell>73.3</cell><cell>80.0</cell><cell>77.1</cell><cell>78.4</cell><cell>80.1</cell></row><row><cell></cell><cell>Success (%)</cell><cell>55.4</cell><cell>57.1</cell><cell>60.6</cell><cell>61.1</cell><cell>63.8</cell><cell>73.3</cell><cell>70.3</cell><cell>72.0</cell><cell>74.5</cell></row><row><cell></cell><cell>OP0.50 (%)</cell><cell>64.0</cell><cell>-</cell><cell>-</cell><cell>66.8</cell><cell>73.6</cell><cell>75  †</cell><cell>78.9</cell><cell>79.0</cell><cell>80.8</cell></row><row><cell>UAV123</cell><cell>OP0.75 (%)</cell><cell>32.8</cell><cell>-</cell><cell>-</cell><cell>32.9</cell><cell>41.1</cell><cell>56  †</cell><cell>55.7</cell><cell>56.5</cell><cell>60.2</cell></row><row><cell></cell><cell>AUC (%)</cell><cell>53.7</cell><cell>-</cell><cell>52.8</cell><cell>55.0</cell><cell>58.4</cell><cell>61.3</cell><cell>65.0</cell><cell>64.9</cell><cell>67.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>+Refine Niu et al. [48] Cao et al. [4] Direct Gaussian Laplace Softmax (CE, L 2 ) Softmax (CE, L 2 , Var) 5.74 ± 0.05 5.47 ± 0.01 4.81 ± 0.02 4.79 ± 0.06 4.85 ± 0.04 4.78 ± 0.05 4.81 ± 0.03 --4.65 ± 0.02 4.66 ± 0.04 4.81 ± 0.04 4.65 ± 0.04 4.69 ± 0.03</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>+Refine Gu et al. [19] Yang et al. [66] Direct Gaussian Laplace Softmax (CE, L 2 ) Softmax (CE, L 2 , Var) 3.66 3.60 3.09 ± 0.07 3.12 ± 0.08 3.21 ± 0.06 3.04 ± 0.08 3.15 ± 0.07 --3.07 ± 0.07 3.11 ± 0.07 3.19 ± 0.06 3.01 ± 0.07 3.11 ± 0.06</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>At test time, targets are predicted by maximizing the DNN output f θ (x, y) w.r.t. y via gradient-based refinement. Extensive experiments performed on four diverse computer vision tasks demonstrate the high accuracy and wide applicability of our method. Future directions include exploring improved architectural designs, studying other regression applications, and investigating our proposed method's potential for aleatoric uncertainty estimation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1548-1557 (2017) 14, 31 20. Gustafsson, F.K., Danelljan, M., Schön, T.B.: Evaluating scalable Bayesian deep learning methods for robust computer vision. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.: A tutorial on energybased learning. Predicting structured data 1(0) (2006) 3, 6 37. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.: SiamRPN++: Evolution of siamese visual tracking with very deep networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp.</figDesc><table><row><cell>35. Lawson, D., Tucker, G., Dai, B., Ranganath, R.: Energy-inspired models: Learning</cell></row><row><cell>with sampler-induced distributions. In: Advances in Neural Information Processing</cell></row><row><cell>Systems (NeurIPS) (2019) 6</cell></row><row><cell>(2020) 5 21. He, K., Gkioxari, G., Dollár, P., Girshick, R.B.: Mask R-CNN. Proceedings of the 36. 4282-4291</cell></row><row><cell>IEEE International Conference on Computer Vision (ICCV) pp. 2980-2988 (2017) (2019) 1, 11, 12, 13</cell></row><row><cell>21 38. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature</cell></row><row><cell>22. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: pyramid networks for object detection. In: Proceedings of the IEEE Conference on</cell></row><row><cell>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Computer Vision and Pattern Recognition (CVPR). pp. 2117-2125 (2017) 4, 10,</cell></row><row><cell>(CVPR). pp. 770-778 (2016) 13 11, 21</cell></row><row><cell>23. He, Y., Zhu, C., Wang, J., Savvides, M., Zhang, X.: Bounding box regression with 39. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,</cell></row><row><cell>uncertainty for accurate object detection. In: Proceedings of the IEEE Conference Zitnick, C.L.: Microsoft COCO: Common objects in context. In: Proceedings of</cell></row><row><cell>on Computer Vision and Pattern Recognition (CVPR). pp. 2888-2897 (2019) 5 the European Conference on Computer Vision (ECCV). pp. 740-755 (2014) 4, 9,</cell></row><row><cell>10, 11, 12, 22, 24 24. Hinton, G., Osindero, S., Welling, M., Teh, Y.W.: Unsupervised discovery of nonlin-40. Makansi, O., Ilg, E., Cicek, O., Brox, T.: Overcoming limitations of mixture density ear structure using contrastive backpropagation. Cognitive science 30(4), 725-731 networks: A sampling and fitting framework for multimodal future prediction. In: (2006) 6 Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 25. Huang, L., Zhao, X., Huang, K.: GOT-10k: A large high-diversity benchmark for (CVPR). pp. 7144-7153 (2019) 4, 5 generic object tracking in the wild. IEEE Transactions on Pattern Analysis and 41. Massa, F., Girshick, R.: maskrcnn-benchmark: Fast, modular reference implemen-Machine Intelligence (TPAMI) (2019) 25 tation of Instance Segmentation and Object Detection algorithms in PyTorch. 26. Huber, P.J.: Robust estimation of a location parameter. The Annals of Mathemat-https://github.com/facebookresearch/maskrcnn-benchmark (2018), accessed: ical Statistics pp. 73-101 (1964) 4, 22 04/09/2019 21, 22 27. Ilg, E., Cicek, O., Galesso, S., Klein, A., Makansi, O., Hutter, F., Bro, T.: Uncer-42. Mnih, A., Hinton, G.: Learning nonlinear constraints with contrastive backprop-tainty estimates and multi-hypotheses networks for optical flow. In: Proceedings agation. In: Proceedings of the IEEE International Joint Conference on Neural of the European Conference on Computer Vision (ECCV). pp. 652-667 (2018) 2, Networks. vol. 2, pp. 1302-1307. IEEE (2005) 6 4, 5 43. Mueller, M., Smith, N., Ghanem, B.: A benchmark and simulator for UAV tracking. 28. Jiang, B., Luo, R., Mao, J., Xiao, T., Jiang, Y.: Acquisition of localization confi-In: Proceedings of the European Conference on Computer Vision (ECCV). pp. dence for accurate object detection. In: Proceedings of the European Conference 445-461 (2016) 4, 11, 12 on Computer Vision (ECCV). pp. 784-799 (2018) 1, 3, 4, 5, 6, 9, 10, 22, 23, 24 44. Muller, M., Bibi, A., Giancola, S., Alsubaihi, S., Ghanem, B.: TrackingNet: A 29. Kendall, A., Gal, Y.: What uncertainties do we need in Bayesian deep learn-large-scale dataset and benchmark for object tracking in the wild. In: Proceedings ing for computer vision? In: Advances in Neural Information Processing Systems of the European Conference on Computer Vision (ECCV). pp. 300-317 (2018) 4, (NeurIPS). pp. 5574-5584 (2017) 2, 4, 5 11, 12, 25</cell></row><row><cell>30. Kiani Galoogahi, H., Fagg, A., Huang, C., Ramanan, D., Lucey, S.: Need for speed: 45. Nam, H., Han, B.: Learning multi-domain convolutional neural networks for visual</cell></row><row><cell>A benchmark for higher frame rate object tracking. In: Proceedings of the IEEE tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern</cell></row><row><cell>International Conference on Computer Vision (ICCV). pp. 1125-1134 (2017) 12, Recognition (CVPR). pp. 4293-4302 (2016) 1, 11</cell></row><row><cell>25 46. Niethammer, M., Huang, Y., Vialard, F.X.: Geodesic regression for image time-</cell></row><row><cell>31. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint series. In: International conference on medical image computing and computer-</cell></row><row><cell>arXiv:1412.6980 (2014) 20 assisted intervention. pp. 655-662. Springer (2011) 1</cell></row><row><cell>32. Lakshminarayanan, B., Pritzel, A., Blundell, C.: Simple and scalable predictive 47. Nijkamp, E., Hill, M., Han, T., Zhu, S.C., Wu, Y.N.: On the anatomy of MCMC-</cell></row><row><cell>uncertainty estimation using deep ensembles. In: Advances in Neural Information based maximum likelihood learning of energy-based models. In: Thirty-Fourth</cell></row><row><cell>Processing Systems (NeurIPS). pp. 6402-6413 (2017) 2, 4 AAAI Conference on Artificial Intelligence (2020) 6</cell></row><row><cell>33. Lathuilière, S., Mesejo, P., Alameda-Pineda, X., Horaud, R.: A comprehensive 48. Niu, Z.</cell></row><row><cell>analysis of deep regression. IEEE Transactions on Pattern Analysis and Machine</cell></row><row><cell>Intelligence (TPAMI) (2019) 2, 4</cell></row><row><cell>34. Law, H.2018)</cell></row><row><cell>1, 3, 4, 5</cell></row></table><note>It models the conditional target density p(y|x) by predicting the un-normalized density through a DNN f θ (x, y), taking the input- target pair (x, y) as input. This energy-based model p(y|x; θ) = e f θ (x,y) /Z(x, θ) of p(y|x) is trained by directly minimizing the associated negative log-likelihood, employing Monte Carlo importance sampling to approximate the partition func- tion Z(x, θ).Acknowledgments This research was supported by the Swedish Founda- tion for Strategic Research via ASSEMBLE, the Swedish Research Council via Learning flexible models for nonlinear dynamics, the ETH Zürich Fund (OK), a Huawei Technologies Oy (Finland) project, an Amazon AWS grant, and Nvidia.19. Gu, J., Yang, X., De Mello, S., Kautz, J.: Dynamic facial analysis: From bayesian filtering to recurrent neural network. In:, Deng, J.: Cornernet: Detecting objects as paired keypoints. In: Proceed- ings of the European Conference on Computer Vision (ECCV). pp. 734-750 (, Zhou, M., Wang, L., Gao, X., Hua, G.: Ordinal regression with multi- ple output cnn for age estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4920-4928 (2016) 13, 28 49. Pan, H., Han, H., Shan, S., Chen, X.: Mean-variance loss for deep age estimation from a face. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5285-5294 (2018) 1, 4, 6, 29, 32</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>and NFS [30] datasets.</figDesc><table><row><cell>L</cell><cell>{σ l } L l=1</cell><cell>OP0.50 (%)</cell><cell>OP0.75 (%)</cell><cell>AUC (%)</cell></row><row><cell>1</cell><cell>{0.05}</cell><cell>75.77</cell><cell>45.72</cell><cell>63.37</cell></row><row><cell>2</cell><cell>{0.01, 0.1}</cell><cell>77.25</cell><cell>46.09</cell><cell>61.48</cell></row><row><cell>2</cell><cell>{0.03, 0.3}</cell><cell>79.27</cell><cell>48.59</cell><cell>63.65</cell></row><row><cell>2</cell><cell>{0.05, 0.5}</cell><cell>79.90</cell><cell>48.71</cell><cell>64.10</cell></row><row><cell>2</cell><cell>{0.07, 0.7}</cell><cell>78.41</cell><cell>47.72</cell><cell>62.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S4 .</head><label>S4</label><figDesc>Full results for the age estimation experiments. Gradient-based refinement using our proposed method consistently improves MAE (lower is better) for the age predictions outputted by a number of baselines.</figDesc><table><row><cell>Method</cell><cell>MAE</cell></row><row><cell>Niu et al. [48]</cell><cell>5.74 ± 0.05</cell></row><row><cell>Cao et al. [4]</cell><cell>5.47 ± 0.01</cell></row><row><cell>Direct -Huber</cell><cell>4.80 ± 0.06</cell></row><row><cell>Direct -Huber + Refinement</cell><cell>4.74 ± 0.06</cell></row><row><cell>Direct -L2</cell><cell>4.81 ± 0.02</cell></row><row><cell>Direct -L2 + Refinement</cell><cell>4.65 ± 0.02</cell></row><row><cell>Gaussian</cell><cell>4.79 ± 0.06</cell></row><row><cell>Gaussian + Refinement</cell><cell>4.66 ± 0.04</cell></row><row><cell>Laplace</cell><cell>4.85 ± 0.04</cell></row><row><cell>Laplace + Refinement</cell><cell>4.81 ± 0.04</cell></row><row><cell>Softmax -CE &amp; L2</cell><cell>4.78 ± 0.05</cell></row><row><cell>Softmax -CE &amp; L2 + Refinement</cell><cell>4.65 ± 0.04</cell></row><row><cell>Softmax -CE, L2 &amp; Var</cell><cell>4.81 ± 0.03</cell></row><row><cell cols="2">Softmax -CE, L2 &amp; Var + Refinement 4.69 ± 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>dataset. {σ l } 2</figDesc><table><row><cell>l=1</cell><cell>Average MAE</cell></row><row><cell>{0.1, 20}</cell><cell>6.96</cell></row><row><cell>{1, 20}</cell><cell>5.08</cell></row><row><cell>{1, 30}</cell><cell>5.24</cell></row><row><cell>{2, 20}</cell><cell>7.02</cell></row><row><cell>{1, 10}</cell><cell>7.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Prediction Algorithm</head><p>Our prediction procedure (Section 3.3) is detailed in Algorithm S1, where λ denotes the gradient ascent step-length, η is a decay of the step-length and T is the number of iterations. In our experiments, we fix T (typically, T = 10) and select {λ, η} using grid search on a validation set.</p><p>Algorithm S1 Prediction via gradient-based refinement.</p><p>Input: x ,ŷ, T , λ, η.</p><p>1: y ←ŷ. 2: for t = 1, . . . , T do 3:</p><p>PrevValue ← f θ (x , y). 4:ỹ ← y + λ∇yf θ (x , y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>NewValue ← f θ (x ,ỹ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if NewValue &gt; PrevValue then 7:</p><p>y ←ỹ. 8: else 9:</p><p>λ ← ηλ. 10: Return y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Illustrative Example</head><p>The ground truth conditional target density p(y|x) in <ref type="figure">Figure 2</ref> is defined by a mixture of two Gaussian components (with weights 0.2 and 0.8) for x &lt; 0, and a log-normal distribution (with µ = 0.0, σ = 0.25) for x ≥ 0. The training data</p><p>i=1 was generated by uniform random sampling of x, x i ∼ U (−3, 3). Both models were trained for 75 epochs with a batch size of 32 using the ADAM [31] optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Prediction</head><p>For this experiment, we also use the prediction procedure detailed in Algorithm S2. Again following IoU-Net, we set T = 5, Ω 1 = 0.001 and Ω 2 = −0.01. Based on the validation set, we select λ = 0.1. We refine a single estimateŷ, predicted by each baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Baselines</head><p>All baselines are trained for 75 epochs with a batch size of 32, using the ADAM optimizer with weight decay of 0.001. Identical data augmentation and parameter initialization as for our proposed model is used. Direct The DNN architecture of Direct first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two fully-connected layers (2048 → 2048, 2048 → 3), outputting the predictionŷ ∈ R 3 . It is trained by minimizing either the Huber or L 2 loss. Gaussian The Gaussian model is defined using a DNN f θ (x) according to,</p><p>It is trained by minimizing the negative log-likelihood, corresponding to the loss,</p><p>The DNN architecture of f θ (x) first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two heads of two fully-connected layers (2048 → 2048, 2048 → 3) to output µ θ (x) ∈ R 3 and log σ 2 θ (x) ∈ R 3 . The mean µ θ (x) is taken as the predictionŷ.</p><p>Laplace Following <ref type="bibr" target="#b16">[17]</ref>, the Laplace model is defined using a DNN f θ (x) according to,</p><p>It is trained by minimizing the negative log-likelihood, corresponding to the loss,</p><p>The DNN architecture of f θ (x) first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by two heads of two fully-connected layers (2048 → 2048, 2048 → 3) to output µ θ (x) ∈ R 3 and log β θ (x) ∈ R 3 . The mean µ θ (x) is taken as the predictionŷ. Softmax The DNN architecture of Softmax first extracts ResNet50 features g x ∈ R 2048 from the input image x. The feature vector g x is then processed by three heads of two fully-connected layers (2048 → 2048, 2048 → C), outputting logits for C = 151 discretized classes {−75, −74, . . . , 75} for the yaw, pitch and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Mixture density networks</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07884</idno>
		<title level="m">Rank-consistent ordinal regression for neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2D/3D image registration using regression learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mageras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PyTracking: Visual tracking library based on PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<idno>accessed: 30/04/2020 25</idno>
		<ptr target="https://github.com/visionml/pytracking" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random forests for real time 3d face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging heteroscedastic aleatoric uncertainties for robust real-time lidar 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning generative convnets via multi-grid modeling and sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep directional statistics: Pose estimation with uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prokudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixture dense regression for object detection and human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varamesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FSA-Net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SSR-Net: A compact soft stagewise regression network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<ptr target="https://susanqq.github.io/UTKFace/13" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Method</forename><surname>Yaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mae</forename><surname>Pitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mae</forename><surname>Roll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mae</forename><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Direct -Huber + Refine</surname></persName>
		</author>
		<idno>2.75 ± 0.08 3.70 ± 0.11 2.87 ± 0.09 3.11 ± 0.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaussian + Refine</surname></persName>
		</author>
		<idno>2.84 ± 0.08 3.67 ± 0.12 2.81 ± 0.08 3.11 ± 0.07</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laplace + Refine</surname></persName>
		</author>
		<idno>2.89 ± 0.07 3.81 ± 0.13 2.88 ± 0.06 3.19 ± 0.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ce &amp;amp; L2 +</forename><surname>Softmax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Refine</surname></persName>
		</author>
		<idno>2.67 ± 0.08 3.61 ± 0.12 2.75 ± 0.10 3.01 ± 0.07</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Softmax -Ce</surname></persName>
		</author>
		<idno>L2 &amp; Var 2.83 ± 0.12 3.79 ± 0.10 2.84 ± 0.11 3.15 ± 0.07</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ce</forename><surname>Softmax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Var + Refine</surname></persName>
		</author>
		<idno>2.76 ± 0</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
