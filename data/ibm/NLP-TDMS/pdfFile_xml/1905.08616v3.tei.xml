<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Depth Completion from Visual Inertial Odometry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Fei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Tsuei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Depth Completion from Visual Inertial Odometry</title>
					</analytic>
					<monogr>
						<title level="m">IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY</title>
						<imprint>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Visual Learning, Sensor Fusion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a method to infer dense depth from camera motion and sparse depth as estimated using a visualinertial odometry system. Unlike other scenarios using point clouds from lidar or structured light sensors, we have few hundreds to few thousand points, insufficient to inform the topology of the scene. Our method first constructs a piecewise planar scaffolding of the scene, and then uses it to infer dense depth using the image along with the sparse points. We use a predictive cross-modal criterion, akin to "self-supervision," measuring photometric consistency across time, forward-backward pose consistency, and geometric compatibility with the sparse point cloud. We also present the first visual-inertial + depth dataset, which we hope will foster additional exploration into combining the complementary strengths of visual and inertial sensors. To compare our method to prior work, we adopt the unsupervised KITTI depth completion benchmark, where we achieve state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A sequence of images is a rich source of information about both the three-dimensional (3D) shape of the environment and the motion of the sensor within. Motion can be inferred at most up to a scale and a global Euclidean reference frame, provided sufficient parallax and a number of visually discriminative Lambertian regions that are fixed in the environment and visible from the camera. The position of such regions in the scene defines the Euclidean reference frame, with respect to which motion is estimated. Scale, as well as two directions of orientation, can be further identified by fusion with inertial measurements (accelerometers and gyroscopes) and, if available, a magnetometer can fix the last (Gauge) degree of freedom.</p><p>Because the regions defining the reference frame have to be visually distinctive ("features"), they are typically sparse. In theory, three points are sufficient to define a Euclidean Gauge if visible at all times. In practice, because of occlusions, any Structure From Motion (SFM) or simultaneous localization and mapping (SLAM) system maintains an estimate of the location of a sparse set of features, or "sparse point cloud," typically in the hundreds to thousands. These are sufficient to support a point-estimate of motion, but a rather poor representation of shape as they do not reveal the topology of the scene: The empty space between points could be empty, or occupied by a solid with a smooth surface radiance (appearance). Attempts to densify the sparse point cloud, by interpolation or regularization with generic priors such as smoothness, piecewise planarity and the like, typically fail since SFM yields far too sparse a reconstruction to inform topology. This is where the image comes back in.</p><p>Inferring shape is ill-posed, even if the point cloud was generated with a lidar or structured light sensor. Filling the gaps relies on assumptions about the environment. Rather than designing ad-hoc priors, we wish to use the image to inform and restrict the set of possible scenes that are compatible with the given sparse points.</p><p>Summary of contributions: We use a predictive cross-modal criterion to score dense depth from images and sparse depth. This kind of approach is sometimes referred to as "selfsupervised." Specifically, our method (i) exploits a set of constraints from temporal consistency (a.k.a. photometric consistency across temporally adjacent frames) to pose (forwardbackward) consistency in a combination that has not been previously explored. To enable our pose consistency term, we introduce (ii) a set of logarithmic and exponential mapping layers for our network to represent motion using exponential arXiv:1905.08616v3 [cs.CV] 11 Feb 2020 coordinates, which we found to improve reconstruction compared to other parametrizations.</p><p>The challenge in using sparse depth as a supervisory (feedback) signal is precisely that it is sparse. Information at the points does not propagate to fill the domain where depth is defined. Some computational mechanism to "diffuse the information" from the sparse points to their neighbors is needed. Our approach proposes (iii) a simple method akin to using a piecewise planar "scaffolding" of the scene, sufficient to transfer the supervisory signal from sparse points to their neighbors. This yields a two-stage approach, where the sparse points are first processed to design the scaffolding ("meshing and interpolation") and then "refined" using the images as well as priors from the constraints just described.</p><p>One additional contribution of our approach is (iv) to introduce the first visual-inertial + depth dataset. The role of inertials is to enable reconstruction in metric scale, which is critical for robotic applications. Although scale can be obtained via other sensors, e.g., stereo, lidar, and RGB-D, we note they are not as widely available as monocular cameras with inertials (almost every modern phone has it) and consume more power. Since inertial sensors are now ubiquitous and typically co-located with cameras in many mobile devices from phones to cars, we hope this dataset will foster additional exploration into combining the complementary strengths of visual and inertial sensors.</p><p>To evaluate our method, since no other visual-inertial + depth benchmark is available, and to facilitate comparison with similar methods, we adopt the KITTI benchmark, where a Velodyne (lidar) sensor provides sparse points with scale, unlike monocular SFM, but like visual-inertial odometry (VIO). Although the biases in lidar are different from VIO, this can be considered a baseline. Note that we only use the monocular stream of KITTI (not stereo) for fair comparison.</p><p>The result is a (v) two-stage approach of scaffolding and refining with a network that contains much fewer parameters than competing methods, yet achieves state-of-the-art performance in the "unsupervised" KITTI benchmark (a misnomer). The supervision in the KITTI benchmark is really fusion from separate sensory channels, combined with ad-hoc interpolation and extrapolation. It is unclear whether the benefit from having such data is outweighed by the biases it induces on the estimate, and in any case such supervision does not scale; hence, we forgo (pseudo) ground truth annotations altogether.</p><p>II. RELATED WORK Supervised Depth Completion minimizes the discrepancy between ground truth depth and depth predicted from an RGB image and sparse depth measurements. Methods focus on network topology <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, optimization <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and modeling <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. To handle sparse depth, <ref type="bibr" target="#b0">[1]</ref> employed early fusion, where the image and sparse depth are convolved separately and the results concatenated as the input to a ResNet encoder. <ref type="bibr" target="#b8">[9]</ref> proposed late fusion via a U-net containing two NASNet encoders for image and sparse depth and jointly learned depth and semantic segmentation, whereas <ref type="bibr" target="#b2">[3]</ref> used ResNet encoders for late fusion. <ref type="bibr" target="#b6">[7]</ref> proposed a normalized convolutional layer to propagate sparse depth and used a binary validity map as a confidence measure. <ref type="bibr" target="#b7">[8]</ref> proposed an upsampling layer and joint concatenation and convolution to deal with sparse inputs. All these methods require per-pixel ground-truth annotation. What is called "ground truth" in the benchmarks is actually the result of data processing and aggregation of many consecutive frames. We skip such supervision and just infer dense depth by learning the cross-modal fusion from the virtually infinite volume of un-annotated data. Unsupervised Depth Completion methods, such as <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref> predict depth by minimizing the discrepancy between prediction and sparse depth input as well as the photometric error between the input image and its reconstruction from other viewpoints available only during training. <ref type="bibr" target="#b0">[1]</ref> used Perspective-n-Point (PnP) <ref type="bibr" target="#b11">[11]</ref> and Random Sample Consensus (RANSAC) <ref type="bibr" target="#b12">[12]</ref> to align monocular image sequences for their photometric term with a second-order smoothness prior. Yet, <ref type="bibr" target="#b0">[1]</ref> does not generalize well to indoor scenes that contains many textureless regions (e.g. walls), where PnP with RANSAC may fail. <ref type="bibr" target="#b9">[10]</ref> used a local smoothness term, but instead minimized the photometric error between rectified stereo-pairs where pose is known. <ref type="bibr" target="#b2">[3]</ref> also leveraged stereo pairs and a more sophisticated photometric loss <ref type="bibr" target="#b13">[13]</ref>. <ref type="bibr" target="#b2">[3]</ref> replaced the generic smoothness term with a learned prior to regularize their prediction. To accomplish this, <ref type="bibr" target="#b2">[3]</ref> requires a conditional prior network (CPN) that is trained on an additional dataset (representative of the depth completion dataset) in a fully-supervised manner using ground-truth depth. The CPN does not generalize well outside its training domain (e.g. one cannot use a CPN trained on outdoors scenes to regularize depth predictions for indoors). Hence, <ref type="bibr" target="#b2">[3]</ref> is essentially not unsupervised and has limited applicability. In contrast, our method is trained on monocular sequences, is fully unsupervised and does not use any auxiliary groundtruth supervision. Unlike previous works, our method does not require large networks ( <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref>) nor any complex network operations ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>). Moreover, our method outperforms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> on the unsupervised KITTI depth completion benchmark <ref type="bibr" target="#b1">[2]</ref> while using fewer parameters. Rotation Parameterization. To construct the photometric consistency loss during training, an auxiliary pose network is needed if no camera poses are available. While the translational part of the relative pose can be modeled as T ∈ R 3 , the rotational part belongs to the special orthogonal group</p><formula xml:id="formula_0">R ∈ SO(3) . = {R ∈ R 3×3 |R R = I, det(R) = +1} [14]</formula><p>, which is represented by a 3 × 3 matrix. <ref type="bibr" target="#b15">[15]</ref> uses quaternions, which require an additional norm constraint; this is a soft constraint imposed in the loss function, and thus is not guaranteed. <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref> use Euler angles which may result in a non-orthogonal rotation matrix due to rounding error from the multiplication of many sine and cosine terms. We use the exponential map on SO(3) to map the output of the pose network to a rotation matrix. Though theoretically similar, we empirically found that the exponential map is more beneficial than the Euler angles in Sec. VII.</p><p>Our contributions are a simple, yet effective two-stage approach resulting in a large reduction in network parameters while achieving state-of-the-art performance on the unsupervised KITTI depth completion benchmark; using exponential (best viewed in color at 5×). We first build a scaffolding z i from sparse depth zs estimated by VIO. Then together with the image It, z i is fed to the refinement network as input to produce output z. Note: the pose network (blue) is only needed in one operation mode and is only used in training. In the other operation mode, VIO poses are used instead. The scaffolding module (red) is parameter-free -leading to our light-weight two-stage approach. parameterization of rotation for our pose network; a pose consistency term that enforces forward and backward motion to be the inverse of each other; and finally a new depth completion benchmark for visual-inertial odometry systems with indoor and outdoor scenes and challenging motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD FORMULATION</head><p>We reconstruct a 3D scene given an RGB image I t : R 2 ⊃ Ω → R 3 + and the associated set of sparse depth measurements z s : Ω ⊃ Ω s → R + .</p><p>We begin by assuming that world surfaces are graphs of smooth functions (charts) locally supported on a piecewise planar domain (scaffolding). We construct the scaffolding from the sparse point cloud ("Scaffolding" in <ref type="figure">Fig. 3</ref>) to obtain z i : Ω → R + , then learn a completion model refining z i by leveraging the monocular sequences (I t−1 , I t , I t+1 ), of frames before and after the given time t, and the sparse depth z s . We compose a surrogate loss L (Eqn. 2) for driving the training process, using an encoder-decoder architecture f θ (·) parameterized by weights θ, where the input is an image with its scaffolding (I t , z i ), and the output is the dense deptĥ z = f θ (I t , z i ). <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Two-Stage Approach</head><p>Depth completion is a challenging problem due to the sparsity level of the depth input, z s . As the density of sparse depth measurements covers ≈ 5% of the image plane for the outdoor self-driving scenario (Sec. V-A) and less than ≈ 1% for the indoor setting (Sec. VII-C), generally only a single measurement will be present within a local neighborhood and in most instances none. This renders conventional convolutions ineffective as each sparse depth measurement can be seen as a Dirac delta and convolving a kernel over the entire sparse depth input will give mostly zero activations. Hence, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b1">[2]</ref> proposed specialized operations to propagate the information from the sparse depth input through the network. We, instead, propose a two-stage approach that circumvents this problem by first approximating a coarse scene geometry with scaffolding and training a network to refine the approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scaffolding</head><p>Given sparse depth measurements z s , our goal is to create a coarse approximation of the scene; yet, the topology of the scene is not informed by z s . Hence, we must rely on a prior or an assumption -that surfaces are locally smooth and piecewise planar. We begin by applying the lifting transform <ref type="bibr" target="#b19">[19]</ref> to z s , mapping z s from 2-d to 3-d space. We then compute its convex hull <ref type="bibr" target="#b20">[20]</ref>, of which the lower envelope is taken as the Delaunay triangulation of the points in z s -resulting in a triangular mesh in Barycentric coordinates.</p><p>To form the tessellation of the triangular mesh, we approximate each surface using linear interpolation within the Barycentric coordinates and the resulting scaffolding is projected back onto the image plane to produce z i . For a given triangle, simple interpolation is sufficient for recovering the plane as a linear combination of the co-planar points. For sets of points not co-planar, interpolation will give an approximation, with which we refine using a network. We note that our approximation cannot be achieved by simply filtering (e.g. Gaussian) z s to propagate depth values as the filter would produce mostly zeros and even destroy the sparse depth signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Refinement</head><p>Given an RGB image and its corresponding piece-wise planar scaffolding (I t , z i ), we train a network to recover the 3-d scene by refining z i based on information from I t . Our network learns to refine without ground-truth supervision by minimizing Eqn. 2 (see Sec. IV). Network Architecture. We propose two encoder-decoder architectures with skip connections following the late fusion paradigm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Each encoder has an image branch and a depth branch, where each contains 75% and 25% of the total encoder parameters, respectively. The latent representation of the branches are concatenated and fed to the decoder. We propose a VGG11 encoder (≈ 5.7M parameters) containing 8 convolution layers for each branch as our best performing model, and a VGG8 encoder (≈ 2.4M parameters) containing only 5 convolution layers for each branch as our light-weight model. This is in contrast to other unsupervised methods <ref type="bibr" target="#b0">[1]</ref> (early fusion) and <ref type="bibr" target="#b2">[3]</ref> (late fusion) -both of which use ResNet34 encoders with ≈ 23.8M and ≈ 14.8M parameters, respectively. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and our approach share the same decoder architecture containing ≈ 4M parameters. We show in Sec. VII that despite having 76.1% and 61.5% fewer encoder parameters than <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>, our VGG11 model outperforms both <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>. Moreover, performance does not degrade by much from VGG11 to VGG8 and VGG8 still surpasses <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref> while having a 89.9% and 83.9% reduction in the encoder parameters. Unlike <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which requires high energy consumption hardware, our approach is computationally cheap, and can be deployed to low-powered agents using an Nvidia Jetson. Logarithmic and Exponential Map Layers. To construct our objective (Eqn. 2), we leverage a pose network <ref type="bibr" target="#b15">[15]</ref> to regress the relative camera poses g = (R, T ) ∈ SE <ref type="bibr" target="#b2">(3)</ref> .</p><formula xml:id="formula_1">= {(R, T )|R ∈ SO(3), T ∈ R 3 }.</formula><p>We present a novel logarithmic map layer: log : SO(3) → so(3), where so <ref type="formula" target="#formula_4">(3)</ref> is the tangent space of SO(3), and an exponential map layer: exp : so(3) → SO(3) -for mapping R between SO(3) and so(3). We use the logarithmic map to construct the pose consistency loss (Eqn. 6), and the exponential to map the output of the pose Image Scaffolding After 1 Epoch After 12 Epochs After 26 Epochs <ref type="figure">Fig. 3</ref>. Learning to refine (best viewed at 5× with color). Our network learns to refine the input scaffolding. Green rectangles highlight the regions for comparison throughout the course of training. The network first learns to copy the input and later learns to fuse information from RGB image to refine the approximated depth from scaffolding (see row 1 pedestrian and row 2 street signs).</p><p>network ω . = [ω 1 , ω 2 , ω 3 ] ∈ R 3 as coordinates in so(3) to a rotation matrix:</p><formula xml:id="formula_2">R(ω) = exp(ω) . = I +ω sin ω 2 +ω 2 (1 − cos ω 2 ) (1)</formula><p>where the hat operator· maps ω ∈ R 3 to a skew-symmetric matrix <ref type="bibr" target="#b14">[14]</ref>. We train of our pose network using a surrogate loss (Eqn. 3) without explicit supervision. Ablation studies on the use of exponential coordinates and pose consistency for depth completion can be found in <ref type="table" target="#tab_1">Table III</ref> and IV. Our approach contains two stages: (i) we generate a coarse piecewise planar approximation of the scene from the sparse depth inputs z s via scaffolding and (ii) we feed the resulting depth map along with the associated RGB image to our network for refinement ( <ref type="figure">Fig. 3</ref>). This approach alleviates the network from the need of learning from sparse inputs, for which <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref> compensated with parameters. We show the effectiveness of this approach by achieving the state-of-the-art on the unsupervised KITTI depth completion benchmark with half as many parameters as the prior-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LOSS FUNCTION</head><p>Our loss function is a linear combination of four terms that constrain (i) the photometric consistency between the observed image and its reconstructions from the monocular sequence, (ii) the predicted depth to be similar to that of the associated available sparse depth, (iii) the composition of the predicted forward and backward relative poses to be the identity, and (iv) the prediction to adhere to local smoothness.</p><formula xml:id="formula_3">L = w ph L ph + w sz L sz + w pc L pc + w sm L sm<label>(2)</label></formula><p>where L ph denotes photometric consistency, L sz sparse depth consistency, L pc pose consistency, and L sm local smoothness. Each loss term L is described in the next subsections and the associated weight w in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Photometric Consistency</head><p>We enforce temporal consistency by minimizing the discrepancy between each observed image I t and its reconstruction I τ from temporally adjacent images</p><formula xml:id="formula_4">I τ , where τ ∈ T . = {t − 1, t + 1}:Î τ (x) = I τ πg τ t K −1xẑ (x)<label>(3)</label></formula><formula xml:id="formula_5">wherex = [x T 1] T are the homogeneous coordinates of x ∈ Ω , g τ t ∈ SE(3)</formula><p>is the relative pose of the camera from time t to τ , K denotes the camera intrinsics, and π refers to the perspective projection. Our photometric consistency term is a combination of the average per pixel reprojection residual with an L 1 penalty and SSIM <ref type="bibr" target="#b13">[13]</ref>, a perceptual metric that is invariant to local illumination changes:</p><formula xml:id="formula_6">L ph = 1 |Ω| τ ∈T x∈Ω w co |I t (x) −Î τ (x)|+ w st 1 − SSIM(I t (x),Î τ (x))<label>(4)</label></formula><p>We use 3 × 3 image patches centered at location x for SSIM. w co and w st can be found in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse Depth Consistency</head><p>Our sparse depth consistency term provides our predictions with metric scale by encouraging the predictionsẑ to be similar to that of the metric sparse depth z s available from lidar in KITTI dataset (Sec. V-A) and sparse reconstruction in our visual-inertial dataset (Sec. V-B). Our sparse depth consistency loss is the L 1 -norm of the difference between the predicted depthẑ and the sparse depth z s averaged over Ω s (the support of the sparse depth):</p><formula xml:id="formula_7">L sz = 1 |Ω s | x∈Ωs |ẑ(x) − z s (x)|<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pose Consistency</head><p>A pose network takes an ordered pair of images (I t , I τ ) and outputs the relative pose g τ t ∈ SE(3) (forward pose). When a temporally swapped pair (I τ , I t ) is fed to the network, the network is expected to output g tτ (backward pose) -the inverse of g τ t , i.e., g τ t · g tτ = e ∈ SE(3). The forwardbackward pose consistency thus penalizes the deviation of the composed pose from the identity:</p><formula xml:id="formula_8">L pc = log(g τ t · g tτ ) 2 2 (6) where log : SE(3) → se(3) is the logarithmic map.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Local Smoothness</head><p>We impose a smoothness loss on the predicted depthẑ by applying an L 1 penalty to the gradients in both the x and y directions of the predicted depthẑ:</p><formula xml:id="formula_9">L sm = 1 |Ω| x∈Ω λ X (x)|∂ Xẑ (x)| + λ Y (x)|∂ Yẑ (x)|<label>(7)</label></formula><p>where λ X = e −|∂ X It(x)| and λ Y = e −|∂ Y It(x)| are the edge-awareness weights to allow for discontinuities in regions corresponding to object boundaries.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The Role of Inertials</head><p>Although inertials are not directly present in the loss, their role in metric depth completion is crucial. Without inertials, a SLAM system cannot produce sparse point clouds in metric scale, which are then used as both the input to the scaffolding stage (Sec. III-B) and a supervisory signal (Eqn. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASETS A. KITTI Benchmark</head><p>We evaluate our approach on the KITTI depth completion benchmark <ref type="bibr" target="#b1">[2]</ref>. The dataset provides ≈ 80, 000 raw image frames and associated sparse depth maps. The sparse depth maps are the raw output from the Velodyne lidar sensor, each with a density of ≈ 5%. The ground-truth depth map is created by accumulating the neighbouring 11 raw lidar scans, with dense depth corresponding to the bottom 30% of the images. We use the officially selected 1,000 samples for validation and we apply our method to 1,000 testing samples, with which we submit to the official KITTI website for evaluation. The results are reported in <ref type="table" target="#tab_1">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VOID Benchmark</head><p>While KITTI provides a standard benchmark for evaluating depth completion in the driving scenario, there exists no standard depth completion benchmark for the indoor scenario. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> used NYUv2 <ref type="bibr" target="#b21">[21]</ref> -an RGB-D dataset -to develop and evaluate their models on indoor scenes. Yet, each performs a different evaluation protocol with different sparse depth samples -varying densities of depth values were randomly sampled from the depth frame, preventing direct comparisons between methods. Though this is reasonable as a proof of concept, it is not realistic in the sense that no sensor measures depth at random locations. The VOID dataset. We propose a new publicly available dataset for a real world use case of depth completion by bootstrapping sparse reconstruction in metric space from a SLAM system. While it is well known that metric scale is not observable in the purely image-based SLAM and SFM setting, it has been resolved by the recent advances in VIO <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, where metric pose and structure estimation can be realized in a gravity-aligned and scaled reference frame using a inertial measurement unit (IMU). To this end, we leverage an off-theshelf VIO system 1 , atop which we construct our dataset and 1 https://github.com/ucla-vision/xivo develop our depth completion model. While there are some visual-inertial datasets (e.g. TUM-VI <ref type="bibr" target="#b24">[24]</ref> and PennCOSYVIO <ref type="bibr" target="#b25">[25]</ref>), they lack per-frame dense depth measurements for crossmodal validation, and are also relatively small -rendering them unsuitable for training deep learning models. To demonstrate the applicability of our approach, we additionally show qualitative results on the TUM-VI dataset in <ref type="figure">Fig. 5</ref> using sparse depth density level of 0.015%.</p><p>Our dataset is dubbed "Visual Odometry with Inertial and Depth" or "VOID" for short and is comprised of RGB video streams and inertial measurements for metric reconstruction along with per-frame dense depth for cross-modal validation. Data acquisition. Our data was collected using the latest Intel RealSense D435i camera 2 , which was configured to produce synchronized accelerometer and gyroscope measurements at 400 Hz, along with synchronized VGA-size (640 × 480) RGB and depth streams at 30 Hz. The depth frames are acquired using active stereo and is aligned to the RGB frame using the sensor factory calibration (see <ref type="figure">Fig. 8</ref>). All the measurements are time-stamped.</p><p>The SLAM system we use is based on <ref type="bibr" target="#b22">[22]</ref> -an EKFbased VIO model. While the VIO recursively estimates a joint posterior of the state of the sensor platform (e.g. pose, velocity, sensor biases, and camera-to-IMU alignment) and a small set of reliable feature points, the 3D structure it estimates is extremely sparse -typically 20 ∼ 30 feature points (in-state features). To facilitate 3D reconstruction, we track a moderate amount of out-of-state features in addition to the in-state ones, and estimate the depth of the feature points using auxiliary depth sub-filters <ref type="bibr" target="#b14">[14]</ref>. The benchmark. We evaluate our method on the VOID depth completion benchmark, which contains 56 sequences in total, both indoor and outdoor with challenging motion. Typical scenes include classrooms, offices, stairwells, laboratories, and gardens. Of the 56 sequences, 48 sequences (∼ 40K frames) are designated for training and 8 sequences for testing, from which we sampled 800 frames to construct the testing set. Our benchmark provides sparse depth maps at three density levels. We configured our SLAM system to track and estimate depth of 1500, 500 and 150 feature points, corresponding to 0.5%, 0.15% and 0.05% density of VGA size, which are then used in the depth completion task. </p><formula xml:id="formula_10">x∈Ω |ẑ(x) − zgt(x)| RMSE mm 1 |Ω| x∈Ω |ẑ(x) − zgt(x)| 2 1/2 iMAE 1/km 1 |Ω| x∈Ω |1/ẑ(x) − 1/zgt(x)| iRMSE 1/km 1 |Ω| x∈Ω |1/ẑ(x) − 1/zgt(x)| 2 1/2</formula><p>Error metrics for evaluating KITTI and VOID depth completion benchmarks, where zgt is the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. IMPLEMENTATION DETAILS</head><p>Our approach was implemented using TensorFlow <ref type="bibr" target="#b26">[26]</ref>. With a Nvidia GTX 1080Ti, training takes ≈ 42 hours for our VGG11 model and ≈ 34 hours for our VGG8 model on KITTI depth completion benchmark (Sec. V-A) for 30 epochs; whereas training takes ≈ 10 hours and ≈ 7 hours on the VOID benchmark (Sec. V-B) for 10 epochs. Inference takes ≈ 22 ms per image. We used Adam <ref type="bibr" target="#b27">[27]</ref> with β 1 = 0.9 and β 2 = 0.999 to optimize our network end-to-end with a base learning rates of 1.2×10 −4 for KITTI and 1×10 −4 for VOID . We decrease the learning rate by half after 18 epochs for KITTI and 6 epochs for VOID , and again after 24 epochs and 8 epochs, respectively. We train our network with a batch size of 8 using a 768 × 320 resolution for KITTI and 640 × 480 for VOID . We are able to achieve our results on the KITTI benchmark using the following set of weights for each term in our loss function: w ph = 1.00, w co = 0.20, w st = 0.40, w sz = 0.20, w pc = 0.10 and w sm = 0.01. For the VOID benchmark, we increased w sz to 1.00 and w sm to 0.10. We do not use any data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. KITTI Depth Completion Benchmark</head><p>We show quantitative and qualitative comparisons on the unsupervised KITTI depth completion benchmark in <ref type="table" target="#tab_1">Table II</ref> and <ref type="figure">Fig. 4</ref>, respectively. The results of the methods listed are taken directly from their papers. We note that <ref type="bibr" target="#b2">[3]</ref> only reported their result in their paper and do have have an entry in KITTI depth completion benchmark for their unsupervised model. Hence, we compare qualitatively with the prior-art <ref type="bibr" target="#b0">[1]</ref>. Our VGG11 model outperforms the state-of-the-art [3] on every metric by as much as 12.8% while using 48.4% fewer parameters. Our light-weight VGG8 model also outperforms <ref type="bibr" target="#b2">[3]</ref> on MAE, RMSE, and iMAE while <ref type="bibr" target="#b2">[3]</ref> beat our VGG8 by 2.2% on iRMSE. We note that <ref type="bibr" target="#b2">[3]</ref> trains a separate network, using ground truth, to supervise their depth completion model. Moreover, <ref type="bibr" target="#b2">[3]</ref> exploits rectified stereo-imagery where the pose of the cameras is known; whereas, we learn our pose by jointly training the pose network with our depth predictor. In comparison to <ref type="bibr" target="#b0">[1]</ref> (who also uses monocular videos), both our VGG11 and VGG8 model outperforms them on every metric while using much fewer paramters. We also note that the qualitative results of <ref type="bibr" target="#b0">[1]</ref> contains artifacts such as apparent scanlines of the Velodyne and "circles" in far regions.</p><p>As an introspective exercise, we plot the mean error of our model at varying distances on the KITTI validation set ( <ref type="figure" target="#fig_3">Fig. 6</ref>) and overlay it with the ground truth depth distribution to show We compare our model to unsupervised methods on the KITTI depth completion benchmark <ref type="bibr" target="#b1">[2]</ref>. Number of parameters used by each are listed for comparison. <ref type="bibr" target="#b28">[28]</ref> stated that they use a fully convolution network, but does not specify the full architecture. Our VGG11 model outperforms state-of-theart <ref type="bibr" target="#b2">[3]</ref> across all metrics. Despite reducing ≈ 3.3M parameters, our VGG8 model does not degrade by much and outperforms VGG11 marginally on the RMSE metric. Moreover, our VGG8 model also outperforms <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>. We compare variants of our model on the KITTI depth completion validation set. Each model is denoted by its loss function. Regions with missing depth in Scaffolding Only is assigned average depth. It is clear that scaffolding alone (row 1) and our baseline model trained without scaffolding (row 2) do poorly compared to our models that combine both (rows 3-6). Our full model using VGG11 produces the best overall results and achieves state-of-the-art on the test set <ref type="table" target="#tab_1">Table II</ref>. Our approach is robust, our lightweight VGG8 model achieves similar performance to our VGG11 model. that our model performs very well in distances that matter in real-life scenarios. Our performance begins to degrade at distances larger than 80 meters; this is due to the lack of sparse measurements and insufficient parallax -problems that plague methods relying on multi-view supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KITTI Depth Completion Ablation Study</head><p>We analyze the effect brought by each of our contributions through a quantitative evaluation on the KITTI depth completion validation set <ref type="table" target="#tab_1">(Table III)</ref>. Our two baseline models, scaffolding and vanilla model trained without scaffolding, perform poorly in comparison to the models that are trained with scaffolding -showcasing the effectiveness of our refinement approach. Although the loss functions are identical, exponential parameterization consistently improves over Euler angles across all metrics. We believe this is due to the regularity of the derivatives of the exponential map <ref type="bibr" target="#b29">[29]</ref> compared to other parameterizations -resulting in faster convergence and wider minima during training. While <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b17">[17]</ref> train their pose network using the photometric error with no additional constraint, we show that it is beneficial to impose our pose consistency term (Sec. 6). By constraining the forward and backward poses to be inverse of each other, we obtain a more accurate pose resulting in better depth prediction. Our experiments verify this claim as we see an improvement in across all metrics in <ref type="table" target="#tab_1">Table III</ref>. We note that the improvement does not seem significant on KITTI as the motion is mostly planar; however, when predicting non-trivial 6 DoF motion (Sec. VII-D), we see a significant boost when employing this term. Our model trained with the full loss function produces <ref type="figure">Fig. 5</ref>. Qualitative results on TUM-VI (best viewed in color at 2×). We apply our method to TUM-VI and obtained our results using sparse depth input at a density level of 0.015%. Unlike KITTI and VOID, TUM-VI images are monochrome, and bear a highly distorted fisheye camera model, which was compensated in training. Color bar shows the depth range. the best results (bolded in <ref type="table" target="#tab_1">Table II</ref>) and is the state-of-theart for unsupervised KITTI depth completion benchmark. We further propose a VGG8 model that only contains ≈ 6.4M parameters. Despite having 34% fewer paramters than VGG11, the performance of VGG8 does not degrade by much (see <ref type="table" target="#tab_1">Table II</ref>, III, V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. VOID Depth Completion Benchmark</head><p>We evaluate our method on the VOID depth completion benchmark for all three density levels <ref type="table" target="#tab_5">(Table V)</ref> using error metrics in <ref type="table" target="#tab_1">Table I</ref>. As the photometric loss (Eqn. 4) is largely dependent on obtaining the correct pose, we additionally propose a hybrid model, where the relative camera poses from our visual-inertial SLAM system are used to construct the photometric loss to show an upper bound on performance. In contrast to the KITTI, which provides ≈ 5% sparse depth density concentrated on the bottom 30% of the image, the VOID benchmark only provides ≈ 0.5%, ≈ 0.15% and ≈ 0.05% densities in sparse depth. Yet, our method is still able to produce reasonable results for indoor scenes with a MAE of ≈ 8.5 cm on 0.5% density and ≈ 17.9 cm when given only 0.05%. Since most scenes contain textureless regions, sparse depth supervision becomes important as photometric reconstruction is unreliable. Hence, performance degrades as density decreases. Yet, we degrade gracefully: as density decreases by 10X, our error only doubles. We note that the scaffolding may poorly represent the scene. In the worst case, where it provides no extra information, our method becomes the common depth completion approach. Also, we observe systematic performance improvement in all the evaluation metrics <ref type="table" target="#tab_5">(Table V)</ref> when replacing the pose network with SLAM pose. This can be largely attributed to the necessity for the correct pose to minimize photometric error during training. Our pose network may not be able to consistently predict the correct pose due to the challenging motion of the dataset. <ref type="figure" target="#fig_4">Fig. 7</ref> shows two sample RGB images with the densified depth images back-projected to 3D, colored, and viewed from a different vantage point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. VOID Depth Completion Ablation Study</head><p>To better understand the effect of rotation parameterization and our pose consistency loss (Eqn. 6) on the depth completion task, we compare variants of our model and again replace the pose network with SLAM pose to show an upper-bound on performance. Although exponential outperforms Euler parameterization, we note that both perform much worse than using SLAM pose. However, we observe a performance boost when applying our pose consistency term and our model improves over exponential without pose consistency by as much as 23.4%. Moreover, it approaches the performance of our model trained using SLAM pose. This trend still holds when density decreases <ref type="table" target="#tab_5">(Table V)</ref>. This suggests that despite the additional constraint, the pose network still has some difficulties predicting the pose due to the challenging motion. This finding, along with results from Table V, highlights the strength of classical SLAM systems in the deep learning era, which also urges us to develop and test pose networks on the VOID dataset which features non-trivial 6 DoF motion -much more challenging than the mostly-planar motion in KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION</head><p>While deep networks have attracted a lot of attention as a general framework to solve an array of problems, we must note that pose may be difficult to learn on datasets with non-trivial 6 DoF motion -which the SLAM community has studied for decades. We hope that VOID will serve as a platform to develop models that can handle challenging motion and further foster fusion of multi-sensory data. Furthermore, we show that a network can recover the scene geometry from extremely We compare the variants of our pose network. SLAM Pose replaces the output of pose network with SLAM estimated pose to gauge an upper bound in performance. When using our pose consistency term with exponential parameterization, our method approaches the performance of our method when using SLAM pose. Note: we trained <ref type="bibr" target="#b0">[1]</ref> from scratch using groundtruth pose and adapted <ref type="bibr" target="#b26">[26]</ref> to train on monocular sequences. The conditional prior network used in <ref type="bibr" target="#b2">[3]</ref> is trained on ground truth from NYUv2 <ref type="bibr" target="#b21">[21]</ref>. The VOID dataset contains VGA size images (480 × 640) of both indoor and outdoor scenes with challenging motion. For "Pose From", SLAM refers to relative poses estimated by a SLAM system, and PoseNet refers to relative poses predicted by a pose network. sparse point clouds (e.g. features tracked by SLAM). We also show that improvements can be obtained by leveraging pose from a SLAM system instead of a pose network. These findings motivate a possible mutually beneficial marriage between classical methods and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A VOID DATASET</head><p>In the main paper, we introduced the "Visual Odometry with Inertial and Depth" (VOID) dataset with which we propose a new depth completion benchmark. We described the data acquisition process, benchmark setup, and evaluation protocols in Sec. V-B and Sec. VII-C. To give some flavor of the VOID dataset, <ref type="figure">Fig. 9</ref> shows a set of images (top inset) sampled from video sequences in VOID, and output of our visual-inertial odometry (VIO) system (bottom), where the blue pointcloud is the sparse reconstruction of the underlying scene and the yellow trace is the estimated camera trajectory.</p><p>Two rows of chairs in a classroom "L" shape formed by desks in a mechanical laboratory a brick wall with plants on the ground underneath stairs <ref type="figure">Fig. 9</ref>. Sample sequences in VOID dataset (best viewed in color at 5×). In each panel, the top inset shows 4 sample images of a video sequence in our VOID dataset; the bottom shows the sparse pointcloud reconstruction (blue) and camera trajectory (yellow) from our VIO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B MORE RESULTS ON VOID DATASET</head><p>In the main paper, we evaluated our approach on the VOID depth completion benchmark in Sec. VII-C, and Sec. VII-D provided quantitative results in <ref type="table" target="#tab_5">Table V</ref> and IV and qualitative results in <ref type="figure" target="#fig_4">Fig. 7</ref>. Here, we provide additional qualitative results in <ref type="figure" target="#fig_0">Fig. 10</ref> to show how our approach performs on a variety of scenes -both indoor and outdoor -from the VOID dataset. The figure is arranged in two panels of 3 × 2 grids, where each panel contains a sample RGB image (left) that is fed to our depth completion network as input, and the corresponding colored pointcloud (right) produced by our approach, viewed at a different vantage point. The pointclouds are obtained by back-projecting the color pixels to the estimated depth. We used an input sparse depth density level of ≈ 0.5% to produce the results. Our approach can provide detailed reconstructions of scenes from both indoor (e.g. right panel, last row: equipment from mechanical lab) and outdoor settings (e.g. left panel: flowers and leaves of plants in garden). It is also able to recover small objects such as the mouse on the desk in the mechanical lab, and structures at very close range (e.g. left panel, last row: staircase located less than half a meter from the camera). <ref type="figure" target="#fig_0">Fig. 10</ref>. Qualitative results on VOID dataset. In each panel, the left shows a sample RGB image fed to our depth completion network as input; the right shows the completed depth map back-projected to 3D, colored, and viewed from a different vantage point. Our method recovers the scene structure with details at various ranges in both indoor and outdoor settings. We perform an ablation study on our pose representation by jointly training our depth completion network and pose network on KITTI depth completion dataset and testing only the pose network on KITTI Odometry sequence 09 and 10. We evaluate the performance of each pose network using metrics described in Sec. C-A. While performance of exponential parameterization and Euler angles are similar on ATE-5F, and RPE, exponential outperforms Euler angles in ATE and RRE on both sequences. Our model using exponential with pose consistency performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C POSE ABLATION STUDY</head><p>In the main paper, we focus on the depth completion task and hence we evaluate the effects of different pose parameterizations and our pose consistency term by computing error metrics relevant to the recovery of the 3D scene on both the VOID and KITTI depth completion benchmarks. Here, we focus specifically on pose by directly evaluating the pose network on the KITTI odometry dataset in <ref type="table" target="#tab_1">Table VI</ref>. We show qualitative results on the trajectory obtained by chaining pairwise camera poses estimated by each pose network in <ref type="figure" target="#fig_0">Fig. 11</ref> and provide an analysis of the results in Sec. C-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose Evaluation Metrics</head><p>To evaluate the performance of the pose network and its variants, we adopt two most widely used metrics in evaluating simultaneous localization and mapping (SLAM) systems: absolute trajectory error (ATE) and relative pose error (RPE) <ref type="bibr" target="#b31">[31]</ref> along with two novel metrics tailored to the evaluation of pose networks.</p><p>Given a list of estimated camera posesĝ T . = {ĝ 1 ,ĝ 2 , · · · ,ĝ T }, whereĝ t ∈ SE(3), relative to a fixed world frame, and the list of corresponding ground truth poses g T .</p><p>= {g 1 , g 2 , · · · , g T }, where g t ∈ SE(3), ATE reads</p><formula xml:id="formula_11">ATE(ĝ T , g T ) = 1 T T t=1 trans(g −1 tĝt ) 2 2<label>(8)</label></formula><p>where the function trans : SE(3) → R 3 extracts the translational part of a rigid body transformation. ATE is essentially the root mean square error (RMSE) of the translational part of the estimated pose over all time indices. <ref type="bibr" target="#b18">[18]</ref> proposed a "5-frame" version of ATE (ATE-5F) -the root mean square of ATE of a 5-frame sliding window over all time indices, which we also incorporate. While ATE measures the overall estimation accuracy of the whole trajectory -suitable for evaluating full-fledged SLAM systems where a loop closure module presents, it does not faithfully reflect the accuracy of our pose network since 1) our pose network is designed to estimate pairwise poses, and 2) thus by simply chaining the pose estimates overtime, the pose errors at earlier time instants are more pronounced. Therefore, we also adopt RPE to measure the estimation accuracy locally:</p><formula xml:id="formula_12">RPE(ĝ T , g T ; ∆) = 1 T − ∆ T −∆ t=1 trans (g −1 t g t+∆ ) −1 (ĝ −1 tĝt+∆ ) 2 2<label>(9)</label></formula><p>which is essentially the end-point relative pose error of a sliding window averaged over time. By measuring the end-point relative poseĝ tτ . =ĝ −1 tĝt+∆ , where τ .</p><p>= t + ∆, over a sliding window [t, t + ∆], we are able to focus more on the relative pose estimator (the pose network) itself rather than the overall localization accuracy. In our evaluation, we choose a sliding window of size 1, i.e., ∆ = 1. However, RPE is affected only by the accuracy of the translational part of the estimated pose, as we expand the relative pose error:</p><formula xml:id="formula_13">g −1 tτĝtτ = (R tτ , T tτ ) −1 · (R tτ ,T tτ )<label>(10)</label></formula><formula xml:id="formula_14">= (R tτRtτ , −R tτTtτ + T tτ )<label>(11)</label></formula><p>KITTI Odometry Sequence 09 KITTI Odometry Sequence 10 <ref type="figure" target="#fig_0">Fig. 11</ref>. Qualitative Pose Ablation Study KITTI Odometry Sequence 09 and 10. We perform an ablation study on our pose representation by jointly training our depth completion network and pose network on KITTI depth completion dataset and testing only the pose network on KITTI Odometry sequence 09 and 10. We obtain the camera trajectories by chaining the pairwise camera poses estimated by our pose network. We observe that the trajectory of our method using exponential parameterization trained with pose consistency (Eqn. 6) is most closely aligned with the ground-truth trajectory.</p><p>leading to trans(g −1 tτĝtτ ) = −R tτTtτ + T tτ , where the rotational partR tτ of the estimated pose disappears! Therefore, to better evaluate the rotation estimation, and, more importantly, to study the effect of different rotation parameterization and the pose consistency term, we propose the relative rotation error (RRE) metric:</p><formula xml:id="formula_15">RRE(ĝ T , g T ; ∆) = 1 T − ∆ T −∆ t=1 log rot(g −1 tτĝtτ ) 2 2<label>(12)</label></formula><p>where rot : SE(3) → SO(3) extracts the rotational part of a rigid body transformation, and log : SO(3) → R 3 is the logarithmic map for rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study on KITTI Odometry</head><p>We perform an ablation study on the effects of our pose parameterizations and our pose consistency in <ref type="table" target="#tab_1">Table VI</ref> and provide qualitative results showing the trajectory predicted by our pose network in <ref type="figure" target="#fig_0">Fig. 11</ref>. We jointly trained our depth completion network and our pose network on the KITTI depth completion dataset and evaluate the pose network on sequence 09 and 10 of the KITTI Odometry dataset.</p><p>For sequence 09, our pose network using exponential parameterization performs comparably to Euler angles on the ATE-5F and RPE metrics while outperforming Euler by ≈ 20% on ATE and ≈ 3.4% on RRE. This result suggests that while within a small window Euler and exponential perform comparably on translation, exponential is a better pose parameterization and globally more correct. We additionally see that exponential outperforms Euler angles on all metrics in sequence 10.</p><p>Our best results are achieved using exponential parameterization with our pose consistency term (Eqn. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D MORE RESULTS ON KITTI DEPTH COMPLETION BENCHMARK</head><p>In the main paper, we evaluated our approach on the KITTI depth completion benchmark test set in Sec. VII-A and performed an ablation study on the validation set in Sec. VII-B. Quantitative results are shown in <ref type="table" target="#tab_1">Table II</ref>, III and qualitative results in <ref type="figure">Fig. 4</ref>. However, as the KITTI online depth completion benchmark only shows the first 20 samples from the test set, we provide additional qualitative results on a variety of scenes in <ref type="figure" target="#fig_0">Fig. 12</ref> to better represent our performance on the test set.</p><p>The results in <ref type="figure" target="#fig_0">Fig. 12</ref> were produced by our VGG11 model trained using the full loss function (Eqn. 2) with exponential parameterization for rotation. Our method is able to recover pedestrians and thin structures well (e.g. the guard rails, and street poles). Additionally, our network is also able to recover structures that do not have any associated sparse lidar points (e.g structures located on the upper half of the image domain). This can be attributed to our photometric data-fidelity term (Sec. IV-A). As show in <ref type="figure">Fig. 3</ref>, our network first learns to copy the input scaffolding and to output it as the prediction. It later learns to fuse information from the input image to produce a prediction that includes elements from the scene that is missing from the scaffolding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion VGG11</head><p>Late Fusion VGG8 Pose Network <ref type="figure" target="#fig_0">Fig. 13</ref>. Network architectures. Green denotes convolution, orange deconvolution, and purple upsampling. Blue denotes the latent representation, and red the output of pose network. Our VGG11 and VGG8 architectures following the late fusion paradigm <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and our auxiliary pose network to predict relative pose between two frames for constructing our photometric and pose consistency loss (Eqn. <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6)</ref>. Our auxiliary pose network is used only in training and not inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E NETWORK ARCHITECTURE</head><p>We trained our model using two network architectures ( <ref type="figure" target="#fig_0">Fig. 13</ref>) following the late fusion paradigm: (i) our main model using a VGG11 <ref type="bibr" target="#b32">[32]</ref> encoder <ref type="table" target="#tab_1">(Table VII)</ref>, and (ii) our light weight model using a VGG8 <ref type="bibr" target="#b32">[32]</ref> encoder <ref type="table" target="#tab_1">(Table VIII)</ref>. Both encoders use the same decoder <ref type="table" target="#tab_1">(Table IX)</ref>.  <ref type="bibr" target="#b32">[32]</ref> encoder following the late fusion paradigm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b2">[3]</ref> contains ≈ 5.7M parameters as opposed to the ≈23.8M and ≈14.8M parameters used by <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively. The symbol denotes concatenation. Resolution ratio with respect to image size.  <ref type="bibr" target="#b32">[32]</ref> encoder following the late fusion paradigm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b2">[3]</ref> contains only ≈ 2.4M parameters as opposed to the ≈23.8M and ≈14.8M parameters used by <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively. The symbol denotes concatenation. Resolution ratio with respect to image size. Note that our light-weight model performs similarly to our VGG11 model.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Depth completion with Visual-Inertial Odometry (VIO) on the proposed VOID dataset (best viewed in color at 5×). Bottom left: sparse reconstruction (blue) and camera trajectory (yellow) from VIO. The highlighted region is densified and zoomed in on the top right. Top left shows an image of the same region which is taken as input, and fused with the sparse depth image by our method. On the bottom right is the same view showing only the sparse points, insufficient to determine scene geometry and topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>System diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig. 4. Qualitative evaluation on KITTI benchmark. Top to bottom: input image and sparse depth, results of [1], our results. Results are taken from KITTI online test server. Warmer colors in the error map denote higher error. Green rectangles highlight regions for detail comparison. We perform better in general, particularly on thin structures and far regions. [1] exhibit artifacts resembling scanlines and "circles" for far away regions (highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Error characteristics of our model on KITTI. The abscissa shows the distance of sparse data points measured by Velodyne, of which the percentage of all the data points is shown in red; the blue curve shows the mean absolute error of the estimated depth at the given distance, of which the 5-th and 95-th percentile enclose the light blue region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative evaluation on VOID benchmark. Top: Input RGB images. Bottom: Densified depth images back-projected to 3D, colored, and viewed from a different vantage point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.<ref type="bibr" target="#b3">4</ref>. Qualitative evaluation on KITTI benchmark. Top to bottom: input image and sparse depth, results of<ref type="bibr" target="#b0">[1]</ref>, our results. Results are taken from KITTI online test server. Warmer colors in the error map denote higher error. Green rectangles highlight regions for detail comparison. We perform better in general, particularly on thin structures and far regions.<ref type="bibr" target="#b0">[1]</ref> exhibit artifacts resembling scanlines and "circles" for far away regions (highlighted in red).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ERROR</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>METRICS.</cell></row><row><cell>Metric</cell><cell>units</cell><cell>Definition</cell></row><row><cell>MAE</cell><cell>mm</cell><cell>1 |Ω|</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II KITTI</head><label>II</label><figDesc>DEPTH COMPLETION BENCHMARK.</figDesc><table><row><cell>Method</cell><cell># Parameters</cell><cell>MAE</cell><cell>RMSE</cell><cell cols="2">iMAE iRMSE</cell></row><row><cell cols="4">Schneider [28] not reported 605.47 2312.57</cell><cell>2.05</cell><cell>7.38</cell></row><row><cell>Ma [1]</cell><cell>≈ 27.8M</cell><cell cols="2">350.32 1299.85</cell><cell>1.57</cell><cell>4.07</cell></row><row><cell>Yang [3]</cell><cell>≈ 18.8M</cell><cell cols="2">343.46 1263.19</cell><cell>1.32</cell><cell>3.58</cell></row><row><cell>Ours VGG11</cell><cell>≈ 9.7M</cell><cell cols="2">299.41 1169.97</cell><cell>1.20</cell><cell>3.56</cell></row><row><cell>Ours VGG8</cell><cell>≈ 6.4M</cell><cell cols="2">304.57 1164.58</cell><cell>1.28</cell><cell>3.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III KITTI</head><label>III</label><figDesc>DEPTH COMPLETION ABLATION STUDY. Lsz + Lsm (vanilla) VGG11 Eul. 347.14 1330.88 1.46 4.22 L Lsm VGG11 Exp. 305.06 1239.06 1.21 3.71 L</figDesc><table><row><cell>Model</cell><cell cols="2">Encoder Rot. MAE RMSE iMAE iRMSE</cell></row><row><cell>Scaffolding</cell><cell>-</cell><cell>-443.57 1990.68 1.72 6.43</cell></row><row><cell>L ph +</cell><cell></cell><cell></cell></row></table><note>ph + Lsz + Lsm VGG11 Eul. 327.84 1262.46 1.31 3.87 L ph + Lsz + Lsm VGG11 Exp. 312.10 1255.21 1.28 3.86 L ph + Lsz + Lpc +ph + Lsz + Lpc + Lsm VGG8 Exp. 308.81 1230.85 1.29 3.84</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV VOID</head><label>IV</label><figDesc>DEPTH COMPLETION BENCHMARK AND ABLATION STUDY.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell cols="2">RMSE iMAE iRMSE</cell></row><row><cell>Ma [1]</cell><cell cols="2">198.76 260.67 88.07</cell><cell>114.96</cell></row><row><cell>Yang [3]</cell><cell cols="2">151.86 222.36 74.59</cell><cell>112.36</cell></row><row><cell>VGG11 PoseNet + Eul.</cell><cell cols="2">108.97 212.16 64.54</cell><cell>142.64</cell></row><row><cell>VGG11 PoseNet + Exp.</cell><cell cols="2">103.31 179.05 63.88</cell><cell>131.06</cell></row><row><cell>VGG11 PoseNet + Exp. + Lpc</cell><cell>85.05</cell><cell>169.79 48.92</cell><cell>104.02</cell></row><row><cell>VGG11 SLAM Pose</cell><cell>73.14</cell><cell>146.40 42.55</cell><cell>93.16</cell></row><row><cell>VGG8 PoseNet + Exp. + Lpc</cell><cell>94.33</cell><cell>168.92 56.01</cell><cell>111.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V DEPTH</head><label>V</label><figDesc>COMPLETION ON VOID WITH VARYING SPARSE DEPTH DENSITY.</figDesc><table><row><cell>Density</cell><cell>Pose From</cell><cell>MAE</cell><cell>RMSE</cell><cell>iMAE</cell><cell>iRMSE</cell></row><row><cell>∼ 0.5%</cell><cell>PoseNet SLAM</cell><cell>85.05 73.14</cell><cell>169.79 146.40</cell><cell>48.92 42.55</cell><cell>104.02 93.16</cell></row><row><cell>∼ 0.15%</cell><cell>PoseNet SLAM</cell><cell>124.11 118.01</cell><cell>217.43 195.32</cell><cell>66.95 59.29</cell><cell>121.23 101.72</cell></row><row><cell>∼ 0.05%</cell><cell>PoseNet SLAM</cell><cell>179.66 174.04</cell><cell>281.09 253.14</cell><cell>95.27 87.39</cell><cell>151.66 126.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>POSE ABLATION STUDY KITTI ODOMETRY SEQUENCE 09 AND 10.</figDesc><table><row><cell>Pose</cell><cell cols="3">ATE (m) ATE-5F (m) RPE (m) RRE ( • )</cell></row><row><cell>Sequence 09</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Euler</cell><cell>34.38</cell><cell>0.091</cell><cell>0.107 0.176</cell></row><row><cell>Exp.</cell><cell>27.57</cell><cell>0.091</cell><cell>0.108 0.170</cell></row><row><cell cols="2">Exp. w/ Consistency 18.18</cell><cell>0.080</cell><cell>0.094 0.157</cell></row><row><cell>Sequence 10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Euler</cell><cell>32.37</cell><cell>0.067</cell><cell>0.094 0.251</cell></row><row><cell>Exp.</cell><cell>25.18</cell><cell>0.059</cell><cell>0.091 0.225</cell></row><row><cell cols="2">Exp. w/ Consistency 24.60</cell><cell>0.059</cell><cell>0.081 0.218</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>6): on sequence 09, it outperformed Euler and exponential without pose consistency by ≈ 47.1% and ≈ 28.9% on ATE, ≈ 12.1% and ≈ 13% on RPE, ≈ 10.8% and ≈ 7.6% on RRE, respectively, and both by ≈ 12.1% on ATE-5F. On sequence 10, it outperformed Euler and exponential by ≈ 24% and ≈ 2.3% on ATE, ≈ 13.8% and ≈ 11% on RPE, and ≈ 13.1% and ≈ 3.1% on RRE, respectively. It also beat Euler by ≈ 12% on RPE and is comparable to exponential on the metric. Qualitative Results on KITTI Depth Completion Test Set. We show results from various scenes on the KITTI test set. The sparse depth input on the KITTI benchmark is concentrated on the lower half of the image domain. Our network learns to predict structures that do not have any sparse points (e.g. street sign in row 3, 5, and 6). Also, we are able to recover predestrians (e.g. rows 2, and 3) and thin structures well (e.g. guard rails in row 1, poles in row 3, 4, 5, and 6, and 7).</figDesc><table><row><cell>Image</cell><cell>Sparse Depth</cell><cell>Completed Depth</cell></row><row><cell>Fig. 12.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">VGG11 ENCODER ARCHITECTURE</cell><cell></cell><cell></cell></row><row><cell>VGG11 Encoder</cell><cell>kernel</cell><cell></cell><cell>channels</cell><cell></cell><cell cols="2">resolution</cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell>size</cell><cell>stride</cell><cell>in</cell><cell>out</cell><cell>in</cell><cell>out</cell><cell># params</cell><cell>input</cell></row><row><cell>Image Branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1 image</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>48</cell><cell>1</cell><cell>1/2</cell><cell>≈ 3.6K</cell><cell>image</cell></row><row><cell>conv2 image</cell><cell>3</cell><cell>2</cell><cell>48</cell><cell>96</cell><cell>1/2</cell><cell>1/4</cell><cell>≈ 41K</cell><cell>conv1 image</cell></row><row><cell>conv3 image</cell><cell>3</cell><cell>1</cell><cell>96</cell><cell>192</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 166K</cell><cell>conv2 image</cell></row><row><cell>conv3b image</cell><cell>3</cell><cell>1</cell><cell>192</cell><cell>192</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 331K</cell><cell>conv3 image</cell></row><row><cell>conv4 image</cell><cell>3</cell><cell>1</cell><cell>192</cell><cell>384</cell><cell>1/8</cell><cell>1/8</cell><cell>≈ 663K</cell><cell>conv3b image</cell></row><row><cell>conv4b image</cell><cell>3</cell><cell>1</cell><cell>384</cell><cell>384</cell><cell>1/8</cell><cell>1/8</cell><cell>≈ 1.3M</cell><cell>conv4 image</cell></row><row><cell>conv5 image</cell><cell>3</cell><cell>1</cell><cell>384</cell><cell>384</cell><cell>1/16</cell><cell>1/16</cell><cell>≈ 1.3M</cell><cell>conv4b image</cell></row><row><cell>conv5b image</cell><cell>3</cell><cell>2</cell><cell>384</cell><cell>384</cell><cell>1/16</cell><cell>1/32</cell><cell>≈ 1.3M</cell><cell>conv5 image</cell></row><row><cell>Depth Branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1 depth</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>16</cell><cell>1</cell><cell>1/2</cell><cell>≈ 0.8K</cell><cell>depth</cell></row><row><cell>conv2 depth</cell><cell>3</cell><cell>2</cell><cell>16</cell><cell>32</cell><cell>1/2</cell><cell>1/4</cell><cell>≈ 4.6K</cell><cell>conv1 depth</cell></row><row><cell>conv3 depth</cell><cell>3</cell><cell>1</cell><cell>32</cell><cell>64</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 18K</cell><cell>conv2 depth</cell></row><row><cell>conv3b depth</cell><cell>3</cell><cell>1</cell><cell>64</cell><cell>64</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 37K</cell><cell>conv3 depth</cell></row><row><cell>conv4 depth</cell><cell>3</cell><cell>1</cell><cell>64</cell><cell>128</cell><cell>1/8</cell><cell>1/8</cell><cell>≈ 74K</cell><cell>conv3b depth</cell></row><row><cell>conv4b depth</cell><cell>3</cell><cell>1</cell><cell>128</cell><cell>128</cell><cell>1/8</cell><cell>1/8</cell><cell>≈ 147K</cell><cell>conv4 depth</cell></row><row><cell>conv5 depth</cell><cell>3</cell><cell>1</cell><cell>128</cell><cell>128</cell><cell>1/16</cell><cell>1/16</cell><cell>≈ 147K</cell><cell>conv4b depth</cell></row><row><cell>conv5b depth</cell><cell>3</cell><cell>2</cell><cell>128</cell><cell>128</cell><cell>1/16</cell><cell>1/32</cell><cell>≈ 147K</cell><cell>conv5 depth</cell></row><row><cell>Latent Encoding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>latent</cell><cell>-</cell><cell>-</cell><cell>384+128</cell><cell>512</cell><cell>1/32</cell><cell>1/32</cell><cell>0</cell><cell>conv5b image conv5b depth</cell></row><row><cell>Total Parameters</cell><cell>≈ 5.7M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our VGG11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">VGG8 ENCODER ARCHITECTURE</cell><cell></cell><cell></cell></row><row><cell>VGG8 Encoder</cell><cell>kernel</cell><cell></cell><cell>channels</cell><cell></cell><cell cols="2">resolution</cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell>size</cell><cell>stride</cell><cell>in</cell><cell>out</cell><cell>in</cell><cell>out</cell><cell># params</cell><cell>input</cell></row><row><cell>Image Branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1 image</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>48</cell><cell>1</cell><cell>1/2</cell><cell>≈ 3.6K</cell><cell>image</cell></row><row><cell>conv2 image</cell><cell>3</cell><cell>2</cell><cell>48</cell><cell>96</cell><cell>1/2</cell><cell>1/4</cell><cell>≈ 41K</cell><cell>conv1 image</cell></row><row><cell>conv3b image</cell><cell>3</cell><cell>2</cell><cell>96</cell><cell>192</cell><cell>1/4</cell><cell>1/8</cell><cell>≈ 166K</cell><cell>conv2 image</cell></row><row><cell>conv4b image</cell><cell>3</cell><cell>2</cell><cell>192</cell><cell>384</cell><cell>1/8</cell><cell>1/16</cell><cell>≈ 663K</cell><cell>conv3b image</cell></row><row><cell>conv5b image</cell><cell>3</cell><cell>2</cell><cell>384</cell><cell>384</cell><cell>1/16</cell><cell>1/32</cell><cell>≈ 1.3M</cell><cell>conv4b image</cell></row><row><cell>Depth Branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv1 depth</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>16</cell><cell>1</cell><cell>1/2</cell><cell>≈ 0.8K</cell><cell>depth</cell></row><row><cell>conv2 depth</cell><cell>3</cell><cell>2</cell><cell>16</cell><cell>32</cell><cell>1/2</cell><cell>1/4</cell><cell>≈ 4.6K</cell><cell>conv1 depth</cell></row><row><cell>conv3b depth</cell><cell>3</cell><cell>1</cell><cell>32</cell><cell>64</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 18K</cell><cell>conv2 depth</cell></row><row><cell>conv4b depth</cell><cell>3</cell><cell>1</cell><cell>64</cell><cell>128</cell><cell>1/8</cell><cell>1/16</cell><cell>≈ 74K</cell><cell>conv3b depth</cell></row><row><cell>conv5b depth</cell><cell>3</cell><cell>2</cell><cell>128</cell><cell>128</cell><cell>1/16</cell><cell>1/32</cell><cell>≈ 147K</cell><cell>conv4b depth</cell></row><row><cell>Latent Encoding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>latent</cell><cell>-</cell><cell>-</cell><cell>384+128</cell><cell>512</cell><cell>1/32</cell><cell>1/32</cell><cell>0</cell><cell>conv5b image conv5b depth</cell></row><row><cell>Total Parameters</cell><cell>≈ 2.4M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Our light-weight VGG8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX</head><label>IX</label><figDesc>4M parameters. The symbol denotes concatenation and the ↑ symbol denotes upsampling. Resolution ratio with respect to image size.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">DECODER ARCHITECTURE</cell><cell></cell><cell></cell></row><row><cell>Decoder</cell><cell cols="2">kernel</cell><cell>channels</cell><cell></cell><cell cols="2">resolution</cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell>size</cell><cell>stride</cell><cell>in</cell><cell>out</cell><cell>in</cell><cell>out</cell><cell># params</cell><cell>input</cell></row><row><cell>deconv5</cell><cell>3</cell><cell>2</cell><cell>512</cell><cell>256</cell><cell>1/32</cell><cell>1/16</cell><cell>≈ 1.2M</cell><cell>latent</cell></row><row><cell>concat5</cell><cell>-</cell><cell>-</cell><cell>256+384+128</cell><cell>768</cell><cell>1/16</cell><cell>1/16</cell><cell>0</cell><cell>deconv5 conv4b image conv4b depth</cell></row><row><cell>conv5</cell><cell>3</cell><cell>1</cell><cell>768</cell><cell>256</cell><cell>1/16</cell><cell>1/16</cell><cell>≈ 1.8M</cell><cell>concat5</cell></row><row><cell>deconv4</cell><cell>3</cell><cell>2</cell><cell>256</cell><cell>128</cell><cell>1/16</cell><cell>1/8</cell><cell>≈ 295K</cell><cell>conv5</cell></row><row><cell>concat4</cell><cell>-</cell><cell>-</cell><cell>128+192+64</cell><cell>384</cell><cell>1/8</cell><cell>1/8</cell><cell>0</cell><cell>deconv4 conv3b image conv3b depth</cell></row><row><cell>conv4</cell><cell>3</cell><cell>1</cell><cell>384</cell><cell>128</cell><cell>1/8</cell><cell>1/8</cell><cell>≈ 442M</cell><cell>concat4</cell></row><row><cell>deconv3</cell><cell>3</cell><cell>2</cell><cell>128</cell><cell>128</cell><cell>1/8</cell><cell>1/4</cell><cell>≈ 147K</cell><cell>conv4</cell></row><row><cell>concat3</cell><cell>-</cell><cell>-</cell><cell>128+96+32</cell><cell>256</cell><cell>1/4</cell><cell>1/4</cell><cell>0</cell><cell>deconv3 conv2 image conv2 depth</cell></row><row><cell>conv3</cell><cell>3</cell><cell>1</cell><cell>256</cell><cell>64</cell><cell>1/4</cell><cell>1/4</cell><cell>≈ 147K</cell><cell>concat3</cell></row><row><cell>deconv2</cell><cell>3</cell><cell>2</cell><cell>64</cell><cell>64</cell><cell>1/4</cell><cell>1/2</cell><cell>≈ 37K</cell><cell>conv3</cell></row><row><cell>concat2</cell><cell>-</cell><cell>-</cell><cell>64+48+16</cell><cell>128</cell><cell>1/2</cell><cell>1/2</cell><cell>0</cell><cell>deconv2 conv1 image conv1 depth</cell></row><row><cell>conv2</cell><cell>3</cell><cell>1</cell><cell>128</cell><cell>1</cell><cell>1/2</cell><cell>1/2</cell><cell>≈ 1.2K</cell><cell>concat2</cell></row><row><cell>output</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1/2</cell><cell>1</cell><cell>0</cell><cell>↑ conv2</cell></row><row><cell>Total Parameters</cell><cell>≈ 4M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Our decoder contains ≈</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X POSE</head><label>X</label><figDesc></figDesc><table /><note>NETWORK ARCHITECTURE</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://realsense.intel.com/depth-camera/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Total Parameters ≈ 1M Our auxiliary pose network contains ≈ 1M parameters and is only used during training to construct the photometric and pose consistency loss (Eqn. <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6)</ref>. The output is averaged along its width and height dimensions to result in a 6 element vector -of which 3 elements are used to compose rotation and the rest for translation.</p><p>Depth completion networks. Our VGG11 and VGG8 model ( <ref type="figure">Fig. 13</ref>) contain a total of ≈ 9.7M and ≈ 6.4M parameters, respectively. In comparison to <ref type="bibr" target="#b0">[1]</ref> with ≈ 27.8M parameters and <ref type="bibr" target="#b2">[3]</ref> with ≈ 18.8M, our VGG11 model have a 65.1% and 48.4% reduction in parameters over <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively; our VGG8 model have a 80% and 66% reduction over <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref>. The image and depth branches of the encoder process the image and depth inputs separately -weights are not shared. The results of the encoders are concatenated as the latent representation and passed to the decoder for depth completion. The decoder makes the prediction at 1/2 resolution. The final layer of the decoder is an upsampling layer. Pose Network. Our pose network takes a pair of images as input and regresses the relative pose between the images. Reversing the order of the image will reverse the relative pose as well. We take the average across the width and height dimensions of the pose network output to produce a 6 element vector. We use 3 elements to model rotation and the rest to model translation. Including Pose Network in Total Parameters. We follow the network parameter computations of <ref type="bibr" target="#b2">[3]</ref> who employs an additional network trained on ground truth for regularization during training. Our pose network <ref type="table">(Table X)</ref> is an auxiliary network that is only used in training, and not during inference. Hence, we do not include it in the total number of parameters. However, even if we do, our pose network has ≈ 1M parameters, making our total for VGG11 to be ≈ 10.7M and VGG8 to be ≈ 7.4M. Our VGG11 model is still has a 61.5% reduction in parameter, and our VGG8 a 73.4% over the 27.8M parameters used by <ref type="bibr" target="#b0">[1]</ref>. If we include the auxiliary prior network of <ref type="bibr" target="#b2">[3]</ref>, containing 10.1M parameters, that is used for regularization during training, then <ref type="bibr" target="#b2">[3]</ref> has a total of 28.8M parameters. Our VGG11 model, therefore, has a 62.8% reduction in parameters over <ref type="bibr" target="#b2">[3]</ref> and our VGG8 has a 74.3% reduction.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Convolutional Compressed Sensing for LiDAR Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning morphological operators for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dimitrievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veelaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hmsnet: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sample RGB + D images in the VOID dataset (best viewed in color at 5×). Color bar shows the depth range</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An invitation to 3-d vision: from images to geometric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geo-supervised visual depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voronoi diagrams from convex hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="223" to="228" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The quickhull algorithm for convex hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huhdanpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual-inertial navigation, mapping and localization: A scalable real-time causal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-state constraint kalman filter for vision-aided inertial navigation,&quot; in Robotics and automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Mourikis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Roumeliotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE international conference on</title>
		<imprint>
			<biblScope unit="page" from="3565" to="3572" />
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The tum vi benchmark for evaluating visual-inertial odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Usenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1680" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Penncosyvio: A challenging visual inertial odometry benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sanket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cleveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-29" />
			<biblScope unit="page" from="3847" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A compact formula for the derivative of a 3-d rotation in exponential coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
