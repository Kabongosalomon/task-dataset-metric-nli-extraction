<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LADDERNET: MULTI-PATH NETWORKS BASED ON U-NET FOR MEDICAL IMAGE SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LADDERNET: MULTI-PATH NETWORKS BASED ON U-NET FOR MEDICAL IMAGE SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-LadderNet, segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>U-Net has been providing state-of-the-art performance in many medical image segmentation problems. Many modifications have been proposed for U-Net, such as attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net with residual blocks or blocks with dense connections. However, all these modifications have an encoderdecoder structure with skip connections, and the number of paths for information flow is limited. We propose Lad-derNet in this paper, which can be viewed as a chain of multiple U-Nets. Instead of only one pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple pairs of encoder-decoder branches, and has skip connections between every pair of adjacent decoder and decoder branches in each level. Inspired by the success of ResNet and R2-UNet, we use modified residual blocks where two convolutional layers in one block share the same weights. A LadderNet has more paths for information flow because of skip connections and residual blocks, and can be viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to an ensemble of FCNs improves segmentation accuracy, while the shared weights within each residual block reduce parameter number. Semantic segmentation is essential for retinal disease detection. We tested LadderNet on two benchmark datasets for blood vessel segmentation in retinal images, and achieved superior performance over methods in the literature. The implementation is provided https: //github.com/juntang-zhuang/LadderNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Deep learning has achieved state-of-the-art performance in many computer vision tasks, such as image classification, semantic segmentation, object recognition, motion tracking and image captioning <ref type="bibr">[1]</ref>. Convolutional neural networks have become popular since the success of AlexNet <ref type="bibr" target="#b3">[2]</ref> on the classification task for ImageNet dataset <ref type="bibr" target="#b4">[3]</ref> mainly for the following reasons: first, large datasets and powerful computational resources are available nowadays; second, convolutional operation on an image is translation-invariant and enables weight sharing for feature extraction; third, the success of activation functions such as rectified linear unit (ReLU); fourth, efficient optimization algorithms such as stochastic gradient descent (SGD) and Adam optimizer.</p><p>Deep convolutional neural networks (CNN) have achieved near-radiologist performance in many semantic segmentation tasks in medical image analysis. Fully convolutional neural network (FCN) <ref type="bibr" target="#b5">[4]</ref> have succeeded in semantic segmentation on Pascal VOC dataset, and U-Net <ref type="bibr" target="#b6">[5]</ref> achieved the top accuracy in the segmentation of neuronal structures in electron microscopic stacks. Other variants of CNN achieve stateof-the-art performance on benchmark semantic segmentation tasks, such as PSPNet <ref type="bibr" target="#b7">[6]</ref> and DeepLab <ref type="bibr" target="#b8">[7]</ref>. Among all these variants, U-Net is the most widely used structure in medical image analysis, mainly because the encoder-decoder structure with skip connections allows efficient information flow, and performs well when sufficiently large datasets are not available.</p><p>Many variants of U-Net have been proposed: Alom et al. proposed to use recurrent convolution in U-Net <ref type="bibr" target="#b9">[8]</ref>; Ozan et al. proposed to use attention module in U-Net to determine where to look at; Simon et al. proposed Tiramisu <ref type="bibr" target="#b10">[9]</ref>, where the convolutional layers in a U-Net are replaced with dense blocks. However, all these variants still fall into the encoderdecoder structure, where the number of paths for information flow is limited. We propose LadderNet, a convolutional network for semantic segmentation with more paths for information flow. We demonstrate that LadderNet can be viewed as an ensemble of FCNs, and validate its superior performance on blood vessel segmentation task in retinal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LadderNet</head><p>U-Net and its variants in the literature all have an encoderdecoder structure. However, the number of paths for information flow in U-Net is limited. We propose LadderNet, a multibranch convolutional neural network for semantic segmentation as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which has more paths of information flow. Features in different spatial scales are named with letters A to E, and columns are named with numbers 1 to 4. We name column 1 and 3 as encoder branches, and name column 2 and 4 as decoder branches. We use convolution with a stride of 2 to go from small-receptive-field features to large-receptive- field features (e.g., A to B), and use transposed convolution with a stride of 2 to go from large-receptive-field features to large-receptive-field features (e.g., B to A). The number of channels is doubled from one level to the next level (e.g., A to B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Connection between LadderNet and U-Net</head><p>LadderNet can be viewed as a chain of U-Nets. Columns 1 and 2 can be viewed as a U-Net, and Columns 3 and 4 can be viewed as another U-Net. Between two U-Nets, there are skip connections at levels A-D. Different from U-Net, where features from encoder branches are concatenated with features from decoder branches, we sum features from two branches. We demonstrate a LadderNet composed of 2 U-Nets here, but attach more U-Nets to form complicated network structures.</p><p>LadderNet can also be viewed as an ensemble of multiple FCNs. Veit et al. proposed that ResNet behaves like an ensemble of shallow networks <ref type="bibr" target="#b11">[10]</ref>, because the residual connections provide multiple paths of information flow. Similarly, LadderNet provides multiple paths of information flow, and we lists a few paths here as an example:</p><formula xml:id="formula_0">(1) A1 → A2 → A3 → A4, (2) A1 → A2 → A3 → B3 → B4 → A4, (3) A1 → B1 → B2 → B3 → B4 → A4, (4) A1 → B1 → C1 → D1 → E1 → D2 → C2 → B2 → A2 → A3 → A4.</formula><p>Each path can be viewed as a variant of FCN. The total number of paths grows exponentially with the number of encoderdecoder pairs and the number of spatial levels (e.g., A to E in <ref type="figure" target="#fig_0">Fig. 1)</ref>. Therefore, LadderNet has the potential to capture more complicated features and produce a higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Shared-weights residual block</head><p>More encoder-decoder branches will increase the number of parameters and the difficulty of training. To solve this problem, we propose shared-weights residual blocks as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Different from a standard residual convolutional block proposed by He <ref type="bibr" target="#b12">[11]</ref>, the two convolutional layers in the same block share the same weights. Similar to the recurrent convolutional neural network (RCNN) <ref type="bibr" target="#b13">[12]</ref>, the two convolutional layers in the same block can be viewed as one recurrent layer, except that the two batch normalization layers are different. A drop-out layer is added between two convolutional layers to avoid overfitting. The shared-weights residual block combines the strength of skip connection, recurrent convolution and drop-out regularization, and has much fewer parameters that a standard residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluated the proposed LadderNet on two popular datasets for retina blood vessel segmentation: the DRIVE dataset and the CHASE DB1 dataset. The DRIVE dataset consists of 40 color images of the retina, 20 of which were used for training and the remaining 20 images for testing. Each image has 565 × 584 pixels. To increase the number of training samples, we randomly sampled 190,000 patches of size 48 × 48 from the training images, and used 10% of the training samples as validation data.</p><p>The CHASE DB1 dataset was collected from both left and right eyes of 14 school children. It has 28 color images of the retina, 20 of which were used for training and the other 8 images (from 4 children) for testing. The size of each image is 996 × 960. We randomly sampled 760,000 patches of size 48 × 48 from the training images, and used 10% of the training samples as validation.</p><p>All patches were converted to gray-scale for experiments. Field of view (FOV) is provided for the DRIVE dataset but not the CHASE DB1 dataset. We applied similar techniques in <ref type="bibr" target="#b14">[13]</ref> to generate FOV masks, and sampled patches over the entire image including regions outside of the FOV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training of the neural network</head><p>We chose a LadderNet with 5 levels (A-E) and a drop-out rate of 0.25, and set the number of channels of the first level (level A) as 10, resulting in a LadderNet with 1.5M parameters. We used cross-entropy loss for semantic segmentation, and applied Adam optimizer with default parameters and a batch size of 1024. We used "reduce learning rate on plateau" strategy, and set the learning rate as 0.01, 0.001, 0.0001 on epochs 0, 20 and 150 respectively, and set the total learning epochs as 250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation approaches</head><p>We used several metrics to evaluate the performance of Lad-derNet, including accuracy (AC), sensitivity (SE), specificity (SP) and F1-score. First we calculated True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN). Different metrics are calculated as follows:</p><formula xml:id="formula_1">AC = T P + T N T P + T N + F P + F N (1) SE = T P T P + F N (2) SP = T N T N + F P<label>(3)</label></formula><p>The F1-score is calculated as follows:</p><formula xml:id="formula_2">P recision = T P T P + F P (4) Recall = T P T P + F N (5) F 1 = 2 × P recision × Recall P recision + Recall<label>(6)</label></formula><p>To further evaluate the performance of different neural networks, we calculated the receiver operating characteristics (ROC) curve and the are under curve (AUC).    both tasks, and the areas under the precision-recall curves are above 0.88 for both tasks. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the quantitative results of different methods. LadderNet generates the highest F1-score, accuracy and AUC for both tasks. LadderNet also generates high SE and SP on two tasks. It's easy to generate a high SE or SP by predicting image pixels towards one category more easily than the other; in the extreme case, predicting the entire image as blood vessel will generate a SE = 1.0 but SP = 0.0. SE  and SP focus more on one category than the other, while other metrics such as AC, AUC and F1-score evaluates the performance of a model based on performance on two categories; therefore, a higher AC, AUC and F1-score is more convincing than a higher SE or SP. LadderNet achieves the highest AU, AUC and F1-score in both tasks, therefore it performs the best compared to previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION AND CONCLUSION</head><p>We propose LadderNet for semantic segmentation in this paper. Compared to U-Net, LadderNet has more encoderdecoder pairs. The skip connections enable LadderNet to have multiple paths for information flow, and the number of paths increases exponentially with the number of encoderdecoder pairs. Another innovation is the shared-weights residual block, which combines the strengths of residual connection, recurrent convolutional layer and drop-out regularization. The shared-weights design also greatly reduces the number of parameters. Our LadderNet has a superior performance over previous methods in the literature on two public datasets, and improves the performance on the key problem to automatic retinal disease detection. LadderNet can also be used in other semantic segmentation tasks such as tumor segmentation or brain lesion detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>From left to right: Structure of LadderNet and shared-weights residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of LadderNet are shown in Figs. 2-5. LadderNet generates predictions that are visually very close to the ground truth, and the areas under the ROC curves are above 0.97 for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>From left to right: Precision-recall curve and ROC curve on DRIVE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Test results on DRIVE dataset. From Top to bottom: input image, ground truth and predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>From left to right: Precision-recall curve and ROC curve on CHASE DB1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Test results on CHASE DB1 dataset. From Top to bottom: input image, ground truth and predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of LadderNet and other methods on DRIVE and CHASE DB1 datasets.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoliang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoliang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2009</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alom</forename><surname>Md Zahangir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jégou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Inception recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alom</forename><surname>Md Zahangir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-d gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on medical Imaging</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation of fundus images by major vessel extraction and subimage classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohini</forename><surname>Roychowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JBHI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A cross-modality learning approach for vessel segmentation in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
