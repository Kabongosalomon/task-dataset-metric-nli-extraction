<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Data Augmentation for Commonsense Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiben</forename><surname>Yang</surname></persName>
							<email>yiben.yang@</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><surname>Chaitanya</surname></persName>
							<email>chaitanyam@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malaviya</forename><surname>†π</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
							<email>jared.fern@u.</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
							<email>swabhas@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
							<email>ronanlb@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Ping</forename><surname>Wang</surname></persName>
							<email>jzwang@northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><surname>Chandra Bhagavatula</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<email>yejinc@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>dougd@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Π University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<title level="a" type="main">Generative Data Augmentation for Commonsense Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG c , that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG c consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WINOGRANDE, CODAH, and COMMONSENSEQA, and also enhances out-of-distribution generalization, proving to be more robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG c produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While recent advances in large-scale neural language models <ref type="bibr">Radford et al., 2019;</ref><ref type="bibr">Raffel et al., 2019)</ref> have led to strong performance on several commonsense reasoning benchmarks <ref type="bibr">(Talmor et al., 2019;</ref><ref type="bibr" target="#b25">Lv et al., 2020;</ref><ref type="bibr">Sakaguchi et al., 2020)</ref>, their accuracy by and large depends on the availability of large-scale human-authored training data. However, crowdsourcing examples at scale for each new task and domain can be prohibitively expensive. Moreover, human-authored data has been shown to exhibit annotation artifacts <ref type="bibr">(Gururangan et al., 2018;</ref><ref type="bibr">Agrawal Figure 1</ref>: Example of a selected high-quality generated example compared to a human-authored example from the WINOGRANDE dataset. Composing commonsense questions can require creativity. <ref type="bibr">Schwartz et al., 2017)</ref>, leading to models with considerably weaker performance on outof-distribution samples <ref type="bibr">(Jia and Liang, 2017;</ref><ref type="bibr" target="#b8">Belinkov and Bisk, 2017;</ref><ref type="bibr">Iyyer et al., 2018)</ref>.</p><p>A candidate solution that has shown promise in other tasks, such as reading comprehension, is to augment a human-authored training set with a large set of synthetically-generated examples <ref type="bibr" target="#b35">(Zhou et al., 2017;</ref><ref type="bibr" target="#b16">Du et al., 2017;</ref><ref type="bibr" target="#b33">Zhao et al., 2018a)</ref>. But, generating synthetic examples for commonsense reasoning poses a unique challenge. In reading comprehension, for instance, the goal of data augmentation is to generate questions that are directly answerable by a given reference passage. In contrast, answering commonsense questions relies on commonsense notions that are seldom stated explicitly <ref type="bibr">(Gordon and Van Durme, 2013;</ref><ref type="bibr" target="#b19">Forbes and Choi, 2017)</ref>, and authoring such questions can require creativity (see <ref type="figure">Figure 1</ref>). Based on promising evidence from previous work <ref type="bibr" target="#b28">(Yang et al., 2018;</ref><ref type="bibr">Trinh and Le, 2018;</ref><ref type="bibr" target="#b9">Bosselut et al., 2019;</ref><ref type="bibr" target="#b14">Davison et al., 2019)</ref>, we hypothesize that pretrained language models, such as <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>, capture some common sense expressed implicitly in their pretraining corpus. Could questions generated by such models serve as helpful training data? In this work, we explore this question through Generative Data Augmentation for commonsense reasoning (G-DAUG c ; §2): a novel framework for augmenting training data with diverse and informative synthetic training examples to improve both in-distribution performance and out-of-distribution generalization of commonsense reasoning models. <ref type="bibr">1</ref> Although a generative model allows us to produce large pools of synthetic training examples, the generated examples may be noisy or redundant. To ensure that we use the most informative examples for augmentation, we introduce data selection methods based on influence functions <ref type="bibr">(Koh and Liang, 2017)</ref> and a heuristic to maximize the diversity of the generated data pool. Finally, we propose an effective two-stage training scheme for augmentation with synthetic data. In experiments across multiple commonsense benchmarks, we show that G-DAUG c can mitigate the expense and brittleness resulting from large training sets for commonsense reasoning tasks.</p><p>To summarize, our contributions include: 1. G-DAUG c , a generative data augmentation framework for commonsense reasoning ( §2), 2. novel selection methods that identify informative and diverse synthetic training examples from the generated pool ( §3), 3. experiments showing that G-DAUG c improves in-distribution performance, achieving a 1-4% average absolute gain across four commonsense reasoning data sets and state-of-theart results on the WINOGRANDE <ref type="bibr">(Sakaguchi et al., 2020)</ref>, <ref type="bibr">COMMONSENSEQA (Talmor et al., 2019)</ref>, and CODAH  benchmarks, and also improves model robustness in terms of resistance to adversarial attacks <ref type="bibr">(Jin et al., 2020)</ref> and accuracy on perturbed evaluation sets ( §4), and 4. a comprehensive analysis of the factors that influence G-DAUG c 's performance ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">G-DAUG c</head><p>We now describe our framework for Generative Data Augmentation for Commonsense Reasoning (G-DAUG c ). <ref type="figure">Figure 2</ref> shows an overview of the approach. We describe G-DAUG c 's data generation procedure (steps 1 and 2 in the figure) in this section, and cover the data selection and training <ref type="figure">Figure 2</ref>: Illustration of the G-DAUG c process: (1) generate synthetic data and train a task model, (2) relabel the generated data using the task model, (3) filter the generated data based on estimated influence scores, (4) further select a subset based on a diversity-maximizing heuristic, (5) train a new task model using the filtered generations (synthetic training), and (6) further train this model using the original training data (organic training).</p><p>components (steps 3-5) in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Synthetic Training Data Generation</head><p>We will use multiple choice question answering as a running example to describe synthetic data generation. Formally, consider a dataset of N questions D = {(Q i , C i , y i ) : i = 1, 2, ..., N }, where Q i is a sequence of words denoting the i th question, C i = {C i j : j = 1, 2, ..., K} is the corresponding choice set with K choices which are word sequences as well, and a ground truth label y i ∈ {1, 2, ..., K}. We denote the answer as C i y i and the distractors as C i j =y i s. Our text generators are pretrained generative language models, finetuned to maximize the loglikelihood of a sequence of text W, L W (θ) = T t=1 log P (w t |W 1:t−1 ; θ), where W 1:t−1 denotes a subsequence of W and θ denotes the model parameters. 2 Below, we describe how we use variations of this objective to finetune different LMs to generate questions, answers and distractors. 3</p><p>Generating Synthetic Questions To train our question generator, we finetune the LM on the training question set {Q i } to optimize the language modeling objective: L q (θ q ) = N i=1 log P (Q i ; θ q ), where θ q denotes the parameters of the question generator. After finetuning, we generate new questions with nucleus sampling <ref type="bibr">(Holtzman et al., 2020)</ref>, which is suitable for generating long-form text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Synthetic Answers and Distractors</head><p>To generate choice sets, we independently finetune two separate generative LMs, one for answers and the other for distractors. The answer and distractor generators are trained to maximize the conditional log-likelihood of the answer and the distractors, respectively, given the question. Mathematically, we</p><formula xml:id="formula_0">optimize both L a (θ a ) = N i=1 log P (C i y i |Q i ; θ a ) and L d (θ d ) = N i=1 j =y i log P (C i j |Q i ; θ d ),</formula><p>where θ a and θ d denote the parameters of the answer and distractor generators, respectively. For answers, we use nucleus sampling with low temperature (for long answers) or greedy decoding (for short answers). To encourage diversity across generated distractors, we use nucleus sampling without temperature for these.</p><p>Data Relabeling. Our choice of generative LMs naturally defines labels for the synthetic choice sets. Alternatively, we consider using a supervised task model trained on the original training set, to relabel a candidate pool of synthetic answers and distractors. This is similar to treating the synthetic questions as unlabeled data and applying self-training. The utility of this self-training can be task-dependent; in our experiments, we used validation performance to determine whether or not to relabel our synthetic training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Synthetic Data Selection and Training</head><p>The above generation method can produce a large pool of examples, but training on all of them would be computationally expensive and might harm performance due to noisy generations. Here, we propose three data selection methods aimed at choosing more effective training examples from the generated pool ( §3.1). Further, we outline a simple staged training procedure ( §3.2) to mitigate the negative impact from noise in the synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selecting High-quality and Diverse Synthetic Examples</head><p>A randomly sampled synthetic dataset may contain examples that are similar to one another, along with low-quality generations <ref type="bibr">(Holtzman et al., 2020)</ref>.</p><p>We refer to such a random selection approach as G-DAUG c -Rand. We hypothesize that a diverse and high-quality synthetic set would benefit the task model more. We present three data selection algorithms that target quality, diversity and a combination of both.</p><p>Filtering with Influence Functions. We hypothesize that filtering out detrimental synthetic training examples can boost downstream performance <ref type="bibr" target="#b11">(Bras et al., 2020)</ref>. A given training example x is considered detrimental if including x in the training set results in a higher generalization error, approximated by validation loss, i.e.:</p><formula xml:id="formula_1">L(X , θ) = 1 |X | x i ∈X l(x i , θ), L(X val ,θ(X tr ∪ {x})) − L(X val ,θ(X tr )) &gt; 0.</formula><p>This would naively require retraining the model with x, which is computationally prohibitive. Fortunately, the validation loss change can be efficiently approximated through the use of influence functions <ref type="bibr" target="#b5">(Atkinson et al., 1983;</ref><ref type="bibr">Koh and Liang, 2017</ref>  <ref type="bibr" target="#b5">(Atkinson et al., 1983;</ref><ref type="bibr">Koh and Liang, 2017)</ref> tells us that the influence of upweighting a training example x by some small on the model parametersθ with the corresponding parameter space Θ is given by:</p><formula xml:id="formula_2">θ ,x = argmin θ∈Θ l(x, θ) + 1 N i=1 w i N i=1 w i l(x i , θ) I up,params (x) := dθ ,x d =0 = −H −1 θ ∇ θ l(x,θ),</formula><p>where w i is weight for the training example x i and Hθ is the Hessian evaluated atθ. The above result is a slight generalization of Koh and Liang (2017), but it is straightforward to generalize their proof to the weighted empirical risk case. Then, we apply the chain rule to get the influence of upweighting x on the validation loss:</p><formula xml:id="formula_3">I up,loss (x) := dL(X val ,θ ,x ) d =0 = ∇ θ L(X val ,θ) I up,params (x).</formula><p>Note that L(X tr , θ) can be rewritten as the following weighted average form to incorporate a new training example x new :</p><formula xml:id="formula_4">L(X tr , θ) = 1 N +1 i=1 w i N +1 i=1 w i l(x i , θ),</formula><p>where w i = 1∀i = N + 1, w N +1 = 0 and x N +1 = x new . Adding the new training example x new is equivalent to upweighting x N +1 by 1 N :</p><formula xml:id="formula_5">L(X tr ∪ {x new }, θ) ∝ 1 N l(x N +1 , θ) + 1 N +1 i=1 w i N +1 i=1 w i l(x i , θ).</formula><p>Applying the influence function I up,loss (x), we obtain the following linear approximation of the validation loss change upon adding the training example x new :</p><formula xml:id="formula_6">L(X val ,θ(X tr ∪ {x new })) − L(X val ,θ(X tr )) ≈ 1 N I up,loss (x new ).</formula><p>We adopt the stochastic estimation method described in <ref type="bibr">Koh and Liang (2017)</ref> to efficiently compute I up,loss . Detrimental synthetic data will have 1 N I up,loss &gt; 0.</p><p>Another distinction between our approach and Koh and Liang (2017) is that they compute the influence of a single training example on a single test example, whereas we estimate influence of a synthetic training example on all validation examples at once, which makes our approach scalable to large pools of synthetic data. Our approach, referred to as G-DAUG c -Influence, filters out detrimental synthetic data (i.e., the examples that have a positive estimated influence on the validation loss).</p><p>Selecting Diverse Examples. While G-DAUG c -Influence promotes training data quality, it ignores diversity; we hypothesize that better diversity can provide a more reliable training signal. We propose a simple greedy algorithm that iteratively selects a synthetic training example from the pool that maximizes a diversity measure. Here, we use a simple measure of diversity equal to the number of unique unigrams in the selected training set. Surprisingly, preliminary experiments with a more sophisticated diversity method based on embedding distance did not improve results (see Appendix E for details).</p><p>We refer to this approach as G-DAUG c -Diversity (see Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 G-DAUG c -Diversity</head><p>Input: Synthetic data pool D pool , Target size N Output: Synthetic dataset Initialization:</p><formula xml:id="formula_7">D synthetic ← − {} repeat x max = argmax x∈D pool #n-grams(D synthetic ∪{x}) − #n-grams(D synthetic ) Add x max to D synthetic Remove x max from D pool until |D synthetic | = N return D synthetic</formula><p>Combining Influence Filtering and Diversity Maximization G-DAUG c -Influence and G-DAUG c -Diversity have complementary benefits-the former aims at improving the quality of individual examples by filtering out detrimental ones, and the latter is designed to compose a diverse training set but does not consider quality. To reap both benefits, we propose a combined selection technique, G-DAUG c -Combo, that first filters the data using G-DAUG c -Influence, then selects examples according to G-DAUG c -Diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Synthetic Data</head><p>In traditional data augmentation, new data is usually mixed with the original training examples to create an augmented training set <ref type="bibr">(Wei and Zou, 2019;</ref><ref type="bibr">Kafle et al., 2017)</ref>. However, when augmenting with data produced using a generative model, label noise can be detrimental to learning <ref type="bibr">(Kafle et al., 2017)</ref>. Moreover, the generated questions themselves can be noisy, i.e. nonsensical or ambiguous (see <ref type="table">Table 7</ref> under §4.2). To address this issue, we propose a simple training procedure that treats the synthetic and original data differently. We first train a model on the synthetic data (Synthetic Training), then further train on the original, human-authored training set (Organic Training). The motivation is to correct any unfavorable noise that may have been learnt during the first stage, by subsequently training on original data as more recent training data is favored by neural models <ref type="bibr" target="#b20">(Goodfellow et al., 2014)</ref> .</p><p>We also experiment with a mixing approach that minimizes a weighted average of the loss for the synthetic data and the original data, with an importance weight to downweight the synthetic examples to mitigate noise. We find that two-stage training performs better than the importance-weighted loss (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present experiments on four commonsense multiple choice QA benchmarks: COMMONSENSEQA <ref type="bibr">(Talmor et al., 2019)</ref>, <ref type="bibr">WINOGRANDE (Sakaguchi et al., 2020)</ref>, CODAH  and Hel-laSwag <ref type="bibr" target="#b31">(Zellers et al., 2019)</ref>. Our techniques are also directly applicable to other closed-book multiple choice QA setups, such as science QA, and to textual entailment tasks with minor modifications. To evaluate G-DAUG c 's extensibility to these settings, we also experiment with a textual entailment task, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, and a closedbook version of the ARC-Challenge Scientific QA task <ref type="bibr" target="#b13">(Clark et al., 2018)</ref> in which access to the scientific corpus for the ARC dataset (or any other information sources) is disallowed during test. We simulate low-resource settings on the large Hel-laSwag and SNLI datasets by downsampling these to 2K and 3K training samples respectively; the other data sets are either already low-resource or have a low-resource component. Dataset details are provided in Appendix A.</p><p>Robustness Evaluation In addition to measuring in-distribution performance, we also analyze robustness to perturbed or adversarial data. Following Wei and Zou (2019), we perform WordNetbased <ref type="bibr" target="#b18">(Fellbaum, 1998)</ref> synonym replacement on the validation or test set (when test labels are available) with a 10% replacement rate. 5 Our second evaluation with TextFooler (Jin et al., 2020) identifies the most important words and replaces these with the most semantically and grammatically correct substitutes, until the model prediction is altered. We adopt two metrics to measure robustness under TextFooler's attacks: 1) failure rate: the proportion of examples for which TextFooler fails to change the prediction and 2) average perturbation ratio: the average fraction of words replaced when TextFooler succeeds in altering a prediction. We re-implement TextFooler with two minor changes: we only swap words in questions, not answers, and we replace the Universal Sentence Encoder with SROBERTA (Reimers and Gurevych, 4 https://leaderboard.allenai.org/ winogrande/submissions/public, https: //www.tau-nlp.org/csqa-leaderboard 5 https://github.com/jasonwei20/eda_nlp 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We use ROBERTA  as our pretrained task model, and <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> as our pretrained generator. <ref type="bibr">6</ref> We use validation performance to decide whether to do relabeling for COMMONSENSEQA and WINOGRANDE, and apply relabeling by default on all other tasks (tuning this choice may boost performance). To perform a controlled comparison, we restrict the synthetic set size to be equal across all methods. We repeat all experiments with 10 random restarts and pick the best model based on validation performance. Additional experimental details, with hyperparameters, are provided in Appendix C.</p><p>Baselines Our first baseline is a finetuned ROBERTA model with no augmentation. We compare with existing work on data augmentation via a BACKTRANSLATION approach from <ref type="bibr" target="#b27">Xie et al. (2019)</ref>; under our setting the original and backtranslated data are mixed at random. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">In-Distribution Results</head><p>Our main results for commonsense question answering are reported in <ref type="table">Table 1</ref>. All G-DAUG c variants outperform the baselines, highlighting the impact of generative data augmentation. On average, every other variant achieves higher test performance than G-DAUG c -Rand, which further highlights the importance of our data selection approaches. In addition, influence and diversity selection methods score similarly, however, their combination (in G-DAUG c -combo) outperforms either alone, which suggests that they are complementary selection approaches. More specifically, G-DAUG c -Combo performs the best on 3/4 tasks and obtains the highest average score. Further, G-DAUG c -Combo provides a 5.0% absolute gain over previously published state-of-the-art results on WINOGRANDE. 8 For COMMONSENSEQA, G-DAUG c -Combo outperforms the previous nonensemble state-of-the-art <ref type="bibr" target="#b36">(Zhu et al., 2020)</ref> by 0.4%. We also achieve a new state-of-the-art on CODAH, where the previous best (BERT-based) score was 67.5%   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness Results</head><p>Table 2 presents our evaluation on synonym replacement sets. The G-DAUG c variants outperform the baselines, and G-DAUG c -Combo obtains the best average performance. <ref type="table" target="#tab_3">Table 3</ref> shows results on the TextFooler adversarial attacks. Models trained with data augmentation are more robust to adversarial attacks, as all G-DAUG c variants and BACKTRANSLATION outperform the ROBERTA baseline on both metrics. G-DAUG c -Diversity obtains the best failure rate and average perturbation ratio (higher is better, in both metrics), and G-DAUG c -Combo performs comparably with slightly worse numbers. Overall, the findings suggest that optimizing diversity increases robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on ARC and SNLI</head><p>We explore G-DAUG c 's applicability outside of the commonsense domain in <ref type="table" target="#tab_4">Table 4</ref>, via evaluation on the closed-book ARC-Challenge Scientific QA. Valid science questions are hard to generate because their semantics need to be precise, and we find that many of G-DAUG c 's generations for ARC are noisy. Perhaps surprisingly, nonetheless G-DAUG c outperforms the baselines by a large margin. G-DAUG c -Influence achieves the best in-distribution performance, while G-DAUG c -Diversity is the most robust against TextFooler but has worse accuracy than G-DAUG c -Rand. This may suggest that optimizing for quality is more important when the synthetic data is noisier. We also evaluate G-DAUG c on a textual entailment using the SNLI dataset <ref type="bibr" target="#b10">(Bowman et al., 2015)</ref> in <ref type="table" target="#tab_4">Table 4</ref>. This task has a different format; it is a pair-wise classification task with 3 labels (details in Appendix A). We find that G-DAUG c slightly improves accuracy and robustness over baselines. The performance is likely affected by a label skew introduced by influence-based filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>We now analyze G-DAUG c 's performance, focusing on WINOGRANDE where G-DAUG c offers the most benefit. We first identify several factors that affect performance, and then present evidence that G-DAUG c works by transferring knowledge from the pretrained generator to the task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Factors that Affect G-DAUG c 's Performance</head><p>G-DAUG c is effective at different training sizes. <ref type="figure">Figure 3</ref> illustrates that our winning strategy, G-DAUG c -Combo, remains effective as the amount of training data varies, for WINOGRANDE.   Filtering synthetic data does not hurt accuracy. G-DAUG c 's filtering methods are designed to identify a high-quality and diverse subset of the generated data, to reduce training cost (compared to training on the entire generated pool) without harming accuracy. We evaluate whether G-DAUG c is successful at achieving this in <ref type="table" target="#tab_5">Table 5</ref>, by comparing G-DAUG c against using the entire synthetic data pool for G-DAUG c -Influence and G-DAUG c -Diversity. <ref type="bibr">9</ref> The selection approaches provide comparable or better accuracy compared to using the entire pool, despite using three times less data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Why Does G-DAUG c Work?</head><p>Below, we present analysis suggesting that G-DAUG c works by transferring knowledge from the pretrained model to the task model. In particular, we find that using a pre-trained generator is critical, and that the generated questions are often coherent, include new semantic units, and carry informative labels.</p><p>Using a Pretrained Generator is critical. We analyze the impact of the pretrained generator by comparing our standard G-DAUG c -Rand setting with a setting where the generator is not pretrained, but instead trained from scratch. We find that using GPT-2 trained from scratch results in a score of 67.8% on the WINOGRANDE-M validation set. This is a slight improvement (by 0.2%) over the unaugmented baseline, but is far inferior to the 3.9% improvement obtained when using the pretrained GPT-2. This suggests that using a pretrained generator is critical for G-DAUG c .  Synthetic data labels are important. Even fully unsupervised language model pretraining can improve performance, when using task-relevant data <ref type="bibr">(Gururangan et al., 2020)</ref>. This raises the question of whether G-DAUG c boosts performance by simply exposing the model to more task-relevant text, or if the generated labels are in fact informative. A related question is whether G-DAUG c 's optional self-supervised relabeling improves performance. We analyze these questions for WINO-GRANDE-L and COMMONSENSEQA in <ref type="table" target="#tab_7">Table 6</ref>, evaluating G-DAUG c with three labeling methods: (i) generator labels, (ii) random relabeling, and (iii) relabeling with a task model. When the generator labels are flipped randomly, G-DAUG c is unable to outperform the baselines for either dataset (in fact, it dramatically underperforms on WINOGRANDE-L). This implies that the correctness of labels is crucial for G-DAUG c . Self-supervised relabeling provides a 1.5% absolute gain in WINOGRANDE-L, but a 0.4% drop in COMMONSENSEQA, which suggests its utility is task-dependent.</p><p>G-DAUG c introduces new semantic units. We investigate how distinct the generated questions are from each other and from the original training data. We observe that G-DAUG c only rarely generates exact duplicate questions (e.g., on COMMON-SENSEQA, 0.06% of the questions are duplicates). We further investigate if G-DAUG c introduces new entities and relations to the training data, or if it merely reuses the ones found in the original training set. We quantify the diversity of our synthetic dataset compared to the original data by counting the number of unique semantic units produced by performing Open Information Extraction <ref type="bibr" target="#b7">(Banko et al., 2007)</ref> on the data. Specifically, we run the Stanford Open IE package  and report the number of unique triplets, relations and entities extracted from our WINOGRANDE-M datasets in <ref type="figure" target="#fig_0">Figure 4</ref>. The synthetic data includes many more unique semantic units than the original training data, suggesting that G-DAUG c does introduce new semantic units in the training set.</p><p>G-DAUG c produces mostly fluent questions.</p><p>To evaluate G-DAUG c 's output for fluency, we employ three human annotators to rate generated COMMONSENSEQA questions for their coherence and answerability on a scale of 1 to 4, where a rating of 3 denotes an acceptable question. We obtained a total of 1,387 labels. We measured annotator agreement on a separate set of 50 questions, obtaining a Fleiss' Kappa of 0.41, which is at the low end of moderate annotator agreement, acceptable given the subjective nature of the task. A large (74.04%) majority of questions met the acceptability threshold, with an overall average rating of 3.34. Examples are shown in <ref type="table">Table 7</ref>. Next, we ask annotators to answer the 1,027 acceptable questions, where they can edit choices (but not questions) if they are unable to pick a unique correct answer from the given choices. The  <ref type="table">Table 7</ref>: Examples and prevalence of generated commonsense questions with different manually-assigned fluency ratings, for the COMMONSENSEQA dataset. Ratings of 3 and higher correspond to questions that are answerable and address common sense, and most of G-DAUG c 's generated questions fall into this category.</p><p>editing rate is relatively high, at 55.3%. We mix these human-labeled examples with the original training set to train a ROBERTA model, and obtain 78.1% validation accuracy, which is comparable to G-DAUG c , despite using approximately 50x fewer questions. This suggests that human labels can provide higher leverage than the noisy labels from G-DAUG c , although human labeling is expensive. Additional analyses, provided in Appendix F, show that model sharpness approximated by the Hessian trace <ref type="bibr" target="#b29">(Yao et al., 2019)</ref> does not completely explain G-DAUG c 's performance; and, G-DAUG c is more effective than ensembling with a finetuned generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Data augmentation is a common practice in computer vision, where it takes the form of image transformations like translation and rotation <ref type="bibr">(Perez and Wang, 2017)</ref>. For language tasks, data augmentation is less straightforward. Broadly, previous augmentation methods have used back-translation architectures <ref type="bibr">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b27">Xie et al., 2019)</ref>, heuristics based on syntactic and semantic properties of text including word replacements using a thesaurus <ref type="bibr" target="#b32">(Zhang et al., 2015;</ref><ref type="bibr">Wei and Zou, 2019)</ref> and word embeddings (Wang and <ref type="bibr">Yang, 2015;</ref><ref type="bibr" target="#b17">Fadaee et al., 2017;</ref><ref type="bibr">Kobayashi, 2018;</ref><ref type="bibr" target="#b26">Wu et al., 2019)</ref>, and recently, generative models for synthesizing novel examples for text classification and reading comprehension <ref type="bibr">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b21">Kumar et al., 2020;</ref><ref type="bibr">Puri et al., 2020b)</ref>. Our framework is similar to the last of these as we focus on generative models for data augmentation, but our work is the first to present a generative approach for the challenging commonsense QA setting, and we introduce new data selection approaches to improve the informativeness and diversity of synthetic data.</p><p>Concurrently, there has been work on generat-</p><p>ing adversarial examples for analyzing black-box classifiers. These approaches use generative adversarial networks <ref type="bibr" target="#b34">(Zhao et al., 2018b)</ref> and populationbased optimization algorithms <ref type="bibr" target="#b2">(Alzantot et al., 2018)</ref>. Previous work has also presented methods to generate questions for reading comprehension <ref type="formula">(</ref> Our work is distinct in that it targets question generation in a closed-book setting, investigates the generation of answers as well as distractors, and is aimed at data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced G-DAUG c , a novel data augmentation framework to generate synthetic training data, preserving quality and diversity. We demonstrate that G-DAUG c is effective on multiple commonsense reasoning benchmarks, with improvements on in-distribution performance, as well as robustness against perturbed evaluation sets and challenge sets. Our analysis shows that G-DAUG c tends to perform better in low-resource settings and that our data selection strategies are important for performance. Future work might explore more sophisticated methods to enhance quality and diversity of generated training data, including having humans-in-the-loop for relabeling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>CommonsenseQA <ref type="bibr">(Talmor et al., 2019)</ref>: Com-monsenseQA is a multiple choice QA dataset that consists of 12,247 examples, which aims to test commonsense reasoning capabilities. We use the official random split 1.11 which is an 80/10/10 split. We apply greedy decoding to generate answers, as answers are fairly short for this dataset.</p><p>WINOGRANDE <ref type="figure">(Sakaguchi et al., 2020)</ref>: WINOGRANDE is a benchmark for commonsense reasoning, inspired by the original Winograd Schema Challenge design <ref type="bibr">(Levesque et al., 2011)</ref>, with a larger dataset size and higher difficulty level. It consists of 44K questions with five different training sizes: 160, 640, 2,558, 10,234 and 40,398 questions. The evaluation metric is Area Under the (learning) Curve. We observe that applying top-2 greedy decoding on the answer generator is able to yield a satisfactory set of choices, so the distractor generator is not used in this task. The Winograd schema requires that questions in twin pairs have opposite labels <ref type="bibr">(Levesque et al., 2011)</ref>. We use the following method to generate twin questions: 1. generate a sequence until a blank symbol " " is produced. 2. use two independent runs of sampling to complete the question in two different ways to form twins. The above process does not guarantee that the labels will differ for the two twins, so we further filter out generated pairs that do not have different labels.</p><p>CODAH : CODAH is an adversarially-constructed benchmark which tests commonsense reasoning using sentencecompletion questions, inspired by the Swag dataset <ref type="bibr" target="#b30">(Zellers et al., 2018)</ref>. It contains 2,801 questions in total, and uses 5-fold cross validation for evaluation. <ref type="bibr">10</ref> We lower the temperature to 0.5 for the answer generation in order to increase the confidence of the generated answers.</p><p>HellaSwag <ref type="bibr" target="#b31">(Zellers et al., 2019)</ref>: HellaSwag is a more challenging version of the Swag dataset <ref type="bibr" target="#b30">(Zellers et al., 2018)</ref>, and the task is similar to CO-DAH. The dataset consists of 70K questions where each question comes from one of two domains: Ac-tivityNet or WikiHow. In order to test our methods under a low-resource setting, we downsample the training set to 2,000 examples. We take a random <ref type="bibr">10</ref> The original CODAH work does not specify a particular 5-fold split, so we choose these randomly. We will release our splits for replicability. sample of 1000 questions from the original validation set to serve as our validation data, and another non-overlapping random sample of 5,000 questions from the same set as our test data. The generation settings are the same as CODAH's. <ref type="bibr" target="#b10">(Bowman et al., 2015)</ref>: SNLI is a natural language inference dataset with 570K pairs of labeled sentences. The label assigned to each sentence pair is one of entailment, contradiction or neutral. For low-resource experiments, we downsample the dataset to 3K training examples, which contains 1K unique premises and a hypothesis for all three labels. Similarly, we use a downsampled development set with 999 examples (333 premises and 3 hypotheses for each label). The generative model is fine-tuned by providing the premise, label and hypothesis, separated by special delimiters marking the beginning and end of each element. <ref type="bibr" target="#b13">(Clark et al., 2018)</ref>: The ARC Dataset consists of 7,787 natural grade-school science questions that are used on standardized tests. The ARC-Challenge Set contains 2,590 questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. We use the official split, which has 1,119 train, 299 validation, and 1,172 test examples. The generation settings are the same as COMMONSENSEQA's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARC-Challenge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Validation Set Results</head><p>In <ref type="table">Table 8</ref>, we summarize our main results on the validation sets, comparing the G-DAUG c methods against an unaugmented baseline and a backtranslation augmentation baseline. All G-DAUG c methods consistently outperform the baseline methods in every benchmark. The proposed selection methods provide an extra boost on average, compared to G-DAUG c -Rand. Among those, G-DAUG c -Influence achieves the best performance across all tasks, which is expected as G-DAUG c -Influence selects examples which are helpful in reducing validation loss. Interestingly, G-DAUG c -Combo scores lower than G-DAUG c -Influence, although it outperforms G-DAUG c -Diversity. Finally, backtranslation does not demonstrate any benefit and obtains lower results compared to the augmented baseline in all benchmarks.  <ref type="table">Table 8</ref>: Results on the validation sets of four commonsense benchmarks. All G-DAUG c methods outperform the baseline methods, in particular, G-DAUG c -Influence performs the best on all tasks, which is expected as it selects examples which are helpful in reducing validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameter Settings and Input Formats</head><p>Hyperparameter settings for finetuning GPT-2, ROBERTA and G-DAUG c are shown in <ref type="bibr">Tables 11,</ref><ref type="bibr">12,</ref><ref type="bibr">14,</ref><ref type="bibr">15 and 16</ref>. We manually tune the learning rate and the number of epochs for GPT-2 finetuning based on validation perplexity. For finetuning ROBERTA baseline models, we select the number of epochs from {1,3,5,8,10} based on validation accuracy for CSQA, WINOGRANDE and HellaSwag-2K. For CODAH, SNLI-3K and ARC-Challenge, we simply use 5 epochs. For G-DAUG c synthetic training, we train all models using a learning rate of 5e-6 for one epoch. For G-DAUG c organic training, we use the same hyperparameter settings as ROBERTA baselines (except for CSQA and HellaSwag-2K, where we find reducing 2 epochs gives significantly better results). In <ref type="table" target="#tab_12">Tables 9 and  10</ref>, we specify the input formats for finetuning GPT-2 and ROBERTA. Finally, we benchmark the running time of our implementations of the influence and diversity selection methods on the task of selecting 127,478 examples from a pool consisting of 380,700 candidates for WINOGRANDE-M. We use one Nvidia 2080 Ti GPU and one Intel Core I9-7900X with 10 cores and a clockspeed of 3.3 GHz. The running time of the influence and diversity algorithms is about 8.3 hours and 2.9 hours, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Influence Functions</head><p>In practice, since the generalization error is usually approximated by validation loss, a training example x i is considered detrimental if it increases validation loss, i.e.:</p><formula xml:id="formula_8">L(X , θ) = 1 |X | x∈X l(x, θ),<label>(1)</label></formula><formula xml:id="formula_9">L(X val ,θ(X train ∪ {x i })) − L(X val ,θ(X train )) &gt; 0,<label>(2)</label></formula><p>where</p><formula xml:id="formula_10">X train = {x i } N i=1 is a training set, X val = {x i } M i=1</formula><p>is a validation set, l is a loss function, and θ(X train ) = argmin θ∈Θ L(X train , θ) is an empirical risk minimizer.</p><p>The main result from previous work <ref type="bibr" target="#b5">(Atkinson et al., 1983;</ref><ref type="bibr">Koh and Liang, 2017)</ref> tells us that the influence of upweighting a training example x by some small on the model parametersθ with the corresponding parameter space Θ is given by:</p><formula xml:id="formula_11">θ ,x = argmin θ∈Θ l(x, θ) + 1 N i=1 w i N i=1 w i l(x i , θ)<label>(3)</label></formula><formula xml:id="formula_12">I up,params (x) := dθ ,x d =0 = −H −1 θ ∇ θ l(x,θ),<label>(4)</label></formula><p>where w i is weight for the training example x i and</p><formula xml:id="formula_13">Hθ = 1 N i=1 w i N i=1 w i ∇ 2 θ l(x i ,θ)</formula><p>is the Hessian evaluated atθ. The above result is a slight generalization of Koh and Liang (2017), since the simple average used in that work is a special case of our weighted average, but it is straightforward to generalize their proof to our weighted empirical risk case and we omit the details of the proof in this paper. Then, we apply the chain rule to get the influence of upweighting x on the validation loss:</p><formula xml:id="formula_14">I up,loss (x) := dL(X val ,θ ,x ) d =0<label>(5)</label></formula><p>= ∇ θ L(X val ,θ) I up,params (x).   </p><formula xml:id="formula_16">L(X train , θ) = 1 N +1 i=1 w i N +1 i=1 w i l(x i , θ),</formula><p>where w i = 1∀i = N + 1, w N +1 = 0 and x N +1 = x new . Adding the new training example x new is equivalent to upweighting x N +1 by 1 N :</p><formula xml:id="formula_17">L(X train ∪ {x new }, θ) = N N + 1 ( 1 N l(x N +1 , θ) + 1 N +1 i=1 w i N +1 i=1 w i l(x i , θ)) ∝ 1 N l(x N +1 , θ) + 1 N +1 i=1 w i N +1 i=1 w i l(x i , θ).</formula><p>Applying the influence function I up,loss (x), we obtain the following linear approximation of the validation loss change upon adding the training example x new :</p><formula xml:id="formula_18">L(X val ,θ(X train ∪ {x new })) − L(X val ,θ(X train )) (7) ≈ 1 N I up,loss (x new ).<label>(8)</label></formula><p>We adopt the stochastic estimation method described in <ref type="bibr">Koh and Liang (2017)</ref> to efficiently compute I up,loss . Detrimental synthetic data will have 1 N I up,loss &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Diversity Selection using Embedding Distance</head><p>We define our embedding distance based diversity measure as the sum of the cosine distances between every pair of selected examples. To attempt to maximize this measure, we use a greedy algorithm that at each iteration randomly samples 10K candidate examples from the pool, and selects the candidate that maximizes the distance between it and its nearest neighbor in the set of examples selected so far. We use SROBERTA (Reimers and Gurevych, 2019) as our sentence embedding method and Faiss (Johnson et al., 2017) as our nearest neighbor searcher. We compare the embedding distance based measure with the unigram approach on WINOGRANDE dataset. The embedding distance based diversity selection is not found to be more effective than the unigram approach, in fact it performs 0.6% worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Analysis</head><p>Sharpness Analysis. Previous work <ref type="bibr">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Keskar et al., 2016;</ref><ref type="bibr" target="#b29">Yao et al., 2019)</ref> has shown that models with flatter local minima tend to generalize better. <ref type="bibr">Moreover, Hao et al. (2019)</ref> show that pretraining helps BERT to achieve flat and wide optima in the finetuning stage, which partially explains its performance benefits. We investigate whether G-DAUG c 's data augmentation may also encourage flatter optima. Specifically, using the fact that a larger Hessian trace for a model implies a sharper local minimum <ref type="bibr" target="#b29">(Yao et al., 2019)</ref>, we compute the Hessian trace of 10 baseline and 10 G-DAUG c -Combo methods using 1e-5/5e-6/2e-5 * 4e-5/5e-5/5e-5 4e-5/5e-5/5e-5 5e-5 2e-5/1e-5/1e-5 Epochs (q/a/d) 3/5/3 * 3/3/3 3/3/3 3 3/5/5 Grad Clipping  LR (q/a) 5e-5/5e-5 2e-5/5e-5 2e-5/5e-5 2e-5/5e-5 1e-5/5e-5 Epochs (q/a) 8/12 6/6 3/3 3/3 3/1  <ref type="table" target="#tab_3">Table 13</ref>: Test performance of an unaugmented baseline model and the same model ensembled with a finetuned GPT-2 generator on WINOGRANDE. We use weighted average ensemble with weights tuned on validation data.</p><p>the Hutchinson Method <ref type="bibr" target="#b6">(Avron and Toledo, 2011)</ref> and find an average relative decrease of 9.5% for G-DAUG c -Combo, suggesting that G-DAUG c does find slightly flatter optima. Likewise, when comparing the best performing models of each approach, G-DAUG c -Combo's best model is slightly flatter than the baseline (a relative decrease of 0.2%). However, we also find the contradictory fact that, over the 20 models, flatter optima tend to be associated with worse task performance (Spearman correlation of 0.39, p ≈ 0.09). So, it does not appear that sharpness explains G-DAUG c 's performance advantage over the baseline. A more thorough analysis of this hypothesis is an item of future work.</p><p>Generator/Task Model Ensemble. G-DAUG c harnesses pretrained knowledge from GPT-2 in order to improve a ROBERTA-based task model. A more standard approach for model combination (albeit, with twice the computational cost at runtime) would be to ensemble the two models instead. We evaluate ensembling a baseline ROBERTA model with a finetuned GPT-2 generator for WINOGRANDE in <ref type="table" target="#tab_3">Table 13</ref>. We adopt a weighted-average ensemble method, where the weights are tuned on validation data (the tuning is important to achieve peak performance). The ensemble model performs same as the baseline model, and G-DAUG c -Combo outperforms both of them by 3.9%. This suggests that G-DAUG c is more effective than simply ensembling the finetuned generator.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>OpenIE analysis on the original data and synthetic data used by G-DAUG c -Combo on WINO-GRANDE-M. The synthetic dataset contains many more unique semantic units compared to the original dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Heilman and Smith, 2010; Rus et al., 2011;<ref type="bibr" target="#b1">Alberti et al., 2019;</ref> Puri et al., 2020a), online tutoring<ref type="bibr" target="#b23">(Lindberg et al., 2013)</ref>, factual QA (Serban et al., 2016) and visual question generation (Mostafazadeh et al., 2016). A comprehensive survey on neural question generation can be found in Pan et al. (2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Results on the test sets of four commonsense benchmarks. ROBERTA (reported) is the result for the ROBERTA-large baseline reported on public leaderboards. 4 ROBERTA (ours) is re-evaluation of the ROBERTAlarge model using our setup. All G-DAUG c methods outperform the baseline methods, and G-DAUG c -Combo performs the best overall.</figDesc><table><row><cell></cell><cell>CSQA (Acc)</cell><cell>WINOGRANDE (AUC)</cell><cell>CODAH (Acc)</cell><cell>HellaSwag-2K (Acc)</cell><cell>Average</cell></row><row><cell cols="2">ROBERTA (reported) 72.1</cell><cell>66.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ROBERTA (ours)</cell><cell>71.6</cell><cell>67.5</cell><cell>82.3</cell><cell>75.4</cell><cell>74.2</cell></row><row><cell cols="2">BACKTRANSLATION 70.2</cell><cell>67.2</cell><cell>81.8</cell><cell>73.0</cell><cell>73.1</cell></row><row><cell>G-DAUG c -Rand</cell><cell>71.8</cell><cell>70.9</cell><cell>83.6</cell><cell>75.9</cell><cell>75.6</cell></row><row><cell>G-DAUG c -Influence</cell><cell>72.1</cell><cell>70.9</cell><cell>84.3</cell><cell>75.8</cell><cell>75.8</cell></row><row><cell>G-DAUG c -Diversity</cell><cell>72.3</cell><cell>71.2</cell><cell>83.5</cell><cell>76.1</cell><cell>75.8</cell></row><row><cell>G-DAUG c -Combo</cell><cell>72.6</cell><cell>71.4</cell><cell>84.0</cell><cell>76.8</cell><cell>76.2</cell></row><row><cell></cell><cell cols="5">CSQA WINOGRANDE CODAH HellaSwag-2K Average</cell></row><row><cell>ROBERTA (ours)</cell><cell>69.9</cell><cell>63.8</cell><cell>74.7</cell><cell>63.2</cell><cell>67.9</cell></row><row><cell cols="2">BACKTRANSLATION 69.0</cell><cell>62.3</cell><cell>75.5</cell><cell>65.4</cell><cell>68.1</cell></row><row><cell>G-DAUG c -Rand</cell><cell>72.1</cell><cell>65.5</cell><cell>75.9</cell><cell>64.1</cell><cell>69.4</cell></row><row><cell>G-DAUG c -Influence</cell><cell>71.0</cell><cell>65.7</cell><cell>76.2</cell><cell>64.3</cell><cell>69.3</cell></row><row><cell>G-DAUG c -Diversity</cell><cell>71.6</cell><cell>66.0</cell><cell>76.0</cell><cell>64.8</cell><cell>69.6</cell></row><row><cell>G-DAUG c -Combo</cell><cell>72.0</cell><cell>66.0</cell><cell>76.0</cell><cell>65.2</cell><cell>69.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. We find</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on WordNet-based synonym replacement sets. For CODAH and HellaSwag-2K, we perturb test sets, as the labels are available. G-DAUG c -Combo achieves the highest average score.that BACKTRANSLATION hurts performance, and uniformly underperforms the ROBERTA baseline. See Appendix B for validation set results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Robustness to TextFooler-based adversarial attacks (failure rate / average perturbation ratio, higher is better for both). Models trained with augmented data are more robust to TextFooler's attacks compared to models without data augmentation. On average, G-DAUG c -Diversity performs the best.</figDesc><table><row><cell></cell><cell cols="2">ARC-Challenge Scientific QA</cell><cell></cell><cell>SNLI-3K</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Val. Test Syn. TF:Fail TF:Pert Val. Test Syn. TF:Fail TF:Pert NLI Diag.</cell></row><row><cell>RoBERTa (ours)</cell><cell>43.5 39.4 35.2 6.6</cell><cell>9.3</cell><cell cols="2">91.8 88.6 77.5 17.0</cell><cell>20.2</cell><cell>56.7</cell></row><row><cell>Backtranslation</cell><cell>43.1 43.1 42.4 6.6</cell><cell>9.3</cell><cell>91.2 8.1</cell><cell>81.0 18.8</cell><cell>21.7</cell><cell>54.0</cell></row><row><cell>G-DAUG c -Rand</cell><cell>50.8 48.1 43.4 12.9</cell><cell>10.8</cell><cell cols="2">91.8 89.0 78.6 17.7</cell><cell>20.6</cell><cell>57.4</cell></row><row><cell cols="2">G-DAUG c -Influence 51.5 48.5 45.2 12.4</cell><cell>11.0</cell><cell cols="2">92.3 88.7 78.6 18.0</cell><cell>20.7</cell><cell>56.9</cell></row><row><cell cols="2">G-DAUG c -Diversity 49.5 47.5 42.2 13.9</cell><cell>10.8</cell><cell cols="2">92.0 89.0 79.4 19.0</cell><cell>20.5</cell><cell>57.7</cell></row><row><cell>G-DAUG c -Combo</cell><cell>50.8 48.2 43.8 13.1</cell><cell>10.7</cell><cell cols="2">91.9 88.7 78.7 16.7</cell><cell>20.5</cell><cell>57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on closed-book ARC-Challenge Scientific QA and SNLI-3K, along with robustness to synonym replacement, TextFooler (TF) attacks and NLI Diagnostics. G-DAUG c improves accuracy and robustness.Figure 3: Validation results for different training set sizes on the WINOGRANDE dataset (in log scale). G-DAUG c helps more for smaller training sizes.The improvement over baseline is largest in the low-resource (small training size) regime. For the smallest sizes, XS and S, G-DAUG c -Combo increases the effective training size by a factor of 4 (i.e. training on XS or S matches unaugmented ROBERTA's performance on S or M, respectively). In contrast, BACKTRANSLATION only helps for the XS size, but hurts performance on larger sizes.</figDesc><table><row><cell>Staged training is essential. G-DAUG c uses a</cell></row><row><cell>two-staged training method (Section 3.2) aimed</cell></row><row><cell>at mitigating the effect of noise in the generated</cell></row><row><cell>data. We analyze alternative training protocols</cell></row><row><cell>on the WINOGRANDE-L dataset: Mixing (train-</cell></row><row><cell>ing on the union of generated and original data)</cell></row><row><cell>and Importance Weighted Loss. Compared to a</cell></row><row><cell>no-augmentation baseline (with accuracy of 75.9),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results comparing G-DAUG c 's filtering methods against using the entire synthetic data pool for augmentation, on WINOGRANDE-M.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Validation accuracy of G-DAUG c with differ- ent labeling methods on WINOGRANDE-L and COM- MONSENSEQA. Random labels hurt accuracy, and model relabeling helps on WINOGRANDE but not on COMMONSENSEQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 15-25, Vancouver, Canada. Association for Computational Linguistics.</figDesc><table><row><cell>reasoning over heterogeneous external knowledge</cell><cell>Robin Jia and Percy Liang. 2017. Adversarial exam-Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila</cell></row><row><cell>Jonathan Gordon and Benjamin Van Durme. 2013. Re-porting bias and knowledge acquisition. In Proceed-ings of the 2013 workshop on Automated knowledge base construction, pages 25-30. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of ACL. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural lan-guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), for commonsense question answering. In Proc. of AAAI. Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar-garet Mitchell, Xiaodong He, and Lucy Vander-wende. 2016. Generating natural questions about an image. In Proceedings of the 54th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1802-1813, Berlin, Germany. Association for Computational Linguis-tics. Liangming Pan, Wenqiang Lei, Tat-Seng Chua, and Min-Yen Kan. 2019. Recent advances in neural question generation. arXiv preprint arXiv:1905.08949. Luis Perez and Jason Wang. 2017. The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621. R. Puri, Ryan Spring, M. Patwary, M. Shoeybi, and Bryan Catanzaro. 2020a. Training question answering models from synthetic data. ArXiv, abs/2002.09599.</cell><cell>ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri-cal Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics. Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is BERT really robust? natural lan-guage attack on text classification and entailment. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv Zilles, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation mod-els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Compu-preprint arXiv:1702.08734. Kushal Kafle, Mohammed Yousefhussien, and Christo-tational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computa-tional Linguistics. pher Kanan. 2017. Data augmentation for visual question answering. In Proceedings of the 10th In-Iulian Vlad Serban, Alberto García-Durán, Caglar ternational Conference on Natural Language Gen-eration, pages 198-202, Santiago de Compostela, Spain. Association for Computational Linguistics. Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. 2016. Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus. In Pro-Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-cedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2016. On large-batch training for deep learn-ing: Generalization gap and sharp minima. ArXiv, abs/1609.04836. ceedings of the 54th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers), pages 588-598, Berlin, Germany. Associa-tion for Computational Linguistics.</cell></row><row><cell>pages 107-112, New Orleans, Louisiana. Associa-tion for Computational Linguistics. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-alizing and understanding the effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 4143-Raul Puri, Ryan Spring, Mostofa Patwary, Moham-mad Shoeybi, and Bryan Catanzaro. 2020b. Train-ing question answering models from synthetic data. arXiv preprint arXiv:2002.09599. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</cell><cell>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Sosuke Kobayashi. 2018. Contextual augmentation: Jonathan Berant. 2019. CommonsenseQA: A ques-Data augmentation by words with paradigmatic re-tion answering challenge targeting commonsense lations. In Proceedings of the 2018 Conference of knowledge. In Proceedings of the 2019 Conference the North American Chapter of the Association for of the North American Chapter of the Association Computational Linguistics: Human Language Tech-for Computational Linguistics: Human Language nologies, Volume 2 (Short Papers), pages 452-457, Technologies, Volume 1 (Long and Short Papers), New Orleans, Louisiana. Association for Computa-pages 4149-4158, Minneapolis, Minnesota. Associ-tional Linguistics. ation for Computational Linguistics.</cell></row><row><cell>4152, Hong Kong, China. Association for Computa-tional Linguistics. Michael Heilman and Noah A. Smith. 2010. Good question! statistical ranking for question genera-tion. In Human Language Technologies: The 2010 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans-former. arXiv e-prints. Annual Conference of the North American Chap-ter of the Association for Computational Linguistics, pages 609-617, Los Angeles, California. Associa-tion for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on</cell><cell>Pang Wei Koh and Percy Liang. 2017. Understand-Trieu H. Trinh and Quoc V. Le. 2018. A sim-ing black-box predictions via influence functions. In ple method for commonsense reasoning. ArXiv, ICML, volume 70 of Proceedings of Machine Learn-abs/1806.02847. ing Research, pages 1885-1894. PMLR. Tianyang Wang, Jun Huan, and Bo Li. 2018. Data dropout: Optimizing training data for convolutional neural networks. 2018 IEEE 30th International Con-ference on Tools with Artificial Intelligence (ICTAI), pages 39-46.</cell></row><row><cell>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. Neural Computation, 9:1-42. Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics. Choi. 2020. The curious case of neural text degener-ation. International Conference on Learning Repre-sentations. Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, Svetlana Stoyanchev, and Cristian Moldovan. 2011. Question generation shared task and evaluation chal-</cell><cell>William Yang Wang and Diyi Yang. 2015. That's so an-noying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic cat-egorization of annoying behaviors using #petpeeve tweets. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2557-2563, Lisbon, Portugal. Association for Computational Linguistics.</cell></row><row><cell>Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. lenge -status report. In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation. Association for Computational Linguistics.</cell><cell>Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proc. of EMNLP-</cell></row><row><cell>In Proceedings of the 2018 Conference of the North</cell><cell>IJCNLP.</cell></row><row><cell>American Chapter of the Association for Computa-Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-</cell><cell></cell></row><row><cell>tional Linguistics: Human Language Technologies, vatula, and Yejin Choi. 2020. WINOGRANDE: An</cell><cell></cell></row><row><cell>Volume 1 (Long Papers), pages 1875-1885, New adversarial winograd schema challenge at scale. In</cell><cell></cell></row><row><cell>Orleans, Louisiana. Association for Computational Thirty-Fourth AAAI Conference on Artificial Intelli-</cell><cell></cell></row><row><cell>Linguistics. gence (AAAI), New York, USA.</cell><cell></cell></row></table><note>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R'emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Where can I stand on a river to see water falling without getting wet? A: waterfall /s WINOGRANDE /s Feeling a draft, William asked Neil to please close the front door because was closer. /s Neil /s CODAH /s I am always very hungry before I go to bed. I am /s concerned that this is an illness. /s HellaSwag-2K /s A man is on a sandy beach, playing croquette. he /s is parasailing, making a random move. /s SNLI-3KPREM Five black dogs run in a field. /PREM ANS entailment /ANS HYP Some animals running. /HYP ARC-Challenge Q: Which of the following is an example of a physical change? A: breaking a glass /s</figDesc><table><row><cell>Task</cell><cell>Format</cell></row><row><cell>CSQA</cell><cell>Q:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Input formats for GPT-2. "Q:" and "A:" are the prefix for a question and a candidate answer (choice). Where can I stand on a river to see water falling without getting wet? /s A: waterfall /s WINOGRANDE s Feeling a draft, William asked Neil to please close the front door because was closer. /s Neil /s CODAH s I am always very hungry before I go to bed. I am /s concerned that this is an illness. /s</figDesc><table><row><cell>Task</cell><cell>Format</cell></row><row><cell cols="2">CSQA s Q: HellaSwag-2K s A man is on a sandy beach, playing croquette. he /s is parasailing, making a random move. /s</cell></row><row><cell>SNLI-3K</cell><cell>s Five black dogs run in a field. /s Some animals running. /s</cell></row><row><cell>ARC-Challenge</cell><cell>s Q: Which of the following is an example of a physical change? /s A: breaking a glass /s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Input formats for ROBERTA. "Q:" and "A:" are the prefix for a question and a candidate answer</cell></row><row><cell>(choice).</cell></row><row><cell>Note that L(X train , θ) can be rewritten as the</cell></row><row><cell>following weighted average form to incorporate a</cell></row><row><cell>new training example x new :</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameter settings for finetuning GPT-2. "q/a/d" stands for "question/answer/distractor". Some hyperparameters for WINOGRANDE is shown in a separate table as they vary with the train size.</figDesc><table><row><cell>Hyperparam</cell><cell>XS</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameter settings for finetuning GPT-2 on WINOGRANDE.</figDesc><table><row><cell></cell><cell>Test AUC</cell></row><row><cell>Baseline</cell><cell>67.5</cell></row><row><cell cols="2">Baseline + Generator 67.5</cell></row><row><cell>G-DAUG c -Combo</cell><cell>71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameter settings for finetuning ROBERTA. Some hyperparameters for WINOGRANDE are shown in a separate table as they vary with the training set size.</figDesc><table><row><cell>Hyperparam</cell><cell>XS</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell></row><row><cell>LR</cell><cell cols="5">1e-5 1e-5 1e-5 1e-5 1e-5</cell></row><row><cell>Epochs</cell><cell>10</cell><cell>8</cell><cell>5</cell><cell>5</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Hyperparameter settings for finetuning ROBERTA on WINOGRANDE.</figDesc><table><row><cell>Hyperparam</cell><cell cols="6">CSQA WINOGRANDE CODAH HellaSwag-2K SNLI-3K ARC-Challenge</cell></row><row><cell>Synthetic Data Size</cell><cell>50K</cell><cell>∼ 50K-130K 11</cell><cell>100K</cell><cell>50K</cell><cell>100K</cell><cell>50K</cell></row><row><cell>LR (synthetic)</cell><cell>5e-6</cell><cell>5e-6</cell><cell>5e-6</cell><cell>5e-6</cell><cell>5e-6</cell><cell>5e-6</cell></row><row><cell>Epochs (synthetic)</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Additional hyperparameter settings for G-DAUG c Two-Stage Training. For finetuning on the original data, we use the same settings as ROBERTA (except for CSQA and HellaSwag-2K, where we find reducing 2 epochs gives significantly better results). For Winogrande, we generate 400K examples before the rejection procedure (seeAppendix A). The examples retained after the rejection procedure approximately ranges from 50K-130K depending on the training size.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/yangyiben/G-DAUG-c-Generative-Data-Augmentation-for-Commonsense-Reasoning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">W1:0 denotes an empty sequence 3 Specific modifications for other tasks, e.g. textual entailment, are discussed in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We used the HuggingFace library(Wolf et al., 2019). 7 https://github.com/google-research/ uda/ 8 These results are state-of-the-art for our model class; higher scores have been obtained using a T5 model with roughly an order of magnitude more parameters than ours.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">G-DAUG c -Combo utilizes a larger pool, so it is not comparable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by NSF Grant IIS-1351029. We thank Iz Beltagy, Jonathan Bragg, Isabel Cachola, Arman Cohan, Mike D'Arcy, Daniel King, Kyle Lo, and Lucy Lu Wang for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<title level="m">Naama Tepper, and Naama Zwerdling. 2020. Not enough data? Deep learning to the rescue! In Proc. of AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin Jose Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Residuals and influence in regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Dennis</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanford</forename><surname>Weisberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Toledo</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/1944345.1944349</idno>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Asli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<title level="m">Adversarial filters of dataset biases</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CODAH: An adversarially-authored question answering dataset for common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Arcy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 3rd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Minneapolis, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="63" to="69" />
		</imprint>
	</monogr>
	<note>Jared Fernandez, and Doug Downey</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>abs/1803.05457</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Verb physics: Relative physical knowledge of actions and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="266" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgeting in gradientbased neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1312.6211</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/2003.02245</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
			<affiliation>
				<orgName type="collaboration">KR</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating natural language questions to support learning on-line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Popowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Winne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation</title>
		<meeting>the 14th European Workshop on Natural Language Generation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-based formers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation for consistency training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting commonsense properties from embeddings with limited human guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiben</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="644" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyhessian: Neural networks through the lens of the hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno>abs/1912.07145</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SWAG: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Paragraph-level neural question generation with maxout pointer and gated self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1424</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3901" to="3910" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01792</idno>
		<title level="m">Neural question generation from text: A preliminary study</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
