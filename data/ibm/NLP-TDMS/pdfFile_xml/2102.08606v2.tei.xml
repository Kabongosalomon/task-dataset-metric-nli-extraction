<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Centroid Transformers: Learning to Abstract with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
							<email>lmwu@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
							<email>xcliu@utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Centroid Transformers: Learning to Abstract with Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs.</p><p>In essence, what self-attention does is to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between the input pairs. As a result, it maps N inputs to N outputs and casts a quadratic O(N 2 ) memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs (M ≤ N ), such that the key information in the inputs is summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals a underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and image processing. Empirical results demonstrate the effectiveness of our method over the standard transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> has emerged to be one of the most important neural architectures and has achieved remarkable successes on various tasks such as language modeling <ref type="bibr" target="#b14">(Irie et al., 2019;</ref><ref type="bibr" target="#b15">Jiao et al., 2020)</ref>, machine translation <ref type="bibr" target="#b31">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b41">Zhang et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019a)</ref>, computer vision <ref type="bibr" target="#b2">(Carion et al., 2020;</ref><ref type="bibr" target="#b6">Dosovitskiy et al., 2020)</ref>, and many others.</p><p>What makes transformers unique is the extensive usage of the self-attention mechanism <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. A self-attention block is placed in each stage of the transformer to gather information globally from the input sequence. A self-attention module takes in N inputs, and returns N outputs of the same size. For each element in the input, it assigns an attention weight to every other element in the input to find out who it should pay more attention to, and perform a weighted sum to aggregate the information from the relevant inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Equal contribution</head><p>Intuitively, the self-attention modules can be viewed as conducting interactive reasoning, inferring the pairwise interacting relations between the elements of inputs and propagating information between pairs of elements. Naturally, a key drawback of the pairwise interaction is that it casts an O(N 2 ) memory and time complexity, where N is the number of input elements, making it a major computational bottleneck in transformers. This necessitates an emerging line of research on approximating self-attention modules to gain higher computational efficiency <ref type="bibr" target="#b17">(Kitaev et al., 2019;</ref><ref type="bibr" target="#b16">Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b34">Wang et al., 2020)</ref>.</p><p>In this work, we develop a variant of attention module for conducting summative reasoning rather than interactive reasoning. Our goal is to take N input elements and return a smaller number M of outputs (M ≤ N ), such that the key information in the N inputs are summarized in the M outputs. If N = M , our new module reduces to standard self-attention (except an extra skip connection link). However, by setting M &lt; N , we creates an information bottleneck and enforce the network to filter out the useless information (i.e., dimension reduction), and also improve the computational cost from O(N 2 ) to O(N M ). In addition, once the number of inputs is reduced, the complexity of the subsequent modules/layers is also reduced accordingly (e.g., applying self-attention on the output will yield a O(M 2 ) complexity).</p><p>Intuitively, we hope that the M output elements are representative of the N inputs. This can be viewed as a clustering process in which we find M "centroids" for the inputs. Our new module, which we call centroid attention layer, is obtained by "amortizing" the gradient descent update rule on a clustering objective function. It exploits a novel connection between self-attention and clustering algorithms: we write down a clustering objective function on the input data under a trainable similarity metric, and derive its gradient descent update rule; we then observe that the gradient descent update yields a generalization of self-attention layer and use it to motivate the design of our new module.</p><p>Using our new modules, we build centroid transformers, in which we insert our centroid attention modules between typical self-attention modules. We apply centroid transformers on several challenging scenarios, ranging from natural language processing to computer vision. On ab- <ref type="figure">Figure 1</ref>: The vanilla transformer (a) which maps N inputs to N outputs; and our centroid transformer (b) which summarizes N inputs into M "centroid" outputs (M ≤ N ) to save computational cost and filter out useless information simultaneously. stractive text summarization, centroid transformer beats the vanilla transformer with about only 50% computational cost. On 3D point cloud classification and image classification tasks, centroid transformer achieves substantially higher accuracy as well as computational efficiency compared with the state-of-the-art transformer networks. We also use centroid attention to replace the dynamic routing module in the 3D Capsule Network <ref type="bibr" target="#b42">(Zhao et al., 2019)</ref> for point cloud reconstruction, which we find yields lower construction error, reduced storage consumption, and more semantically meaningful latent representation.</p><formula xml:id="formula_0">Self- Attention ( ! ) Input Sequence Output Sequence × × Self- Attention ( ! ) Input Sequence × Centroid Attention ( ) Centroids × Self- Attention ( ! ) Output Sequence × (a) Transformer (b) Centroid transformer Self- Attention ( ! ) Self- Attention ( ! ) MLP MLP MLP MLP MLP MLP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CENTROID TRANSFORMER</head><p>We first introduce the standard self-attention mechanism used in vanilla transformers in Section 2.1, and then derive the centroid attention mechanism by drawing inspiration from the gradient descent update of a clustering objective function and discuss the related centroid transformer in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SELF-ATTENTION MECHANISM</head><formula xml:id="formula_1">Let {x i } N i=1 ∈ R N</formula><p>×d be a set of input vectors, where we may view each vector x i as a data point or "particle". Each x i can be a word embedding vector in natural language processing, an image patch in computer vision, or a point in 3D point cloud. A self-attention module can be viewed as updating {x i } in parallel via</p><formula xml:id="formula_2">x i ← x i + N j=1 L =1 sim (x i , x j ) × v (x j ), ∀i ∈ [N ]</formula><p>(1) Here, each sim (·, ·) is a similarity function and v (x i ) ∈ R d is a value function; each is considered to be a head in the attention and process a specific aspect of the inputs; is a positive constant.</p><p>Intuitively, the self-attention module evaluates the similarity (or attention score) between pairs of input vectors, and updates each vector with the sum of the inputs weighted by the similarity scores. In practice, the similarity function is often defined as</p><formula xml:id="formula_3">sim (x i , x j ) = exp(Q (x i )K (x j )) N k=1 exp(Q (x i )K (x k )) ,<label>(2)</label></formula><p>where Q (x) and K (x) are the "query" and "key" functions of the -th head. Eq. <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_3">(2)</ref> illustrate how the features are processed by one head. A complete transformer encoder with T layers is the composition of selfattention layers and multi-layer perceptrons (MLPs); see <ref type="figure">Figure 1</ref>. Obviously, the time and memory complexities of self-attention are quadratic on N , i.e., O(N 2 ), which form the key computational bottleneck when applying self-attention on long sequences and large batches of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CENTROID ATTENTION</head><p>Self-attention updates the inputs {x i } based on their interacting relations, obtaining an output of the same size.</p><p>In this work, we propose a mechanism that maps the N inputs</p><formula xml:id="formula_4">{x i } n i=1 to M output vectors {u i } M i=1 , M ≤ N , such that each u i ∈ R d can be viewed as a centroid of the inputs {x i } N i=1</formula><p>. Namely, we hope the module to be able to effectively perform a clustering-like operation on the inputs, where</p><formula xml:id="formula_5">{u i } M i=1 is a compression of {x i } M i=1</formula><p>while inheriting the key information.</p><formula xml:id="formula_6">Let {φ (x i , u j ) : ∀ ∈ [L]</formula><p>} be a set of measures of similarity between centroid u j and input x i . Ideally, we may want to construct the centroids by optimizing the following general "soft K-means" objective function for clustering,</p><formula xml:id="formula_7">{u * j } = arg max {uj } L =1 N i=1 1 α log   M j=1 exp (αφ (x i , u j ))   .</formula><p>(3) Here, α &gt; 0 is a positive coefficient. If α → +∞ and L = 1, then the objective reduces to i max j φ 1 (x i , u j ), which coincides with the typical k-means objective function when −φ 1 (·, ·) is a distance measure. In general, the objective (3) obtains centroids to represent the inputs based on multiple similarity functions {φ }.</p><p>By solving the optimization with gradient descent, we can unroll (3) into an iterative update of form:</p><formula xml:id="formula_8">Initialization: {u 0 j } M j=1 = I({x i } N i=1 ) For t = 1, 2, . . . , T , u t+1 j ← u t j + L =1 N i=1 sim (x i , u t j )V (x i , u t j ),<label>(4)</label></formula><p>where I(·) denotes a mechanism for initializing the centroids and each of the following T steps conducts gradient descent of (3), with</p><formula xml:id="formula_9">sim (x i , u j ) = exp(αφ (x i , u j )) M k=1 exp(αφ (x i , u k )) , V (x i , u j ) = ∇ uj φ (x i , u j ).</formula><p>Clearly, the gradient update above can be interpreted as a multi-head attention mechanism, with sim (·, ·) and V (·, ·) being the similarity function and the value function of the -th head. The initialization I(·) together with the T steps of attention like updates in (4) above form a centroid attention module. See <ref type="figure">Figure 2</ref> for an illustration.</p><p>In practice, we find it works well to set = 1/T by default. Moreover, in settings when computational efficiency is important, we set T = 1 for a good performanceefficiency trade off. The initialization step can vary in different tasks: we can, for example, draw {u 0</p><formula xml:id="formula_10">j } M j=1 from {x i } N i=1</formula><p>by random sampling without replacement or farthest point sampling; we can also directly define I(·) to be a trainable fully connected or convolution layer. See more discussion in the experiment section.</p><p>Although both sim (·, ·) and V (·, ·) are determined by φ following the derivation above. In practice, we can define them separately in more flexible forms based on practical needs. For example, we may define sim (·, ·) by set-</p><formula xml:id="formula_11">ting φ(x i , u j ) = Q (u j )K(x i ) as typical self-attention, while setting V (x i , u j ) with a separate trainable value function.</formula><p>Our module includes self-attention as a special case when we i) set M = N , ii) initialize {u i } to be the same as x i , and iii) iterate for one step (T = 1). Therefore, our derivation also reveals a close connection between gradientbased clustering and self attention. Note that <ref type="bibr" target="#b24">Ramsauer et al. (2020)</ref> discusses a connection between Hopfield update rule and self-attention, which is related but different from our perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KNN Approximation</head><p>The computational cost of the attention is O(N M ), which can still be expensive if N and M are large, To further save the computational cost, we apply a K-nearest neighbor (KNN) approximation to (4), yielding</p><formula xml:id="formula_12">u t+1 j ← u t j + L =1 i∈N j,k sim (x i , u t j )V (x i , u t j ), where N j,k denotes the K-nearest neighbor of u j in- side {x i } N i=1 , that is, N j,k = {i (1) , . . . , i (k) }, where {i (1) , . . . , i (n) } is the ascending reordering of {1, . . . , n} according to some distance metric d(x i , u j ). In practice, we define the distance by d(x i , u j ) = ||x i − u j || 2 .</formula><p>As shown in our experiments, the KNN approximation is particularly useful for the point cloud classification task, in which the length of the inputs elements is N = 1024, which is beyond the capacity of our GPU memory even for a single data point (i.e., a batch size of 1). <ref type="figure">Figure 2</ref>: (a) The self-attention module, which modifies the input sequence {x i } into {x i } by updating them with pairwise interactions (see Eq <ref type="formula">(1)</ref>). (b) The centroid attention module, which transforms the input sequence {x i } into a set of centroids {u i } by first initializing the centroids and then updating them by interacting with the elements in the inputs (see Eq <ref type="formula" target="#formula_8">(4)</ref>).</p><formula xml:id="formula_13">(a) Self-Attention (b) Centroid Attention ! " # $ ! $ " $ % $ # % # ! " # % ! times</formula><p>Centroid Transformer As shown in <ref type="figure">Fig. 1</ref>, we construct a centroid transformer architecture by alternating between typical self-attention blocks and centroid attention blocks. This allows us to gradually abstract the input sequence to increasingly short representations, which filters out useless information and simultaneously saves the computation cost. As vanilla transformer, we insert fully connected MLP layers with residual links between the attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We apply our centroid attention block on different kind of tasks with transformer or capsule structures to show our powerful abstract ability as well as the computational efficiency. We show that our centroid transformer structure has a strong performance in various of tasks including text summarization, point cloud classification, point cloud reconstruction and image classification. On all the tasks, we outperform baseline transformer with lower computational and storage cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ABSTRACTIVE TEXT SUMMARIZATION</head><p>We test centroid transformer on abstractive text summarization, a classic task in natural language processing. The task is to provide a short summary text (in several words) for a paragraph or a long sentence.  <ref type="table">Table 1</ref>: Results on Gigaword text summarization task (MBS=Maximal Batch Size, MP = Mean-Pooling). The MACs (Multiply-add ACcumulation) is only computed for the encoder, assuming the length of sequence is 45 (the maximal length of sequence in the dataset). Though centroid transformer with random initialization (Ours-Random) performs worse than the baseline, centroid transformer with mean-pooling being the initialization method (Ours-MP) yields the best ROUGE score (See <ref type="bibr" target="#b19">(Lin, 2004)</ref> for its definition) with 50% computational cost compared to the original transformer.</p><p>evaluates the quality of unigrams/bigrams/whole sentence in the generated summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We construct our centroid transformer by replacing the second self-attention module in the baseline transformer encoder with our centroid attention module.</p><p>M is set to N/3 so that our centroid attention module compresses N input points into N/3 centroids. The rest of the parts are kept unchanged. When decoding, the cross-attention is applied between the generated sequence and the centroids. We test two initialization schemes for the centroid attention, random sampling and mean pooling. Random sampling means we randomly sample N/3 elements from the original sequence as the initial centroids, while mean pooling refers to apply mean pooling on every three elements.</p><p>Our baseline transformer follows the original encoderdecoder structure in <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, with 4 layers in both encoder and decoder. We use a word embedding with 512 dimensions. All the models are trained for 30 epochs with Adam optimizer and the same learning rate. The number of warm-up steps is set to 4,000. At decode time, beam search with size 10 is exploited to generate the summary. Both self-attention and cross-attention is enabled in the decoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of the experiments is shown in Table. 1. Centroid transformer with mean-pooling initialization yields the highest score in all three metrics. Moreover, our centroid transformer reduces 50% MACs against the original transformer on the encoder side. For the same Titan XP GPU, centroid transformer takes less storage space, allowing 38 more samples in a single batch. We also find the initialization scheme to be important. Random sampling scheme results in a centroid transformer that is worse than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">POINT CLOUD CLASSIFICATION</head><p>We apply our centroid transformer on point cloud classification. Point cloud data usually has over 1,000 points as input, which is a long sequence and hard to directly train with an original transformer. By applying our centroid transformer, we can gradually aggregate the thousands points into several abstract clusters, which brings powerful feature extraction ability and saves computational cost. We compare our method with several baselines including the state-of-the-art attention based method SepNet-W15 (Ran &amp; Lu, 2020) on ModelNet40 dataset <ref type="bibr" target="#b36">(Wu et al., 2015)</ref>. We outperform the existing methods on classification accuracy and consume much less resource comparing with the attention based method SepNet-W15.</p><p>Data We use ModelNet40 as our benchmark, which has 40 classes with 9843 training shapes and 2468 test shapes. Consider that many of the ModelNet40 classification works use their specifically processed data, for a fair comparison, we use the same data as <ref type="bibr" target="#b22">(Qi et al., 2017a)</ref>, which is an uniformly sampled point cloud data from ModelNet40 raw data without any other specific normalization and preprocess.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We design a transformer architecture with 4 attention blocks and a 3-layer MLP classification head as <ref type="figure" target="#fig_0">Figure 3</ref> shows. The first centroid attention block cluster N points to N/2 and the second one abstract N/2 to N/8. We use Farthest Point Sampling (FPS) as our initialization function. The dimension of the 4 attention locks is 64-128-128-512. We set K = 40 in all the KNN masks and the KNN is calculated in feature space in the centroid attention blocks. After we extract features, we fuse the features in each layer by upsampling their number of points to N using nearest-neighbor sampling . Then we concatenate all the layers' feature together and pass through a fully connected layer. The output feature will pass through a max pooling and a average pooling layer separately. At last, we concatenate them together as the final feature of the input point cloud. The feature will pass a 3-layer MLP to get the final classification score. All the activation functions in the network are LeakyReLU with a negative slope of 0.2. We train the network with Adam optimizer start with 1e −3 learning rate and decay by 0.7 every 20 epochs for 250 epochs in total.</p><p>Models Acc MACs(G) Params(M) PointNet <ref type="bibr" target="#b22">(Qi et al., 2017a)</ref> 89.2 0.3 0.8 PointNet++ <ref type="bibr" target="#b23">(Qi et al., 2017b)</ref> 91.9 7.9 12.1 DGCNN <ref type="bibr" target="#b35">(Wang et al., 2019b)</ref> 92   <ref type="table">Table 3</ref>: Results on ModelNet40 of centroid attention when using different numbers of iterations T and different initialization strategies (see Eq <ref type="formula" target="#formula_8">(4)</ref>).</p><p>Result From <ref type="table" target="#tab_2">Table 2</ref>, we can see that our model outperforms all the baseline on classification accuracy on ModelNet40. In addition, when comparing with SepNet-W15, our model can achieve a higher accuracy with only 1/4 MACs and fewer parameters. <ref type="table">Table 3</ref>, we show the result of our method using different T in centroid attention blocks and compare them with other downsampling strategies. For the different T , the performance do not have a big difference. However, larger T means more computation operations, in practice, we choose T = 1 for a performanceefficiency balance. For the downsampling strategies, we compare our model with Random Sampling, Farthest Point Sampling and K-means. We apply K-means for 3 iterations to guarantee the computation complexity is comparable with our centroid attention. Comparing with farthest point sampling strategy, our speed only slow a little bit while getting a 0.8 improvement in the classification accuracy. Comparing with the random sampling, we have a 1.5 accuracy improvement. In addition, we are 0.9 better than K-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study In</head><p>Visualization We visualize our second centroid attention blocks' feature cluster in <ref type="figure">Figure 4</ref>. We plot the sampled point in white star and its K-Nearest-Neighbour (KNNs) in red point. We set K = 40 and use L2 distance in KNNs. From <ref type="figure">Figure 4</ref> we can see that rather than gathering around the sampled point in the 3D space, the KNNs in our features space tend to distribute within the same se-mantic part of the query part, which indicates our feature captures high-level concept of the shapes. <ref type="figure">Figure 4</ref>: Learning classification on ModelNet40 with centroid transformer. We visualize the K-nearestneighbours (KNNs) points of some sampled points (white stars). For the KNNs' distance, for ours, we use the L2 distance in feature space learned in the second centroid attention blocks and for 3D space, we use the 3D euclidean distance. Here, K = 40, The red points indicates the KNNs points of the sampled white point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">POINT CLOUD RECONSTRUCTION</head><p>We further test our centroid attention block on point cloud reconstruction task and make a visualization plot to illustrate our layer's efficiency and abstract ability. We use ShapeNet part and ShapeNet Core13 <ref type="bibr" target="#b3">(Chang et al., 2015)</ref> as our dataset and use 3D Capsule Network <ref type="bibr" target="#b42">(Zhao et al., 2019)</ref> as our backbone. We replace the Dynamic Routing module in the original capsule network with our centroid attention block to abstract the information from the prime capsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>We follow the 3D Capsule Network <ref type="bibr" target="#b42">(Zhao et al., 2019)</ref> settinga and construct an autoencoder to reconstruct the 3D point cloud with 2048 points. We set up two model scales with 12 and 64 latent caspules. In our centroid attention block, we treat the prime capsules as our input sequence and latent capsules as the abstract sequence. We use a linear projection to learn the initialization of the latent capsules. We set T = 3 to match number of iterations in Dynamic Routing module.</p><p>For the 3D Capsule Network, we setup two network sizes, the small one contains 512 prime capsules with 16 dimensions and 12 latent capsules with 128 dimensions. The larger one contains 1024 prime capsules with 16 dimensions and 64 latent capsules with 64 dimensions. The 64 latent capsuls setting keeps the same with the original setting in 3D capsule network. We train small setting for 200 epochs using ShapeNet Part dataset and base setting for 300 epochs using ShapeNet Core13 dataset. The other hyperparameter keeps the same as <ref type="bibr" target="#b42">Zhao et al. (2019)</ref>.  <ref type="figure">Figure 5</ref>: Point cloud reconstruction using 3D Capsule Network <ref type="bibr" target="#b42">(Zhao et al., 2019)</ref> as backbone, with 12 latent capsules. We decode the 12 latent capsules one by one and highlight the corresponding decoded points with bright colors while keep the other parts gray. (a) shows the latent capsules learned by replacing the original dynamic routing module with centroid attention blocks, which can capture semantically meaningful parts of the plane, and are grouped into three clusters that represents plane body, plane front and back wings, and middle engine, respectively. (b) shows the capsules learned by dynamic routing in <ref type="bibr" target="#b42">(Zhao et al., 2019)</ref>, which distribute randomly and yield no semantic meanings.</p><p>(a) Capsules of our method on a table (b) Capsules of our method on a car <ref type="figure">Figure 6</ref>: More visualization of capsules learned by our method. In both (a) and (b), the capsules map to semantically meaningful parties of the inputs.</p><p>Result From <ref type="table">Table 4</ref> we can see that in the 12 latent capsules setting, our model greatly outperforms the Dynamic Routing module to abstract the latent capsule and leads a Chamfer Distance 1.6 × 10 −3 comparing with 2.7 × 10 −3 in baseline. When comparing with Dynamic Routing in 64 latent capsules setting, which is same as the oringal paper, we still has 10 −3 better Chamfer Distance score comparing with baseline. The results indicate the Dynamic Routing module fails to abstract many prime capsules into a small amount of latent capsules. On the contrast, our centorid transformer can aggregate information no matter the output length is short or long.</p><p>Further, our centroid attention block has a much smaller parameter size as well as a larger max batch size per GPU.</p><p>Tjos means we can process more data in the same time and lead a much faster training speed comparing with the capsule network using Dynamic Routing. This roughly leads a 3× training speed boosting when training with same amount of GPUs.</p><p>Visualization We further visualize the decoded points from each latent capsule. To clearly show the semantic meaning, we use 12 latent capsules to plot the reconstruction visualization, in which each capsules can decode into 170 points. Each time, we highlight the decoded points from specific capsules in bright colors and keep the rest of the reconstruction points in gray. In <ref type="figure">Figure 5 (a)</ref>, we show our learned latent capsules can capture semantic meanings and distribute on the specific part of the plane  <ref type="table">Table 4</ref>: Comparison between the performance of dynamic routing and our centroid attention method under different latent capsules number settings (MBS/GPU = Maximum Batch Size per GPU, CD = Chamfer Distance).</p><p>Here the Chamfer Distance is a metric for evaluating the quality of reconstruction.</p><p>shape. If we group several capsules together, we can further get the whole semantic parts. <ref type="figure">Figure 5 (b)</ref> shows the decoded points learned by Dynamic Routing. We can find that the reconstruction shape is in a low quality and the highlight points are in a random distribution across the whole shape. That means Dynamic Routing failed to learn a meaningful latent capsules under this setting. We plot two more visualization using our centroid attention blocks to show the result of different shapes with different number of semantic parts. <ref type="figure">Figure 6</ref> (a) clearly shows that the table are decomposed into the surface and legs. Furthermore, <ref type="figure">Figure 6</ref> (b) decomposes the car into body, surrounding, wheel, roof and door parts, these illustrate our centroid attention's strong ability in clustering the semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">VISION TRANSFORMER</head><p>We apply our centroid transformer structure on ImageNet <ref type="bibr" target="#b5">(Deng et al., 2009</ref>) classification task using vision transformer to show our capacities among various transformer with large-scale data. We choose DeiT <ref type="bibr" target="#b30">(Touvron et al., 2020)</ref> as our baseline, which is the state-of-the-art vision classification transformer structure on ImageNet dataset.</p><p>By applying centroid attention block in one specific layer, we build up our centroid transformer as <ref type="figure" target="#fig_2">Figure 7</ref> shows. Our transformer abstract the image patches sequence into a shorter one in the middle, which reduces the computational cost comparing with the original DeiT. Further, we allow some overlaps in the first convolution layer to create a longer initial token sequence with richer represent ability.</p><p>We compare our centroid transformer with DeiT-tiny and DeiT-small, which are two vision transformers with different model scales. We also apply two efficient transformers, Set Transformer <ref type="bibr" target="#b18">(Lee et al., 2019)</ref> and Linformer  on DeiT as baseline to compare the performance under an energy efficient setting.</p><p>Experiment Setup In the overlap setting, we set the first convolution layer with kernel size 16, stride 14, and padding 1. This can create a patch sequence with 256 tokens. In the centroid attention block, we first initialize our shorter sequence by reshaping the N length patch tokens into a √ N × √ N order. We then apply a depth-wise  <ref type="table">Table 5</ref>: Architecture design of different models for image classification tasks on ImageNet, centroid @ means replacing the self-attention block with the centroid attention block at specific layer. In comparison, thanks to the ability of down-sampling, the centroid transformer with comparable computational cost can take a larger number of overlapping patches in the early stage and hence capture more information from the input images.</p><p>convolution layer with kernel size 3, stride 2, padding 1 to downsample the input tokens into √ N /2 × √ N /2 and reshape it back to N/4 tokens as the initialization. We set T = 1 for a better performance-efficiency trade off. The rest of the training setting are the same as DeiT. We setup different model scales. The overall architectures design is listed in <ref type="table">Table 5</ref>. For fast transformer baselines, we set Linformer's project dimension k = 128 and Set Transformer's induce point set length m = 128.</p><p>Result Our results are shown in Tabel 6. In "Ours-1" and "Ours-4" setting, by replacing one self-attention blocks into centroid attention block, our model has only 64% MACs comparing the original DeiT-tiny and DeiT-small while only drop the Top-1 Accuracy by 0.4 and 0.3. Further, when we allow a longer initial sequence by overlapping the image patchs and adjust the centroid attention blocks to the fifth layer, the centorid block"Ours-2" and "Ours-5" achieves a same and 0.2 higher top-1 with 81% MACs. When we further increase the MACs by adjusting the centroid attention block's layer, we can get larger models "Ours-3" and "Ours-6" with the close MACs comparing with DeiT-tiny and DeiT-small baseline and gains 1.2 and 1.0 performance improvement.</p><p>Whatsmore, when we compare centroid transformer with other fast transformers, we outperform set transformer by over 5.0 and linformer by over 2.0 Top-1 accuracy with smaller MACs. Our good performance comparing with these fast transformers further shows our structures' powerful ability and efficiency to handle even this kind of large-scale image classification task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>Fast Transformers A line of recent works have been developed to approximate self-attention layers to improve over the O(N 2 ) complexity. These methods keep the basic design of self-attention, mapping N inputs to N outputs, and is hence different in purpose with our method, which compresses N inputs to a smaller number M of outputs. Among these works, Reformer <ref type="bibr" target="#b17">(Kitaev et al., 2019)</ref>   <ref type="bibr" target="#b18">(Lee et al., 2019)</ref> reduces the computation by introducing a set of induced points, which plays a role similar to centroids, only serve as the intermediary layer between N inputs and N outputs. Another related work is Routing Transformers <ref type="bibr" target="#b25">(Roy et al., 2020)</ref>, which introduces a sparse routing module based on online k-means, reducing the complexity of attention to O(n 1.5 ).</p><p>PoWER-BERT <ref type="bibr" target="#b8">(Goyal et al., 2020)</ref> also reduces the number of tokens gradually to improve the efficiency of BERT. However, their work fundamentally differs from ours from three aspects: (1) Their work is motivated by the phenomenon of information diffusion in BERT specifically, which may not be the case in other transformers.</p><p>(2) Their work focus on select a subset of tokens from the original sequence, while the emphasis of our work is summarize the information into several centroids. This leads to completely distinct structure design.</p><p>(3) Their scoring function is human-designed. In contrast, we start from clustering algorithm, and derives a novel connection between gradient-based clustering and attention.</p><p>Capsule Networks Similar to our method, capsule networks <ref type="bibr" target="#b10">(Hinton et al., 2011)</ref> are also based on the idea of "building clustering algorithms into neural networks". Different from our method, which is based on amortizing gradient-based clustering algorithms, Dynamic routing and EM routing <ref type="bibr" target="#b26">(Sabour et al., 2017;</ref><ref type="bibr" target="#b32">Wang &amp; Liu, 2018;</ref><ref type="bibr" target="#b11">Hinton et al., 2018)</ref> in capsule networks are based on amortizing EM like algorithms. However, unlike our method, dynamic routing and EM routing are not trainable modules and hence not as efficient as our method in extracting data information. In addition, our method does not need to store the pairwise assignment information like dynamic/EM routing and hence reduces the runtime space consumption.</p><p>Adaptive Down-sampling At a high level, our method can be viewed as an attention-like mechanism for adaptively down-sampling the inputs, which forms a key building block for deep neural networks in computer vision. In convolutional neural networks, various techniques have been proposed, including fixed strategies such as pooling <ref type="bibr" target="#b28">(Simonyan &amp; Zisserman, 2014)</ref>, strided convolution <ref type="bibr" target="#b29">(Springenberg et al., 2014)</ref>, dilated convolution <ref type="bibr" target="#b40">(Yu &amp; Koltun, 2015)</ref>, learnable down-sampling techniques such as Local-importance based pooling <ref type="bibr" target="#b7">(Gao et al., 2019)</ref>, deformable convolution <ref type="bibr" target="#b4">(Dai et al., 2017)</ref>, and trainable Region-of-Interest (ROI) Pooling <ref type="bibr" target="#b9">(Gu et al., 2018)</ref>. In addition, <ref type="bibr" target="#b21">Nezhadarya et al. (2020)</ref> proposes an adaptive down-sampling layer for point cloud classification. RandLA-Net <ref type="bibr" target="#b13">(Hu et al., 2020)</ref> uses random-sampling to process large-scale point cloud. Comparing these methods, which mostly focus on convolutional neural networks (CNNs), our method provides a general and basic adaptive down-sampling strategy for transformers, which is expected to find important applications as the counterparts in CNNs.</p><p>Metric Learning for Clustering A line works have been developed to learn domain-specific similarity functions to boost the performance of the clustering algorithms based on the learned metrics <ref type="bibr" target="#b38">(Yang et al., 2016</ref><ref type="bibr" target="#b37">(Yang et al., , 2017</ref><ref type="bibr" target="#b12">Hsu et al., 2018;</ref><ref type="bibr" target="#b0">Aljalbout et al., 2018;</ref><ref type="bibr" target="#b39">Yang et al., 2019)</ref>. This yields a metric learning task. Our work is fundamentally different we use clustering to inspire the design of a new transformer architecture, and our goal is not to actually optimize the clustering quality for a specific problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose centroid attention, which performs summative reasoning for sequence modeling. Centroid attention takes the original sequence as input, and provides a shorter sequence of centroids that absorbs the information of the input sequence. We use ceontroid attention to construct centroid transformer. By using centroids for later stages, the computational and memory consumption of centroid transformers is significantly reduced against their full-length counterparts. Experiments conducted on text summarization, 3D vision and image processing demonstrates centroid transformers can yield similar / better performance over the original transformers with high efficiency.</p><p>A AN ENERGY VIEW OF SELF-ATTENTION We provide another view to draw connection between attention mechanism and learning to abstract with energy-based models. Let's first rewrite the self-attention operation in an energy-view. We start by defining the following energy function on the sequence {x i } N i=1 ,</p><formula xml:id="formula_14">E {x i } N i=1 = N i=1 N j=1 ζ(x i x j ),<label>(5)</label></formula><p>where ζ(·, ·) is a pairwise energy function. To find the sequence with the lowest energy, we can perform gradient descent yielding,</p><formula xml:id="formula_15">x i ← x i − N j=1</formula><p>∇ xi ζ(x i x j )x j , ∀i = 1, . . . , N.</p><p>Properly setting ζ(x i x j ) will recover the single-headed self-attention operation with v(x j ) = x j and a similarity function without the normalization denominator in Eq.</p><p>(2). In this sense, self-attention can be explained as one gradient step towards the sequence with the lowest energy.</p><p>The energy function above yields a fully observed pairwise energy function. The centroid attention can be viewed as corresponding to the energy function of restricted Boltzmann machine (RBM) in which {u j } M j=1 are viewed as hidden variables,</p><formula xml:id="formula_17">E {x i } N i=1 , {u j } M j=1 = N i=1 M j=1 ζ(u j x i ).<label>(7)</label></formula><p>Here, {x i } N i=1 are the visible variables, and {u j } M j=1 are the hidden variables. Given fixed visible variables, we can also find the hidden variables that minimizes the energy by gradient descent,</p><formula xml:id="formula_18">u j ← u j − N i=1 ∇ uj ζ(x i u j )x i , ∀j = 1, . . . , M.<label>(8)</label></formula><p>Therefore, centroid attention is like finding the most likely hidden variable for a given observed variable. Note that the derivation here is different from the one in the main text, because the similarity function is not normalized here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of the centroid transformer we used for point cloud classification. "CA" represents centroid attention and "SA" vanilla self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Capsules learned by our method (b) Capsules learned by dynamic routing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Comparing DeiT and centroid transformer for image inputs. Upper panel: DeiT partitions the input image into non-lapping patches and keeps the same number of patches throughout the layers, which may lose information of the image because the patches do not overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Data We use the annotated Gigaword corpus<ref type="bibr" target="#b25">(Rush et al., 2015)</ref> as our benchmark to compare different methods. The dataset is pre-processed with the tools provided by the authors. The corpus contains about 3.8 millions of training examples. The script also has 400,000 extra examples for validation and test. We randomly sample 2,000 examples for validation and test respectively, as in<ref type="bibr" target="#b20">Nallapati et al. (2016)</ref>. All the models are validated and tested with the same validation and test set.</figDesc><table><row><cell>Models</cell><cell cols="5">MACs(M) MBS/GPU ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>Transformer</cell><cell>523.2</cell><cell>192</cell><cell>32.987</cell><cell>15.286</cell><cell>30.771</cell></row><row><cell>Ours-Random</cell><cell>262.9</cell><cell>230</cell><cell>30.310</cell><cell>12.752</cell><cell>27.823</cell></row><row><cell>Ours-MP</cell><cell>262.9</cell><cell>230</cell><cell>34.651</cell><cell>16.468</cell><cell>32.415</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">performance of the generated summaries is measured</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">by Recall-Oriented Understudy for Gisting Evaluation</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(ROUGE) (Lin, 2004). ROUGE-1/ ROUGE-2/ROUGE-L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on ModelNet40 of our method and various of baselines. Here MACs denotes multiply-add cumulation and Params means the number of parameters in the model.</figDesc><table><row><cell>Method</cell><cell cols="2">Acc Data/sec</cell></row><row><cell>Ours (T=1)</cell><cell>93.1</cell><cell>60</cell></row><row><cell>Ours (T=2)</cell><cell>93.2</cell><cell>55</cell></row><row><cell>Ours (T=3)</cell><cell>93.2</cell><cell>52</cell></row><row><cell>Random Sampling</cell><cell>91.7</cell><cell>312</cell></row><row><cell cols="2">Farthest Point Sampling 92.3</cell><cell>66</cell></row><row><cell>K-means</cell><cell>92.3</cell><cell>54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Vision Transformer result compared with DeiT. Ours-x indicates different MACs setting of our model. MACs indicates multiply-add cumulation and Params means the number of parameters in the model.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07648</idno>
		<title level="m">Clustering with deep learning: Taxonomy and new methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip: Local importancebased pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power-bert: Accelerating bert inference via progressive word-vector elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chakaravarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3690" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04226</idno>
		<title level="m">guage modeling with deep transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling bert for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tinybert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attentionbased permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequenceto-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive hierarchical down-sampling for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nezhadarya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12956" to="12964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">Deep hierarchical feature learning on point sets in a metric space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<idno>arXiv:2011.14285</idno>
		<title level="m">Deeper or wider networks of point clouds with self-attention? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hopfield networks is all you need</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<idno>arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A neural attention model for abstractive sentence summarization</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09829</idno>
		<title level="m">Dynamic routing between capsules</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient attention: Attention with linear complexities</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An optimization view on dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01787</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to cluster faces on an affinity graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving the transformer translation model with document-level context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03581</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1009" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
