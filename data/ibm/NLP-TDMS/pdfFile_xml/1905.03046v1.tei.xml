<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PiNet: A Permutation Invariant Graph Neural Network for Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meltzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Braintree Ltd</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Daniel Gutierrez Mallea</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Braintree Ltd</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
							<email>p.bentley@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Braintree Ltd</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PiNet: A Permutation Invariant Graph Neural Network for Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph classification, the problem of predicting a label to each graph in a given set, is of significant interest in the bio-and chemo-informatics domains, among others; with typical applications in predicting chemical properties <ref type="bibr" target="#b1">[Li and Zemel, 2016]</ref>, drug effectiveness <ref type="bibr" target="#b2">[Neuhaus et al., 2009]</ref>, protein functions <ref type="bibr" target="#b3">[Shervashidze et al., 2009]</ref>, and classification of segmented images <ref type="bibr" target="#b3">[Scarselli et al., 2009</ref>].</p><p>There are two major challenges faced by graph classifiers: First, a problem of ordering, i.e. the ability to recognise isomorphic graphs as equivalent when the order of their nodes/edges are permuted, and second, in how to handle instances of varying dimensions, i.e. graphs with different numbers of nodes/edges. In image classification, the ordering of pixels is given, and instances differing in size may be scaled; however, for graphs the ordering of nodes/edges is typically arbitrary, and finding the analogous transformation to scaling an image is evidently non-trivial.</p><p>Typical approaches to solving these challenges include kernel methods, in which implicit kernel spaces circumvent the need to map each instance to a fixed size, ordered representation for classification <ref type="bibr" target="#b5">[Zhang et al., 2018]</ref>, and deep learning architectures with some explicit feature extraction method whereby a fixed size representation is constructed for each graph and passed to a CNN (or similar) classification model <ref type="bibr">[Niepert et al., 2016]</ref>. While the deep learning approaches often outperform the kernel methods with respect to scalability in the number of graphs, they require suitable fixed size representation for each graph, and typically cannot guarantee that isomorphic graphs will be interpreted as the same.</p><p>In order to address these issues, we propose PiNet; an endto-end deep learning graph convolution architecture with guaranteed invariance to permutation of nodes in the input graphs. To present our model, we first review relevant literature, then present the architecture with proof of its invariance to permutation. We conduct three experiment to evaluate PiNet's effectiveness: We verify the utility in its invariance to permutation with a graph isomorphism classification task, we then test its ability to learn appropriate message passing matrices, and we perform a benchmark test against existing classifiers on a set of standard molecule classification datasets. Finally, we draw our conclusions and suggest further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Kernels</head><p>A graph kernel φ is a positive semi-definite function that maps graphs belonging to a space G to an inner product in some Hilbert space H, φ : G → H. Graph classification can be performed in the mapped space H with standard classification algorithms, or using the kernel trick (with SVMs, for example) the mapped feature space may be exploited implicitly. In this sense, kernel methods are well suited to deal with the high and variable dimensions of graph data, where explicit computation of such a feature space may not be possible.</p><p>Despite the large number of graph kernels found in the literature, they typically fall into just three distinct classes <ref type="bibr">[Shervashidze et al., 2011]</ref>: Graph kernels based on random walks and paths <ref type="bibr" target="#b0">[Borgwardt et al., 2005]</ref>, graph kernels based on frequencies of limited size subgraphs or graphlets <ref type="bibr" target="#b3">[Shervashidze et al., 2009]</ref>, and graph kernels based on subtree patterns where a similarity matrix between two graphs is defined by the number of matching subtrees in each graph <ref type="bibr" target="#b1">[Harchaoui and Bach, 2007]</ref>.</p><p>Although kernels are well suited to varying dimensions of graphs, their scalability is limited. In many cases they scale poorly to large graphs <ref type="bibr" target="#b4">[Shervashidze et al., 2010]</ref> and given their reliance on SVMs or full computation of a kernel matrix, they become intractable for large numbers of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Convolutional and recurrent neural networks, while successful in many domains, struggle with a graph inputs because of the arbitrary order in which the nodes of each instance may appear <ref type="bibr" target="#b5">[Ying et al., 2018]</ref>. For node level tasks (i.e. node classification and link prediction) graph neural networks (GNNs) <ref type="bibr" target="#b3">[Scarselli et al., 2009]</ref> handle this issue well by integrating the neural network structure with that of the graph. In each layer, state associated with each node is propagated to its neighbours via a learned filter and then a non linear function is applied. Thus the network's inner layers learn a latent representation for each node. For example, the <ref type="bibr">GCN [Kipf and Welling, 2016]</ref> uses a normalised version of the graph adjacency matrix to propagate node features while learning spectral filters. The GCN model has received significant attention in recent years with several extensions already in the literature <ref type="bibr" target="#b1">[Hamilton et al., 2017;</ref><ref type="bibr" target="#b0">Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b3">Romero, 2018]</ref>.</p><p>However, for graph level tasks the GNN model and its variants do not handle permutations of the nodes well. For example, for a pair of isomorphic graphs, corresponding nodes would receive corresponding outputs, but for the graph as whole, the node level outputs will not necessarily be given in the same order, thus two graphs may be given shuffled representations presenting an obvious challenge for a downstream classifier. One approach to solving this problem is proposed by <ref type="bibr" target="#b5">[Verma and Zhang, 2018]</ref> where a permutation invariant layer based on computing the covariance of the data is added to the GCN architecture; however, computation of the covariance matrix O(n 3 ) in the number of nodes, thus not an attractive solution for large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mixed Models</head><p>Combining elements of kernel methods and neural networks, mixed models use an explicit method in order to generate vectorized representations of the graphs which are then passed to a conventional neural network. One benefit to this approach being that explicit kernels are typically more efficient when dealing with large numbers of graphs <ref type="bibr">[Kriege et al., 2015]</ref>.</p><p>For example, PATCHY-SAN <ref type="bibr">[Niepert et al., 2016]</ref> extracts fixed size localized patches by applying a graph labelling procedure given by the WL algorithm <ref type="bibr" target="#b5">[Weisfeiler and Lehman, 1968</ref>] and a canonical labeling procedure from <ref type="bibr" target="#b2">[McKay, 1981]</ref> to order the nodes. It then uses these patches to form a 3dimensional tensor for each graph that is passed to a standard CNN for classification.</p><p>A similar procedure is presented in [Gutierrez <ref type="bibr" target="#b1">Mallea et al., 2019]</ref> where, in order to overcome the potential loss of information associated with the CNN convolution operation, a Capsule network is used to perform the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PiNet</head><p>Formally, we consider the problem of predicting a label y for an unseen test graph, given a training set G with corresponding labels Y L . Each graph G ∈ G is defined as the pair G = (A, X), where A ∈ R N ×N is the graph adjacency matrix, and X ∈ R N ×d is a corresponding matrix of d-dimensional node features. We fix N to be the maximum number of nodes in each of the graphs in G, padding empty rows and columns of A and X with zeros. Note, however, that these zero entries do not form part of the final graph representations used by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>PiNet is an end-to-end deep neural network architecture that utilizes the permutation equivariance of graph convolutions in order to learn graph representations that are invariant to permutation.  <ref type="figure">Figure 1</ref>: Model Architecture. A is the adjacency matrix of a single graph, X the corresponding feature matrix, and Z the predicted label(s) for the complete batch of input graphs. <ref type="figure">Figure 1</ref>, our model consists of a pair of doublestacked message passing layers combined by a matrix product. Let σ R and σ S denote the rectified linear unit and softmax activations functions,Ã the preprocessed adjacency matrix, and F (1) X and F (1) A the dimensions per node of the latent representation of the features and attention stacks respectively. The features and attention stacks each output a tensor (Z X and Z A ) with each element corresponding to an input graph given by the functions zX :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><formula xml:id="formula_0">(R N ×N , R N ×d ) → R N ×F (1) X andzA : (R N ×N , R N ×d ) → R F (1) A ×N respectively, where z X (A, X) = σ R Ã σ R Ã XW (0) X W (1) X ,<label>(1)</label></formula><formula xml:id="formula_1">z A (A, X) = σ S Ã σ R Ã XW (0) A W (1) A ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">A = (pI + (1 − p)D) − 1 2 (A + qI)(pI + (1 − p)D) − 1 2 ,<label>(3)</label></formula><p>where I is the identity matrix of order N , D is the degree matrix such that</p><formula xml:id="formula_3">D ij = d(A) ij = N k=1 A ik if i = j, 0 otherwise,<label>(4)</label></formula><p>and 0 ≤ p, q ≤ 1.</p><p>The use of softmax activation on the attention stack applies the constraint that outputs sum to 1, thus preventing all node attention weightings from dropping to 0 and keeping the resulting products within a reasonable range.</p><p>The trainable parameters p and q offer an extra attention mechanism that enables the model to weigh the importance of symmetric normalisation of the adjacency matrix, and the addition of self loops. <ref type="table" target="#tab_1">Table 1</ref> shows the four matrices given by the extreme cases of p and q; however, intermediate combinations are of course possible. Also note that p and q may be different for each message passing layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix</head><p>Definition p q As seen in <ref type="figure">Figure 2</ref>, the weighting on the self loops given by q allows the model to include each node's own state in the previous layer as an input to the next layer.</p><formula xml:id="formula_4">Adjacency A 1 0 A with S.L. A + I 1 1 Sym Norm Adjacency D − 1 2 AD − 1 2 0 0 Sym Norm A with S.L. D − 1 2 (A + I)D − 1 2 0 1</formula><p>For each node of the graph, the model learns a vector where each element is a weighted sum of the features of the node's neighbourhood.</p><p>The addition of self loops add the nodes' own previous states to the input of the next layer.</p><p>The extent to which the neighbours' features should be normalised is controlled by the parameter , and the weighting of a node's neighbourhood vs. its own previous state by . The final output of the model is the matrix Z, where each row i is the predicted label given by the function</p><formula xml:id="formula_5">z(A, X) = σS [g (zA (A, X) · zX (A, X)) WD] ∈ R C , (5) where g : R F (1) A ×F (1) X → R F (1) A ·F (1)</formula><p>X is a reshape function. Note that since Z A has been transposed, the columns of each matrix (corresponding directly to the nodes of the graph) align exactly with the rows of each matrix in Z X , thus ∀i ∈ |G| the product</p><formula xml:id="formula_6">[ZA] i · [ZX ] i ∈ R F (1) A ×F (1)</formula><p>X aligns the weights for each node learned by the attention stack with the latent features learned by the features stack, resulting in a weighted sum of the features of each node without being affected by permutations of the nodes at input.</p><p>Finally, to train the model, we minimise the categorical cross-entropy</p><formula xml:id="formula_7">L = − l∈Y L C f =1 Y lf ln Z lf ,<label>(6)</label></formula><p>where Y is the target labels, and C the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Permutation Invariance</head><p>Here, following the necessary definitions, we provide proof of our model's invariance to permutation of the nodes in the input graphs.</p><p>Definition 3.1 (Permutation). A permutation π is a bijection of a set S onto itself, such that it moves an object at position i to position π(i). For example, a permutation of π to the vector (x 1 , x 2 , . . . , x n ) would be (x π −1 (1) , x π −1 (2) , . . . , x π −1 (n) ).</p><p>Definition 3.2 (Permutation Matrix). A permutation matrix P π ∈ {0, 1} n×n is an orthogonal matrix such that:</p><formula xml:id="formula_8">[P π X] ij = X π −1 (i)j , X ∈ R n×m (7) XP π ij = X iπ −1 (j) , X ∈ R m×n<label>(8)</label></formula><p>thus, for a square matrix A ∈ R n×n ,</p><formula xml:id="formula_9">P π AP π ij = A π −1 (i)π −1 (j) .<label>(9)</label></formula><p>Definition 3.3 (Graph Permutation). We define the permutation of π to a graph G as a mapping of the node indices, i.e. P π G = (P π AP π , P π X)</p><p>Definition 3.4 (Permutation Invariance). Let P n be the set of all valid permutation matrices of order n, then a function f is invariant to row permutation iff</p><formula xml:id="formula_11">f (X) = f (P π X), ∀X ∈ R n×m , P π ∈ P n ,<label>(11)</label></formula><p>and f is invariant to column permutation iff</p><formula xml:id="formula_12">f (X) = f (XP π ), ∀X ∈ R m×n , P π ∈ P n .<label>(12)</label></formula><p>Definition 3.5 (Permutation Equivariance). Let P n be the set of all valid permutation matrices of order n, then a function f is equivariant to row permutation iff P π f (X) = f (P π X), ∀X ∈ R n×m , P π ∈ P n , (13) similarly, for column permutation f (X)P π = f (XP π ).</p><p>Lemma 3.1. For any matrices A ∈ R n×m and B ∈ R n×p , and any permutation π, the product A ·B remains unchanged by a permutation of π applied to the rows of A and B, i.e.</p><formula xml:id="formula_13">(P π A) · P π B = A · B.<label>(14)</label></formula><p>Proof. Consider the vector product</p><formula xml:id="formula_14">a · b = n k a k b k .<label>(15)</label></formula><p>We permute the rows of a and the columns of b P π a · bP π = n k a π −1 (k) b π −1 (k)</p><p>and observe a reordering of terms, in which the factor pairs remain in correspondence. Since addition is commutative, then</p><formula xml:id="formula_16">n k a π −1 (k) b π −1 (k) = n k a k b k<label>(17)</label></formula><p>=⇒ P π a · bP π = a · b.</p><p>By the same logic, we see that</p><formula xml:id="formula_18">(P π A) · P π B ij = n k A π −1 (k)i B π −1 (k)j (19) = n k A ki B kj (20) = A · B ij (21) =⇒ (P π A) · P π B = A · B<label>(22)</label></formula><p>Lemma 3.2. IfÃ is the preprocessed version of A, then P πÃ P π is the preprocessed version of P π AP π , i.e. If</p><formula xml:id="formula_19">A = (pI + (1 − p)d(A)) − 1 2 (A + qI)(pI + (1 − p)d(A)) − 1 2 (23) then P πÃ P π = (pI + (1 − p)d(P π AP π )) − 1 2 (P π AP π + qI) (pI + (1 − p)d(P π AP π )) − 1 2 , (24) where d(A) is the diagonal degree matrix of A as defined in Equation 4. Proof. From Equation 4, d(P π AP π ) ij = N k=1 A π −1 (i)π −1 (k) if i = j, 0 otherwise (25) = P π d(A)P π<label>(26)</label></formula><p>Considering each factor of Equation 24 (RHS), we observe that</p><formula xml:id="formula_20">(pI + (1 − p)d(P π AP π )) − 1 2 = (pI + (1 − p)P π d(A)P π ) − 1 2 (27) = (pI + P π [(1 − p)d(A)] P π ) − 1 2 (28) = (P π [pI + (1 − p)d(A)] P π ) − 1 2<label>(29)</label></formula><p>and since the matrix is diagonal = P π (pI + (1 − p)d(A)) − 1 2 P π .</p><p>(30) We also have (P π AP π + qI) = P π (A + qI)P π .</p><p>(31) By Equation 30 and 31,</p><formula xml:id="formula_21">(pI + (1 − p)d(PπAP π )) − 1 2 (PπAP π + qI) (pI + (1 − p)d(PπAP π )) − 1 2 = Pπ(pI + (1 − p)d(A)) − 1 2 P π Pπ(A + qI)P π Pπ(pI + (1 − p)d(A)) − 1 2 P π ,<label>(32)</label></formula><p>and since Pπ is orthogonal, P π Pπ = I, so</p><formula xml:id="formula_22">= Pπ(pI+(1−p)d(A)) − 1 2 (A+qI)(pI+(1−p)d(A)) − 1 2 P π<label>(33)</label></formula><p>Theorem 3.3. For any input graph G, and any permutation π applied to G, the output of PiNet is equal, i.e.</p><formula xml:id="formula_23">z(G) = z(P π G),<label>(34)</label></formula><p>where z : (R N ×N , R N ×d ) → R C is the forward pass function of PiNet given in Equation 5.</p><p>Proof. By Equation 1, Definition 3.3 and Lemma 3.2,</p><formula xml:id="formula_24">z X (P π G) = z X (P π AP π , P π X) (35) = σR PπÃP π σR PπÃP π PπXW (0) X W (1) X<label>(36)</label></formula><p>P π is orthogonal, so P π P π = I, giving</p><formula xml:id="formula_25">z X (P π G) = σ R P πÃ P π σ R P πÃ XW (0) X W (1) X<label>(37)</label></formula><p>Since σ R is an element-wise operation,</p><formula xml:id="formula_26">σ R (P π X) = P π · σ R (X),<label>(38)</label></formula><p>then</p><formula xml:id="formula_27">z X (P π G) = σR PπÃP π Pπ · σR Ã XW (0) X W (1) X (39) = σ R P πÃ σ R Ã XW (0) X W (1) X (40) = P π · σ R Ã σ R Ã XW (0) X W (1) X (41) = P π · z X (A, X) (42) = P π · z X (G).<label>(43)</label></formula><p>By the same logic as Equation 35 to 40,</p><formula xml:id="formula_28">zA(PπG) = σS PπÃP π σR PπÃP π PπXW (0) X W (1) X (44) = σ S P πÃ σ R Ã XW (0) X W (1) X (45) = σ S P πÃ σ R Ã XW (0) X W (1) X ,<label>(46)</label></formula><p>where σ S is a column-wise softmax</p><formula xml:id="formula_29">σ S (X) ij = e Xij k e X ik .<label>(47)</label></formula><p>When the rows of its input are permuted by π,</p><formula xml:id="formula_30">σ S (P π X) ij = e X π −1 (i)j k e X π −1 (i)k<label>(48)</label></formula><p>we observe that rows of the output are also permuted by π, thus by Equation 46 and 48</p><formula xml:id="formula_31">zA(PπG) = Pπ · σ S Ã σR Ã XW (0) X W (1) X .<label>(49)</label></formula><p>From Equation 5</p><formula xml:id="formula_32">z(P π G) = σ S [g (z A (P π G) · z X (P π G)) W D ] ,<label>(50)</label></formula><p>and by Equation 43 and 49 we see that</p><formula xml:id="formula_33">zA (PπG) · zX (PπG) = Pπ · σ S Ã σR Ã XW (0) X W (1) X · [Pπ · zX (G)] , (51) which by Lemma 3.1 = σ S Ã σ R Ã XW (0) X W (1) X · [z X (G)] (52) = σ S Ã σ R Ã XW (0) X W (1) X · [z X (G)] (53) = z A (G) · z X (G)<label>(54)</label></formula><p>Finally, by Equation 50 and 51 to 54,</p><formula xml:id="formula_34">z(P π G) = σ S [g (z A (G) · z X (G)) W D ] (55) = z(G).<label>(56)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>To implement PiNet we use Keras + Tensorflow. The model operates on batches and uses a mixture of Scipy sparse matrices and Numpy arrays to represent the graphs and their features. Full source code for PiNet is available at LINK(reveals authors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct three experiments: We empirically verify the utility of our model's invariance to permutation with a graph isomorphism classification task, we evaluate the effect of different message passing matrices and the model's ability to select an appropriate message passing configuration, and we compare the model's classification performance against existing graph classifiers on a set of standard molecule classification datasets. We next describe the data used followed by a description of each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For the isomorphism test, we generate a dataset of 500 graphs.</p><p>To create sufficient challenge, in each randomly sampled Erdos Reny <ref type="bibr" target="#b0">[Erdõs and Rényi, 1960]</ref> graph, we fix the number of nodes, and the node degree distributions, to be constant. We generate the dataset according to Algorithm 1, with the parameters: N = 50, C = 5, N g = 100, and p = 0.15. For the final two experiments we use the binary classification molecule datasets detailed in <ref type="table">Table 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Isomorphism Test</head><p>We test PiNet's ability to recognise isomorphic graphs -specifically, unseen permutations of given training graphs. For a baseline, we test against two variants of the <ref type="bibr">GCN [Kipf and Welling, 2016]</ref>, one in which the graph level representation is given by a sum of the node representations, and the other in which we apply a dense layer directly to the node level outputs of the GCN. We also compare against two state of the art graph classifiers: the WL Kernel <ref type="bibr">[Shervashidze et al., 2011]</ref> and PATCHY- <ref type="bibr">SAN [Niepert et al., 2016]</ref>. We perform 10 trials for every training sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Message Passing Mechanisms</head><p>We study the impact on classification accuracy of the four matrices shown in <ref type="table" target="#tab_1">Table 1</ref> that facilitate message passing between nodes, alongside the parametrised matrix shown in Equation 3, in which the extent to which to normalise neighbours' values and include the node's own state as input are controlled by the learned parameters p and q. Note that p and q are learned for each message passing layer and are not required to be the same, however, for each of the matrices from <ref type="table" target="#tab_1">Table 1</ref> we test, we use the same matrix in all four message passing layers.</p><p>For each matrix, we perform a 10-fold cross validation and record the classification accuracy. For all runs we use the following hyper-parameters: batch size = 50, epochs = 200, first layer latent feature size F (0) X = F (0) A = 100, second layer latent feature size F (1) X = F (1) A = 64, and learning rate = 10 −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison Against Existing Methods</head><p>As with the isomorphism test, we compare against the GCN with a dense layer applied directly to the node outputs as well as with a sum of the node representations, the Weisfeiler Lehman graph kernel, and PATCHY-SAN. For each model we perform 10-fold cross validation on each dataset. For PiNet, we use the same hyper-parameters as described in Section 4.3. We test the model with p and q as trainable values, and also search over the space (p, q) ∈ {0, 1}×{0, 1}. For the GCN we use two layers of sizes 100 and 64 hidden units, with a learning rate of 10 −3 , and for PATCHY-SAN we search over two labelling procedures, betweenness centrality <ref type="bibr">[Brandes, 2001]</ref> and NAUTY <ref type="bibr" target="#b2">[McKay, 1981]</ref> canonical labelling. As seen in <ref type="figure" target="#fig_1">Figure 3</ref>, PiNet outperforms all competitors tested. Using an independent two-sample t-test we observe statistical significance (p-value &lt; 0.05) in all cases except a single point (circled in red). PiNet fails to achieve 100% accuracy since the neural network learns a surjective function, thus with so few training examples in some cases complete multiple classes become indistinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Message Passing Mechanisms</head><p>In <ref type="figure" target="#fig_3">Figure 4</ref> we observe that the optimal message passing matrix (when p and q are fixed for all layers, and (p, q) ∈ {0, 1}× {0, 1}) varies depending on the particular set of graphs given. With p and q as trainable parameters ((p, q) ∈ [0, 1] × [0, 1], and (p, q) may be different for each layer), we see that for the MUTAG and PROTEINS datasets the model learns values that outperform those found with our manual search. For the others, however, the model is unable to find the optimal ps and qs, suggesting that the model finds only local minima. We note however, that in every case tested, the model is able to learn the values p and q that give better than average classification performance when compared with our manual search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison Against Existing Methods</head><p>As shown in <ref type="table">Table 3</ref>, PiNet achieves competitive classification performance on all datasets tested. An independent twosample t-test indicates PiNet achieves a statistically significant (p-value &lt; 0.05) gain in only a few cases ( * ); however, with competitive performance on all datasets it demonstrates a robustness to the properties of the different sets of graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed PiNet, an end-to-end deep neural network graph classifier invariant to permutations of nodes in the input graphs. We have provided theoretical proof of its invariance to permutation, and demonstrated the utility in such a property empirically with a graph isomorphism classification task against a set of existing graph classifiers, achieving a statistically significant gain in classification accuracy on a range of small training set sizes. The permutation invariance is achieved through a differentiable attention mechanism in which the model learns the weight by which the states associated with each node should be aggregated into the final graph representation.</p><p>We have demonstrated that PiNet is able to learn an effective parametrisation of a message passing matrix that enables it to adapt to different types of graphs with a flexible state propagation and diffusion mechanism. Finally, we have shown PiNet's robustness to the properties of different sets of graphs in achieving consistently competitive classification performance against a set of existing techniques on five commonly used molecule datasets.</p><p>For future work we plan to explore more advanced aggregation mechanisms by which the latent representations learned for each node of the graph may be combined. <ref type="bibr">NCI</ref>  <ref type="table">Table 3</ref>: Mean classification accuracies for each classifier. For manual search the values p and q as follows: MUTAG and PROTEINS p = 1, q = 0, NCI-1 and NCI-109 p = q = 1, PTC p = q = 0. * indicates PiNet (both models) achieved statistically significant gain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Layer propagation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Mean classification accuracy on isomorphic graph classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Each plot shows the mean classification accuracy for each message passing matrix of our search space, alongside the accuracy of PiNet when p and q are learned during training. The dashed lines indicate the mean accuracy of the manual search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1st Layer 2nd Layer Graph Embedding Prediction</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>X</cell><cell></cell></row><row><cell></cell><cell>A</cell><cell></cell></row><row><cell>Attention</cell><cell></cell><cell>Features</cell></row><row><cell>Preprocess</cell><cell></cell><cell></cell></row><row><cell>A</cell><cell></cell><cell></cell></row><row><cell>Message Passing</cell><cell></cell><cell>Message Passing</cell></row><row><cell>Layer</cell><cell></cell><cell>Layer</cell></row><row><cell>σ R</cell><cell></cell><cell>σ R</cell></row><row><cell cols="2">Preprocess</cell><cell></cell></row><row><cell>Message Passing</cell><cell>A</cell><cell>Message Passing</cell></row><row><cell>Layer</cell><cell></cell><cell>Layer</cell></row><row><cell>⊤</cell><cell></cell><cell>σ</cell></row><row><cell>σ S</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Product Layer</cell><cell></cell></row><row><cell></cell><cell>Dense Layer</cell><cell></cell></row><row><cell></cell><cell>σ S</cell><cell></cell></row><row><cell></cell><cell>Z</cell><cell></cell></row></table><note>R PreprocessA PreprocessA Input</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The four message passing matrices given by the extreme values of p and q, i.e. (p, q) ∈ {0, 1} × {0, 1}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Binary classification molecule datasets. |G| is the number of graphs, |V | the number of nodes, and d the dimensions of the node features. Number of nodes N , number of classes C, number of graphs per class Ng, edge probability p seedGraph ← SampleErdosRenyiGraph(N , p) while seedGraph not fully connected do</figDesc><table><row><cell>MUTAG NCI1 NCI109 188 4110 4127 28 111 111 18 29.8 29.6 25.56 PTC PROTEINS 344 1113 109 620 39.06 7 37 38 18 3 66.49 50.05 50.38 39.51 59.57 Table 2: Algorithm 1 Graph Dataset Generation |G| Max. |V | Mean |V | d % of +ve Input: SampleErdosRenyiGraph(N , p) end while D ← Array[] for each c in C do Gc ← Array[] S ← getDegreeSequence(seedGraph) sampleGraph ← GenerateGraph(S) for each i in Ng do pg ← permute(sampleGraph.adjacencyMatrix) pg ← relabel(pg.nodes) Gc[i] ← pg end for D[c] ← Gc end for Output: D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>WLKernel .68 ± .00 * .53 ± .02 * .53 ± .03 * .61 ± .01 * .62 ± .03 PiNet (Manual p and q) .87 ± .08 .74 ± .03 .73 ± .03 .75 ± .06 .63 ± .06 PiNet (Learned p and q) .88 ± .07 .74 ± .02 .71 ± .04 .75 ± .06 .63 ± .04</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-109</cell><cell>PROTEINS</cell><cell>PTC</cell></row><row><cell>GCN + Dense</cell><cell>.86 ± .06</cell><cell>.73 ± .03</cell><cell>.72 ± .02</cell><cell>.71 ± .04</cell><cell>.63 ± .07</cell></row><row><cell>GCN + Sum</cell><cell>.86 ± .05</cell><cell>.72 ± .03</cell><cell>.73 ± .03</cell><cell>.74 ± .04</cell><cell>.61 ± .05</cell></row><row><cell>PATCHY-SAN</cell><cell>.85 ± .06</cell><cell>.58 ± .02</cell><cell>.58 ± .03</cell><cell>.70 ± .02</cell><cell>.58 ± .02</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Braintree Ltd. for providing the full funding for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUTAG</head><p>NCI-1</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ulrik Brandes. A faster algorithm for betweenness centrality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towsley ; James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="17" to="61" />
		</imprint>
	</monogr>
	<note>Journal of Mathematical Sociology</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Capsule Neural Networks for Graph Classification using Explicit Tensorial Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gutierrez</forename><surname>Mallea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<editor>Marion Neumann, Kristian Kersting, and Petra Mutzel</editor>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Li and Zemel</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="881" to="886" />
		</imprint>
		<respStmt>
			<orgName>Kipf and Welling</orgName>
		</respStmt>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Novel kernels for error-tolerant graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mckay ; Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neuhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mohamed Ahmed, and Konstantin Kutzkov. Learning Convolutional Neural Networks for Graphs</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="425" to="441" />
			<date type="published" when="1981" />
			<publisher>Mathias Niepert</publisher>
		</imprint>
	</monogr>
	<note>Spatial Vision</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>Pascal</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR. Shervashidze et al., 2010] Nino Shervashidze</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten M Borgwardt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leeuwen</surname></persName>
		</author>
		<editor>Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Shervashidze et al., 2011</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reduction of a graph to a canonical form and an algebra which appears in the process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang ; Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; B Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hierarchical Graph Representation Learning with Differentiable Pooling. 32nd Conference on Neural Information Processing Systems</title>
		<editor>Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. RetGK</editor>
		<meeting><address><addrLine>Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec; Montréal; Montréal</addrLine></address></meeting>
		<imprint>
			<publisher>Weisfeiler and Lehman</publisher>
			<date type="published" when="1968" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Canada.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
