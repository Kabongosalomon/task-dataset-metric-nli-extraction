<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reviving Iterative Training with Mask Guidance for Interactive Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Understanding lab</orgName>
								<orgName type="institution" key="instit1">AI Center Moscow</orgName>
								<orgName type="institution" key="instit2">Samsung Electronics Co</orgName>
								<address>
									<postCode>5C</postCode>
									<settlement>Lesnaya, Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Understanding lab</orgName>
								<orgName type="institution" key="instit1">AI Center Moscow</orgName>
								<orgName type="institution" key="instit2">Samsung Electronics Co</orgName>
								<address>
									<postCode>5C</postCode>
									<settlement>Lesnaya, Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Understanding lab</orgName>
								<orgName type="institution" key="instit1">AI Center Moscow</orgName>
								<orgName type="institution" key="instit2">Samsung Electronics Co</orgName>
								<address>
									<postCode>5C</postCode>
									<settlement>Lesnaya, Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reviving Iterative Training with Mask Guidance for Interactive Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A R T I C L E I N F O</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>interactive segmentation segmentation mask refinement</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Recent works on click-based interactive segmentation have demonstrated state-of-the-art results by using various inference-time optimization schemes. These methods are considerably more computationally expensive compared to feedforward approaches, as they require performing backward passes through a network during inference and are hard to deploy on mobile frameworks that usually support only forward passes. In this paper, we extensively evaluate various design choices for interactive segmentation and discover that new state-of-the-art results can be obtained without any additional optimization schemes. Thus, we propose a simple feedforward model for click-based interactive segmentation that employs the segmentation masks from previous steps. It allows not only to segment an entirely new object, but also to start with an external mask and correct it. When analyzing the performance of models trained on different datasets, we observe that the choice of a training dataset greatly impacts the quality of interactive segmentation. We find that the models trained on a combination of COCO and LVIS with diverse and highquality annotations show performance superior to all existing models. The code and trained models are available at https://github.com/saic-vul/ritm_interactive_segmentation. <ref type="figure">Figure 1</ref>: Besides segmenting new objects, proposed method allows to correct external masks, e.g.produced by other instance or semantic segmentation models. A user can fix false negative and false positive regions with positive (green) and negative (red) clicks, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interactive segmentation algorithms allow users to explicitly control the predictions using interactive input at several iterations, in contrast to common semantic and instance segmentation algorithms that can only input an image and output a segmentation mask in one pass. Such interaction makes it possible to select an object of interest and correct prediction errors. Another important feature of this group of algorithms is the capability to segment objects of previously unseen classes.</p><p>User input can be formalized via various representations: scribbles, clicks, extreme points, etc. Click-based interactive segmentation is one of the most well-studied topics among other deep learning-based interactive segmentation approaches, as it has well-established protocols of training and evaluation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. It also employs quite an intuitive and simple way to specify the desired object. Scribble-based methods often use heuristic and complicated procedures for simulating user input, which makes it hard to fairly evaluate them <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Approaches based on extreme points are not intuitive for users and are not flexible enough due to a limited number of user interactions <ref type="bibr" target="#b7">[8]</ref>. In our work, we focus on click-based interactive segmentation.</p><p>In general, the development of deep learning-based algorithms for semantic and instance segmentation requires a huge amount of annotated data. Annotating data with segmentation masks is very time-consuming and therefore expensive. Interactive segmentation can significantly simplify and speed up this process <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, which is one of the most important applications of interactive segmentation. It can also be used in photo editing, allowing users to select objects easily, which is especially important for smartphone applications where user input is often limited by finger or stylus activities.</p><p>Recent works on click-based interactive segmentation propose complicated inference-time optimization procedures to improve the quality of interactive segmentation even further <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The deployment of these models is substantially limited, for instance, on mobile devices, as it requires the implementation of backward passes with gradients that are not provided by popular frameworks. Surprisingly, we find that properly chosen baselines of click-based interactive segmentation models without any without any explicit refinement techniques and other specific modifications can be trained using the standard random points sampling procedure and show state-of-the-art performance. We observe that modifications of poorly tuned baselines can provide false performance improvements, while strong baselines with the same modifications do not show any improvement at all. At the same time, datasets with coarse masks or containing a small amount of images may become a bottleneck for the performance of interactive segmentation models. Our experiments show that usage of diverse large datasets with fine masks for training plays a crucial role for the performance of discussed models. Therefore, we choose a strong baseline model for implementing and evaluating new modifications. We train models on a combination of LVIS and COCO datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, that, to the best of our knowledge, are the most suitable for training interactive segmentation models.</p><p>We propose an extension of click-based interactive segmentation that allows to modify existing instance segmentation masks interactively. We revive an iterative training procedure, and make a network aware of the mask from a previous step <ref type="bibr" target="#b13">[14]</ref>. We show that such awareness improves the models' stability, i.e. allows to avoid accuracy dropping when adding new clicks. We propose a new training dataset obtained by combining the LVIS and COCO datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Interactive segmentation methods. Interactive segmentation is a longstanding problem in computer vision. Early methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> tackle the problem using optimization-based approaches minimizing a specifically constructed cost function defined on a graph over image pixels. GrabCut proposed in <ref type="bibr" target="#b17">[18]</ref> is a classic approach based on iterative energy minimization of a cost function, that is modeled using a Gaussian mixture. Xu et al. <ref type="bibr" target="#b0">[1]</ref> first propose a CNNbased model for interactive segmentation and introduced a clicks simulation strategy for training that is adopted in some further works. Later, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> propose various CNN-based methods for interactive segmentation that aimed at the refinement of predictions by increasing the diversity of predicted masks, and using the attention mechanism. The novel subgroup of methods emerges with an introduction of the Backpropagating Refinement Scheme (BRS) in <ref type="bibr" target="#b1">[2]</ref>. The authors propose an optimization procedure that minimizes a discrepancy between the predicted mask and the map of input clicks after each click with respect to the input distance maps. This refinement technique improves the segmentation quality at the cost of increasing the runtime. Sofiiuk et al. <ref type="bibr" target="#b3">[4]</ref> address this issue by proposing f-BRS, a lightweight version of BRS that uses a similar optimization procedure, though with respect to internal parameters introduced to the higher levels of a network, reducing runtime on an order of magnitude. Kontogianni et al. <ref type="bibr" target="#b2">[3]</ref> introduce another method of test time refinement targeting the optimization process on network parameters. Different types of interactive feedback. While clicks are the mainstream form of input in interactive segmentation, a lot of works explore other variations of interactive feedback from the user. For example, one of the simplest types of interactions is a bounding box, that is used in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. The main drawbacks of these approaches are lack of specific object reference inside the selected area and lack of interface for correction of the predicted mask. The limitations of bounding box-based interaction are addressed in <ref type="bibr" target="#b24">[25]</ref>. The authors propose to combine clicks with bounding boxes to provide more specific object guidance and allow corrections of the predicted mask. DEXTR <ref type="bibr" target="#b7">[8]</ref> uses extreme points of the target object, i.e. left-most, right-most, top, and bottom pixels as an input. On the one hand, such an interaction is compact and is limited by 4 clicks. On the other hand, placing extreme points in the right locations is harder than making an arbitrary click on an object and there is no support for corrections, similar to interaction via bounding boxes. Scribbles are used in many early works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. This type of feedback provides richer prior information for the algorithm, compared to the others. In contrast, putting a stroke requires more effort from the user, compared to simpler forms of interactions. Another drawback that refrains CNN-based methods from the wide adoption of scribbles is that realistic strokes simulation for training the neural networks is a rather hard task. Therefore, the vast majority of scribble-based methods employ graphical models or similar training-free techniques. Apart from the aforementioned forms of interaction, some works use a combination of traditional input representation with other modalities, e.g. PhraseClick <ref type="bibr" target="#b26">[27]</ref> combines clicks with text input to better infer the attributes of the target object, thus requiring fewer clicks.</p><p>Segmentation mask refinement. The majority of segmentation mask refinement techniques are targeted to refine the boundaries of a mask by making local corrections without changing the mask globally, i.e. excluding or including large parts of an object. These methods can either be a part of the network architecture <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> or serve as a postprocessing step <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Local and boundary refinement tasks are often addressed without any additional input from the user, as all the required features are automatically extracted from the input image and mask. Global refinement, on the contrary, may require additional feedback from the user to make improvements to the mask. We propose to combine interactive segmentation and mask refinement allowing to refine external segmentation mask with user clicks as guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revising Network Architecture</head><p>The task of interactive segmentation is very similar to instance or semantic segmentation in terms of the network architecture. In these tasks, networks take high-resolution input and produce high-resolution segmentation masks aligned with the input. The key difference is in the user input: its main aspects are the encoding and processing of the encoded input inside the network. Therefore, there is no need to reinvent general segmentation architectures. Instead, it is reasonable to rely on time-tested state-of-the-art segmentation networks and focus on interactive-specific parts.</p><p>We consider the DeepLabV3+ <ref type="bibr" target="#b31">[32]</ref> and HRNet+OCR <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> semantic segmentation architectures as a backbone for our interactive segmentation model. While DeepLabV3+ is a well-studied segmentation architecture that proved efficient in many segmentation-related tasks, HRNet is a relatively new promising one that was specially designed to produce high-resolution output. According to our experiments, HRNet is a more preferable architecture for this task. We provide the ablation study for the backbone architecture in Section 5.2 and show the results in <ref type="table" target="#tab_2">Tables 1, 3 and 4</ref>. Clicks encoding. There are positive and negative clicks in click-based interactive segmentation. These clicks are represented by their coordinates in an image. In order to feed them to a convolutional network, we should encode them in a spatial form first. Encoding the clicks via a distance transform from Xu et al. <ref type="bibr" target="#b0">[1]</ref> is the most common approach to clicks encoding. However, they can also be represented by gaussians or disks with a fixed radius. Benenson et al. <ref type="bibr" target="#b9">[10]</ref> perform a detailed ablation study on clicks encoding and find that disks with a small radius surpass the other encodings in terms of model performance.</p><p>We conduct our ablation study to compare the distance transform encoding with the disks encoding and find that the latter outperforms the former. We assume that the following observation explains the superiority of disks. The changes in disk encoding caused by adding new points or moving existing ones are always local and only slightly affect the encoding map. At the same time, a distance transform map can change drastically when a new point is added, especially if there are only a few points. In turn, such sudden considerable changes might confuse a network. We show a visualization of both the distance transform and disk encodings in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Feeding encoded clicks to a backbone. Semantic segmentation backbones are usually pre-trained on ImageNet <ref type="bibr" target="#b34">[35]</ref> and can take only RGB images as an input. The most common way to handle additional input, e.g. encoded user clicks, is to augment the weights of the first convolutional layer of a pre-trained model to accept N-channel input instead of only an RGB image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. In our work, we denote this modification of network architecture by Conv1E. Sofiiuk et al. <ref type="bibr" target="#b3">[4]</ref> propose the Distance Maps Fusion (DMF) module which transforms an image concatenated with additional user input channels into 3-channel input.</p><p>We propose a new simple approach to solving this problem, which is described as follows. We introduce a convolutional block that outputs the tensor of exactly the same shape as the first convolutional block in the backbone does. This tensor is then summed element-wise with the output of the first backbone convolutional layer, which usually has 64 channels. We denote this modification of network architecture by Conv1S. While being similar to Conv1E, it allows to choose a different learning rate for new weights without affecting the weights of a pre-trained backbone. We show the schemes of these architectural modifications in <ref type="figure" target="#fig_1">Figure 3</ref> and provide an ablation study in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Iterative Sampling Strategy</head><p>Most recent works on click-based interactive segmentation use the sampling strategy for simulating user clicks during training, where a set of positive and negative clicks is randomly generated without considering any relations between them <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>. In practice, every new click is placed in the erroneous region of a prediction produced by a network using the set of previous clicks. This fact is completely ignored in the random sampling strategy. It also makes it impossible to integrate masks from previous interactions into a model, as we need to have ordered interactions and a sequence of corresponding predictions to successfully train such a model.</p><p>The iterative sampling strategy, which resembles the interaction with a real user, is employed to a certain extent <ref type="table">Table 1</ref> Ablation studies of the network architecture choices described in Section 3.1. Each cell consists of two results "X/Y", where "X" and "Y" correspond to evaluation without and with f-BRS-B <ref type="bibr" target="#b3">[4]</ref>, respectively. "DT" stands for the distance transform clicks encoding. All models are trained on SBD. in several works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3]</ref>. As full iterative sampling is very computationally expensive, in these works random sampling is used for initialization and then a few clicks are added using the iterative sampling procedure. We adopt the iterative sampling procedure proposed by Mahadevan et al. <ref type="bibr" target="#b13">[14]</ref> with the following changes. First, we sample each point not just from the center of a mislabelled region, but from the region obtained by applying morphological erosion operation to the mislabelled region, so that the eroded region has 4 times less area. We observe that choosing center points leads to overfitting to the NoC evaluation metric (see Section 5 for the details). In practice, however, these models demonstrate worse performance and unstable behavior when a user places a click near the borders of an object or a mislabelled region. Second, we do not save simulated clicks for dataset samples during training and simulate user clicks for each batch individually. For that reason, we limit the maximum number of sampling iterations to , and each batch can uniformly get from 0 to iterations. We use a combination of the random and iterative sampling strategies for training our iterative models. First, we simulate user clicks with the random sampling strategy <ref type="bibr" target="#b0">[1]</ref> just as we do for our non-iterative baselines. Then, we add from 0 to iteratively simulated clicks. We provide an ablation study of the number of iteratively sampled clicks in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Incorporating Masks From Previous Steps</head><p>In interactive segmentation, it seems natural to incorporate output segmentation masks from previous interactions as an input for the next correction, providing additional prior information that can help improve the quality of prediction. In case of incorporating the mask from a previous interaction, it is necessary to use the iterative sampling for simulating user interactions during training. Along with the iterative sampling, Mahadevan et al. <ref type="bibr" target="#b13">[14]</ref> propose passing an output mask from a previous iteration into the model as an optional extension of their method.</p><p>To train a model with the mask from a previous step, we use a combination of the random and iterative user interactions simulation described in Section 3.2. Our model takes this mask as the third channel together with two channels for positive and negative encoded clicks, respectively. For the first interaction, as well as for the batches with skipped iterative sampling, we feed an empty mask to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Normalized Focal Loss</head><p>Binary cross entropy (BCE) loss is one of the standard loss functions for training semantic segmentation algorithms. Besides, some state-of-the-art interactive segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> adopt it for training the network. The main drawback of BCE is that it treats all examples equally, slowing the training during later epochs as the gradient from almost correctly segmented areas propagates similarly to the gradient from erroneous regions. Focal loss was introduced in <ref type="bibr" target="#b36">[37]</ref> to alleviate this problem. Let̂ denote the output of the network and , denote the confidence of prediction at the point ( , ). Then the FL is formulated as follows:</p><formula xml:id="formula_0">( , ) = −(1 − , ) log ,<label>(1)</label></formula><p>One can notice that the total weight (̂ ) = ∑ , (1− , ) decreases when the accuracy of the prediction increases. This means that the total gradient of the FL fades over time, slowing the training process. To mitigate this problem, <ref type="bibr" target="#b37">[38]</ref> proposed normalized focal loss (NFL) that is formulated as follows:</p><formula xml:id="formula_1">( , ,̂ ) = − 1 (̂ ) (1 − , ) log ,<label>(2)</label></formula><p>The gradient of NFL does not fade over time due to normalization and remains equal to the total gradient of BCE. This allows for faster convergence and better accuracy compared to training with BCE. We choose NFL as a loss function for training the proposed methods and provide an ablation study with respect to loss function choice in Section 5.2 (results are provided in <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset for Interactive Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reviewing Existing Datasets</head><p>The vast majority of recent CNN-based interactive segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> are trained either using Semantic Boundaries Dataset <ref type="bibr" target="#b38">[39]</ref>, or the Pascal VOC dataset <ref type="bibr" target="#b39">[40]</ref>, or the combination of these two datasets, as they share the same set of images. The augmented dataset contains a total of 10582 images with 25832 instance-level masks. The total number of classes in this dataset is twenty: 7 categories of transportation, 6 species of animals, 6 types of indoor objects, and a separate class for persons. These classes cover only general types of objects, implying limitations on the variety of predictable classes. Recently, large-scale segmentation datasets OpenImages <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref> and LVIS <ref type="bibr" target="#b12">[13]</ref> were introduced. These datasets stimulated the development of state-of-the-art instance segmentation algorithms <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>, providing a large variety of labeled classes with a sufficient number of examples. LVIS contains around 1.2M instances on 100k images embracing more than a thousand object classes, while OpenImages has 2.6M instances on 944k images covering 350 object categories. We believe that having such a diverse dataset is one of the key components in training of the state-of-the-art interactive segmentation model. Another important characteristic of a segmentation dataset is annotation quality. Gupta et al. <ref type="bibr" target="#b12">[13]</ref> present a study of annotation quality in some of the instance segmentation benchmarks, comparing a subset of masks from each dataset to experts' annotations. According to this study, the LVIS dataset has the highest annotation quality among reported datasets, presumably allowing to achieve higher prediction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combination of COCO and LVIS</head><p>Considering the observations from Section 4.1, the LVIS dataset appears to be the best choice for training the models except for one drawback: it is long-tailed and therefore lacking general object categories, which can affect the accuracy and generalization of the trained model. As a solution to this problem, we propose to augment LVIS labels with masks from the COCO <ref type="bibr" target="#b11">[12]</ref> dataset, as these two datasets share the same set of images. The COCO segmentation dataset contains a total of 1.2M instance masks on 118k training images with 80 object classes. These categories represent more common and general objects, complementing the long-tailed object classes from the LVIS dataset. We propose the following procedure to construct the combined COCO+LVIS dataset with diverse object classes from the most common to less frequent ones. All masks from both datasets are joined together except for those masks from COCO that have a corresponding mask from LVIS with an intersection over union (IoU) score between them larger than 80%. In that case, we only keep the mask from LVIS, since it presumably has better overall and especially boundary quality. As a result of the described procedure, we obtained a dataset with 104k images and 1.6M instance-level masks. We argue that the further development of interactive segmentation algorithms relies heavily on the training data. Thus, we provide the comparative study of the training datasets in Section 5.2, results are provided in <ref type="table" target="#tab_2">Table 3</ref>. Based on our experiments and the aforementioned observations, we conclude that the proposed COCO+LVIS dataset is the best choice for training interactive segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform an extensive evaluation of the proposed approach by conducting ablation studies for all sufficient parts of the method, exploring the convergence properties with an increasing number of clicks and comparing our method with current state-of-the-art works.</p><p>Datasets. We evaluate the performance of our method on five common benchmarks for interactive segmentation with instance-level annotations. The GrabCut <ref type="bibr" target="#b17">[18]</ref> dataset contains 50 images with a single object mask for each image. We adopt the test subset of Berkeley <ref type="bibr" target="#b45">[46]</ref> introduced in <ref type="bibr" target="#b46">[47]</ref>, which consists of 100 masks for 96 images. The DAVIS <ref type="bibr" target="#b47">[48]</ref> dataset was introduced for the evaluation of video segmentation datasets. We use the subset of 345 randomly sampled frames of video sequences that was introduced in [2] for evaluation. We follow the common protocol and combine all instance-level masks for one image into one segmentation mask. We also use the validation part of the Pascal VOC <ref type="bibr" target="#b39">[40]</ref> dataset that consists of 1449 images with 3417 instances. Each instance mask is used separately during evaluation. The Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b38">[39]</ref> contains 6671 instance-level masks for 2820 images. This dataset has been used for evaluating the interactive segmentation algorithms since <ref type="bibr" target="#b0">[1]</ref>. To test the algorithm, we use each of instance masks separately and do not combine them into one segmentation mask for one image.</p><p>Evaluation metric. We perform the evaluation using the standard Number of Clicks (NoC) measure, reporting the number of clicks required to achieve the predefined Intersection over Union (IoU) threshold between predicted and ground truth masks. We denote NoC with IoU threshold set to 85% and 90% as NoC@85 and NoC@90, respectively. To generate clicks during the evaluation procedure, we follow the strategy used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>. The next click is placed at the center of the region with any type of prediction error (false positive or false negative) with the largest area among other erroneous regions. The region center is defined as the point farthest from the boundaries of the corresponding region.</p><p>Segmentation backbones. We consider two different segmentation architectures: DeepLab-V3+ <ref type="bibr" target="#b31">[32]</ref> with ResNet and HRNet+OCR <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. To implement DeepLab-V3+ for interactive segmentation we follow <ref type="bibr" target="#b3">[4]</ref>. We use the implementation of HRNet and models pre-trained on ImageNet presented in the official repository 1 . There are several versions of HRNet which differ in terms of their capacity: HRNet-W18-C, HRNet-W30-C, HRNet-W32-C, etc. In our work we employ HRNet-W18-C-Small-v2, HRNet-W18-C, HRNet-W32-C, referring to them as HRNet-18s, HRNet- <ref type="table">Table 4</ref> Comparison of different models in terms of the number of FLOPs and parameters. All models take an image with a resolution of 400 × 400 to compute the number of FLOPs. We use exactly the same architecture of the DeepLab models as it was proposed in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>#Params Ratio-to-HRNet18 #FLOPs Ratio-to-HRNet18 18, HRNet-32 for the sake of brevity. The numbers of parameters and FLOPs of our models is shown in <ref type="table">Table 4</ref>. All the networks are initialized by the weights pre-trained on ImageNet weights. We use the OCR module proposed in <ref type="bibr" target="#b33">[34]</ref> for all HRNets. The original OCR module always produces 512 output channels regardless of the size of a backbone. We proportionally reduce the width of all the OCR layers, so that it produces 48, 64 and 128 channels for HRNet-18s, HRNet-18 and HRNet-32, respectively. Implementation details. The training task is binary segmentation with normalized focal loss, described in Section 3.4, as an objective. We use image crops with a size of 320 × 480 and randomly resize images with a scale factor from 0.75 to 1.40 before cropping. We use horizontal flip and random jittering of brightness, contrast, and RGB values as augmentations during training. We adopt test time augmentations from f-BRS method <ref type="bibr" target="#b3">[4]</ref>, using Zoom-In technique and averaging predictions from original and horizontally flipped image during evaluation.</p><p>In all our experiments, we use Adam optimizer with 1 = 0.9, 2 = 0.999 and train the models for 55 epochs on the proposed COCO+LVIS dataset (49 epochs with learning rate 5 × 10 −4 , then learning rate is decreased by 10 times on 50th and 53rd epochs). The learning rate for backbone networks is set 10 times lower than the learning rate for the rest of the network. We denote one pass through each image of the dataset as an epoch. The batch size is set to 32. We train the networks based on HRNet-18s, HRNet-18 and HRNet-32 on 1, 2 and 4 Tesla P40, respectively. To train the model based on ResNet-34, we use 1 Tesla P40.</p><p>We implement the proposed method using the PyTorch [49] framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Convergence Analysis</head><p>One of the key properties of an interactive segmentation algorithm is convergence to sufficient accuracy with an increasing number of clicks. Previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> improve the convergence using inference time optimization schemes, that force the predictions to match with input clicks. Our method is free from any refinement schemes and makes use of the proposed architecture and the iterative training scheme to achieve the desired level of convergence.</p><p>Sofiiuk et al. <ref type="bibr" target="#b3">[4]</ref> introduced the evaluation protocol for analysis of convergence for interactive segmentation methods. The main aspect of this protocol is the NoC 100 evaluation metric, which is similar to the NoC described earlier but has the clicks limit set to 100. Such size of interactive feedback should give an algorithm enough information to converge. In case the algorithm can not reach the IoU threshold, the image is supposedly too hard for the method to handle, therefore increasing the click limit is unlikely to change it. <ref type="table">Table 5</ref> shows the comparison of the the results of BRS <ref type="bibr" target="#b1">[2]</ref>, f-BRS-B <ref type="bibr" target="#b3">[4]</ref> and the proposed method with HR-Net18+OCR backbone, reporting NoC 100 , number of images for which the 90% threshold on IoU was not achieved after 20 and 100 clicks. All models are trained on the SBD <ref type="bibr" target="#b38">[39]</ref> dataset for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies</head><p>Network architecture ablations. In Section 3.1, we discuss different architectural choices for interactive segmentation networks. First, we explore different strategies of feeding encoded clicks to the model (DMF, Conv1E and Conv1S). We find that the HRNet-18 and ResNet-34 models with Conv1S show better performance in general and we use it in all further experiments. Then, we evaluate different clicks encoding strategies. We find that changing the distance transform encoding to the disk encoding significantly improves results of both HRNet-18 and ResNet-34, which confirms the findings of Benenson et al. <ref type="bibr" target="#b9">[10]</ref> whose model performed best with disks with a radius of 3. However, our experiments show that a radius of 5 is better than 3. The results of all ablations can be found in <ref type="table">Table 1</ref>. <ref type="table">Table 5</ref> Convergence analysis on Berkeley, SBD and DAVIS. We report the number of images that were not correctly segmented after 20 and 100 clicks and the NoC 100 @90 metric. All models are trained on SBD.  In addition, these experiments clearly demonstrate the superiority of HRNet-18 over DeepLabV3+ with ResNet-34, especially when the former has several times fewer parameters and FLOPs than the latter (the number of parameters and FLOPs can be found in <ref type="table">Table 4</ref>). We use HRNet with the Conv1S input scheme and encode clicks with disks of radius 5 in all other experiments.</p><p>Training datasets comparison. To further research the findings from Section 4, we evaluate the models trained on each of the six common segmentation datasets: ADE20k <ref type="bibr" target="#b49">[50]</ref>, OpenImages <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>, SBD <ref type="bibr" target="#b38">[39]</ref>, Pascal VOC <ref type="bibr" target="#b39">[40]</ref> augmented with labels from SBD (denoted as Pascal VOC+SBD), LVIS <ref type="bibr" target="#b12">[13]</ref>, COCO <ref type="bibr" target="#b11">[12]</ref> and also the COCO+LVIS dataset, proposed in Section 4.2. We provide results for non-iterative baseline methods with HRNet-18+OCR and ResNet-34 backbones, trained with NFL loss. We report NoC@90 on Berkeley, SBD and DAVIS in <ref type="table" target="#tab_2">Table 3</ref>. The performance on COCO and COCO+LVIS is relatively close, but the model trained on the proposed dataset generalizes better due to wider class distribution in the training set. Another observation is that the models trained on SBD and Pascal VOC+SBD show the best performance on the SBD dataset in terms of NoC@90, which is most likely caused by the similar distribution of training and testing sets. Nonetheless, these models are inferior to the model trained on the COCO+LVIS dataset in terms of performance on other benchmarks. All models trained for dataset comparison share the same training parameters and augmentations, that are described at the beginning of Section 5. We train the models for the following number of epochs on each dataset: 180 for ADE20k, 5 for OpenImages, 120 for SBD and Pascal VOC+SBD, 55 for LVIS and COCO+LVIS, 40 for COCO. We denote one pass through each image of the dataset as an epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss functions comparison.</head><p>We compare four loss functions that were used for training segmentation methods in recent works: binary cross entropy (BCE) loss, focal loss (FL) <ref type="bibr" target="#b36">[37]</ref>, soft IoU loss <ref type="bibr" target="#b50">[51]</ref> and normalized focal loss (NFL) <ref type="bibr" target="#b37">[38]</ref>. We evaluate the performance of the baseline HRNet-18+OCR model trained on the COCO+LVIS dataset, described in Section 4.2. The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. We provide only NoC@90 on all standard datasets for simplicity. The evaluation results support the reasoning in Section 3.4, demonstrating that training with NFL leads to better accuracy and convergence on all 4 datasets. Iterative training ablations. The main hyperparameter of our iterative sampling scheme is the maximum number of iteratively sampled clicks in addition to some set of randomly sampled clicks. Our experiments show that = 3 is an optimal value of that parameter. Surprisingly, too high values (&gt; 4) lead to instability during training and to worse results. We had to train models with = 5, 6 for several times, as they collapsed after 10-20 epochs of training on COCO+LVIS and showed poor results. We provide the ablation study in <ref type="table" target="#tab_5">Table 6</ref>.</p><p>We study the impact of feeding a mask from the previous click to the iterative model with = 3. Apart from metrics improvement, we observe substantial improvement of the stability of the model when new clicks are added. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, both the iterative model without a previous mask and the non-iterative model sometimes have drops of mean IoU when the number of clicks increases. It indicates that adding a new click during the process of interactive segmentation can even make the mask worse, contradicting user expectations. This effect is better illustrated on mean IoU plots for small datasets such as GrabCut or Berkeley, as the smoothness of the curve is proportional to the dataset size. When the model relies on a mask from a previous iteration, it can take into account the segmentation result of a previous iteration and avoid unexpected collapse of the current mask. Moreover, it allows to modify existing masks without additional effort. We discover that the trained model can be successfully initialized with an external inaccurate mask without the history of previous clicks to correct the errors of this mask. Several examples of applying our to this use case are shown in <ref type="figure">Figure 1</ref>.  <ref type="bibr" target="#b0">[1]</ref>. Green and red dots denote positive and negative clicks, respectively. There are only 2 images from Berkeley on which our model does not converge to 90% IoU in 20 clicks. One of them is shown in the third row. <ref type="table">Table 7</ref> Evaluation results on GrabCut, Berkeley, SBD, DAVIS and Pascal VOC datasets. The best and the second best results are set in bold and underlined, respectively. "H18s", "H18" and "H32" stand for "HRNet-18s", "HRNet-18" and "HRNet-32", respectively. Our models with the "IT-M" suffix are iterative models that take a mask from a previous step with = 3, otherwise they are our non-iterative baselines. The name of the training dataset is indicated below the word "Ours". "C+L" stands for COCO+LVIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GrabCut </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Previous Works</head><p>The quantitative results are summarized in <ref type="table">Table 7</ref>. We notice that even the proposed baseline model with HR-Net18+OCR backbone outperforms all previous methods. The proposed iterative method that uses a mask from the previous click sets a new state-of-the-art in interactive segmentation on all five benchmarks. Another notable fact is that the smallest among the proposed models with HRNet18s+OCR backbone performs on par with heavier ones, making it possible to use the proposed method on devices with low computational capability.</p><p>We provide plots of mean IoU with respect to the number of clicks for the GrabCut, Berkeley, DAVIS and SBD datasets in <ref type="figure" target="#fig_4">Figure 6</ref>. The proposed method outperforms previous state-of-the-art works and shows great improvement in accuracy and stability while avoiding accuracy drops after any click and converging to a better result.</p><p>We also visualise evaluation process of the proposed iteratively trained HRNet-18 model on some images from the Berkeley dataset in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Most of recent studies on interactive segmentation propose models that heavily rely on additional inference-time optimization schemes. In our work, we have demonstrated that a pure feedforward model with a modern backbone architecture can achieve or even surpass current state-of-the-art results. We have introduced a new model able to modify existing segmentation masks as well as segment new objects without any prior masks. It sets a new state-of-the-art on all common interactive segmentation benchmarks.</p><p>Our experiments have proved that a training dataset has a major impact on the model performance. We have proposed to combine the two existing instance segmentation datasets, COCO <ref type="bibr" target="#b11">[12]</ref> and LVIS <ref type="bibr" target="#b12">[13]</ref>, and use it for interactive segmentation. The resulting large and diverse dataset with high-quality annotations has significantly improved the generalization ability of our model. Training on this dataset allows to push the state-of-the-art results achieved by our model even further. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of two different approaches for encoding user clicks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Different architecture choices of feeding encoded clicks to a backbone, described in Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Mean IoU@ for varying number of clicks on GrabCut, Berkeley, DAVIS and SBD. The iterative model that takes a mask from a previous step is much more stable and converges to a better IoU. All the results are reported for the model with the HRNet-18+OCR backbone trained on COCO+LVIS, iterative models are trained with = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of interactive segmentation for the Berkeley images with a different number of clicks fed to the HRNet-18 ITER-M model and obtained by the NoC evaluation procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Mean IoU@ for varying number of clicks on GrabCut, Berkeley, DAVIS and SBD. The iterative models (denoted as ITER-M) show stable performance without accuracy drops and converge to a better IoU. Names of the training datasets are enclosed in parentheses. "C+L" stands for COCO+LVIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Evaluation of the non-iterative baseline method with the HRNet-18+OCR backbone trained on COCO+LVIS (proposed in Section 4.2) with different loss functions. We report NoC@90 on four datasets.</figDesc><table><row><cell>Method</cell><cell cols="4">NoC 20 @90 GrabCut Berkeley SBD DAVIS</cell></row><row><cell>BCE</cell><cell>1.82</cell><cell>3.13</cell><cell>7.58</cell><cell>6.31</cell></row><row><cell>Soft IoU</cell><cell>2.02</cell><cell>3.03</cell><cell>7.94</cell><cell>6.45</cell></row><row><cell>FL</cell><cell>1.80</cell><cell>3.28</cell><cell>7.56</cell><cell>6.40</cell></row><row><cell>NFL</cell><cell>1.70</cell><cell>2.48</cell><cell>6.72</cell><cell>5.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Evaluation results for the non-iterative baseline method with the ResNet-34 and HRNet-18+OCR backbones trained on various datasets. NoC@90 is reported.</figDesc><table><row><cell>Train Dataset</cell><cell>Backbone</cell><cell>GrabCut</cell><cell cols="2">NoC 20 @90 Berkeley SBD</cell><cell>DAVIS</cell></row><row><cell>ADE20k</cell><cell>ResNet-34 HRNet-18</cell><cell>2.70 2.68</cell><cell>5.09 4.78</cell><cell>8.27 8.02</cell><cell>8.56 8.32</cell></row><row><cell>OpenImages</cell><cell>ResNet-34 HRNet-18</cell><cell>2.20 2.02</cell><cell>4.68 4.47</cell><cell>8.16 7.95</cell><cell>7.47 8.08</cell></row><row><cell>SBD</cell><cell>ResNet-34 HRNet-18</cell><cell>2.94 2.41</cell><cell>4.73 3.95</cell><cell>6.94 6.66</cell><cell>7.56 7.17</cell></row><row><cell>Pascal VOC+SBD</cell><cell>ResNet-34 HRNet-18</cell><cell>2.77 2.25</cell><cell>4.53 3.63</cell><cell>6.92 6.63</cell><cell>6.48 6.16</cell></row><row><cell>LVIS</cell><cell>ResNet-34 HRNet-18</cell><cell>2.59 2.44</cell><cell>3.61 3.13</cell><cell>7.99 8.14</cell><cell>6.98 7.18</cell></row><row><cell>COCO</cell><cell>ResNet-34 HRNet-18</cell><cell>1.80 1.77</cell><cell>3.34 2.90</cell><cell>6.29 6.32</cell><cell>6.11 5.85</cell></row><row><cell>COCO+LVIS</cell><cell>ResNet-34 HRNet-18</cell><cell>1.74 1.70</cell><cell>2.91 2.48</cell><cell>6.53 6.86</cell><cell>6.01 6.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Ablation studies on the maximum number of iterations in the iterative sampling. All the results are reported for the model with the HRNet-18+OCR backbone trained on COCO+LVIS.</figDesc><table><row><cell></cell><cell>Prev</cell><cell cols="2">NoC 20 @90</cell><cell></cell></row><row><cell></cell><cell>Mask</cell><cell>Berkeley</cell><cell>DAVIS</cell><cell>SBD</cell></row><row><cell>3</cell><cell>-</cell><cell>2.38</cell><cell>5.92</cell><cell>6.49</cell></row><row><cell>3</cell><cell>+</cell><cell>2.26</cell><cell>5.74</cell><cell>6.06</cell></row><row><cell>1</cell><cell>+</cell><cell>2.57</cell><cell>5.81</cell><cell>6.15</cell></row><row><cell>2</cell><cell>+</cell><cell>2.48</cell><cell>5.70</cell><cell>6.10</cell></row><row><cell>3</cell><cell>+</cell><cell>2.26</cell><cell>5.74</cell><cell>6.06</cell></row><row><cell>4</cell><cell>+</cell><cell>2.52</cell><cell>6.03</cell><cell>6.04</cell></row><row><cell>5</cell><cell>+</cell><cell>2.49</cell><cell>5.98</cell><cell>6.24</cell></row><row><cell>6</cell><cell>+</cell><cell>2.55</cell><cell>6.11</cell><cell>6.82</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/HRNet/HRNet-Image-Classification</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment.</head><p>We thank Julia Churkina for her assistance with editing and for comments that greatly improved the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.47</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00544</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5297" to="5306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Continuous adaptation for interactive object segmentation by learning from corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58517-4_34</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="579" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F-Brs</forename></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00865</idno>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8623" to="8632" />
		</imprint>
	</monogr>
	<note>Rethinking backpropagating refinement for interactive segmentation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Error-tolerant scribbles based interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.57</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ScribbleSup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.344</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive graph cut based segmentation with shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2005.191</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00071</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-RNN++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00096</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01197</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11700" to="11709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive full image segmentation by considering all regions jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01189</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11622" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00550</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in n-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2001.937505</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2010.5540073</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3129" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GrabCut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00067</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regional interactive image segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.297</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MultiSeg: Semantically meaningful, scale-diverse segmentations from minimal user input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00075</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="662" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with first click attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01335</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13339" to="13348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DenseCut: Densely connected CRFs for realtime GrabCut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MILCut: A sweeping line multiple instance learning paradigm for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.40</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive object segmentation with inside-outside guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01225</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12234" to="12244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative image segmentation using random walks with restart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88690-7_20</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="264" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PhraseClick: Toward achieving flexible interactive segmentation by phrase and click</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58580-8_25</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="417" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_5</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SegFix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58610-2_29</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01057</idno>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10558" to="10567" />
		</imprint>
	</monogr>
	<note>Deepstrip: High-resolution boundary refinement</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58539-6_11</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Content-aware multi-level guidance for interactive instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01187</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11602" to="11611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AdaptIS: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2019.00745</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7355" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2011.6126343</idno>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01406</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14045" to="14054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.01168</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11662" to="11671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2001.937655</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comparative evaluation of interactive segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="434" to="444" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sorkine-Hornung, A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.85</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. B. Fox, R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.544</idno>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
	<note>Scene parsing through ADE20k dataset</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Optimizing intersection-over-union in deep neural networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50835-1_22</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
