<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Ranade</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
							<email>aaagrawa@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering 3D human pose from 2D joints is a highly unconstrained problem. We propose a novel neural network framework, PoseNet3D, that takes 2D joints as input and outputs 3D skeletons and SMPL body model parameters. By casting our learning approach in a student-teacher framework, we avoid using any 3D data such as paired/unpaired 3D data, motion capture sequences, depth images or multi-view images during training. We first train a teacher network that outputs 3D skeletons, using only 2D poses for training. The teacher network distills its knowledge to a student network that predicts 3D pose in SMPL representation. Finally, both the teacher and the student networks are jointly fine-tuned in an end-to-end manner using temporal, self-consistency and adversarial losses, improving the accuracy of each individual network. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach reduces the 3D joint prediction error by 18% compared to previous unsupervised methods. Qualitative results on in-the-wild datasets show that the recovered 3D poses and meshes are natural, realistic, and flow smoothly over consecutive frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurately estimating 3D pose from 2D landmarks is a classical ill-posed problem in computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. Due to projective ambiguity, there exists an infinite number of 3D poses corresponding to a given 2D skeleton <ref type="bibr" target="#b5">[6]</ref>. This makes prediction of 3D joints from 2D landmarks (lifting) a challenging task. To address these issues, previous 2D to 3D approaches have used various kinds of additional 3D supervision, including paired 2D-3D correspondences <ref type="bibr" target="#b39">[40]</ref>, unpaired 3D data <ref type="bibr" target="#b27">[28]</ref>, multi-view images <ref type="bibr" target="#b55">[56]</ref> and synthetic data generated using motion capture (MoCap) sequences <ref type="bibr" target="#b51">[52]</ref>. Acquiring MoCap data is expensive and timeconsuming, and hence not scalable to new applications. Moreover, since 3D datasets do not represent all dimensions of variability in human motion, such as human shapes and sizes, appearance and clothing, environment and light-Equal Contribution ing, limb articulations etc., models trained on these datasets don't generalize to real-world scenarios <ref type="bibr" target="#b28">[29]</ref>. In this work, we present a novel training framework, PoseNet3D, to estimate 3D human pose and shape using only 2D data as input. By starting from 2D poses, our approach allows us to train on video datasets, enabling generalization across diverse in-the-wild scenarios and emancipating us from the data-bottleneck of supervised approaches.</p><p>Previous methods for 3D pose prediction can be classified as model-free and model-based. Typically, model-free approaches directly learn a mapping from 2D landmarks to 3D joints <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40]</ref>. Model-based approaches fit 3D parametric models such as SMPL <ref type="bibr" target="#b37">[38]</ref> to estimate 3D shape and pose. This is typically done by minimizing the 2D error between the projection of the predicted 3D pose and the given 2D landmarks. However, as shown in <ref type="bibr" target="#b27">[28]</ref>, 2D reprojection error alone is highly under-constrained and can be minimized via non-natural joint angles. Lack of 3D supervision further aggravates this problem.</p><p>In this paper, our goal is to train a neural network that takes 2D pose (landmarks) as input and outputs SMPL parameters and 3D skeletons, without requiring any additional 3D data or iterative fitting during training. We first train a lifting network (aka teacher) using only 2D inputs to predict model-free 3D poses. The 3D pose output from the teacher is then used as pseudo ground truth to train a student network to predict SMPL pose parameters. Thus, our teacherstudent formulation allows training the network in the absence of additional 3D data. In fact, we show that training the student network by directly minimizing the 2D reprojection error (without using knowledge from the teacher) fails due to inherent ambiguities in 2D projection, resulting in incorrect depth predictions and unnatural poses.</p><p>When using a parametric model such as SMPL, there often exists a semantic gap between the SMPL 3D joints and the 2D landmarks obtained from RGB images (e.g. using OpenPose <ref type="bibr" target="#b3">[4]</ref>). For example, the 3D hip joints in SMPL are close to the center of the pelvis, while in the Hu-man3.6M <ref type="bibr" target="#b23">[24]</ref> dataset, the 2D hip joints are close to the body surface. In previous works, this semantic adaptation is learned offline by fitting SMPL meshes to specific 3D datasets and is used during evaluation. Thus, 3D data is also required implicitly for bridging the aforementioned se-  <ref type="figure" target="#fig_1">Figure 1</ref>: Overview of the proposed PoseNet3D approach. Input 2D poses are fed to a temporal backbone, followed by a teacher branch and a student branch, which output model-free 3D poses and SMPL parameters respectively. mantic gap. In contrast, we demonstrate that the semantic adaptation can be automatically learned during training to bridge the gap between the SMPL 3D joints and the 2D landmarks. Bridging this semantic gap, which is often ignored in previous works is crucial, otherwise the network can minimize joint error by twisting the body, resulting in unnatural poses.</p><p>Our approach builds upon Chen et al. <ref type="bibr" target="#b5">[6]</ref> who train the teacher (lifter) network in an unsupervised manner. However, different from <ref type="bibr" target="#b5">[6]</ref>, the primary contribution of our work demonstrates how to recover SMPL parameters from video, without requiring any 3D data or 3D pose priors for training. To the best of our knowledge, ours is the first work that shows this is feasible. It is important to note that <ref type="bibr" target="#b5">[6]</ref> do not output SMPL parameters and we only use lifting as a component in our pipeline. By estimating SMPL parameters, we solve a different and arguably harder problem beyond lifting, similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Our secondary contribution is to improve the lifting component used in our pipeline (over <ref type="bibr" target="#b5">[6]</ref>), by incorporating temporal information via dilated convolutions, a temporal discriminator, and novel temporal consistency losses.</p><p>We evaluate our approach on 3D human pose estimation tasks on Human3.6M, MPI-INF-3DHP and 3DPW datasets, reducing the mean per joint position error by 18% compared to the state-of-the-art unsupervised method of <ref type="bibr" target="#b5">[6]</ref> (47mm vs 58mm) as shown in Sect. 4. Qualitative results confirm that our method is able to recover complex 3D pose articulations on previously unseen in-the-wild images (e.g. <ref type="figure" target="#fig_2">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several deep learning techniques have been proposed to estimate 3D joints directly from 2D images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>. We build upon approaches that decompose the problem into estimation of 2D joints from images followed by the estimation of 3D pose. Obtaining 2D joints from images is a mature area in itself and several approaches such as CPM <ref type="bibr" target="#b69">[70]</ref>, Stacked-Hourglass (SH) <ref type="bibr" target="#b44">[45]</ref>, Mask-RCNN <ref type="bibr" target="#b19">[20]</ref> or affinity models <ref type="bibr" target="#b3">[4]</ref> can be used.</p><p>Previous 2D to 3D approaches can be broadly classified into (a) model-free methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref> and (b) model-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b71">72]</ref>. Several such approaches have used 3D supervision during training. The 3D information has been used in various forms such as paired 2D-3D data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref>, 3D pose priors (e.g. Gaussian Mixture Model) built using 3D Mo-Cap sequences <ref type="bibr">[2,</ref><ref type="bibr" target="#b31">32]</ref>, learned priors using 3D data via a discriminator <ref type="bibr" target="#b27">[28]</ref>, and synthetic 2D-3D pairings <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b71">72]</ref>.</p><p>Our key contribution is a novel combination of model-based and model-free predictions, without requiring additional 3D data during training.</p><p>Approaches such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67]</ref> have primarily used 2D joints from single/multi-view images without explicit 3D supervision to learn 3D pose. Chen et al. <ref type="bibr" target="#b5">[6]</ref> proposed an unsupervised algorithm for lifting 2D poses to 3D. Our teacher network builds upon the work of Chen et al. <ref type="bibr" target="#b5">[6]</ref>, but differs in the following respects. Firstly, unlike <ref type="bibr" target="#b5">[6]</ref>, our approach is able to estimate SMPL parameters. Secondly, in <ref type="bibr" target="#b5">[6]</ref>, inference uses a single frame as input and weak temporal consistency is enforced using an additional discriminator on frame differences. Their architecture only employs fully connected layers. In contrast, we use dilated convolutions (similar to <ref type="bibr" target="#b52">[53]</ref>) to model temporal dynamics in the lifter as well as in the discriminator and train/test on multiframe inputs. Video based approaches such as Li et al. <ref type="bibr" target="#b35">[36]</ref> employ 3D trajectory optimization via low rank property and temporal smoothness of video sequences. Instead, we propose novel loss terms to account for the consistency of predicted skeletons on common frames across neighboring time-steps and show improvement in accuracy on the Hu-man3.6M dataset over <ref type="bibr" target="#b5">[6]</ref>. Deep Learning with SMPL: Deep learning approaches such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64]</ref> have utilized SMPL to directly regress to the underlying shape and pose parameters by training a feed-forward network. The 3D joints are computed via linear regression on the estimated mesh vertices <ref type="bibr" target="#b27">[28]</ref>. Our student network also predicts SMPL parameters but differs from these approaches in following respects.</p><p>Firstly, approaches such as <ref type="bibr" target="#b27">[28]</ref> minimize the 2D reprojection error between the projection of the SMPL 3D joints and the predicted 2D joints from images. However, as noted in <ref type="bibr" target="#b27">[28]</ref>, 2D keypoint loss is highly unconstrained and thus <ref type="bibr" target="#b27">[28]</ref> learns the limits of joint angles using a dataset of 3D scans. Since we do not assume access to any additional 3D data at training time, we address this problem by first training a teacher network to predict 3D joint positions. We then use the output of the teacher as pseudo groundtruth to train the student network to predict SMPL parameters. By using knowledge distillation from the teacher along with simple regularizers on the SMPL parameters, we can recover realistic 3D pose without requiring additional 3D information during training. Our ablation studies show that the proposed strategy significantly outperforms the baseline strategy of directly minimizing the 2D re-projection error.</p><p>Secondly, previous works typically ignore the semantic gap between the SMPL 3D joints and the 2D landmarks while training. Instead, a regressor from vertices to joints <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref> is obtained offline by fitting SMPL meshes to specific 3D datasets (e.g. Human3.6M). In contrast, we bridge this gap by using online semantic joint adaptation (SJA) during training of the student network. We demonstrate that SJA improves the accuracy as well as naturalness of the predicted 3D pose. SMPL based Optimization: Classical optimization techniques have also been used to fit the SMPL model to 2D landmarks/silhouettes <ref type="bibr">[2,</ref><ref type="bibr">18,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b41">42]</ref>. The optimization based approaches are typically slow, prone to error and require good initialization as well as 3D pose priors built using MoCap sequences. In contrast, our approach trains a feed-forward network allowing for a faster and more robust inference. Recently, Kolotouros et al. <ref type="bibr" target="#b31">[32]</ref> (SPIN) have used iterative optimization within the training loop of a deep neural network to generate pseudo ground truth, which is used to provide direct supervision on SMPL parameters for regression. Instead, our teacher network provides supervision on 3D joints, obtained by SMPL forward kinematics. In contrast to SPIN, we do not use a 3D pose prior learned using CMU MoCap sequences (similar to <ref type="bibr">[2]</ref>). Our approach can also be extended to use optimization in the loop to generate additional supervision on SMPL parameters, which we leave for future work. Knowledge Distillation: Distilling the knowledge in neural networks has been used for applications such as network compression, combining an ensemble of models into a single model <ref type="bibr" target="#b20">[21]</ref>, enhancing privacy <ref type="bibr" target="#b46">[47]</ref>, large scale semi-supervised learning <ref type="bibr" target="#b72">[73]</ref> etc. Wang et al. <ref type="bibr" target="#b66">[67]</ref> propose a knowledge-distillation framework to use non-rigid structure from motion (NRSfM) alogrithm as the teacher to generate pseudo ground truth for training a student network. We borrow the same terminology but our 2D-3D lifter as a teacher shows better results on Human3.6M than <ref type="bibr" target="#b66">[67]</ref>. Unlike <ref type="bibr" target="#b66">[67]</ref>, our approach also outputs SMPL parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Our PoseNet3D approach is a combination of model-free 3D pose estimation followed by knowledge distillation to predict SMPL pose parameters. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the input to our network is a set of T 2D skeletons from T consecutive frames of a video. The architecture consists of a temporal backbone, which utilizes dilated convolutions over time to model the temporal dynamics and produces a feature vector. The feature vector is fed to two branches: (a) Teacher branch, which outputs 3D poses, and (b) Student branch, which outputs SMPL parameters. The 3D joints from the student branch are computed as described in Section 3.3. The two sets of 3D joints from the student and teacher branches are compared to ensure consistency. The predicted 3D joints from the teacher and the student branches are re-projected to 2D after random rotations and are fed to a temporal discriminator. In the following sections, we describe the teacher and student networks and associated training losses in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Teacher: Temporally Consistent Lifting</head><p>Let x j i = (x j i , y j i ), i = 1, . . . , N denote the i th 2D pose landmark of a skeleton in frame j with the root joint (midpoint between the hip joints) as origin. The 2D skeleton for frame j is x j = x j 1 , . . . , x j N . The input to the network at time step t is a set of T 2D skeleton frames of the same subject, represented as x(t) = x t , . . . , x t+T −1 . For simplicity, we drop the dependence on time-step to describe the lifter. Similar to <ref type="bibr" target="#b5">[6]</ref>, we assume a perspective camera with unit focal length centered at the origin and fix the distance of the 3D skeleton to the camera to a constant c units. The 2D skeletons are normalized such that the mean distance from the head joint to the root joint is 1 c units in 2D. At each time-step t, the teacher branch predicts a depth offset o j i for each x j i . The 3D joints are computed as</p><formula xml:id="formula_0">X j i = (x j i z j i , y j i z j i , z j i ), where z j i = max(1, c+o j i ).</formula><p>The generated skeletons are projected back to 2D via random projections. Let Q be a random rotation matrix. The rotated 3D skeleton Y j i is obtained as</p><formula xml:id="formula_1">Y j i = Q(X j i − X j r ) + C,<label>(1)</label></formula><p>where X j r is the predicted root joint of j th skeleton and C = (0, 0, c) T . Let y j i denote the 2D projection of Y j i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Losses for the Teacher Network</head><p>Multi-Frame Self-Supervision Loss: Let G T denote the teacher network that predicts the model-free 3D pose X j i = G T (x j i ) as defined in Section 3.1. We also lift the reprojected 2D skeletons to obtain</p><formula xml:id="formula_2">Y j i = G T (y j i ) using the same network. If G T (·) is accurate, Y j i should match Y j i .</formula><p>Therefore, we define our multi-frame self-supervision loss as</p><formula xml:id="formula_3">L mss = N i T j Y j i − Y j i 2 .<label>(2)</label></formula><p>Temporal Consistency Loss: Since we predict T 3D skeletons at each time step t, common frames exist between neighboring time-windows. Using a sliding window with temporal stride 1, we have T − 1 frames in common between time-step t and t + 1. We use an L 2 loss to enforce consistency between these common frames in 3D,</p><formula xml:id="formula_4">L tc = T −1 j=1 X j+1 i (t) − X j i (t + 1) 2 .<label>(3)</label></formula><p>Bone Length Loss: At each time step t, we enforce that the bone lengths for the T predicted 3D skeletons be consistent by minimizing the variance of bone lengths over the T frames. Let b(m, n, j) = X j m − X j n denote the bone length between the m th and n th predicted 3D joints for frame j. Bone length loss is defined as</p><formula xml:id="formula_5">L bl = N m=1 n∈N (m) Var j (b(m, n, j)),<label>(4)</label></formula><p>where N (m) denotes the set of connected skeleton joints for joint m and Var j denotes variance over T frames. Temporal Discriminator: The discriminator provides feedback to the lifter regarding the realism of projected 2D skeletons. In contrast to <ref type="bibr" target="#b5">[6]</ref>, which uses a single frame discriminator and a frame-difference discriminator, we use a single temporal discriminator that takes a set of T reprojected/real 2D skeletons as input. Previous approaches have used RNN and LSTM to model sequential/temporal data. A challenge in using RNN/LSTM is delayed feedback which requires the use of a policy gradient to back-propagate the feedback from the discriminator <ref type="bibr" target="#b10">[11]</ref>. In contrast, our temporal discriminator(D) uses dilated convolutions and provides feedback at each time-step, simplifying the training. Formally, the discriminator is trained to distinguish between sequences of T real 2D skeletons r(t) = r 1 , . . . , r T (target probability of 1) and fake (projected) 2D skeletons y(t) = y 1 , . . . , y T (target probability of 0). We utilize a standard adversarial loss <ref type="bibr" target="#b16">[17]</ref> defined as</p><formula xml:id="formula_6">L T D = min Θ T max Θ D E(log(D(r(t))) + E(log(1 − D(y(t)))),<label>(5)</label></formula><p>where Θ T and Θ D denote the parameters of the teacher and the discriminator networks, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Student: Estimating SMPL Parameters</head><p>For our model-based approach, we use the Skinned Multi-Person Linear (SMPL) representation <ref type="bibr" target="#b37">[38]</ref>. SMPL is a parametric model that factors human bodies into a shape (body proportions) and pose (articulation) representation. The shape is parameterized using a PCA subspace with 300 basis shapes and shape coefficients (β). The human pose is modeled as a set of 24 local joint angles corresponding to K = 24 3D joints (including root joint) and is represented as 72 axis-angle coefficients. We directly predict the rotation matrix corresponding to each joint from the network, on which we perform a differentiable ortho-normalization. Let R = {R 1 , . . . , R K } denote the set of K rotation matrices. Given a set of parameters β and R, SMPL produces a mesh V = M(β, R), V ∈ R 6890×3 with 6890 vertices, where M is differentiable. Note that the 3D joints by themselves do not fully constrain the shape of the body and it is not possible to predict accurate shape using 3D joints alone. Approaches such as <ref type="bibr" target="#b51">[52]</ref> have additionally used silhouettes to estimate shape and thus accurate shape prediction is not a goal of this paper. We only predict the first 10 β parameters (common for all T frames) and set the remaining to zero. Thus, the student network has a total of 10 + 24 × 9 × T = 10 + 216 × T outputs at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Semantic Joint Adaptation (SJA)</head><p>The 3D joints J ∈ R 24×3 are obtained by linear regression from the final mesh vertices V . The linear regressor is a sparse matrix W ∈ R 24×6890 , which represents a convex combination of vertices for each 3D joint. Hence, J = W V . The pre-trained linear regressor in SMPL produces 3D joints that are often semantically different from the 2D joints obtained from 2D pose detectors or annotations on datasets. For example, in SMPL the 3D hip joints are closer to the center of the body. However, in Hu-man3.6M 2D annotations, the hip landmarks are closer to the periphery. Our SJA module learns the adaptation of the SMPL 3D joints to 2D joints used for training.</p><p>We first experimented with a linear layer that learns a weight matrix A ∈ R 72×72 and a bias vector b ∈ R 72×1 , which is applied to the 72 × 1 vectorized representation of J to adapt the SMPL joints (referred to as Linear-SJA),</p><formula xml:id="formula_7">J = AJ + b.<label>(6)</label></formula><p>However, such an approach fails in practice. Since there is no constraint on joints, the network can potentially minimize joint error by moving the SMPL joints outside the body <ref type="figure" target="#fig_0">(Fig. 2)</ref>. To avoid such pitfalls, similar to SMPL, we learn a convex combination of vertices W , resulting in 24 × 6890 = 165, 360 additional learnable parameters and obtain the new joints as J = W V . Visualization in <ref type="figure">Fig. 3</ref> show that weights for the learned regressor on Human3.6M shifts from the center of body towards the surface, corresponding to a similar shift in 2D hip landmarks. For the rest of the paper, SJA refers to the learned convex combination of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses for the Student</head><p>The following losses are used to train the student network via knowledge distillation. Knowledge Distillation Loss: We define a loss between the model-free prediction of 3D joints X j i and the 3D joints obtained via the SMPL model. To account for the mismatch between the number of joints, we choose N (14 in our case) relevant joints from the 24 SMPL joints. Let I(i) denote the index of the SMPL joint corresponding to i th input 2D joint. L KD is computed as a sum of individual losses over each joint i and each frame j,</p><formula xml:id="formula_8">L KD = T j=1 N i=1 X j i − W I(i) M(β, R j ) 2 ,<label>(7)</label></formula><p>where W I(i) denote the row of matrix W corresponding to the regressor weights for joint I(i) and R j denotes the set of predicted rotation matrices for frame j.</p><p>Regularization of SMPL Parameters: In absence of any 3D data, we use a simple regularizer for pose parameters to avoid over-twisting by penalizing the deviation of the predicted rotation matrices from identity rotation.</p><formula xml:id="formula_9">L R = T j=1 K i=1 R j i − I 3×3 2 ,<label>(8)</label></formula><p>where I 3×3 is the 3 × 3 identity matrix. We use a similar L 2 regularizer for β, L β = β 2 , since β = 0 represents the average human shape. The β regularizer is used with a relatively larger weight during training to keep the shape close to the average shape. However, we show in Section 4.5 ( <ref type="figure" target="#fig_5">Fig. 7(a)</ref>) that without SJA, these regularizers by themselves are not sufficient to avoid unnatural predictions. Our novel SJA module helps improve the realism and naturalness of predicted pose parameters.</p><p>(a) (b) (c) <ref type="figure">Figure 3</ref>: (a) Original SMPL 3D joints (b) SMPL regression weights for the left-hip joint #2 are visualized by assigning a color to each vertex (dark red corresponds to higher weight) (c) Updated regressor weights for hip joint #2 after SJA.</p><p>Discriminator: Similar to the teacher network, the predicted 3D joints from the student network are reprojected to random views and fed to the discriminator. The corresponding discriminator loss is L S D , similar to L T D in Eqn. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training PoseNet3D</head><p>We train PoseNet3D following these steps: Hyper-parameters λ mss , λ tc , λ bl , λ R , λ β , λ S are defined in Sect. 4.1. Note that in step 4, we feed the re-projection of the 3D pose predicted from both the teacher and student networks to the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We evaluate on the widely used Human3.6M <ref type="bibr" target="#b22">[23]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">[41]</ref> and 3DPW <ref type="bibr" target="#b64">[65]</ref> datasets and show quantitative and qualitative results. We also show qualitative visualizations of reconstructed skeletons and meshes on in-thewild datasets such as Leeds Sports Pose <ref type="bibr" target="#b26">[27]</ref> (LSP) where ground-truth 3D data is not available. Since our approach takes temporal sequences, for inference on single frame input (e.g. LSP), we simply copy the frame T times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use N = 14 joints and randomly sample T = 9 frame sub-sequences of 2D poses from videos for training. The input poses are normalized such that the mean distance from the head joint to the root joint is 0.1 units, corresponding to placing the 3D skeletons at c = 10 units from the camera. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, our temporal backbone takes a 2N × T input followed by a conv-block, comprising convolution filter, batchnorm, ReLU and dropout. Each convolution filter has 1024 channels with a kernel size of 3 × 1 and temporal dilation factor of d = 1. The output of the conv-block is fed to a residual block with two conv-blocks, with a dilation ratio of d = 3 and d = 1, respectively. The teacher branch consists of an additional residual block with 2 fully-connected (FC) layers of size 1024 each. Similarly, the student branch consists of 4 FC residual blocks. The temporal discriminator architecture is identical to the temporal backbone architecture but does not use BatchNorm. We train on TitanX GPUs using the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with batchsize of 6000 and learning rate of 0.0001 for 150 epochs. The loss weights are empirically set as λ mss = 2, λ tc = 1, λ bl = 2, λ R = 30, λ β = 10 and λ S = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Metrics</head><p>Human3.6M (H3.6M): This is one of the largest 3D human pose datasets, consisting of 3.6 million 3D human poses. The dataset contains video and MoCap data from 11 subjects performing typical activities such as walking, sitting, etc. Similar to previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>, we report the mean per joint position error in mm after scaling and rigid alignment to the ground truth skeleton (P-MPJPE) on subjects S9 and S11 (all cameras). We only use 2D data from subjects S1, S5, S6, S7 and S8 for training a single activityagnostic model. To evaluate the smoothness of predicted 3D pose, we report mean per joint velocity error (MPJVE), which is calculated as the mean per joint error of the first derivative of the aligned 3D pose sequences (in mm/frame at 50 Hz). We also propose the mean bone-length stan-dard deviation (MBLSTD) metric as the average standard deviation (in mm) of 8 bone segments (corresponding to upper/lower/left/right hand/leg) over all sequences. Lower values of MPJPE, MPJVE, and MBLSTD signify better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP:</head><p>The MPI-INF-3DHP dataset consists of 3D data captured using a markerless MoCap system. We evaluate on valid images from test-set containing 2929 frames following <ref type="bibr" target="#b27">[28]</ref> and report P-MPJPE, Percentage of Correct Keypoints (PCK) @150mm, and Area Under the Curve (AUC) computed for a range of PCK thesholds. 3DPW: 3DPW [65] is a recent outdoor 3D dataset containing 60 videos wherein 3D ground-truth annotations are extracted using IMUs attached to body limbs. Similar to <ref type="bibr" target="#b31">[32]</ref>, we only use this dataset for evaluation. To handle missing joints, we follow <ref type="bibr" target="#b5">[6]</ref> and train a supervised joint filler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>We denote our results obtained by taking the average of the predicted 3D poses from the teacher and the student networks as PoseNet3D. Averaging dampens output noise and improves upon both branches (  <ref type="table" target="#tab_1">Table 1</ref>: Human3.6M. Comparison of P-MPJPE (in mm) for model-free 3D pose estimation. GT and IMG denote results obtained using ground truth 2D annotations and estimated 2D pose by SH/CPM <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b69">70]</ref> respectively. Best and second best results are bolded and underlined, respectively.</p><p>( †) using temporal information, ( ‡) using multi-view data. (17j) using 17 joints. (+) using additional data for training.</p><p>H3.6M only). Our best model reduces the P-MPJPE error from 58mm to 47mm (18% improvement). We also outperform several previous weakly-supervised approaches that use 3D information in training and with just 5% 3D data, our approach performs competitively with fully supervised approaches. Similarly, on H3.6M, our results are better than previous model-based approaches such as HMR and SPIN that use unpaired 3D data and produce SMPL meshes as output ( <ref type="table" target="#tab_3">Table 2)</ref>. On in-the-wild 3DPW dataset, we show comparable performance to many recent approaches that use 3D data or 3D pose priors <ref type="table" target="#tab_4">(Table 3)</ref>. Finally, <ref type="table" target="#tab_5">Table 4</ref> summarizes results on MPI-INF-3DHP. PoseNet3D model trained on H3.6M outperforms HMR <ref type="bibr" target="#b27">[28]</ref> and comes close to the results from SPIN <ref type="bibr" target="#b31">[32]</ref>, both of which were trained on MPI-INF-3DHP and used unpaired 3D data for training. This offers a strong evidence that PoseNet3D generalizes well to out-of-domain datasets (e.g., in this case trained on H3.6M and tested on MPI-INF-3DHP).      <ref type="bibr" target="#b44">[45]</ref>. ‡ denotes our implementation of <ref type="bibr" target="#b5">[6]</ref>.</p><p>discussed earlier, since our approach uses only 2D landmarks and cannot estimate accurate shape, the projected mesh may not align well with the human silhouette in the image. However, note that our approach is able to recover complicated articulations of the human body. <ref type="figure" target="#fig_3">Fig. 5</ref> shows predicted 3D skeletons from the teacher network on examples from H3.6M and LSP datasets. Finally, <ref type="figure" target="#fig_4">Fig. 6</ref> presents a few failure cases of our approach from H3.6M dataset. Please see supplementary material for additional examples.  <ref type="table" target="#tab_6">Table 5</ref> analyzes the impact of number of frames for the teacher network with and without fine-tuning in terms of MPJVE and MBLSTD. We implemented the approach of <ref type="bibr" target="#b5">[6]</ref> to compute similar metrics and our 9-frame teacher network outperforms their approach, reducing MPJVE and MBLSTD by more than 60% and 30%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>For the student branch, we first define two baselines. Since SMPL is a parametric model, a trivial baseline is to train the student network directly by minimizing the 2D re-projection error (Baseline 1). We also propose an additional baseline by employing SJA on top (Baseline 2). As  Without knowledge distillation, directly training the student network with 2D re-projection loss results in monster meshes, even when the 2D loss is small. noted in <ref type="bibr" target="#b0">[1]</ref> and specifically in <ref type="bibr" target="#b27">[28]</ref>, minimizing the 2D reprojection error without any 3D supervision can result in monster meshes with high P-MPJPE. We observe a similar phenomenon. As shown in <ref type="table" target="#tab_6">Table 5</ref> and <ref type="figure" target="#fig_5">Fig. 7</ref>(b), these baselines result in a high P-MPJPE and do not predict high quality poses. Finally, we analyze the effect of SJA on the student network using KD. Linear-SJA (Eqn. 6) results in low 3D error on training data but high 3D error on test data (PoseNet3D-S-LinearSJA). The network severely overfits on the training data by moving joints outside the body (see <ref type="figure" target="#fig_0">Fig. 2</ref> for examples). Without using SJA (PoseNet3D-S-noSJA), the error is higher than using SJA (PoseNet3D-S-SJA). Visualization in <ref type="figure" target="#fig_5">Fig. 7(a)</ref> compares the output of student network with and without SJA. Notice how the re-projected 3D skeleton is semantically closer to the input 2D skeleton with SJA (especially hip and head joints). SJA results in better 3D pose predictions confirming that in absence of any paired/unpaired 3D supervision, our semantic joint adaptation module is essential for training the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a knowledge distillation algorithm to learn SMPL pose parameters from 2D joints, without requiring additional 3D data for training. Our approach trains a feedforward network to predict SMPL parameters and does not require any iterative fitting. We first learn a teacher network to lift 2D joints to model-free 3D pose in a temporally consistent manner. The temporal dynamics are modeled using dilated convolutions in both lifter and discriminator, allowing feedback at every time-step and avoids common pitfalls in using LSTM/RNN in such settings. The teacher network provides pseudo ground truth to the student network which learns to predict SMPL pose parameters. We demonstrate how to bridge the semantic gap between the SMPL 3D joints and 2D pose landmarks during training, which has been largely ignored in previous literature. We believe that our paper has significantly improved the state-of-art in learning of 3D pose from 2D skeletons in absence of explicit 3D supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Our pipeline is implemented in python using the Py-Torch deep-learning library. Training takes 30 hrs on 4 Nvidia TitanX GPUs. Inference time, given 2D pose input, is 20ms on the same GPU. In the following subsections, we furnish relevant empirical details for our experiments in the manuscript.</p><p>Architecture of the Temporal Generator. The generator network consists of a shared temporal backbone, followed by a model-free teacher branch and a model-based student branch. We attach the pytorch model dump of the generator below.</p><p>The acronyms used in the pytorch model dump are described in <ref type="table">Table.</ref> 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acronym Meaning</head><p>ksz kernel size st stride pdng padding mmntm momentum in f in features out f out features dil dilation in(n) input to layer n   <ref type="formula" target="#formula_8">( 7 )</ref> : ReLU ( i n p l a c e ) <ref type="bibr" target="#b10">11</ref>  <ref type="bibr" target="#b7">( 8 )</ref> : Dropout ( p = 0 . 2 5 ) ) <ref type="bibr" target="#b11">12</ref> S h o r t c u t ( r e s + i n ( 3 ) ) <ref type="bibr" target="#b12">13</ref>  <ref type="bibr" target="#b8">( 9 )</ref> : L i n e a r ( ( i n f =1024 , o u t f =1 , b i a s = T r u e ) Camera Assumptions. Due to the fundamental perspective ambiguity, absolute metric depths cannot be obtained from a single view. To resolve this, we assume a camera with unit focal length centered at origin (0,0,0) and normalize the distance of the ground-truth 3D skeletons from the camera to a constant c = 10m and a constant scale (head to root joint distance) of 1m. We also normalize the input 2D skeletons such that the mean distance from the head joint to the root joint is 1 c = 0.1 units in 2D. This ensures that 3D skeletons will be generated with a constant scale of ≈ 1 m (head to root joint distance). z j i = max(1, c + o j i ) further constrains the predicted 3D skeleton to lie in front of the camera, with a margin of 1m from the camera. For 2D reprojections of generated skeletons, we restrict random camera rotation by uniformly sampling an azimuth angle between [−π, π] and an elevation angle between [−π/9, π/9].</p><p>We provide supplementary qualitative and quantitative results for our PoseNet3D approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Per-activity Evaluation</head><p>In <ref type="table">Table 7</ref>, we present P-MPJPE for each class in Hu-man3.6M dataset. The results shown use the 2D pose detections extracted by SH <ref type="bibr" target="#b44">[45]</ref> as input. We compare our results with the previous unsupervised approach of Chen et al. <ref type="bibr" target="#b5">[6]</ref> and other weakly-supervised methods <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b66">67]</ref>. We outperform previous unsupervised approach of <ref type="bibr" target="#b5">[6]</ref> on all activity classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporal Evaluation</head><p>In <ref type="figure" target="#fig_1">Fig. 11</ref>, we present a side-by-side comparison of the qualitative performance of our approach with two recently published approaches: HMR-Video <ref type="bibr" target="#b28">[29]</ref> and SPIN <ref type="bibr" target="#b31">[32]</ref>. For obtaining the corresponding meshes from <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b31">[32]</ref>, we use their publicly available code and do not post-process their results in any form.</p><p>Please also find video results of our method on temporal sequences here. We use publicly available in-the-wild videos provided by <ref type="bibr" target="#b28">[29]</ref>, PennAction and 3DPW datasets to generate video sequences of our predicted SMPL meshes. It is worth noting that our method is only trained on 2D poses from Human3.6M dataset, while the results are shown on in-the-wild video sequences, which have no overlap in terms of the action categories between the training and evaluation datasets. <ref type="table">Table 7</ref>: Human3.6M. Comparison of P-MPJPE for model-free 3D pose estimation. Results obtained using estimated 2D pose by SH/CPM <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b69">70]</ref>. Best results amongst the unsupervised approaches are shown in bold. ( †) using temporal information, (+) using additional data for training.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Linear correction on SMPL 3D joints for SJA (Equation 6) could lead to network placing joints outside the mesh. Visualization shows rendering of predicted mesh with projection of 3D joints overlayed on top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Train Teacher: Train the shared temporal convolution backbone and the teacher branch by minimizing L T = λ mss L mss + λ tc L tc + λ bl L bl + L T D . 2. Knowledge Distillation: Freeze the shared temporal backbone and the teacher branch. Train the student branch by minimizing L S = L KD + λ R L R + λ β L β . 3. Learn SJA: Initialize W to W . Fine-tune W and the student branch by minimizing L S . 4. Fine-tune the entire network by minimizing L = L T + λ S L S + L S D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of SMPL mesh obtained using predicted parameters on challenging examples. Each example shows input image, recovered SMPL mesh, and the same mesh from a different view. The student network is able to recover complicated articulations of the human body. First row: H3.6M. Second row: LSP. Third row: MPI-INF-3DHP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of predicted 3D pose on H3.6M (top) and LSP (bottom). For H3.6M, the first skeleton in each example shows ground-truth 3D skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Some failure examples from Human3.6M, depicting front/back depth ambiguity in predicting joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Effect of SJA on the student network. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Architecture of the Temporal Discriminator. The temporal discriminator architecture is identical to the temporal backbone with the exception of not having BatchNorm. The pytorch model dump is attached below: 1 ( T e m p o r a l D i s c r i m i n a t o r ) : 2 ( 0 ) : Conv1d ( 2 8 , 1 0 2 4 , k s z =3 , s t =1 , d i l = 1 ) 3 ( 1 ) : ReLU ( i n p l a c e ) 4 ( 2 ) : Dropout ( p = 0 . 2 5 ) 5 ModuleList ( 6 ( 3 ) : Conv1d ( 1 0 2 4 , 1 0 2 4 , k s z =3 , s t =1 , d i l = 3 ) 7 ( 4 ) : ReLU ( i n p l a c e ) 8 ( 5 ) : Dropout ( p = 0 . 2 5 ) 9 ( 6 ) : Conv1d ( 1 0 2 4 , 1 0 2 4 , k s z =3 , s t =1 , d i l = 1 ) 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 ,</head><label>8</label><figDesc>9 and 10 show additional visualizations of the predicted SMPL meshes on challenging examples from Human 3.6M, LSP and 3DPW datasets. Note that our approach handles a variety of pose articulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of SMPL mesh obtained using predicted parameters on challenging examples from Human3.6M. Each example shows input image with input 2D landmarks, recovered SMPL mesh with reprojected 2d pose predictions, and the same mesh from a different view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of SMPL mesh obtained using predicted parameters on challenging examples from LSP. Each example shows input image, recovered SMPL mesh, and the same mesh from a different view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Visualization of SMPL mesh obtained using predicted parameters on challenging examples from 3DPW. Each example shows input image, recovered SMPL mesh, and the same mesh from a different view. Qualitative comparison with HMR-Video [29] and SPIN [32] on (a) davis-hike (b) insta-variety-dunking (c) insta-variety-hammerthrow (d) insta-variety-javelinthrow. (→) indicates the passage of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>, 2), thereby showing an implicit ensemble effect of the two branches. For a fair comparison with model-free methods inTable 1, we also report the results from the teacher branch (PoseNet3D-Teacher) after Step 1 of training (Sect. 3.5). Similarly, results from the student branch after Step 3 of training are denoted as PoseNet3D-Student inTable 2. The corresponding results from the two branches after fine tuning (Step 4, Sect. 3.5) are denoted by PoseNet3D-Teacher-FT and PoseNet3D-Student-FT, respectively.As evident fromTable 1, all variations of our approach trained using H3.6M data outperform the state-of-the-art unsupervised algorithm of Chen et al.<ref type="bibr" target="#b5">[6]</ref> (trained using</figDesc><table><row><cell>Supervision</cell><cell>Method</cell><cell cols="2">P-MPJPE</cell></row><row><cell></cell><cell></cell><cell>GT</cell><cell>IMG</cell></row><row><cell>Full</cell><cell>Chen and Ramanan [5]</cell><cell cols="2">57.5 82.7</cell></row><row><cell></cell><cell>Martinez et al. [40]</cell><cell cols="2">37.1 52.1</cell></row><row><cell></cell><cell>IGE-Net [26] (17j)</cell><cell cols="2">35.8 47.9</cell></row><row><cell></cell><cell>Li and Lee [35]</cell><cell>-</cell><cell>42.6</cell></row><row><cell></cell><cell>Ci et al. [10]</cell><cell cols="2">27.9 42.2</cell></row><row><cell></cell><cell>Hossain &amp; Little [54] (17j)  †</cell><cell></cell><cell>42.0</cell></row><row><cell></cell><cell>Pavllo et al. [53]  †</cell><cell cols="2">22.7 40.1</cell></row><row><cell></cell><cell>Cai et al. [3]  †</cell><cell>-</cell><cell>39.0</cell></row><row><cell></cell><cell>Yang et al. [74](+)</cell><cell>-</cell><cell>37.7</cell></row><row><cell>Weak/Self</cell><cell>3DInterpreter [71]</cell><cell cols="2">88.6 98.4</cell></row><row><cell></cell><cell>Tung et al. [63]</cell><cell>-</cell><cell>98.4</cell></row><row><cell></cell><cell>AIGN [15]</cell><cell cols="2">79.0 97.2</cell></row><row><cell></cell><cell>RepNet [66]</cell><cell cols="2">38.2 65.1</cell></row><row><cell></cell><cell>Drover et al. [14]</cell><cell cols="2">38.2 64.6</cell></row><row><cell></cell><cell>Wang et al. [67]</cell><cell>-</cell><cell>62.8</cell></row><row><cell></cell><cell>Kocabas et al. [31]( ‡)</cell><cell>-</cell><cell>60.2</cell></row><row><cell>Unsupervised</cell><cell>Rhodin et al. [55]( ‡)</cell><cell>-</cell><cell>98.2</cell></row><row><cell></cell><cell>Chen et al. [6]</cell><cell>58.0</cell><cell>-</cell></row><row><cell></cell><cell>Chen et al. [6]( †)(+)</cell><cell cols="2">51.0 68.0</cell></row><row><cell></cell><cell>PoseNet3D-Teacher( †)</cell><cell cols="2">50.6 66.6</cell></row><row><cell></cell><cell>PoseNet3D-Teacher-FT( †)</cell><cell cols="2">46.7 62.1</cell></row><row><cell></cell><cell>PoseNet3D( †)</cell><cell cols="2">47.0 59.4</cell></row><row><cell cols="2">Semi-supervised Chen et al. [6]</cell><cell>37</cell><cell>-</cell></row><row><cell>(5% 3D data)</cell><cell>PoseNet3D-Teacher( †)</cell><cell>35</cell><cell>57.1</cell></row><row><cell></cell><cell>PoseNet3D( †)</cell><cell cols="2">33.7 53.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Human3.6M.</figDesc><table><row><cell cols="2">Comparison of our student network</cell></row><row><cell cols="2">with previous approaches that output SMPL parameters (in</cell></row><row><cell cols="2">mm). Best and second best results are bolded and under-</cell></row><row><cell cols="2">lined, respectively. Our results use SH [45] for 2D pose in-</cell></row><row><cell cols="2">puts. 3D data refers to the use of additional 3D data during</cell></row><row><cell cols="2">training. ( †): using temporal information.</cell></row><row><cell>Method</cell><cell>P-MPJPE</cell></row><row><cell>HMR [28]</cell><cell>81.3</cell></row><row><cell>Doersch et al. [13]</cell><cell>74.7</cell></row><row><cell>HMR-Video [29]</cell><cell>72.6</cell></row><row><cell>Arnab et al. [1]</cell><cell>72.2</cell></row><row><cell>Kolotouros et al. [33]</cell><cell>70.2</cell></row><row><cell>Sun et al. [61]</cell><cell>69.5</cell></row><row><cell>SPIN [32] -static fits</cell><cell>66.3</cell></row><row><cell>SPIN [32] -in the loop</cell><cell>59.2</cell></row><row><cell>PoseNet3D (IMG)</cell><cell>73.6</cell></row><row><cell>PoseNet3D (GT)</cell><cell>63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>3DPW.</figDesc><table><row><cell></cell><cell cols="4">Comparison with previous apporaches</cell></row><row><cell cols="5">that output SMPL parameters (in mm). Unlike other ap-</cell></row><row><cell cols="4">proaches, our approach does not use any 3D data</cell><cell></cell></row><row><cell>Method</cell><cell>3D Data</cell><cell>Training Datasets</cell><cell cols="2">Rigid Alignment</cell></row><row><cell></cell><cell>(for Training)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PCK AUC P-MPJPE</cell></row><row><cell>Vnect [42]</cell><cell>Paired</cell><cell cols="2">H3.6M+MPI-INF-3DHP 83.9 47.3</cell><cell>98.0</cell></row><row><cell>HMR [28]</cell><cell>Paired</cell><cell cols="2">H3.6M+MPI-INF-3DHP 86.3 47.8</cell><cell>89.8</cell></row><row><cell cols="3">DenseRaC [72] Paired+Unpaired Synthetic+Various</cell><cell>89.0 49.1</cell><cell>83.5</cell></row><row><cell>SPIN [32]</cell><cell>Paired</cell><cell>Various</cell><cell>92.5 55.6</cell><cell>67.5</cell></row><row><cell>HMR [28]</cell><cell>Unpaired</cell><cell cols="2">H3.6M+MPI-INF-3DHP 77.1 40.7</cell><cell>113.2</cell></row><row><cell>SPIN [32]</cell><cell>Unpaired</cell><cell>Various</cell><cell>87.0 48.5</cell><cell>80.4</cell></row><row><cell>PoseNet3D</cell><cell>None</cell><cell>H3.6M</cell><cell>81.9 43.2</cell><cell>102.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>MPI-INF-3DHP. Comparison with previous approaches that output SMPL parameters. Metrics for<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref> are taken from<ref type="bibr" target="#b31">[32]</ref>. Various refers to combination of datasets such as H3.6M, MPI-INF-3DHP and LSP. PCK and AUC: higher is better. P-MPJPE (mm): lower is better.</figDesc><table><row><cell>4.4. Qualitative Results</cell></row><row><cell>Figure 4 shows overlay of generated mesh using pre-</cell></row><row><cell>dicted SMPL parameters on the corresponding image, for</cell></row><row><cell>a few examples from H3.6M, LSP and 3DHP datasets. As</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Ablation studies. (Left) Temporal consistency.</cell></row><row><cell>(Right) Effect of SJA on student network. Reported met-</cell></row><row><cell>rics use 2D joints obtained from SH</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Acronyms used within PyTorch model dumps.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real time multiperson 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised 3D pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weaklysupervised discovery of geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3D human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Temporal-3D-Pose-Kinetics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12929" to="12941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can 3D pose be learned from 2D projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018 PeopleCap Workshop</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2D-to-3D lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y. Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computational Studies of Human Motion: Tracking and Motion Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2D features and intermediate 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Model-based vision: a program to see a walking person. Image and Vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ige-net: Inverse graphics energy networks for human pose estimation and single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3D human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3D human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On boosting singleframe 3D human pose estimation via monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shape-aware human pose and shape reconstruction using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bertiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10766</idno>
		<title level="m">Smplr: Deep smpl reverse for 3D human pose and shape recovery</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Camera distanceaware top-down approach for 3D multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3D human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised knowledge transfer for deep learning from private training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Úlfar Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3D human pose estimation using convolutional neural networks with 2D pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3D human pose estimation with relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning monocular 3D human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeletondisentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeletondisentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Distill knowledge from NRSfM for weakly supervised 3D pose learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Not all parts are created equal: 3D pose estimation by modelling bidirectional dependencies of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generalizing monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Single image 3D interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno>abs/1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">HEMlets pose: Learning part-centric heatmap triplets for accurate 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>TemporalBackBone</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Conv1d ( 1 0 2 4 , 1 0 2 4 , k s z =3 , s t =1 , d i l = 3</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
		<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<title level="m">L i n e a r ( i n f =1024 , o u t f =14 , b i a s = T r u e )</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>StudentBranch</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">BatchNorm1d ( 1 0 2 4 , e p s =1e −05 , mmntm = 0 . 1 ) 5 ( 2 ) : ReLU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">L i n e a r ( i n f =1024 , o u t f =1024 , b i a s = T r u e )</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>L i n e a r. i n f =1024 , o u t f =1024 , b i a s = T r u e ) 7 ( 4 ) : BatchNorm1d ( 1 0 2 4 , e p s =1e −05 , mmntm = 0 . 1 ) 8 ( 5 ) : ReLU</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
		<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
		<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modulelist</surname></persName>
		</author>
		<title level="m">L i n e a r ( i n f =1024</title>
		<imprint/>
	</monogr>
	<note>o u t f =1024 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<title level="m">L i n e a r ( i n f =1024 , o u t f =14 , b i a s = T r u e )</title>
		<imprint>
			<date type="published" when="1924" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
	<note>L i n e a r ( i n f =1024. o u t f =1944 , b i a s = T r u e ) 36 ( 2 6 ) : L i n e a r ( i n f =1024 , o u t f =10 , b i a s = T r u e</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Eat Greet Phone Photo Pose Purch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Disc</surname></persName>
		</author>
		<idno>3Dinterp. [71] 78.6 90.8 92.5 89.4 108.9 112.4 77.1 106.7 127.4 139.0 103.4 91.4 79.1 - -98.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">WalkT. Avg</note>
	<note>Sit SitD. Smoke Wait Walk WalkD</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drover</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
