<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
							<email>pfwang@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
							<email>zhangchengquan@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhang</surname></persName>
							<email>zhangxiaoqiang01@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
							<email>gmshi@xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, scene text reading has attracted extensive attention in both academia and industry for its numerous applications, such as scene understanding, image retrieval, augmented reality translation <ref type="bibr" target="#b32">(Wu et al. 2019)</ref>, and robot navigation. Thanks to the surge of deep neural networks, significant progress has been made in detection and recognition separable solutions <ref type="bibr" target="#b33">(Wu and Natarajan 2017;</ref><ref type="bibr" target="#b17">Long et al. 2018;</ref><ref type="bibr">Wang et al. 2019b,a;</ref><ref type="bibr" target="#b39">Zhan and Lu 2019;</ref><ref type="bibr" target="#b25">Shi et al. 2018;</ref><ref type="bibr" target="#b38">Yu et al. 2020;</ref><ref type="bibr" target="#b27">Wan et al. 2019)</ref>, as well as end-to-end text spotting methods. However, existing end-to-end models <ref type="bibr" target="#b26">(Sun et al. 2018;</ref><ref type="bibr" target="#b15">Liu et al. 2018;</ref><ref type="bibr" target="#b5">Feng et al. 2019</ref>) are mostly built on two-stage frameworks or character-based methods <ref type="bibr" target="#b36">(Xing et al. 2019;</ref><ref type="bibr" target="#b18">Lyu et al. 2018)</ref> with complex pipelines, which are inefficient for real-time applications. In this paper, we try to investigate a real-time text spotter for arbitrarily-shaped text. Recognition Accuracy on Total-Text: Our PGNet-E achieves at least two times faster than the most recent state-of-the-art method ABCNet  with competitive recognition accuracy. Complete results are in <ref type="table">Table.</ref> 3.</p><p>Reading of arbitrarily-shaped scene text is a challenging task, as compared in <ref type="figure">Fig. 2</ref>, and the most recent works may suffer from the following disadvantages: (1) The pipelines of two-stage methods <ref type="bibr" target="#b26">(Sun et al. 2018;</ref><ref type="bibr" target="#b18">Lyu et al. 2018;</ref><ref type="bibr" target="#b5">Feng et al. 2019;</ref><ref type="bibr" target="#b16">Liu et al. 2020)</ref> are inefficient, which may involve time-consuming Non-maximum Suppression (NMS) and Region of Interest (RoI) operations. Especially for arbitrarily-shaped text spotter, specific RoI transformation operation, such as RoISlide <ref type="bibr" target="#b5">(Feng et al. 2019)</ref> or Bezier-Align , brings non-negligible computational overhead. (2) In Mask TextSpotter  and CharNet <ref type="bibr" target="#b36">(Xing et al. 2019)</ref>, character-level annotations are required for training, which is too expensive to afford. Though CharNet could be trained in a weakly supervised manner by character-level annotations in synthetic datasets, free synthesized data is not completely replaceable for real data in practice. (3) The recognition of text in non-traditional reading directions would be failed with pre-defined rules. For example, TextDragon <ref type="bibr" target="#b5">(Feng et al. 2019)</ref> and Mask TextSpotter ) make a strong assumption that the reading direction of text region is either from left to right or from up to down, which precludes correct recognition of more challenging text. Rec.</p><p>(f) <ref type="bibr">(Feng et al., ICCV2019)</ref> H Q A <ref type="figure">Figure 2</ref>: Overview of some end-to-end scene text spotting methods that are most relevant to ours, and the blue and green boxes represent their detection and recognition results. Inside the GT (ground-truth) box, 'W' and 'C' represent word-level and character-level annotation. The 'H', 'Q', and 'A' represent that the method can detect horizontal, quadrilateral, and arbitrarilyshaped text, respectively. Our method is free from character-level annotations, NMS, and RoI operations.</p><p>In this paper, we propose a novel framework for reading text in real-time speed with point gathering operation, namely PGNet. The PGNet is a single-shot text spotter based on multi-task learning. The architecture of PGNet is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We employ an FCN <ref type="bibr" target="#b19">(Milletari, Navab, and Ahmadi 2016</ref>) model to learn various information of text regions simultaneously, including text center line (TCL), text border offset (TBO), text direction offset (TDO), and text character classification map (TCC). The pixel-level character classification map is trained with a proposed Point Gathering CTC (PG-CTC) loss, making it free from characterlevel annotations. In the post-processing, we extract the center point sequence in the reading order of each text instance with TCL and TDO maps, and the detection results can be obtained with the corresponding boundary offset information from TBO map. Using the PG-CTC decoder, we serialize the high-level two-dimensional TCC map to character classification probability vector sequences which can be further decoded to the recognition results. The details will be discussed in Sec.3.1. As depicted in <ref type="figure">Fig. 2</ref> and <ref type="figure" target="#fig_0">Fig. 1</ref>, our pipeline is simple yet efficient, and experiments on public benchmarks prove that PGNet achieves better or competitive performance with excellent running speed.</p><p>Moreover, inspired by SRN <ref type="bibr" target="#b38">(Yu et al. 2020)</ref> and GTC <ref type="bibr" target="#b9">(Hu et al. 2020)</ref>, we propose a graph refinement module (GRM) to make secondary reasoning to improve the end-to-end performance further. The points in a text sequence are formulated as nodes in a graph, where the representation of each node is enhanced with semantic context and visual context information from its neighbors, and the character classification result should be more accurate.</p><p>The contributions of this paper are three-fold:</p><p>• We propose a simple yet powerful arbitrarily-shaped text spotter (PGNet), which is free from character-level annotations, NMS, and RoI operations, and it achieves better or competitive performance in end-to-end performance with excellent running speed;</p><p>• We introduce a mechanism to restore the reading order of characters in each text instance, making our method able to correctly recognize text in more challenging situations and non-traditional reading directions;</p><p>• We also propose an efficient graph refinement module (GRM) to improve the CTC recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will review some representative scene text spotters, as well as some recent progress in graph neural networks. A comprehensive review of recent scene text spotters can be found in <ref type="bibr" target="#b37">(Ye and Doermann 2015;</ref><ref type="bibr" target="#b44">Zhu, Yao, and Bai 2016;</ref><ref type="bibr" target="#b0">Baek et al. 2019;</ref><ref type="bibr" target="#b12">Liao et al. 2021)</ref>. Scene Text Spotting. Inspired by the generic object detection methods <ref type="bibr" target="#b14">(Liu et al. 2016;</ref><ref type="bibr" target="#b23">Ren et al. 2015;</ref><ref type="bibr" target="#b22">Redmon et al. 2016</ref>) and segmentation methods <ref type="bibr" target="#b19">Milletari, Navab, and Ahmadi 2016)</ref>, the text spotting methods are developed from spotting regular scene text to spotting arbitrarily-shaped scene text. <ref type="bibr" target="#b11">Lee and Osindero (2016)</ref> proposed the first successful end-to-end text recognition model, which only supports horizontal text and requires relatively complex training procedures. To address the multi-orientation problem of text, <ref type="bibr" target="#b1">Bušta et al. (2017)</ref> utilize YOLO <ref type="bibr" target="#b22">(Redmon et al. 2016)</ref> to generate rotational proposals, and train RoI sampled features with CTC loss. Inspired by the Faster RCNN <ref type="bibr" target="#b23">(Ren et al. 2015)</ref>, TextNet <ref type="bibr" target="#b26">(Sun et al. 2018</ref>) generates text proposals in quadrangles, and encodes the aligned RoI features into context information with a simple recurrent neural network to generate the text sequences, which contains some background information; thus, it may suffer from reading curve texts.</p><p>For the spotting of arbitrarily-shaped scene text, Mask TextSpotter ) detects and recognizes text instances of arbitrary shapes by segmenting the text regions and character regions. However, the character-level annotations are required for training, which is too expensive to afford. Considering the arbitrarily-shaped region of text as a series of quadrangles, TextDragon <ref type="bibr" target="#b5">(Feng et al. 2019</ref>) extracts the components of text feature through RoISlide and  2) The detection and recognition of each text instance can be achieved in a single shot by polygon restoration and PG-CTC decoding mechanism with the center point sequence of each text region.</p><formula xml:id="formula_0">RR-A-N-DD--Y-SS D-OO-N--UU-T-</formula><p>recognizes each cropped feature with CTC based text recognizer. ABCNet  attempts to adaptively fit arbitrarily-shaped text by a parameterized Bezier curve to reduce the computation overhead, and they propose a BezierAlign layer for extracting accurate convolution features of a text instance, making it suitable for some realtime scenarios. Mask Textspotter, TextDragon, and ABC-Net are capable of spotting arbitrarily-shaped scene text, but they are all RoI-based methods and may involve NMS, RoI cropping, and pooling operations, which is time-consuming and may reduce the performance. CharNet is the first onestage arbitrarily-shaped scene text spotting method, which requires character-level ground truth data for training and its backbone is too heavy to run in a real-time speed. Graph Neural Networks. The unstructured data is common in many machine learning tasks and can be organized as graphs. Thanks to the previous effort, we can use the convolutional neural network for graph-structured data. The graph convolutional networks (GCNs) can be categorized into spectral methods and spatial methods. Spectral GCNs generalize convolution by Graph Fourier Transform, while spatial GCNs directly perform manually-defined convolution on graph nodes and their neighbors. For more details about GCNs, please refer to <ref type="bibr" target="#b21">(Wu et al. 2020)</ref>. In terms of applications, GCNs is proved to be efficient in many computer vision tasks, including scene graph generation, point clouds classification, and action recognition. For example, <ref type="bibr" target="#b41">Zhang et al. (2020)</ref> propose a relational reasoning graph network for arbitrary shape text detection by predicting linkages of text components. In this paper, we adopt the Spatial GCN to reasoning the semantic information between point and its neighbors to improve the CTC recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The architecture of our proposed method is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Firstly, the input image is fed into a stem backbone with FPN to produce feature F visual . Then, the F visual is used to predict TCL, TBO, TDO, and pixel-level TCC map by multi-task learning in parallel at 1/4 size of the input images. In the training period, the TCL, TBO, and TDO are supervised by the same scale label maps, while a PG-CTC loss is proposed to train the pixel-level TCC map to solve the lack of character-level annotations. In the inference period, we extract the center point sequence of each text instance from TCL, and sort them with TDO information to recover the right reading order, making our method recognize text in non-traditional reading directions correctly. With the aid of the corresponding boundary offset information from TBO, the detection of each text instance can be achieved in a single shot by polygon restoration. Simultaneously, the PG-CTC decoder can serialize high-level two-dimensional TCC map to character classification probability sequences and decode them into final text recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Point Gathering CTC</head><p>The point gathering (PG) operation plays an important role in both the training and inference process of PGNet and helps to get rid of character-level annotations, NMS, and RoI operations. The TCC maps of PGNet are maps of 37 characters, including 26 letters, 10 Arabic numerals, and one background class. The point gathering operation is employed to gather the character classification probability sequence from the TCC map according to the center points in the center of each text region, which can be formulated as</p><formula xml:id="formula_1">P π = gather(T CC, π),<label>(1)</label></formula><p>where π = {p 1 , p 2 , . . . , p N } is a center point sequence of length N , and p i = (x i , y i ). The output P π is the character classification probability sequence with size N × 37.</p><p>In the training process, the proposed PG-CTC loss makes the training of pixel-level TCC map free from characterlevel annotations. The typical CTC loss function tackles the training problem of the source and target sequences with inconsistent lengths with a background class. The CRNN <ref type="bibr" target="#b24">(Shi, Bai, and Yao 2017)</ref> framework transforms the height of the feature map to 1, which may suffer from the background noise while recognizing the curved text. The 2D-CTC <ref type="bibr" target="#b27">(Wan et al. 2019</ref>) expands the search path of CTC to two-dimensional space, but it still can not handle an image with multi text instances. Here we address the problem by PG-CTC and formulate the classic CTC loss as CT C loss(P, L), where P is a character classification probability sequence and L is its transcript label. For an image with M text instances, suppose the center point coordinate sequences are {π 1 , π 1 , ..., π M }, and corresponding transcript labels are {L 1 , L 1 , ..., L M }, then we define the PG-CTC loss as</p><formula xml:id="formula_2">L P G−CT C = M i=1 CT C loss(P πi , L i ),<label>(2)</label></formula><p>where we can calculate the centerline of polygonal wordlevel annotations and sample it densely to obtain the center point sequences π i in the training process, instead of using character-level annotations. With the training of big data, the character classification information of each pixel in TCC could be learned.</p><p>In the inference process, the PG-CTC decoder dramatically simplifies the overall pipeline of an end-to-end arbitrarily-shaped text spotter, and the NMS and RoI operations are not required in PGNet. For a text region in the TCL map, we extract a center point sequence and sort it in the right reading order, which can be donated as π. Specifically, we adopt a morphological method <ref type="bibr" target="#b42">(Zhang and Suen 1984)</ref> to get the skeleton of a text region and treat it as the center point sequence. The text direction of each point can be extracted from TDO maps. We calculate an average direction of all points and sort them according to the length of projection along the direction to obtain the center point sequence π. The character classification probability sequence P π can be extracted with Eq. (1), and the PG-CTC decoder can be denoted as</p><formula xml:id="formula_3">R π = CT C decoder(P π ),<label>(3)</label></formula><p>where R π represents the transcription of π. For the polygon restoration, we obtain the corresponding border point pairs of π with TBO maps in the same position, and link all the border points clockwise to obtain a complete polygon representation. For more details about polygon restoration, please refer to our previous SAST <ref type="bibr" target="#b29">(Wang et al. 2019a)</ref>. Compared with the CTC-based CRNN framework, the PG-CTC could handle images with multi-text instances of arbitrary shape, where the application of CTC loss is expanded a lot. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>Considering the limitation of computing resources in different scenarios, we propose two versions of PGNet, that is, PGNet-Accuracy and PGNet-Efficient, denoted by PGNet-A and PGNet-E in the following sections. The only difference is stem network, and the PGNet-A adopts ResNet-50 as the backbone network, while PGNet-E employs EfficientNet-B0. With different levels of feature map from the stem network gradually merged three-times in the FPN manner, a fused feature map F visual is produced at 1/4 size of the input images. The TCL and the other maps are predicted in parallel, where we adopt a 1 × 1 convolution layer with the number of output channels set to {1, 2, 4, 37 } for TCL, TDO, TBO, TCC map, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label Generation</head><p>The label generation of arbitrarily-shaped text is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The TCL map is the shrunk version segmentation of the text region. The TBO map indicates the offset between each pixel in TCL and the corresponding point pair in the upper and lower edge of its text region, which helps to determine the boundaries of text regions in the inference. We follow SAST <ref type="bibr" target="#b29">(Wang et al. 2019a)</ref> to generate TCL and TBO map, where more details are introduced. Inspired by the reading mechanism of humans that the eye moves from one character to the next character along the centerline of text region while reading, the TDO map is estimated to recover the reading order of scene text components, which benefits both detection and recognition tasks, especially for those scene text in non-traditional reading directions. The TDO map indicates the offset vector of each pixel in the TCL map to the next reading position. For a quadrilateral region annotation, the direction of the offset vector is from the center point of the left edge to the center point of the right edge, and its magnitude is the length of text region normalized by the number of characters. Polygonal annotations of more than four vertices are treated as a series of quadrangles connected together, and TBO map and TDO map can be generated gradually from quadrangles as described before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objectives</head><p>The loss of multi-task learning can be formulated as</p><formula xml:id="formula_4">L = λ 1 L tcl + λ 2 L tbo + λ 3 L tdo + λ 4 L tcc ,<label>(4)</label></formula><p>where L tcl , L tbo , L tdo and L tcc represent the loss of TCL, TBO, TDO and TCC maps. We train TCL branch by minimizing the Dice loss <ref type="bibr" target="#b19">(Milletari, Navab, and Ahmadi 2016)</ref>, and the Smooth L 1 loss <ref type="bibr" target="#b6">(Girshick 2015</ref>) is adopted for TBO and TDO map, while TCC map is trained with PG-CTC loss as mentioned before. The loss weights λ 1 , λ 2 , λ 3 , and λ 4 are set to {1.0, 1.0, 1.0, 5.0} empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Refinement Module</head><p>We also propose a graph refinement module to perceive the word-level semantic context and visual context to improve the end-to-end reading performance with GCNs further. The coarse recognition results is refined in text instance level, and we construct a visual reasoning graph and a semantic reasoning graph for a point sequence π, of which the points are considered as the nodes in a graph. We use the same structure of graph convolution layer as <ref type="bibr" target="#b31">(Wang et al. 2019c</ref>) and . Especially, the F visual and TCC map are both required as inputs. F visual is the output feature of FPN, as illustrated in <ref type="figure" target="#fig_1">Fig.3</ref>. For a point sequence π = {p 1 , p 2 , . . . , p N }, the adjacency matrix is defined as</p><formula xml:id="formula_5">A ij = 1 − D(p i , p j )/ max(A),<label>(5)</label></formula><p>where D(p i , p j ) is a L2 distance between p i and p j , and each node is self-connected. The structure of GRM is depicted in <ref type="figure" target="#fig_3">Fig.5</ref>, and the two numbers in brackets indicate d in and d out of a neural layer. In the semantic reasoning graph, the feature map F s is obtained by point gathering operation from TCC map and further embedded to X s of shape N × 256.</p><p>With three graph layers, the input X s is transformed to produce Y s of shape N × 64; In the visual reasoning graph, the features F v is gathered from F visual , and its channel is transformed from 128 to 256 with several convolutional layers to get X v . With a similar network, we can get the visual reasoning output Y v of shape N × 64; Finally, we concatenate Y v and Y s , and treat it as a classification problem with several FC layers to produce the refined probability sequence, where the GRM is also optimized with CTC loss. It is worth mentioning that we pad the coarse recognition sequence to the same length and batched together for efficient training, and the max length is set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The benchmark datasets used for the experiments in this paper are briefly introduced below. ICDAR 2015. The ICDAR 2015 dataset <ref type="bibr" target="#b10">(Karatzas et al. 2015)</ref> is collected for the ICDAR 2015 Robust Reading Competition, with 1,000 natural images for training and 500 for testing. The text instances are annotated in word-level.</p><p>Total-Text. The Total-Text (Ch'ng and Chan 2017) is another curved text benchmark, which consists of 1,255 training images and 300 testing images with multiple orientations: horizontal, multi-Oriented, and curved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training. The stem network is initialized with pre-trained weight on ImageNet <ref type="bibr" target="#b4">(Deng et al. 2009</ref>). The training process is mainly divided into the warming-up step, fine-tuning step, and training step of the GRM module. In the warmingup step, we apply Adam optimizer to train our model with learning rate 1e-3, and the learning rate decay factor is 0.94 on the SynthText <ref type="bibr" target="#b7">(Gupta, Vedaldi, and Zisserman 2016)</ref>; In the fine-tuning step, the learning rate is re-initiated to 1e-3, and the model is tuned on ICDAR2015 and Total-Text data; The GRM is an additional module in our pipeline, and in the last step, we only train the learnable parameters in GRM module to provide both the metrics on ICDAR 2015 and Total-Text with/without GRM for a fair comparison.</p><p>The experiments are performed on a workstation with the following configuration, CPU: Intel(R) Xeon(R) CPU E5-2620; GPU: NVIDIA TITAN Xp ×4; RAM: 64GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, comprehensive experiments are conducted to prove the strengths of the proposed method.</p><p>The Effectiveness of TDO. The TDO map is adopted to recover the sequence information of text in non-traditional reading directions, which benefits both detection and recognition tasks. To verify the efficiency of TDO, we conduct several experiments in ICDAR2015 and Total-Text for PGNet-A and PGNet-E. As shown in Tab. 1, the benefits of TDO in Total-Text are apparent, and the recognition accuracy in PGNet-E obtains 1.6% improvement, while it is less  than 0.5% in ICDAR2015. We claim that the non-traditional texts are more likely to appear in the curved Total-Text, while the texts in ICDAR2015 are mostly quadrangles, and the predefined rules may cover most situations.</p><p>The Effectiveness of GRM. The GRM is proposed to model word-level semantic context implicitly and improve the end-to-end reading performance. To verify its efficiency, we conduct several experiments in ICDAR2015 and Total-Text on both PGNet-A and PGNet-E, respectively. As depicted in Tab. 2, the efficiency of GRM is proved, and the end-to-end reading performance is improved significantly. Especially, it gains 2.2% for end-to-end recognition tasks for PGNet-E in Total-Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation for Curved Text</head><p>On Total-Text, we evaluate the performance of PGNet for spotting scene text of arbitrary shapes. We fine-tune our model with Total-Text training set and partial ICDAR2019 ArT <ref type="bibr" target="#b3">(Chng et al. 2019)</ref>, where we remove the images from the Total-Text test set by comparing the MD5 value of each image. The ratio of these two datasets is set to 3: 2 during training. The GRM is further trained with SynthText, partial ICDAR2019 ArT, and Total-Text, of which the data ratio is set to 6: 2: 2, while the learnable parameters outside GRM are frozen. In the inference phase, the longer side of images is resized to 640, with the aspect ratio kept.</p><p>The results are shown in Tab. 3 and <ref type="figure">Fig. 6</ref>. Our method achieves the state-of-the-art detection result and is superior to the other competitors by almost 1.5%. Besides, with the help of efficient post-processing, our PGNet-A also achieves comparable end-to-end recognition results. Specifically, compared with the previous most accurate method CharNet H-57, our method has almost the same recognition accuracy, but it is over 30 times faster and free from character-level annotations. Compared with the ABCNet, which is the current fastest arbitrarily-shaped text spotter with a speed of 20+ FPS, our PGNet-E runs nearly two times faster than ABCNet-F with better recognition accuracy. The comparisons with the previous state-of-the-arts demonstrate the efficiency and effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation for Multi-oriented Text</head><p>We also conduct experiments on ICDAR2015 to confirm the superiority of the proposed method on the multi-oriented scene text. Following the same training strategy as Total-Text, The ICDAR2017-MLT-Latin and ICDAR2015 are used in fine-tune stage, of which the data ratio is 3:7, while SynthText, ICDAR2017-MLT-Latin <ref type="bibr" target="#b20">(Nayef et al. 2017</ref>) and ICDAR2015 with a ratio of 6:2:2 are adopted for training GRM. In the test phase, the longer side of images is resized to 1536, with the aspect ratio kept.</p><p>The results are shown in Tab. 4 and <ref type="figure">Fig. 6</ref>. Our method achieves 88.2% F-Measure on the text detection task and is better than most previous methods. Besides, on the endto-end recognition task, the PGNet-A with strong and weak lexicon achieves state-of-the-art accuracy. Particularly, our method exceeds the previous best one-stage text spotter CharNet R-50 by 3.2%, 3.8%, and 1.2% when evaluated with strong, weak, and general lexicon, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Runtime</head><p>In this paper, we propose a simple yet powerful arbitrarilyshaped text spotter, and considering the limitation of computing resources in different application scenarios, we propose PGNet-A and PGNet-E with different stem networks. The runtime of our method can be roughly divided into three parts: network inference stage, post-processing stage, and graph refinement stage. The runtime of PGNet on Total-Text is evaluated with NVIDIA Tesla V100-SXM2-16GB, which is the same as ABCNet. The longer side of the test image is resized to 640, and the batch size is set to 1 on a single GPU. It takes 15.1 ms, 6.4 ms, and 3.2 ms in the three stages, respectively. It is worth noting that the postprocessing stage, which accounted for a large proportion of time cost, is executed with Python code and can be further optimized. In Total-Text, PGNet-E runs at 46.7 FPS with 84.8% and 58.4% F-Measure respectively on the detection and end-to-end recognition without lexicon, surpassing the previous arbitrarily-shaped text spotters in efficiency significantly, as depicted in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a real-time arbitrarily-shaped text spotter PGNet together with a novel graph refinement module to relieve the inefficient problem of previous methods. Optimizing TCC maps with proposed PG-CTC loss, our PGNet is free from character-level annotations and gets rid of NMS and RoI operations in the inference stage which is single-shot, efficient and straightforward. Moreover, a novel graph refinement module is proposed to improve the endto-end performance further by reasoning the word-level semantic information via a spatial graph neural network. The contributions of our paper are confirmed by comprehensive experiments and qualitative visualization in public benchmarks. In the future, we are interested in promoting the deployment of our novel real-time arbitrarily-shaped text spotter on smart edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Detection Recognition FPS Recall Precious F-score None FOTS  ResNet-50 38.0 52.3 44.0 32.2 -Textboxes <ref type="bibr" target="#b13">(Liao et al. 2017)</ref> ResNet-50-FPN 45.5 62.1 52.5 36.3 1.4 Mask TextSpotter  ResNet-50-FPN 55.0 69.0 61.3 52.9 4.8 TextNet <ref type="bibr" target="#b26">(Sun et al. 2018)</ref> ResNet-50-SAM 59.5 68.2 63.5 54.0 2.7 TextSnake <ref type="bibr" target="#b17">(Long et al. 2018</ref>   <ref type="table">Table 4</ref>: Evaluation on ICDAR 2015 for detecting oriented text. "P", "R", "F" represent "Precision", "Recall", "F-measure" respectively. "S", "W", "G" represent recognition with "Strong", "Weak", "Generic" lexicon respectively. <ref type="figure">Figure 6</ref>: Qualitative results of our method on Total-Text (left two columns) and ICDAR15 (right two columns) datasets.</p><p>6 Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Graph Convolution Layer</head><p>A graph convolution layer takes as input a node feature matrix X together with an adjacency matrix A and outputs a transformed node feature matrix Y . Formally, a graph convolution layer in our case has the following formulation,</p><formula xml:id="formula_6">Y = σ([X GX]W ),<label>(6)</label></formula><p>where X ∈ R N ×din , Y ∈ R N ×dout , N is the number of nodes, and d in , d out are the dimension of input/output node features. G = g(X, A) is a symmetric normalized Laplacian matrix of size N × N , and g(·) is a function of X and A. Operator represents matrix concatenation along the feature dimension. W is the learnable weight matrix with size 2d in × d out , and σ(·) is the non-linear activation function. In our paper, mean aggregation is adopted as g(·) for the aggregation operation,</p><formula xml:id="formula_7">G = Λ − 1 2 AΛ − 1 2 ,<label>(7)</label></formula><p>where Λ is a diagonal matrix with Λ ii = j A ij . The topological structure is implicit in the adjacency matrix, and A ij indicates the affinity between the i-th node and j-th node. For a point sequence π = {p 1 , p 2 , . . . , p N }, the adjacency matrix is defined as</p><formula xml:id="formula_8">A ij = 1 − D(p i , p j )/ max(A),<label>(8)</label></formula><p>where D(p i , p j ) is a L2 distance between p i and p j , and the diagonal elements of the adjacency matrix is always 1, which means that each node is self-connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visualization and Discussion</head><p>We will do some visualization and case analysis to prove the effectiveness of PGNet and GRM qualitatively. In <ref type="figure">Fig. 6</ref>, <ref type="figure" target="#fig_4">Fig. 7</ref> and <ref type="figure" target="#fig_5">Fig. 8</ref>, a green polygon means that detection and recognition both hit the ground truth. A blue polygon with red text indicates that the detection result hits, but the recognition result is incorrect. The right recognition results are not displayed for clear visualization. Besides, the upper left vertex of the text polygon in the right reading order is marked with a pink dot, if it exists.</p><p>Qualitative results of PGNet. The visualization of spotting results in ICDAR2015 and Total-Text are shown in <ref type="figure">Fig. 6</ref>. As can be seen, the proposed arbitrarily-shaped text spotter PGNet can handle curved and multi-oriented text regions well. The superiority of TDO is proved in <ref type="figure" target="#fig_5">Fig. 8</ref>. The reading order information of group A is recovered by predefined rules, which is the same as <ref type="bibr" target="#b5">(Feng et al. 2019</ref>). Furthermore, group B is yielded by TDO map. As shown in <ref type="figure">Fig. 6</ref>,  the word "MOVE" is misidentified as "EVOM" with predefined rules, while is recognized correctly with the TDO map.</p><p>Qualitative results of GRM. The capability of GRM is depicted in <ref type="figure" target="#fig_4">Fig. 7</ref>, and group A and group B is the end-toend results without/with GRM. We show that the GRM can correct the recognition results in different scenarios. In the first case, the wrong recognition "PRT" is revised to the correct "PORT", proving the ability of GRM to make up for missing characters. In the second case, the wrong recognition "NARKET" is refined to the correct "MARKET", indicating the capability of correcting the wrong recognition of characters caused by similar appearance. It is shown in the third case that the ability of GRM to delete extra characters by correcting "UNIVERSTITY" to "UNIVERSITY". The cases mentioned above show that the GRM can add, modify and delete the error characters, which fully prove the potential of the GRM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model Speed vs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The pipeline of PGNet: 1) Extract feature from an input image, and learn TCL, TBO, TDO, TCC maps as a multi-task problem;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Label Generation: (a) is the ground truth annotation of curved text in yellow; (b)-(d) are the generation of TCL, TBO, and TDO maps, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The structure of GRM. For each text sequence, we construct a visual graph and a semantic graph, respectively, and their output Y v and Y s are concatenated together for further character classification with several FC layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of our GRM to correct error recognition, where group A/B is the end-to-end results without/with GRM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of our TDO to correctly recognize text in nontraditional reading directions, where group B/A is the end-to-end results with/without TDO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation the effectiveness of GRM to improve the recognition performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on Total-Text for detecting text lines of arbitrary shapes.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>Detection P</cell><cell>F</cell><cell>Method</cell><cell>E2E Recognition S W G</cell></row><row><cell>EAST (Zhou et al. 2017)</cell><cell cols="4">78.3 83.3 80.7 OpenCV 3.0 (Karatzas et al. 2015)</cell><cell>13.8 12.0 8.0</cell></row><row><cell>LOMO (Zhang et al. 2019)</cell><cell cols="5">81.0 91.6 86.0 Deep TextSpotter (Bušta et al. 2017) 54.0 51.0 47.0</cell></row><row><cell cols="5">Mask TextSpotter (Lyu et al. 2018) 81.0 91.6 86.0 Mask TextSpotter (Lyu et al. 2018)</cell><cell>79.3 73.0 62.4</cell></row><row><cell>TextNet (Sun et al. 2018)</cell><cell cols="4">85.4 89.4 87.4 TextNet (Sun et al. 2018)</cell><cell>78.7 74.9 60.5</cell></row><row><cell>FOTS R-50 (Liu et al. 2018)</cell><cell cols="4">85.2 91.0 88.0 FOTS R-50 (Liu et al. 2018)</cell><cell>81.1 75.9 60.8</cell></row><row><cell>TextDragon (Feng et al. 2019)</cell><cell cols="4">83.8 92.5 87.9 TextDragon (Feng et al. 2019)</cell><cell>82.5 78.3 65.2</cell></row><row><cell>CharNet R-50 (Xing et al. 2019)</cell><cell cols="4">88.3 91.2 89.7 CharNet R-50 (Xing et al. 2019)</cell><cell>80.1 74.5 62.2</cell></row><row><cell>Boundary (Wang et al. 2020)</cell><cell cols="4">87.5 89.8 88.6 Boundary (Wang et al. 2020)</cell><cell>79.7 75.2 64.1</cell></row><row><cell>TextPerceptron (Qiao et al. 2020)</cell><cell cols="4">82.5 92.3 87.1 TextPerceptron (Qiao et al. 2020)</cell><cell>80.5 76.6 65.1</cell></row><row><cell>PGNet-A</cell><cell cols="4">84.3 92.4 88.1 PGNet-A</cell><cell>82.9 77.7 62.3</cell></row><row><cell>PGNet-A with GRM</cell><cell cols="4">84.8 91.8 88.2 PGNet-A with GRM</cell><cell>83.3 78.3 63.5</cell></row><row><cell>PGNet-E</cell><cell cols="4">83.6 85.6 84.6 PGNet-E</cell><cell>80.2 74.9 57.4</cell></row><row><cell>PGNet-E with GRM</cell><cell cols="4">83.6 85.8 84.7 PGNet-E with GRM</cell><cell>80.5 75.3 58.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is done when Pengfei Wang is an intern at Baidu Inc., and is also supported in part by the National Natural Science Foundation of China (NSFC) under Grant Nos. 61572387, 61632019, 61871304, and the Foundation for Innovative Research Groups of the NSFC under Grant 61621005.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bušta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comp. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Total-Text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Doc. Anal. Recognit. (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text -RRC-ArT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1571" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comp. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9076" to="9085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comp. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">GTC: Guided training of ctc towards efficient and accurate scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01276</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading. In Int. Conf. Doc. Anal. Recognit. (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2937086</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="532" to="548" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TextBoxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell. (AAAI)</title>
		<meeting>AAAI Conf. Artif. Intell. (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comp. Vis. (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FOTS: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10200</idno>
		<title level="m">ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TextSnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comp. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comp. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int. Conf. 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">IC-DAR2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Doc. Anal. Recognit. (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06820</idno>
		<title level="m">Text perceptron: Towards end-to-end arbitraryshaped text spotting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst. (NIPS</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TextNet: Irregular Text Reading from Images with an End-to-End Trainable Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comp. Vis. (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09705</idno>
		<title level="m">2D-CTC for Scene Text Recognition. arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6896</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12160" to="12167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia (MM)</title>
		<meeting>ACM Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1277" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape Robust Text Detection With Progressive Scale Expansion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linkagebased Face Clustering via Graph Convolution Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Editing text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia (MM)</title>
		<meeting>ACM Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1500" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-organized text detection with minimal post-processing via border learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<idno>doi:10.1109/ TNNLS.2020.2978386. ArXiv:1901.00596</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst. 1-21</title>
		<imprint/>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional character networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9126" to="9136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards Accurate Scene Text Recognition With Semantic Reasoning Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ESIR: End-To-End Scene Text Recognition via Iterative Image Rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A fast parallel algorithm for thinning digital patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="239" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">EAST: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Patt. Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
