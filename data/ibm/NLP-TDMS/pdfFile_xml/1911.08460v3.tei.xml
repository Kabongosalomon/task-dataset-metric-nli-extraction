<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END ASR: FROM SUPERVISED TO SEMI-SUPERVISED LEARNING WITH MODERN ARCHITECTURES A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-15">15 Jul 2020 July 16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
							<email>qiantong@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
							<email>jacobkahn@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
							<email>egrave@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Facebook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
							<email>vineelkpratap@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
							<email>anuroops@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
							<email>vitaliy888@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Facebook</settlement>
									<region>NYC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<settlement>Menlo Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END ASR: FROM SUPERVISED TO SEMI-SUPERVISED LEARNING WITH MODERN ARCHITECTURES A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-15">15 Jul 2020 July 16, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable Con-vNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LIBRISPEECH dataset, and leverage additional unlabeled data from LIBRIVOX through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end speech recognition models are simpler to implement and train than bootstrapped systems. Even given recent promising results from these systems, best-results for common benchmarks are still dominated by classical ASR models; systems requiring force alignment may leave some performance aside for each training step. We set out to study end-to-end systems on LIBRISPEECH <ref type="bibr" target="#b36">[37]</ref> and, without any algorithmic contribution, see if they can be made to perform as well as more complex training pipelines. The difficulties involved in properly optimizing acoustic models with Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b15">[16]</ref> or sequence-to-sequence (Seq2Seq) <ref type="bibr" target="#b46">[47]</ref> (v.s. crossentropy, for instance) combined with more readily-available regularization techniques for classical pipelines make this comparison challenging. Our best acoustic models nonetheless reach 5.17% WER on test-other, showing that end-to-end models can compete with traditional pipelines.</p><p>As in other domains, self and semi-supervised learning in ASR, where a pretrained network generates and trains on its own labels, yields improvements <ref type="bibr" target="#b48">[49]</ref>. In end-to-end ASR, pseudo-labeling and self-training can be quite * Equal contribution. <ref type="figure">Figure 1</ref>: WERs on dev-other across AM architectures and loss functions. Left: WERs of different models trained on LIBRISPEECH with and without beam-search decoding ("no LM" refers to the greedy decoding). Transformer AM architectures outperform others by a large margin. Right: WERs of models trained on LIBRIVOX. All models trained on LIBRIVOX significantly outperform their LIBRISPEECH counterparts. The gap between Transformer AMs and other models is much smaller with LIBRIVOX data. <ref type="bibr">ResNet</ref>   effective, and its effectiveness is further improved when more data is available <ref type="bibr" target="#b24">[25]</ref>. In this setting, we train a model on LIBRISPEECH, then use that model in conjunction with a language model to generate pseudo-labels from unlabeled audio. We show that with this training scheme, our results without an external language model (LM) reach state-of-theart results that use an external language model, with 2.28% and 4.88% Word Error Rate (WER) on test-clean and test-other respectively. With LM beam-search decoding and rescoring, we reach 2.09% and 4.11% WER on the test set.</p><p>While many advances in end-to-end ASR come as the result of neural architecture search <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b5">6]</ref>, we additionally show that simple semi-supervision via pseudo-labeling significantly bridges the performance gap between a variety of different model architectures and loss functions, as shown in <ref type="figure">Figure 1</ref>. In particular, with enough unlabeled audio, Transformer, ResNet, and depthwise-separable convolution-based acoustic models give similar performance with both CTC and Seq2Seq loss functions, suggesting that new techniques in semi-supervision may facilitate equally-significant gains in ASR performance while being applicable to a multitude of end-to-end setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Acoustic Models</head><p>In this section, we present the three families of acoustic models (AMs) studied. All AMs output probability distributions over tokens. In particular, we use a set of 10k word pieces <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b28">29]</ref> generated from the SentencePiece toolkit <ref type="bibr" target="#b0">1</ref> . The choice to use a fixed set of 10k word pieces is made for the simplicity of the comparative study, not the result of a limitation. Similarly, all AMs take 80-channel log-mel filterbanks as input, with STFTs computed on Hamming windows strided by 10ms. This window size is 25ms for Transformer models and 30ms for TDS and ResNet models. All models are trained end-to-end with either CTC or Seq2Seq loss. Given the huge difference between the amounts of data, we prepare two sets of architectures: one for training only on labeled LIBRISPEECH and one for unlabeled LIBRIVOX.</p><p>ResNet Acoustic Models. ResNets were first introduced in the domain of computer vision <ref type="bibr" target="#b20">[21]</ref> and have since been successfully applied to speech recognition <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50]</ref>. ResNets are composed of several blocks of convolutions (in our case only 1-D convolutions), with skip connections to reduce the effect of vanishing gradients in deep neural networks. In particular, our ResNet encoder includes 42 convolutional layers each with a kernel size of 3. The encoder first maps the input to an embedding space of size 1024 using a single convolutional layer with stride 2; 12 blocks of three 1-D convolutions each follow. Each of the convolutional layers is followed by ReLU, dropout and LayerNorm <ref type="bibr" target="#b1">[2]</ref>. Both the dropout and the number of hidden units increases with the depth of the network. Specific convolution layers are inserted between ResNet blocks in order to upsample when the hidden representation size increases. Our architecture performs significant pooling with respect to the input (16 frames in total, equating to 160 milliseconds) -in addition to the first strided convolutional layer, 3 max pooling layers (each with stride 2) are distributed across the depth of the network (after blocks 3, 7 and 10). Nearly identical encoder architectures are used in front of CTC and Seq2Seq loss functions; the Seq2Seq encoder has its last bottleneck layer removed and lower dropout in deeper layers. The Seq2Seq self-attention decoder for the ResNet architecture is the same as that used with the TDS convolutional AM described below. To better fit the unlabeled data, we increase the model size by increasing the number of channels in each convolution layer.</p><p>Time-Depth Separable (TDS) Convolution Acoustic Models. We extend the TDS block <ref type="bibr" target="#b19">[20]</ref> (which is composed of one 2-D convolution layer and two fully-connected layers with ReLU, LayerNorm and residual connections in between), by increasing the number of channels in the feature maps spanning the two internal fully-connected layers by a factor F &gt; 1, so as to increase model capacity. Following <ref type="bibr" target="#b19">[20]</ref>, 3 sub-sampling layers, i.e. 1-D convolution layers with stride 2, are adopted to ensure an optimal context size for the encoder. For training with only labeled data, we have three groups of TDS blocks with F = 3 after each sub-sampling layers. There are 5, 6, and 10 blocks in each group, containing 10, 14, and 18 channels, respectively. To increase model capacity for unlabeled data, the three groups of TDS blocks, having fewer 4, 5, and 6 blocks and F = 2 in each, are equipped with much larger 16, 32, and 48 channels. All convolutions in both TDS and sub-sampling layers have kernel shapes of 21 × 1. Identical encoder architectures are shared between CTC and Seq2Seq.</p><p>Our Seq2Seq self-attention decoder performs R rounds of attention through the same N -layers of RNN-GRU each with a hidden unit size of 512 in conjunction with the same efficient key-value attention as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>:</p><formula xml:id="formula_0">S r t = SOFTMAX( 1 √ d K ⊤ Q r−1 t )V,<label>(1)</label></formula><p>where [K, V] is 512-dimensional encoder activation and Q r t = g(Q r t−1 , Q r−1 t ) + S r t is the query vector at time t in round r, generated by the GRU g(·). The initial Q 0 t is a 512-dimensional token embedding, and the final Q R t is linearly projected to output classes for token classification. In our experiments, N and R are both set to either 2 or 3 based on validation performance. We use dropout in all TDS blocks and GRUs to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-Based Acoustic Models.</head><p>Our transformer-based acoustic models have a small front-end: , with D c = 1024 or 2048. Each convolution is followed by a GLU activation function <ref type="bibr" target="#b11">[12]</ref> and are striding by 2 each (for 3 consecutive layers), or every other layer (for 6 layers). The output of the front-end for all models is thus strided by 8 frames (80 ms). After the front-end, each Transformer block has 4 attention heads followed by a feedforward network (FFN) with one hidden layer and a ReLU non-linearity. There are two configurations of Transformer blocks: one 24 layer configuration (only for the LIBRISPEECH CTC AM) with dimension D tr = 1024 for the self-attention and 4096 for the FFN, and one 36 layer configuration with dimension D tr = 768 for the self-attention and 3072 for the FFN. Specifically, given a sequence of T vectors of dimension d, the input is represented by the matrix H 0 ∈ R d×T , following exactly <ref type="bibr" target="#b47">[48]</ref>:</p><formula xml:id="formula_1">Z i = NORM(SELFATTENTION(H i−1 ) + H i−1 )</formula><p>,</p><formula xml:id="formula_2">H i = NORM(FFN(Z i ) + Z i ),</formula><p>where Z is the output of the self-attention layer, with a skip connection, and H is the output of the FFN layer, with a skip connection. As is standard: our NORM is LayerNorm, and self-attention is defined as in Eq. 1, but with K = W K H, Q = W Q H, and V = W V H. For CTC-trained models, the output of the encoder H Le is followed by a linear layer to the output classes. For Seq2Seq models, we have an additional decoder, which is a stack of 6 Transformers with encoding dimension 256 and 4 attention heads. The probability distribution of the transcription is factorized as:</p><formula xml:id="formula_3">p(y 1 , ..., y n ) = n i=1 p(y i | y 0 , ..., y i−1 , H Le ),<label>(2)</label></formula><p>where y 0 is a special symbol indicating the beginning of the transcription. For all layers (encoder and decoder -when present), we use dropout on the self-attention. We also use layer drop <ref type="bibr" target="#b13">[14]</ref>, dropping entire layers at the FFN level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Models</head><p>In this section, we present external language models (LMs) used in beam-search decoding. We consider n-gram LMs as well as convolutional <ref type="bibr" target="#b11">[12]</ref> (GCNN) and Transformer-based LMs. For n-gram and GCNN LMs, we train both word-based and word-piece models, and only a word-level Transformer LM. All word-piece LMs are trained on the set of 10k word pieces as outlined in Section 2.1. This ensures that the set of word pieces is consistent across both of the output distributions of the acoustic models and the candidates the language model scores during beam-search decoding.</p><p>For the word-piece and word-level GCNN models, we use the GCNN-14B architecture from <ref type="bibr" target="#b11">[12]</ref> with embedding size 1024 and dropout 0.1. The word-level Transformer LM has the same architecture as <ref type="bibr" target="#b2">[3]</ref>'s Google Billion Words model; we use 16 attention heads and 20 decoder layers with embedding, input and output dimensions of 1280 and 6144 for the FFN with dropout of 0.1.</p><p>3 Dataset and Language Models for Pseudo-Labeling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unlabeled Audio Dataset Preparation</head><p>LIBRIVOX 2 is a large collection of freely-available audio books. Using tools provided with the LIBRI-LIGHT dataset <ref type="bibr" target="#b25">[26]</ref>, we select 72K hours of read speech from English book listings and run several preprocessing steps. After filtering samples to remove readings of duplicate text and corrupted audio, we remove all audio for which the speaker has overlap with a sample in LIBRISPEECH. We run voice activity detection (VAD) using the wav2letter++ framework <ref type="bibr" target="#b41">[42]</ref> on the resulting collection of audio with a CTC model trained on LIBRISPEECH, and segment the result into chunks no greater than 36 seconds; the resulting audio corpus contains 53.8K hours of read speech.</p><p>We then generate pseudo-labels for this audio using the recipe described in <ref type="bibr" target="#b24">[25]</ref>. To generate the pseudo-labels, we use a Transformer AM trained on LIBRISPEECH with CTC loss that achieves a 6.20% WER on dev-other when decoded with a 4-gram word LM -the same model as is listed in <ref type="table" target="#tab_5">Table 5</ref> in Appendix. We pseudo-label all audio using this AM and run beam-search decoding with a 4-gram word LM described below in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Corpus Preparation and n-gram LM Training</head><p>The LIBRISPEECH language model corpus 3 contains text from 14500 public domain books taken from the Gutenberg project <ref type="bibr" target="#b3">4</ref> . Given that pseudo-labels are generated with a beam-search decoding procedure that integrates a language model, it is important that the corpus used to train the language model does not have overlap with the unlabeled audio, else information about the ground truth labels for that unlabeled audio may be explicitly embedded in the LM. We remove all text from the LIBRISPEECH language model training corpus that is ground truth for any of the unlabeled audio from the subset of LIBRIVOX.</p><p>To do so, we follow several steps. Firstly, we filter out all books from the LIBRISPEECH LM corpus with IDs present in LIBRIVOX. Secondly, after normalizing all titles (removing punctuation, casing, and non-alphanumeric tokens), we remove all titles with zero Levenshtein distance between titles from the LIBRIVOX and the LIBRISPEECH LM corpuses. We use a Levenshtein metric over words rather than tokens for improved performance. We then find titles with nonzero but low similarity scores isolated via the following conditions. Given two book title strings s 1 and s 2 , and constants α and β:</p><formula xml:id="formula_4">max{|s 1 |, |s 2 |} − min{|s 1 |, |s 2 |} &lt; α · min{|s 1 |, |s 2 |} &amp; Levenshtein(s 1 , s 2 ) ≤ β · max{|s 1 |, |s 2 |}</formula><p>where notation |s| refers to the number of words in the string |s|, and 0.75 and 0.3 were used as values for α and β, respectively. These constants are found empirically to remove obviously different titles and to have reasonable number of pairs ( 10k) for further manual check. Titles that are manually matched are removed to create the final corpus; 13% of the original LIBRISPEECH-LM corpus was filtered with the aforementioned steps.</p><p>Before training LMs, we normalize the filtered corpus so as to mimic the original normalization procedure found in LIBRISPEECH. 88% of our normalized/filtered corpus has identical normalized text compared to the original LIB-RISPEECH LM corpus. As a result of our using a different tokenizer, sentence boundaries may differ across corpuses, as may abbreviations (e.g. we map '&amp;c' to 'et cetera').</p><p>A 4-gram language model is trained with the resulting corpus using the KenLM toolkit <ref type="bibr" target="#b22">[23]</ref> and the top 200k words as vocabulary. The model is trained without pruning (183k of the top 200k words are the same as the original LIB-RISPEECH LM corpus). This model is then used at beam-search decoding time in conjunction with an acoustic model trained on LIBRISPEECH to generate pseudo-labels on the subset of LIBRIVOX detailed in Section 3. During beamsearch decoding we use a lexicon which is constructed from the LIBRISPEECH train sets only.</p><p>The perplexity difference between the 4-gram LM trained on the filtered corpus and the 4-gram LM trained on original LIBRISPEECH LM corpus is small. The word perplexity of each model is shown in <ref type="table" target="#tab_1">Table 1</ref>. Beam-search decoding of a Transformer AM trained on LIBRISPEECH with an LM trained on the filtered corpus results in only a 0.05% absolute WER regression on dev-other compared to decoding with an n-gram trained on the full corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding</head><p>Decoding is designed to select the best transcription by leveraging both the posteriors of an acoustic model (AM) and the perplexity of a language model (LM). We perform one-pass beam-search decoding with a single external LM. Optionally, to further improve performance, we use stronger NN-based LMs to rescore the beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Beam-search Decoder</head><p>In our experiments, we use lexicon-based and lexicon-free beam-search decoders following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> with either n-gram or GCNN LMs. The lexicon-based decoder, whose search space is limited to the words in the lexicon, is used for CTC models with a word-level LM. The lexicon-free decoder is capable of generating words with arbitrary spelling and is used for S2S models with a word-piece LM. The decoder takes as input posteriors from an acoustic model, a prefix trie built on a lexicon, and an external LM. We tune the language model weight α and the word insertion penalty β on validation sets (dev-clean and dev-other). The decoder outputs a transcriptionŷ that maximizes:</p><formula xml:id="formula_5">log P AM (ŷ|x) + α log P LM (ŷ) + β|ŷ|.</formula><p>To stabilize the Seq2Seq beam search, we introduce an EOS-penalty γ to hypothesis that have finished in an end-ofsentence token. γ is tuned together with other hyper-parameters and our experiments show that this strategy effectively prevents the decoder from early-stopping. To improve decoding efficiency, we also incorporate the thresholding technique in <ref type="bibr" target="#b19">[20]</ref> and strategies mentioned in <ref type="bibr" target="#b53">[54]</ref> including hypothesis merging, score caching, and batched LM forwarding. For CTC decoding, following <ref type="bibr" target="#b38">[39]</ref>, only the blank token is considered if its posterior probability is greater than 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rescoring</head><p>After acquiring the transcriptions of the N -best hypotheses from the one-pass beam-search decoder, we use an external word-level GCNN LM and a Transformer LM to evaluate their log-probabilities, denoted as log P 1 (ŷ) and log P 2 (ŷ) respectively. We then perform rescoring to reorder the hypotheses according to the following score:</p><formula xml:id="formula_6">log P AM (ŷ|x) + α 1 log P 1 (ŷ) + α 2 log P 2 (ŷ) + β|ŷ|,</formula><p>where α 1 , α 2 , β are hyper-parameters of the rescoring algorithm optimized on the validation set and |ŷ| is the transcription length in characters (including the spaces between words). In order to diversify the hypotheses in the beam, to increase the probability that the correct transcription is included, we usually relax the threshold in the decoder and increase beam size when dumping beam candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Technical Details</head><p>We use the standard splits for LIBRISPEECH and the standard LIBRISPEECH LM corpus for LM training. Models are trained using the wav2letter++ 5 toolkit <ref type="bibr" target="#b41">[42]</ref>; reproduction steps and pre-trained models are open-sourced 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Acoustic Model Training</head><p>All hyper-parameters including model architecture are cross-validated on dev-clean and dev-other. Given that we have a large family of models, for simplicity and clarity, we only report hyper-parameters ranges in which we search their best values. . We also use 10% dropout in the decoder for TDS (and 0.1 dropout and 0.1 layer drop in the decoder for Transformers), together with 5% label smoothing, 1% random sampling and 1% word piece sampling. All models use SpecAugment <ref type="bibr" target="#b37">[38]</ref> with an LD policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Language Model Training</head><p>All LMs in this section are trained on the standard LIBRISPEECH LM corpus. All word-level LMs use the same vocabulary for training. n-gram LMs are trained with the KenLM toolkit <ref type="bibr" target="#b22">[23]</ref>, while the GCNN and Transformer LMs are trained with fairseq toolkit 7 <ref type="bibr" target="#b35">[36]</ref>. The word-level 4-gram and GCNN are trained in the same way as <ref type="bibr" target="#b31">[32]</ref>. We also train a 6-gram word-piece LM, which has a similar context size to a word-level 4-gram LM, and prunes 5-grams appearing once and 6-gram appearing twice or fewer. The word-piece and word-level GCNN models are trained with Nesterov accelerated gradient descent <ref type="bibr" target="#b34">[35]</ref> on 8 GPUs for 22 epochs with a step-wise learning rate schedule starting from 1 and decreasing by a factor of 5 when loss stabilized. Gradient clipping and weight normalization are used following <ref type="bibr" target="#b11">[12]</ref>. The word-level Transformer LM is trained with Nesterov accelerated gradient descent on 128 GPUs for 100 epochs with an inverse square root learning rate schedule. During the first 16000 iterations, a warm-up schedule that linearly increases the learning rate from 0 to 1 is used. Word-level perplexities of all LM variants can be found in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>LIBRISPEECH Results. All our results for LIBRISPEECH are listed in Appendix <ref type="table" target="#tab_5">Table 5</ref>. We present results under three scenarios: without any decoding nor external LM, with one-pass decoding only, and with decoding followed by beam rescoring. The decoding beam size is usually 50 and 500 for Seq2Seq and CTC respectively. We use a beam size of 250 for CTC decoding with a GCNN LM. We train strong baselines on simple ResNet architectures and improve the TDS models significantly compared to past results <ref type="bibr" target="#b19">[20]</ref>. These convolutional models outperform endto-end biLSTM models from <ref type="bibr" target="#b32">[33]</ref>. Our best acoustic models are Transformers-based and reach 6.98% without any decoding on test-otherand 5.17% with decoding and rescoring, demonstrating that end-to-end training can perform as well as traditional bootstrapped systems.</p><p>LIBRIVOX Results. Assuming all pseudo-labels are ground-truth, we train acoustic models on a combination of the 960 hours of labeled audio from LIBRISPEECH in conjunction the pseudo-labeled audio from LIBRIVOX, where batches are uniformly sampled (without weighting) from both LIBRISPEECH and LIBRIVOX datasets. Transformer AMs with both CTC and Seq2Seq loss were trained for 5 days on this combined dataset, achieving WERs on test-other of 4.88% and 2.28% on test-clean without decoding or use of an LM, which is state-of-the-art even amongst pipelines that use an LM. Results with decoding/rescoring are shown in <ref type="table" target="#tab_2">Table 2</ref>, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablations</head><p>Varying the amount of unlabeled audio. In this study, we train on several different randomly-selected subsets of pseudo-labels from the original collection generated as described in Section 3. Results are given in <ref type="table" target="#tab_3">Table 3</ref>. Increasing the amount of pseudo-labels strictly improves performance. The listed 53.8k hour result is using the fully-prepared dataset as outlined in Section 3.1. WERs given are without decoding after 800k iterations of training.</p><p>Generating pseudo-labels with an LM containing overlapping text. As discussed in Section 3.1, using an LM to generate pseudo-labels that was trained with a corpus that includes ground truth text from unlabeled audio introduces an overlap that may unrealistically improve the quality of pseudo-labels. We show that the effect of using an LM trained with a small amount of overlapping text to generate pseudo-labels has only a small effect on the performance of models trained on those pseudo-labels. <ref type="table" target="#tab_4">Table 4</ref> contains results for Transformer AMs with both CTC and Seq2Seq loss as described in 2.1 trained on pseudolabels generated with a decoding step that uses an LM trained on an overlapping versus non-overlapping corpus. The models used are of the same architecture as described in Section 2.1. There is a small improvement in dev-other performance for pseudo-labels generated from an overlapping LM, but both models generalize very similarly.</p><p>Training on pseudo-labels only. Models trained on LIBRIVOX pseudo-labels alone outperform models trained on LIBRISPEECH. As outlined in Section 5, all acoustic models are trained on a combination of LIBRISPEECH and pseudo-labeled LIBRIVOX audio. In this setup, it is difficult to disambiguate the importance of the pseudo-labeled audio compared to supervised data from LIBRISPEECH. To test the quality of pseudo-labels in isolation, we trained a CTC-based Transformer model similar to that described in Section 2.1 to compare directly with the CTC-based transformer AM used to generate the pseudo-labels described in Section 3. We compare the resulting AM-only performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">End-to-End Acoustic Models Learn a Language Model: Removing the LM from ASR</head><p>In the sections that follow, we show two results. We first give a simple experimental framework to demonstrate that acoustic models trained on speech learn nontrivial language models, and that training on additional audio facilitates learning better acoustic representations. We then show that with a large collection of pseudo-labeled audio, welltrained acoustic models no longer benefit much from decoding with an external language model in most cases.</p><p>AMs learning LM: transcribing shuffled audio. The language modeling properties of end-to-end acoustic models are briefly discussed in <ref type="bibr" target="#b3">[4]</ref>, where an AM trained with CTC is shown to learn an implicit language model based on its predicted posteriors for words with multiple spelling variants. Still other results show that fusing an LM with an AM during training can improve performance <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b51">52]</ref>. These previous works use RNN-based acoustic models, which possess infinite receptive fields and processes most or all of an input utterance during a single forward pass. We show that modern convolutional architectures have large receptive fields and also condition on internal word representations learned directly from audio.</p><p>If an AM learns a robust LM, the acoustic model will less effectively predict utterances of high underlying wordperplexity; the model will rely on its acoustic representations to predict words without context, providing a good proxy for the quality of its learned acoustic representations. In the experiments that follow, we introduce a simple   <ref type="bibr" target="#b26">[27]</ref> trained on the LJSpeech dataset 8 using the Mozilla TTS toolkit <ref type="bibr" target="#b8">9</ref> . In the second setting, audio is segmented at the word level using a convolutional stride 2 letter-based AM trained with ASG loss <ref type="bibr" target="#b7">[8]</ref>, then re-spliced together in the given shuffled order. Finally, the paper's authors recorded unshuffled and shuffled utterances from a subset of dev-other. <ref type="figure" target="#fig_2">Figure 2</ref> contains the WERs across audio settings on dev-other without decoding. Both CTC and Seq2Seq models perform poorly across the board on shuffled audio which is expected. As soon as we are interested not in the absolute WER values but in the relative WER values across models / losses / datasets, the main outcome from <ref type="figure" target="#fig_2">Figure 2</ref> is that AMs trained with LIBRIVOX pseudo-labels are able to learn better acoustic representations which improve performance on shuffled inputs for which their internal LMs is not predictive. Full results can be found in Appendix <ref type="table" target="#tab_9">Table  9</ref>.</p><p>With enough unlabeled audio, decoding with an LM doesn't improve performance. The importance of the language model to the success of the pseudo-labeling is known; <ref type="bibr" target="#b24">[25]</ref> show that in the end-to-end setting, as the quality of the language model used to generate the pseudo-label decreases even marginally, the quality of the model trained on the resulting pseudo-labels decreases. In what follows, we show that through the self-training procedure, decoding an acoustic model trained on LIBRIVOX pseudo-labels generated with the help of a language model gives very small improvements compared to models trained only on LIBRISPEECH.</p><p>Results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. We use a beam-search decoding procedure without an LM ("Zero-LM") to disambiguate the effect of beam search on WER, and evaluate on dev-other to provide a better lower bound for how much decoding with the LM can improve performance (decoder parameters are also optimized on dev-other ). The models for which results are shown are trained on pseudo-labels from LIBRIVOX generated with an n-gram language model without an overlapping text corpus (see the ablation in Sections 5.3 and 2.2). Decoding with the LM gives little to no gain for models trained on LIBRISPEECH + LIBRIVOX and a much more significant gain for those models trained only on LIBRISPEECH, suggesting information from the 4-gram LM used to generate pseudo-labels on LIBRIVOX has thoroughly diffused into AMs trained with those labels. Full results can be found Appendix in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Deep neural networks were reintroduced in ASR with HMMs <ref type="bibr" target="#b23">[24]</ref>, and many of state-of-the-art models still rely on force alignment <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28]</ref>. Nonetheless, there have been increasingly competitive end-to-end results trained with CTC <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1]</ref>, ASG <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">54]</ref>, LF-MMI <ref type="bibr" target="#b16">[17]</ref>, sequence-to-sequence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, transduction <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b21">22]</ref>, and differentiable decoding <ref type="bibr" target="#b8">[9]</ref>. Listen Attend and Spell <ref type="bibr" target="#b3">[4]</ref> is a family of end-to-end models based on biLSTMs which achieved stateof-the-art results with improved regularization through data augmentation <ref type="bibr" target="#b37">[38]</ref>; we consequently use SpecAugment in all of our experiments. Seq2Seq models are not limited to RNNs; time-depth separable convolutions also give strong results <ref type="bibr" target="#b19">[20]</ref>. Our best models are transformer-based, as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28]</ref>, which give good results in Seq2Seq settings even without external LMs <ref type="bibr" target="#b33">[34]</ref>. In ASR, semi-supervised pseudo-label-style self-training has been explored generally in end-to-end settings in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25]</ref> for both low-resource <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11]</ref> and large-scale <ref type="bibr" target="#b39">[40]</ref> setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We presented state-of-the-art results on LIBRISPEECH with end-to-end methods. While allowing for lexicon-free decoding, the 10k word-piece tokens used during training limit the amount of striding we can use in our model architectures and can be replaced by AMs outputting words with an arbitrary lexicon <ref type="bibr" target="#b9">[10]</ref>. As relative WER gains due to language models shrink (from ≈20% relative-WER without LIBRIVOX to ≈10% with, for GCNN decoding), and as we showed that AMs learn LM-level information, differentiable decoding <ref type="bibr" target="#b8">[9]</ref> is a possible avenue for single-stage AM + LM joint training.</p><p>We show the effectiveness of a simple pipeline that does not require many training steps. In light of our semi-supervised results without decoding or an LM, we think Seq2Seq/CTC losses, transducers, and differentiable decoding are viable methods to achieve end-to-end state-of-the-art results, without external LMs, through semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We would like to thank Steven Garan for audio recordings of shuffled sentences from LIBRISPEECH dev-other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head><p>Comprehensive WER results for LIBRISPEECH and LIBRIVOX acoustic models, including with greedy and beamsearch decoding with different LMs and beam rescoring can be found in <ref type="table" target="#tab_5">Table 5</ref>. This section mainly focus on providing details of how we optimize the beam-search decoding and rescoring procedures for our acoustic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Beam-Search Decoding</head><p>When beam-search decoding, we use the dev-clean and dev-other sets as validation sets and use random search to optimize decoding hyper-parameters. The search ranges of those hyper-parameters are listed in <ref type="table" target="#tab_6">Table 6</ref>. We use between 64 and 128 runs in each random search with hyper-parameter values uniformly sampled from the given ranges. It is worth noting that the optimal ranges for language model weight for models trained on LIBRISPEECH are higher than ones found for LIBRIVOX models as shown in <ref type="table" target="#tab_7">Table 7</ref>. This is conceivably additional evidence that models trained with additional audio rely less on language models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Generating Shuffled Audio</head><p>This section provides details of how we generated shuffled utterances used in the experiments in Section 5.4. Each experiment could introduce systematic error. Therefore, we propose several experiments to conclude. For the two methods generating existing or using new audio (TTS and Segmentation), we shuffle dev-other five times and report the mean and standard deviation (as error bars) in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 TTS</head><p>For each sentence in dev-other, we randomly shuffle its words to form a new sentence. We run the resulting text through a TTS model as outlined in Section 5.3 to create synthetic audio for the scrambled sentences. While simple and easy to implement, this method introduces and amplifies intrinsic errors in the TTS model into the ablation. In particular, the model struggles to handle many of the rare words present in dev-other. Also TTS approach is still away from the human speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Segmentation</head><p>With this method, we first force-align the transcriptions of dev-other to the existing audio using a letter-based stridetwo ASG model as outlined in Section 5.3 and collecting the beginning timestamp and duration of each word. Then, to avoid splicing words that are ready closely together, audio samples are only split when silence of longer than 130 milliseconds is detected (split is done in the middle of silence segment). Finally, audio chunks are randomly shuffled and re-assembled into new utterances. Since this ablation aims to remove LM-friendly context from audio, we filter the resulting recombined audio samples. In particular, we filter all utterances that have only one segment, or have at least one segment with more than 6 words in it. After filtering, 1969 out of 2864 samples in dev-other remain. The distribution of the number of words in each of the resulting segments is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Unlike the TTS method described above, the segmentation method reuses audio as much as possible from dev-other. That said, neither the force alignment nor the segmentation techniques handle all the word boundaries. As such, there may be incomplete words in the resulting audio and LM-friendly context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Recording</head><p>The paper's authors recorded 184 randomly selected sentences from dev-other as well as a single set of shuffled utterances. The unshuffled recorded audio has the lowest WER among all the three methods. We plan to complete a collection of unshuffled and shuffled audio for dev-other in future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Perplexity</head><p>As shown in <ref type="table" target="#tab_9">Table 9</ref>, there are large gaps between the perplexity of transcriptions in the original and shuffled sets across all settings. Our shuffling strategy thus removes important word context and breaks the alignment of the audio words distribution with the LM. The WER gap between the two sets is thus a proxy for the amount of language modeling an acoustic model may implicitly perform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 (</head><label>3</label><figDesc>LIBRISPEECH AMs) or 6 (LIBRIVOX AM) layers of 1-D convolutions each of kernel width 3 and respective input and output sizes (80, D c ), (D c /2, D c ), [(D c /2, D c ), (D c /2, D c ), (D c /2, D c ),] (D c /2, D tr × 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>dev-other WERs without decoding across acoustic models and loss functions for original and shuffled versions of dev-other across three settings. Each plot uses the following original and shuffled audio: Left: original and shuffled dev-other audio segmented using ASG. Middle: audio generated by TTS vocoder for the original and shuffled transcriptions from dev-other. Right: original and shuffled audio for a subset of dev-other recorded by the paper's authors.R e s N e t C T C T D S C T C T r a n s f . C T C R e s N e t S 2 S T D S S 2 S T r a n s f . S 2 S R e s N e t C T C T D S C T C T r a n s f . C T C R e s N e t S 2 S T D S S 2 S T r a n s f . S 2 S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 10 15 Figure 3 :</head><label>5153</label><figDesc>original LS shuffled LS + LV shuffled R e s N e t C T C T D S C T C T r a n s f . C T C R e s N e t S 2 S T D S S 2 S T r a n s f . S 2 S WER on dev-other for models trained on LIBRISPEECH and LIBRISPEECH + LIBRIVOX after decoding with and without the 4-gram LM described in Section 2.2. The gain from LM beam-search decoding for models trained on LIBRIVOX is much smaller compared to that for models trained on LIBRISPEECH.T r a n s f . C T C T r a n s f . " task in which models transcribe LIBRISPEECH dev-other with utterances corresponding to both unshuffled and shuffled transcriptions. Experiments are performed in three audio settings to eliminate bias when scrambling words. First, with a TTS model, unshuffled and shuffled sentences are forwarded through a WaveRNN vocoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Distribution of all n-grams in the obtained segments of filtered dev-other (1969 samples with 16,362 segments in total).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Word-level perplexities of LMs on LIBRISPEECH. Perplexity is computed without unknown words.Plain SGD with momentum is used to train ResNet and TDS models, and Adagrad<ref type="bibr" target="#b12">[13]</ref> to train Transformers. Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks. The initial learning rate for ResNet models is chosen from [0.05, 0.5] , while for TDS and Transformer models, the range decreases to [0.01, 0.03]. Specifically, for Transformers, we apply a linear learning rate warm-up schedule for either 32k or 64k updates. For fully-supervised training with LIBRISPEECH, the learning rate is halved</figDesc><table><row><cell>LANGUAGE MODEL</cell><cell>dev-clean</cell><cell>dev-other</cell></row><row><cell>WORD 4-GRAM</cell><cell>148.0</cell><cell>136.6</cell></row><row><cell>W/O LIBRIVOX</cell><cell>152.8</cell><cell>140.0</cell></row><row><cell>WP 6-GRAM</cell><cell>145.4</cell><cell>133.7</cell></row><row><cell>WP GCNN (188M)</cell><cell>61.7</cell><cell>61.9</cell></row><row><cell>WORD GCNN (319M)</cell><cell>57.0</cell><cell>57.9</cell></row><row><cell>WORD TRANSF. (562M)</cell><cell>48.2</cell><cell>50.2</cell></row></table><note>every 90 epochs for Transformer models, and 150 epochs for ResNet and TDS models. With LIBRIVOX, however, we only halve the learning rate once in the middle of the training. For TDS and ResNet models, we use momentum in the range [0.1, 0.6]. With respect to regularization, we use 0.2 dropout everywhere (front-end, encoder, decoder), and layer drop for all Transformer blocks. Dropout in TDS blocks and ResNet convolutions is in the range [0.05, 0.2] and increases with depth. For Seq2Seq training, we run 3 epochs of attention-window pretraining, and use 99% of teacher forcing (1% of uniform output sampling)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>WERs on LIBRISPEECH development and test sets. Our best results are shown in the bottom section (with the number of parameters), and are both trained with Seq2Seq loss. Full results can be found in AppendixTable 5.</figDesc><table><row><cell>AM</cell><cell></cell><cell>LM</cell><cell></cell><cell cols="2">DEV</cell><cell cols="2">TEST</cell></row><row><cell>TYPE</cell><cell>LEXICON</cell><cell>TYPE</cell><cell>LEXICON</cell><cell>CLEAN</cell><cell>OTHER</cell><cell>CLEAN</cell><cell>OTHER</cell></row><row><cell>FULLY CONV. [54]</cell><cell>LETTER</cell><cell>GCNN</cell><cell>WORD</cell><cell>3.1</cell><cell>9.9</cell><cell>3.3</cell><cell>10.5</cell></row><row><cell>CONV. TRANSF. [34]</cell><cell>5K WP</cell><cell>-</cell><cell>-</cell><cell>4.8</cell><cell>12.7</cell><cell>4.7</cell><cell>12.9</cell></row><row><cell>TDS CONV. [20]</cell><cell>10K WP</cell><cell>GCNN</cell><cell>-</cell><cell>5.0</cell><cell>14.5</cell><cell>5.4</cell><cell>15.6</cell></row><row><cell>DECODING</cell><cell>10K WP</cell><cell>GCNN</cell><cell>10K WP</cell><cell>3.0</cell><cell>8.9</cell><cell>3.3</cell><cell>9.8</cell></row><row><cell>LAS [38]</cell><cell>16K WP</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>2.8</cell><cell>6.8</cell></row><row><cell>DECODING</cell><cell>16K WP</cell><cell>RNN</cell><cell>16K WP</cell><cell></cell><cell></cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>BILSTM + ATTN. [33]</cell><cell>10K BPE</cell><cell>-</cell><cell>-</cell><cell>4.3</cell><cell>12.9</cell><cell>4.4</cell><cell>13.5</cell></row><row><cell>+ TRANSF. DECODING</cell><cell>10K BPE</cell><cell>TRANSF.</cell><cell>10K BPE</cell><cell>2.6</cell><cell>8.4</cell><cell>2.8</cell><cell>9.3</cell></row><row><cell>HMM/BILSTM [33]</cell><cell>12K CDP</cell><cell>4GRAM+LSTM</cell><cell>WORD</cell><cell>2.2</cell><cell>5.1</cell><cell>2.6</cell><cell>5.5</cell></row><row><cell>+ TRANSF. RESCORING</cell><cell>12K CDP</cell><cell>+ TRANSF.</cell><cell>WORD</cell><cell>1.9</cell><cell>4.5</cell><cell>2.3</cell><cell>5.0</cell></row><row><cell>TRANSFORMERS [28]</cell><cell>BPE</cell><cell>RNN</cell><cell>WORD</cell><cell>2.2</cell><cell>5.6</cell><cell>2.6</cell><cell>5.7</cell></row><row><cell>CONV. TRANSF. [19]</cell><cell cols="2">6K TRIPHONES 3GRAM, RESCORED</cell><cell>WORD</cell><cell>1.8</cell><cell>5.8</cell><cell>2.2</cell><cell>5.7</cell></row><row><cell></cell><cell></cell><cell>+ TDNN + LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CONV. TRANSF. [51]</cell><cell>CHENONES</cell><cell>4GRAM</cell><cell>WORD</cell><cell></cell><cell></cell><cell>2.60</cell><cell>5.59</cell></row><row><cell>+ TRANSF. RESCORING</cell><cell>CHENONES</cell><cell>TRANSF.</cell><cell>WORD</cell><cell></cell><cell></cell><cell>2.26</cell><cell>4.85</cell></row><row><cell>TRANSF. (270M) -LIBRISPEECH</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.54</cell><cell>6.67</cell><cell>2.89</cell><cell>6.98</cell></row><row><cell>+ DECODING/RESCORING</cell><cell>10K WP</cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.10</cell><cell>4.79</cell><cell>2.33</cell><cell>5.17</cell></row><row><cell>TRANSF. (296M) -LIBRIVOX</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.12</cell><cell>4.59</cell><cell>2.28</cell><cell>4.88</cell></row><row><cell>+ DECODING/RESCORING</cell><cell>10K WP</cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.00</cell><cell>3.65</cell><cell>2.09</cell><cell>4.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>TRAINING DATASET</cell><cell></cell><cell></cell></row><row><cell>(HOURS)</cell><cell>dev-clean</cell><cell>dev-other</cell></row><row><cell>LS ONLY</cell><cell>2.54</cell><cell>6.67</cell></row><row><cell>LS + 1K LV</cell><cell>2.35</cell><cell>5.56</cell></row><row><cell>LS + 3K LV</cell><cell>2.21</cell><cell>5.16</cell></row><row><cell>LS + 10K LV</cell><cell>2.11</cell><cell>4.95</cell></row><row><cell>LS + 53.8K LV</cell><cell>2.11</cell><cell>4.59</cell></row></table><note>WERs of a Transformer AM architecture outlined in section 2.1 trained with Seq2Seq loss on LIB-RISPEECH with different amounts of pseudo-labeled audio from LIBRIVOX.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>WERs of a Transformer AM when trained with pseudo-labels generated with a decoder integrating an LM that contains overlapping text with unlabeled audio versus an LM with no overlap. Results are shown after decoding with the word 4-gram language model described in Section 2.2. 38% and 5.43% on dev-clean and dev-other respectively, which improves over the LIBRISPEECH-only baseline's 2.99% and 7.31%, respectively. The volume, quality, and diversity of the generated pseudo-labels alone are sufficient to generate superior results as compared to a model trained only on LIBRISPEECH. The model trained on LIBRISPEECH and LIBRIVOX pseudo-labels achieves an improved 2.28% and 4.99% on dev-clean and dev-other, respectively.</figDesc><table><row><cell>MODEL</cell><cell cols="2">OVERLAP dev-other</cell><cell>test-other</cell></row><row><cell>TRANS. S2S</cell><cell>NO YES</cell><cell>4.58 4.51</cell><cell>4.90 4.87</cell></row><row><cell>TRANS. CTC</cell><cell>NO YES</cell><cell>4.92 4.80</cell><cell>5.47 5.33</cell></row><row><cell cols="4">on the LIBRISPEECH development sets. Without decoding, the resulting LIBRIVOX pseudo-label-only model achieves</cell></row><row><cell>WERs of 2.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Word error rates on LIBRISPEECH's development and test sets. Our models listed in the middle and bottom blocks are trained with CTC and Seq2seq losses respectively.</figDesc><table><row><cell>AM</cell><cell></cell><cell>LM</cell><cell></cell><cell cols="2">DEV</cell><cell cols="2">TEST</cell></row><row><cell>TYPE</cell><cell>LEXICON</cell><cell>TYPE</cell><cell>LEXICON</cell><cell>CLEAN</cell><cell>OTHER</cell><cell>CLEAN</cell><cell>OTHER</cell></row><row><cell>CTC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RESNET (306M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>3.93</cell><cell>10.13</cell><cell>4.08</cell><cell>10.03</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>3.76</cell><cell>9.7</cell><cell>4.07</cell><cell>9.77</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>3.29</cell><cell>8.56</cell><cell>3.68</cell><cell>8.69</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.99</cell><cell>7.50</cell><cell>3.28</cell><cell>7.53</cell></row><row><cell>RESNET (500M) LIBRIVOX</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.34</cell><cell>5.54</cell><cell>2.55</cell><cell>5.99</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>2.37</cell><cell>5.45</cell><cell>2.73</cell><cell>5.96</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>2.34</cell><cell>5.23</cell><cell>2.68</cell><cell>5.75</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.19</cell><cell>4.64</cell><cell>2.45</cell><cell>5.13</cell></row><row><cell>TDS (200M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>4.22</cell><cell>11.16</cell><cell>4.63</cell><cell>11.16</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>3.93</cell><cell>10.61</cell><cell>4.44</cell><cell>10.67</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>3.49</cell><cell>9.18</cell><cell>3.98</cell><cell>9.53</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.92</cell><cell>7.52</cell><cell>3.40</cell><cell>8.05</cell></row><row><cell>TDS (500M) LIBRIVOX</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.44</cell><cell>5.70</cell><cell>2.66</cell><cell>6.11</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>2.47</cell><cell>5.61</cell><cell>2.86</cell><cell>6.18</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>2.44</cell><cell>5.33</cell><cell>2.81</cell><cell>5.91</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.26</cell><cell>4.71</cell><cell>2.55</cell><cell>5.24</cell></row><row><cell>TRANSF. (322M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.99</cell><cell>7.31</cell><cell>3.09</cell><cell>7.40</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>2.85</cell><cell>6.98</cell><cell>3.14</cell><cell>7.23</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>2.63</cell><cell>6.20</cell><cell>2.86</cell><cell>6.72</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.18</cell><cell>4.90</cell><cell>2.44</cell><cell>5.36</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.35</cell><cell>5.29</cell><cell>2.57</cell><cell>5.85</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.20</cell><cell>4.94</cell><cell>2.47</cell><cell>5.45</cell></row><row><cell cols="2">TRANSF. (299M) LIBRIVOX 10K WP</cell><cell>-</cell><cell>-</cell><cell>2.28</cell><cell>5.00</cell><cell>2.39</cell><cell>5.35</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEX</cell><cell>2.31</cell><cell>4.94</cell><cell>2.58</cell><cell>5.42</cell></row><row><cell>DECODING</cell><cell></cell><cell>4GRAM</cell><cell>WORD</cell><cell>2.24</cell><cell>4.59</cell><cell>2.52</cell><cell>5.22</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>1.99</cell><cell>3.91</cell><cell>2.28</cell><cell>4.50</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>WORD</cell><cell>2.09</cell><cell>4.27</cell><cell>2.41</cell><cell>4.79</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.01</cell><cell>3.95</cell><cell>2.31</cell><cell>4.54</cell></row><row><cell>SEQ2SEQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RESNET (389M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>3.51</cell><cell>9.89</cell><cell>4.92</cell><cell>10.33</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>3.42</cell><cell>9.60</cell><cell>4.31</cell><cell>9.59</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>3.05</cell><cell>8.69</cell><cell>3.88</cell><cell>8.88</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>2.78</cell><cell>7.86</cell><cell>3.79</cell><cell>8.21</cell></row><row><cell>RESNET (500M) LIBRIVOX</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.27</cell><cell>5.29</cell><cell>2.86</cell><cell>5.88</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>2.26</cell><cell>5.28</cell><cell>2.67</cell><cell>5.54</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>2.29</cell><cell>5.25</cell><cell>2.69</cell><cell>5.62</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>2.26</cell><cell>4.91</cell><cell>2.66</cell><cell>5.31</cell></row><row><cell>TDS (190M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>3.20</cell><cell>8.20</cell><cell>3.43</cell><cell>8.30</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>2.89</cell><cell>8.00</cell><cell>3.24</cell><cell>7.99</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>2.76</cell><cell>7.01</cell><cell>3.18</cell><cell>7.16</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>2.54</cell><cell>6.30</cell><cell>2.93</cell><cell>6.43</cell></row><row><cell>TDS (500M) LIBRIVOX</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.17</cell><cell>4.78</cell><cell>2.37</cell><cell>5.15</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>2.20</cell><cell>4.80</cell><cell>2.38</cell><cell>5.11</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>2.18</cell><cell>4.61</cell><cell>2.35</cell><cell>5.02</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>2.08</cell><cell>4.21</cell><cell>2.24</cell><cell>4.61</cell></row><row><cell>TRANSF. (270M)</cell><cell>10K WP</cell><cell>-</cell><cell>-</cell><cell>2.54</cell><cell>6.67</cell><cell>2.89</cell><cell>6.98</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>2.49</cell><cell>6.32</cell><cell>2.75</cell><cell>6.58</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>2.29</cell><cell>5.81</cell><cell>2.72</cell><cell>6.23</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.13</cell><cell>5.00</cell><cell>2.51</cell><cell>5.47</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>2.12</cell><cell>5.20</cell><cell>2.40</cell><cell>5.70</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.10</cell><cell>4.79</cell><cell>2.33</cell><cell>5.17</cell></row><row><cell cols="2">TRANSF. (296M) LIBRIVOX 10K WP</cell><cell>-</cell><cell>-</cell><cell>2.12</cell><cell>4.59</cell><cell>2.28</cell><cell>4.88</cell></row><row><cell>DECODING</cell><cell></cell><cell>ZEROLM</cell><cell>LEXFREE</cell><cell>2.10</cell><cell>4.53</cell><cell>2.27</cell><cell>4.80</cell></row><row><cell>DECODING</cell><cell></cell><cell>6GRAM</cell><cell>10K WP</cell><cell>2.06</cell><cell>4.32</cell><cell>2.25</cell><cell>4.70</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>1.91</cell><cell>3.76</cell><cell>2.10</cell><cell>4.20</cell></row><row><cell>DECODING</cell><cell></cell><cell>GCNN</cell><cell>10K WP</cell><cell>1.97</cell><cell>3.95</cell><cell>2.17</cell><cell>4.37</cell></row><row><cell>+ RESCORING</cell><cell></cell><cell>GCNN + TRANSF.</cell><cell>WORD</cell><cell>2.00</cell><cell>3.65</cell><cell>2.09</cell><cell>4.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter values and ranges used in a random search for beam-search decoding with n-gram (top block) and GCNN (bottom block) LMs.</figDesc><table><row><cell></cell><cell cols="2">LIBRISPEECH</cell><cell cols="2">LIBRIVOX</cell></row><row><cell>PARAMETERS</cell><cell>CTC</cell><cell>S2S</cell><cell>CTC</cell><cell>S2S</cell></row><row><cell>BEAM</cell><cell>500</cell><cell>50, 100</cell><cell>500</cell><cell>20, 50, 100</cell></row><row><cell>TOKEN BEAM</cell><cell>100</cell><cell>10, 50</cell><cell>100</cell><cell>3, 5, 10</cell></row><row><cell>LM WEIGHT</cell><cell>[0, 3]</cell><cell>[0, 2]</cell><cell>[0, 1.5]</cell><cell>[0, 1]</cell></row><row><cell>THRESHOLD</cell><cell>100</cell><cell>10, 50</cell><cell>100</cell><cell>5, 10, 50</cell></row><row><cell>WORD INSERT.</cell><cell>[−3, 3]</cell><cell>-</cell><cell>[−3, 3]</cell><cell>-</cell></row><row><cell>EOS-PENALTY</cell><cell>-</cell><cell>[−10, 0]</cell><cell>-</cell><cell>[−10, 0]</cell></row><row><cell>BEAM</cell><cell>250</cell><cell>50</cell><cell>250</cell><cell>20, 50, 100</cell></row><row><cell>TOKEN BEAM</cell><cell>100</cell><cell>10, 18</cell><cell>100</cell><cell>3, 5, 10</cell></row><row><cell>LM WEIGHT</cell><cell>[0, 3]</cell><cell>[0, 2]</cell><cell>[0, 1.5]</cell><cell>[0, 0.8]</cell></row><row><cell>THRESHOLD</cell><cell>20</cell><cell>10, 15</cell><cell>20</cell><cell>5, 10, 50</cell></row><row><cell>WORD INSERT.</cell><cell>[−3, 3]</cell><cell>-</cell><cell>[−3, 3]</cell><cell>-</cell></row><row><cell>EOS-PENALTY</cell><cell>-</cell><cell>[−10, 0]</cell><cell>-</cell><cell>[−10, 0]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Optimal LM weight ranges (based on WER) for beam-search decoding with n-gram (top block) and GCNN (bottom block) LMs found via random search.To perform rescoring, we first dump all hypotheses proposed during beam-search decoding using the optimal hyperparameters found with random search. When dumping candidates, beam size, token beam size, and beam threshold are increased so as to increase the number of proposed hypotheses on which to run rescoring. Further details are listed inTable 8. We find optimal values of rescoring hyper-parameters α 1 , α 2 and β (see Section 4.2) via a grid search for CTC models (α 1 , β ∈ [0, 1] and α 2 ∈ [−0.3, 0.3] where the grid step is set to 0.1), and a random search for sequence-to-sequence models (α 1 , ∈ [0, 2.5], α 2 ∈ [−1, 1], β ∈ [−3, 3] with 1000 attempts).</figDesc><table><row><cell></cell><cell cols="2">LIBRISPEECH</cell><cell cols="2">LIBRIVOX</cell></row><row><cell>DATA</cell><cell>CTC</cell><cell>S2S</cell><cell>CTC</cell><cell>S2S</cell></row><row><cell>CLEAN</cell><cell cols="4">[0.8, 1.4] [0.6, 1.1] [0.2, 0.4] [0.0, 0.2]</cell></row><row><cell>OTHER</cell><cell cols="4">[1.1, 1.9] [0.6, 1.2] [0.5, 0.7] [0.1, 0.5]</cell></row><row><cell>CLEAN</cell><cell cols="4">[0.4, 0.8] [0.2, 0.5] [0.2, 0.5] [0.0, 0.4]</cell></row><row><cell>OTHER</cell><cell cols="4">[0.5, 1.1] [0.3, 0.7] [0.3, 0.6] [0.2, 0.4]</cell></row><row><cell>A.2 Rescoring</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Parameters values used when dumping beam candidates for rescoring with n-gram (top block) and GCNN (bottom block) LMs.</figDesc><table><row><cell cols="3">PARAMETERS CTC S2S</cell></row><row><cell>BEAM</cell><cell cols="2">2500 250</cell></row><row><cell>TOKEN BEAM</cell><cell cols="2">1500 150</cell></row><row><cell>THRESHOLD</cell><cell cols="2">5000 150</cell></row><row><cell>BEAM</cell><cell>250</cell><cell>250</cell></row><row><cell>TOKEN BEAM</cell><cell>100</cell><cell>100</cell></row></table><note>THRESHOLD 20 100</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance of word-level 4-gram and Transformer LMs fromTable 1on original and shuffled audio transcriptions generated from LIBRISPEECH dev-other.</figDesc><table><row><cell>SETTING</cell><cell cols="3">SHUFFLED 4-GRAM LM TRANSF. LM</cell></row><row><cell>TTS</cell><cell>NO</cell><cell>147</cell><cell>50</cell></row><row><cell>TTS</cell><cell>YES</cell><cell>749 ± 2</cell><cell>389 ± 2</cell></row><row><cell>SEGMENT.</cell><cell>NO</cell><cell>167</cell><cell>56</cell></row><row><cell>SEGMENT.</cell><cell>YES</cell><cell>827 ± 5</cell><cell>743 ± 9</cell></row><row><cell>RECORDING</cell><cell>NO</cell><cell>162</cell><cell>49</cell></row><row><cell>RECORDING</cell><cell>YES</cell><cell>3807</cell><cell>2995</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/google/sentencepiece</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://librivox.org 3 https://www.openslr.org/11/ 4 https://www.gutenberg.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/facebookresearch/wav2letter 6 https://github.com/facebookresearch/wav2letter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/pytorch/fairseq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://keithito.com/LJ-Speech-Dataset/ 9 https://github.com/mozilla/TTS</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxZX20qFQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fully differentiable beam search decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/collobert19a.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Word-level speech recognition with a dynamic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04323</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge distillation across ensembles of multilingual models for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nussbaum-Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11556</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno>II-1764-II-1772. JMLR.org</idno>
		<ptr target="https://dl.acm.org/citation.cfm?id=3044805.3045089" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition using lattice-free mmi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sameti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The capio 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition using multi-stream self-attention with dilated 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence speech recognition with time-depth separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2019-2460</idno>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2019.8682336</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenlm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Libri-Light</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08435</idno>
		<title level="m">Efficient neural audio synthesis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised training for end-to-end models via weak distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2837" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An end-to-end convolutional neural acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Who needs words? lexicon-free speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04479</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2019-1780</idno>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transformers with convolutional context for asr</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence O (1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady AN USSR</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2019-2680</idno>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully neural network based speech recognition on mobile and embedded devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10620" to="10630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6670" to="6674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07625</idno>
		<title level="m">The fastest open-source speech recognition system</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised DNN training with word selection for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3687" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Residual convolutional ctc networks for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07793</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transformer-based acoustic modeling for hybrid speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1812.06864</idno>
		<ptr target="https://arxiv.org/abs/1812.06864" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
