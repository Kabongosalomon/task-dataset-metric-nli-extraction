<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">year={2017}</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
							<email>wuancong@mail2.sysu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>author={Yu</roleName><forename type="first">Hong-Xing</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ancong</roleName><forename type="first">Wu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi}</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Collaborative Innovation Center of High Performance Computing</orgName>
								<orgName type="institution">NUDT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Guangdong Key Laboratory of Big Data Analysis and Processing</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the IEEE International Conference on Computer Vision. 2017. booktitle={Proceedings of the IEEE International Conference on Computer Vision}</title>
						<meeting>the IEEE International Conference on Computer Vision. 2017. booktitle={ the IEEE International Conference on Computer Vision}						</meeting>
						<imprint>
							<date type="published" when="2017">year={2017}</date>
						</imprint>
					</monogr>
					<note>Code is available at the project page: https://github.com/KovenYu/CAMEL For reference of this work, please cite:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While metric learning is important for Person reidentification (RE-ID), a significant problem in visual surveillance for cross-view pedestrian matching, existing metric models for RE-ID are mostly based on supervised learning that requires quantities of labeled samples in all pairs of camera views for training. However, this limits their scalabilities to realistic applications, in which a large amount of data over multiple disjoint camera views is available but not labelled. To overcome the problem, we propose unsupervised asymmetric metric learning for unsupervised RE-ID. Our model aims to learn an asymmetric metric, i.e., specific projection for each view, based on asymmetric clustering on cross-view person images. Our model finds a shared space where view-specific bias is alleviated and thus better matching performance can be achieved. Extensive experiments have been conducted on a baseline and five large-scale RE-ID datasets to demonstrate the effectiveness of the proposed model. Through the comparison, we show that our model works much more suitable for unsupervised RE-ID compared to classical unsupervised metric learning models. We also compare with existing unsupervised RE-ID methods, and our model outperforms them with notable margins. Specifically, we report the results on large-scale unlabelled RE-ID dataset, which is important but unfortunately less concerned in literatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head><p>Camera-1 Camera-2 Projected by U1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (RE-ID) is a challenging problem focusing on pedestrian matching and ranking across non-overlapping camera views. It remains an open problem although it has received considerable exploration recently, in consideration of its potential significance in security ap-plications, especially in the case of video surveillance. It has not been solved yet principally because of the dramatic intra-class variation and the high inter-class similarity. Existing attempts mainly focus on learning to extract robust and discriminative representations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>, and learning matching functions or metrics <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> in a supervised manner. Recently, deep learning has been adopted to RE-ID community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref> and has gained promising results.</p><p>However, supervised strategies are intrinsically limited due to the requirement of manually labeled cross-view training data, which is very expensive <ref type="bibr" target="#b30">[31]</ref>. In the context of RE-ID, the limitation is even pronounced because (1) manually labeling may not be reliable with a huge number of images to be checked across multiple camera views, and more importantly (2) the astronomical cost of time and money is prohibitive to label the overwhelming amount of data across disjoint camera views. Therefore, in reality supervised methods would be restricted when applied to a new scenario with a huge number of unlabeled data.</p><p>To directly make full use of the cheap and valuable unlabeled data, some existing efforts on exploring unsupervised strategies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12]</ref> have been reported, but they are still not very satisfactory. One of the main reasons is that without the help of labeled data, it is rather difficult to model the dramatic variances across camera views, such as the variances of illumination and occlusion conditions. Such variances lead to view-specific interference/bias which can be very disturbing in finding what is more distinguishable in matching people across views (see <ref type="figure">Figure 1</ref>). In particular, existing unsupervised models treat the samples from different views in the same manner, and thus the effects of view-specific bias could be overlooked.</p><p>In order to better address the problems caused by camera view changes in unsupervised RE-ID scenarios, we pro- <ref type="figure">Figure 1</ref>. Illustration of view-specific interference/bias and our idea. Images from different cameras suffer from view-specific interference, such as occlusions in Camera-1, dull illumination in Camera-2, and the change of viewpoints between them. These factors introduce bias in the original feature space, and therefore unsupervised re-identification is extremely challenging. Our model structures data by clustering and learns view-specific projections jointly, and thus finds a shared space where view-specific bias is alleviated and better performance can be achieved. (Best viewed in color) pose a novel unsupervised RE-ID model named Clusteringbased Asymmetric 1 MEtric Learning (CAMEL). The ideas behind are on the two following considerations. First, although conditions can vary among camera views, we assume that there should be some shared space where the data representations are less affected by view-specific bias. By projecting original data into the shared space, the distance between any pair of samples x i and x j is computed as:</p><formula xml:id="formula_0">d(xi, xj) = U T xi − U T xj 2 = (xi − xj) T M (xi − xj),<label>(1)</label></formula><p>where U is the transformation matrix and M = U U T . However, it can be hard for a universal transformation to implicitly model the view-specific feature distortion from different camera views, especially when we lack label information to guide it. This motivates us to explicitly model the view-specific bias. Inspired by the supervised asymmetric distance model <ref type="bibr" target="#b3">[4]</ref>, we propose to embed the asymmetric metric learning to our unsupervised RE-ID modelling, and thus modify the symmetric form in Eq. (1) to an asymmetric one:</p><formula xml:id="formula_1">d(x p i , x q j ) = U pT x p i − U qT x q j 2,<label>(2)</label></formula><p>where p and q are indices of camera views. An asymmetric metric is more acceptable for unsupervised RE-ID scenarios as it explicitly models the variances among views by treating each view differently. By such an explicit means, we are able to better alleviate the disturbances of view-specific bias.</p><p>The other consideration is that since we are not clear about how to separate similar persons in lack of labeled data, it is reasonable to pay more attention to better separating dissimilar ones. Such consideration motivates us to structure our data by clustering. Therefore, we develop asymmetric metric clustering that clusters cross-view person images. By clustering together with asymmetric modelling, the data can be better characterized in the shared space, contributing to better matching performance (see <ref type="figure">Figure 1</ref>).</p><p>In summary, the proposed CAMEL aims to learn viewspecific projection for each camera view by jointly learning the asymmetric metric and seeking optimal cluster separations. In this way, the data from different views is projected into a shared space where view-specific bias is aligned to an extent, and thus better performance of cross-view matching can be achieved.</p><p>So far in literatures, the unsupervised RE-ID models have only been evaluated on small datasets which contain only hundreds or a few thousands of images. However, in more realistic scenarios we need evaluations of unsupervised methods on much larger datasets, say, consisting of hundreds of thousands of samples, to validate their scalabilities. In our experiments, we have conducted extensive comparison on datasets with their scales ranging widely. In particular, we combined two existing RE-ID datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref> to obtain a larger one which contains over 230,000 samples. Experiments on this dataset (see <ref type="bibr">Sec. 4.4)</ref> show empirically that our model is more scalable to problems of larger scales, which is more realistic and more meaningful for unsupervised RE-ID models, while some existing unsupervised RE-ID models are not scalable due to the expensive cost in either storage or computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>At present, most existing RE-ID models are in a supervised manner. They are mainly based on learning distance metrics or subspace <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>, learning view-invariant and discriminative features <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>, and deep learning frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. However, all these models rely on substantial labeled training data, which is typically required to be pair-wise for each pair of camera views. Their performance depends highly on the quality and quantity of labeled training data. In contrast, our model does not require any labeled data and thus is free from prohibitively high cost of manually labeling and the risk of incorrect labeling.</p><p>To directly utilize unlabeled data for RE-ID, several unsupervised RE-ID models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed. All these models differ from ours in two aspects. On the one hand, these models do not explicitly exploit the information on view-specific bias, i.e., they treat feature transformation/quantization in every distinct camera view in the same manner when modelling. In contrast, our model tries to learn specific transformation for each camera view, aiming to find a shared space where view-specific interference can be alleviated and thus better performance can be achieved. On the other hand, as for the means to learn a metric or a transformation, existing unsupervised methods for RE-ID rarely consider clustering while we introduce an asymmetric metric clustering to characterize data in the learned space. While the methods proposed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> could also learn view-specific mappings, they are supervised methods and more importantly cannot be generalized to handle unsupervised RE-ID. Apart from our model, there have been some clusteringbased metric learning models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b24">25]</ref>. However, to our best knowledge, there is no such attempt in RE-ID community before. This is potentially because clustering is more susceptible to view-specific interference and thus data points from the same view are more inclined to be clustered together, instead of those of a specific person across views. Fortunately, by formulating asymmetric learning and further limiting the discrepancy between view-specific transforms, this problem can be alleviated in our model. Therefore, our model is essentially different from these models not only in formulation but also in that our model is able to better deal with cross-view matching problem by treating each view asymmetrically. We will discuss the differences between our model and the existing ones in detail in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Under a conventional RE-ID setting, suppose we have a surveillance camera network that consists of V camera views, from each of which we have collected N p (p = 1, · · · , V ) images and thus there are N = N 1 + · · · + N V images in total as training samples.</p><p>Let</p><formula xml:id="formula_2">X = [x 1 1 , · · · , x 1 N1 , · · · , x V 1 , · · · , x V N V ] ∈ R M ×N</formula><p>denote the training set, with each column x p i (i = 1, · · · , N p ; p = 1, · · · , V ) corresponding to an Mdimensional representation of the i-th image from the pth camera view. Our goal is to learn V mappings i.e., U 1 , · · · , U V , where U p ∈ R M ×T (p = 1, · · · , V ), corresponding to each camera view, and thus we can project the original representation x p i from the original space R M into a shared space R T in order to alleviate the view-specific interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modelling</head><p>Now we are looking for some transformations to map our data into a shared space where we can better separate the images of one person from those of different persons. Naturally, this goal can be achieved by narrowing intraclass discrepancy and meanwhile pulling the centers of all classes away from each other. In an unsupervised scenario, however, we have no labeled data to tell our model how it can exactly distinguish one person from another who has a confusingly similar appearance with him. Therefore, it is acceptable to relax the original idea: we focus on gathering similar person images together, and hence separating relatively dissimilar ones. Such goal can be modelled by minimizing an objective function like that of k-means clustering <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_3">min U Fintra = K k=1 i∈C k U T xi − c k 2 ,<label>(3)</label></formula><p>where K is the number of clusters, c k denotes the centroid of the k-th cluster and C k = {i|U T x i ∈ k-th cluster}. However, clustering results may be affected extremely by view-specific bias when applied in cross-view problems. In the context of RE-ID, the feature distortion could be view-sensitive due to view-specific interference like different lighting conditions and occlusions <ref type="bibr" target="#b3">[4]</ref>. Such interference might be disturbing or even dominating in searching the similar person images across views during clustering procedure. To address this cross-view problem, we learn specific projection for each view rather than a universal one to explicitly model the effect of view-specific interference and to alleviate it. Therefore, the idea can be further formulated by minimizing an objective function below:</p><formula xml:id="formula_4">min U 1 ,··· ,U V Fintra = K k=1 i∈C k U pT x p i − c k 2 s.t. U pT Σ p U p = I (p = 1, · · · , V ),<label>(4)</label></formula><p>where the notation is similar to Eq. (3), with p denotes the view index, Σ p = X p X pT /N p + αI and I represents the identity matrix which avoids singularity of the covariance matrix. The transformation U p that corresponds to each instance x p i is determined by the camera view which x p i comes from. The quasi-orthogonal constraints on U p ensure that the model will not simply give zero matrices. By combining the asymmetric metric learning, we actually realize an asymmetric metric clustering on RE-ID data across camera views. Intuitively, if we minimize this objective function directly, U p will largely depend on the data distribution from the p-th view. Now that there is specific bias on each view, any U p and U q could be arbitrarily different. This result is very natural, but large inconsistencies among the learned transformations are not what we exactly expect, because the transformations are with respect to person images from different views: they are inherently correlated and homogeneous. More critically, largely different projection basis pairs would fail to capture the discriminative nature of cross-view images, producing an even worse matching result.</p><p>Hence, to strike a balance between the ability to capture discriminative nature and the capability to alleviate viewspecific bias, we embed a cross-view consistency regularization term into our objective function. And then, in consideration of better tractability, we divide the intra-class</p><formula xml:id="formula_5">-1.5 -1 -0.5 0 0.5 1 1.5 -1 -0.5 0 0.5 1 View1, ID1 View2, ID1 View1, ID2 View2, ID2 (a) Original -1.5 -1 -0.5 0 0.5 1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 View1, ID1 View2, ID1 View1, ID2 View2, ID2 (b) Symmetric -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 -1 -0.5 0 0.5 1 View1, ID1 View2, ID1 View1, ID2 View2, ID2</formula><p>(c) Asymmetric <ref type="figure">Figure 2</ref>. Illustration of how symmetric and asymmetric metric clustering structure data using our method for the unsupervised RE-ID problem. The samples are from the SYSU dataset <ref type="bibr" target="#b3">[4]</ref>. We performed PCA for visualization. One shape (triangle or circle) stands for samples from one view, while one color indicates samples of one person. term by its scale N , so that the regulating parameter would not be sensitive to the number of training samples. Thus, our optimization task becomes</p><formula xml:id="formula_6">min U 1 ,··· ,U V F obj = 1 N Fintra + λFconsistency = 1 N K k=1 i∈C k U pT x p i − c k 2 + λ p =q U p − U q 2 F s.t. U pT Σ p U p = I (p = 1, · · · , V ),<label>(5)</label></formula><p>where λ is the cross-view regularizer and · F denotes the Frobenius norm of a matrix. We call the above model the Clustering-based Asymmetric MEtric Learning (CAMEL).</p><p>To illustrate the differences between symmetric and asymmetric metric clustering in structuring data in the RE-ID problem, we further show the data distributions in <ref type="figure">Figure  2</ref>. We can observe from <ref type="figure">Figure 2</ref> that the view-specific bias is obvious in the original space: triangles in the upper left and circles in the lower right. In the common space learned by symmetric metric clustering, the bias is still obvious. In contrast, in the shared space learned by asymmetric metric clustering, the bias is alleviated and thus the data is better characterized according to the identities of the persons, i.e., samples of one person (one color) gather together into a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>For convenience, we denote y i = U pT x p i . Then we have Y ∈ R T ×N , where each column y i corresponds to the projected new representation of that from X. For optimization, we rewrite our objective function in a more compact form. The first term can be rewritten as follow <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_7">1 N K k=1 i∈C k yi − c k 2 = 1 N [Tr(Y T Y ) − Tr(H T Y T Y H)],<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">H = h1, ..., hK , h T k h l = 0 k = l 1 k = l<label>(7)</label></formula><p>h k = 0, · · · , 0, 1, · · · , 1, 0, · · · , 0, 1, · · · T / √ n k</p><p>is an indicator vector with the i-th entry corresponding to the instance y i , indicating that y i is in the k-th cluster if the corresponding entry does not equal zero. Then we construct</p><formula xml:id="formula_10">X =      x 1 1 · · · x 1 N 1 0 · · · 0 · · · 0 0 · · · 0 x 2 1 · · · x 2 N 2 · · · 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 · · · 0 0 · · · 0 · · · x V N V      (9) U = U 1T , · · · , U V T T ,<label>(10)</label></formula><p>so that</p><formula xml:id="formula_11">Y = U T X,<label>(11)</label></formula><p>and thus Eq. (6) becomes</p><formula xml:id="formula_12">1 N K k=1 i∈C k yi − c k 2 = 1 N Tr( X T U U T X) − 1 N Tr(H T X T U U T XH).<label>(12)</label></formula><p>As for the second term, we can also rewrite it as follow:</p><formula xml:id="formula_13">λ p =q U p − U q 2 F = λTr( U T D U ),<label>(13)</label></formula><formula xml:id="formula_14">where D =      (V − 1)I −I −I · · · −I −I (V − 1)I −I · · · −I . . . . . . . . . . . . . . . −I −I −I · · · (V − 1)I      .<label>(14)</label></formula><p>Then, it is reasonable to relax the constraints U pT Σ p U p = I (p = 1, · · · , V )</p><formula xml:id="formula_15">to V p=1 U pT Σ p U p = U T Σ U = V I,<label>(15)</label></formula><p>where Σ = diag(Σ 1 , · · · , Σ V ) because what we expect is to prevent each U p from shrinking to a zero matrix. The relaxed version of constraints is able to satisfy our need, and it bypasses trivial computations. By now we can rewrite our optimization task as follow:</p><formula xml:id="formula_17">min U F obj = 1 N Tr( X T U U T X) + λTr( U T D U ) − 1 N Tr(H T X T U U T XH) s.t. U T Σ U = V I.<label>(17)</label></formula><p>It is easy to realize from Eq. (5) that our objective function is highly non-linear and non-convex. Fortunately, in the form of Eq. (17) we can find that once H is fixed, Lagrange's method can be applied to our optimization task. And again from Eq. (5), it is exactly the objective of kmeans clustering once U is fixed <ref type="bibr" target="#b9">[10]</ref>. Thus, we can adopt an alternating algorithm to solve the optimization problem. Fix H and optimize U . Now we see how we optimize U . After fixing H and applying the method of Lagrange multiplier, our optimization task <ref type="formula" target="#formula_0">(17)</ref> is transformed into an eigen-decomposition problem as follow:</p><formula xml:id="formula_18">Gu = γu,<label>(18)</label></formula><p>where γ is the Lagrange multiplier (and also is the eigenvalue here) and</p><formula xml:id="formula_19">G = Σ −1 (λD + 1 N X X T − 1 N XHH T X T ).<label>(19)</label></formula><p>Then, U can be obtained by solving this eigendecomposition problem. Fix U and optimize H. As for the optimization of H, we can simply fix U and conduct k-means clustering in the learned space. Each column of H, h k , is thus constructed according to the clustering result. Based on the analysis above, we can now propose the main algorithm of CAMEL in Algorithm 1. We set maximum iteration to 100. After obtaining U , we decompose it back into {U 1 , · · · , U V }. The algorithm is guaranteed to convergence, as given in the following proposition: Proposition 1. In Algorithm 1, F obj is guaranteed to convergence.</p><p>Proof. In each iteration, when U is fixed, if H is the local minimizer, k-means remains H unchanged, otherwise it seeks the local minimizer. When H is fixed, U has a closed-form solution which is the global minimizer. Therefore, the F obj decreases step by step. As F obj ≥ 0 has a lower bound 0, it is guaranteed to convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Since unsupervised models are more meaningful when the scale of problem is larger, our experiments were conducted on relatively big datasets except VIPeR <ref type="bibr" target="#b8">[9]</ref> which is small but widely used. Various degrees of view-specific bias can be observed in all these datasets (see <ref type="figure">Figure 3</ref>). The VIPeR dataset contains 632 identities, with two images captured from two camera views of each identity. The CUHK01 dataset <ref type="bibr" target="#b15">[16]</ref> contains 3,884 images of 971 identities captured from two disjoint views. There are two images of every identity from each view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: CAMEL</head><p>Input : X, K, = 10 −8 Output: U 1 Conduct k-means clustering with respect to each column of X to initialize H according to Eq. <ref type="formula" target="#formula_8">(7)</ref> and <ref type="formula" target="#formula_9">(8)</ref>. <ref type="bibr" target="#b1">2</ref> Fix H and solve the eigen-decomposition problem described by Eq. <ref type="formula" target="#formula_0">(18)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Settings</head><p>Experimental protocols: A widely adopted protocol was followed on VIPeR in our experiments <ref type="bibr" target="#b18">[19]</ref>, i.e., randomly dividing the 632 pairs of images into two halves, one of which was used as training set and the other as testing set. This procedure was repeated 10 times to offer average performance. Only single-shot experiments were conducted. The experimental protocol for CUHK01 was the same as that in <ref type="bibr" target="#b18">[19]</ref>. We randomly selected 485 persons as training set and the other 486 ones as testing set. The evaluating procedure was repeated 10 times. Both multi-shot and single-shot settings were conducted.</p><p>The CUHK03 dataset was provided together with its recommended evaluating protocol <ref type="bibr" target="#b16">[17]</ref>. We followed the provided protocol, where images of 1,160 persons were chosen as training set, images of another 100 persons as validation set and the remainders as testing set. This procedure was repeated 20 times. In our experiments, detected samples were adopted since they are closer to real-world settings. Both multi-shot and single-shot experiments were conducted.</p><p>As for the SYSU dataset, we randomly picked 251 pedestrians' images as training set and the others as testing set. In the testing stage, we basically followed the protocol as in <ref type="bibr" target="#b3">[4]</ref>. That is, we randomly chose one and three images of each pedestrian as gallery for single-shot and multi-shot experiments, respectively. We repeated the testing procedure by 10 times.</p><p>Market is somewhat different from others. The evaluation protocol was also provided along with the data <ref type="bibr" target="#b36">[37]</ref>. Since the images of one person came from at most six views, single-shot experiments were not suitable. Instead, multi-shot experiments were conducted and both cumulative matching characteristic (CMC) and mean average precision (MAP) were adopted for evaluation <ref type="bibr" target="#b36">[37]</ref>. The protocol of ExMarket was identical to that of Market since the identities were completely the same as we mentioned above. Data representation: In our experiments we used the deeplearning-based JSTL feature proposed in <ref type="bibr" target="#b31">[32]</ref>. We implemented it using the 56-layer ResNet <ref type="bibr" target="#b10">[11]</ref>, which produced 64-D features. The original JSTL was adopted to our implementation to extract features on SYSU, Market and ExMarket. Note that the training set of the original JSTL contained VIPeR, CUHK01 and CUHK03, violating the unsupervised setting. So we trained a new JSTL model without VIPeR in its training set to extract features on VIPeR. The similar procedures were done for CUHK01 and CUHK03. Parameters: We set λ, the cross-view consistency regularizer, to 0.01. We also evaluated the situation when λ goes to infinite, i.e., the symmetric version of our model in Sec. 4.4, to show how important the asymmetric modelling is.</p><p>Regarding the parameter T which is the feature dimension after the transformation learned by CAMEL, we set T equal to original feature dimension i.e., 64, for simplicity. In our experiments, we found that CAMEL can align data distributions across camera views even without performing any further dimension reduction. This may be due to the fact that, unlike conventional subspace learning models, the transformations learned by CAMEL are view-specific for different camera views and always non-orthogonal. Hence, the learned view-specific transformations can already reduce the discrepancy between the data distributions of different camera views.</p><p>As for K, we found that our model was not sensitive to K when N K and K was not too small (see Sec. 4.4), so we set K = 500. These parameters were fixed for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison</head><p>Unsupervised models are more significant when applied on larger datasets. In order to make comprehensive and fair comparisons, in this section we compare CAMEL with the most comparable unsupervised models on six datasets with their scale orders varying from hundreds to hundreds of thousands. We show the comparative results measured by the rank-1 accuracies of CMC and MAP (%) in <ref type="table">Table 2</ref>. Comparison to Related Unsupervised RE-ID Models. In this subsection we compare CAMEL with the sparse dictionary learning model (denoted as Dic) <ref type="bibr" target="#b12">[13]</ref>, sparse representation learning model ISR <ref type="bibr" target="#b20">[21]</ref>, kernel subspace learning model RKSL <ref type="bibr" target="#b29">[30]</ref> and sparse auto-encoder (SAE) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref>. We tried several sets of parameters for them, and report the best ones. We also adopt the Euclidean distance which is adopted in the original JSTL paper <ref type="bibr" target="#b31">[32]</ref> as a baseline (denoted as JSTL).</p><p>From <ref type="table">Table 2</ref> we can observe that CAMEL outperforms other models on all the datasets on both settings. In addition, we can further see from <ref type="figure">Figure 4</ref> that CAMEL outperforms other models at any rank. One of the main reasons is that the view-specific interference is noticeable in these datasets. For example, we can see in <ref type="figure">Figure 3</ref>(b) that on CUHK01, the changes of illumination are extremely severe and even human beings may have difficulties in recognizing the identities in those images across views. This impedes other symmetric models from achieving higher accuracies, because they potentially hold an assumption that the invari-  <ref type="table">Table 2</ref>. Comparative results of unsupervised models on the six datasets, measured by rank-1 accuracies and MAP (%). "-" means prohibitive time consumption due to time complexities of the models. "SS" represents single-shot setting and "MS" represents multi-shot setting. For Market and ExMarket, MAP is also provided in the parentheses due to the requirement in the protocol <ref type="bibr" target="#b36">[37]</ref>. Such a format is also applied in the following tables.</p><p>ant and discriminative information can be retained and exploited through a universal transformation for all views. But CAMEL relaxes this assumption by learning an asymmetric metric and then can outperform other models significantly. In Sec. 4.4 we will see the performance of CAMEL would drop much when it degrades to a symmetric model. Comparison to Clustering-based Metric Learning Models. In this subsection we compare CAMEL with a typical model AML <ref type="bibr" target="#b33">[34]</ref> and a recently proposed model UsNCA <ref type="bibr" target="#b24">[25]</ref>. We can see from <ref type="figure">Fig. 4</ref> and <ref type="table">Table 2</ref> that compared to them, CAMEL achieves noticeable improvements on all the six datasets. One of the major reasons is that they do not consider the view-specific bias which can be very disturbing in clustering, making them unsuitable for RE-ID problem. In comparison, CAMEL alleviates such disturbances by asymmetrically modelling. This factor contributes to the much better performance of CAMEL.</p><p>Comparison to the State-of-the-Art. In the last subsections, we compared with existing unsupervised RE-ID methods using the same features. In this part, we also compare with the results reported in literatures. Note that most existing unsupervised RE-ID methods have not been evaluated on large datasets like CUHK03, SYSU, or Market, so <ref type="table">Table 3</ref> only reports the comparative results on VIPeR and CUHK01. We additionally compared existing unsupervised RE-ID models, including the hand-craft-feature-based SDALF <ref type="bibr" target="#b7">[8]</ref> and CPS <ref type="bibr" target="#b6">[7]</ref>, the transfer-learning-based UDML <ref type="bibr" target="#b23">[24]</ref>, graph-learning-based model (denoted as GL) <ref type="bibr" target="#b11">[12]</ref>, and local-salience-learning-based GTS <ref type="bibr" target="#b28">[29]</ref> and SDC <ref type="bibr" target="#b34">[35]</ref>. We can observe from <ref type="table">Table 3</ref> that our model CAMEL can outperform the state-of-the-art by large margins on CUHK01.</p><p>Comparison to Supervised Models. Finally, in order to see how well CAMEL can approximate the performance of supervised RE-ID, we additionally compare CAMEL with its supervised version (denoted as CAMEL s ) which is easily    <ref type="table">Table 3</ref>. Results compared to the state-of-the-art reported in literatures, measured by rank-1 accuracies (%). "-" means no reported result.</p><p>derived by substituting the clustering results by true labels, and three standard supervised models, including the widely used KISSME <ref type="bibr" target="#b13">[14]</ref>, XQDA <ref type="bibr" target="#b18">[19]</ref>, the asymmetric distance model CVDCA <ref type="bibr" target="#b3">[4]</ref>. The results are shown in  <ref type="table" target="#tab_3">Table 4</ref>. Results compared to supervised models using the same JSTL features.</p><p>far below the standard supervised RE-ID models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Evaluations</head><p>The Role of Asymmetric Modeling. We show what is going to happen if CAMEL degrades to a common symmetric model in <ref type="table">Table 5</ref>. Apparently, without asymmetrically modelling each camera view, our model would be worsen largely, indicating that the asymmetric modeling for clustering is rather important for addressing the cross-view matching problem in RE-ID as well as in our model.</p><p>Sensitivity to the Number of Clustering Centroids. We take CUHK01, Market and ExMarket datasets as examples of different scales (see <ref type="table" target="#tab_0">Table 1</ref>) for this evaluation. <ref type="table">Table 6</ref> shows how the performance varies with different numbers of clustering centroids, K. It is obvious that the performance only fluctuates mildly when N K and K is not too small. Therefore CAMEL is not very sensitive to K especially when applied to large-scale problems. To further explore the reason behind, we show in <ref type="table">Table 7</ref> the rate of clusters which contain more than one persons, in the initial stage and convergence stage in Algorithm 1. We can see that (1) in spite of that K is varying, there is always a number of clusters containing more than one persons in both the initial stage and convergence stage. This indicates that our model works without the requirement of perfect clustering results. And (2), although the number is various, in the convergence stage the number is consistently decreased compared to initialization stage. This shows that the cluster results are improved consistently. These two observations suggests that the clustering should be a mean to learn the asymmetric metric, rather than an ultimate objective.</p><p>Adaptation Ability to Different Features. At last, we show that CAMEL can be effective not only when adopting deep-learning-based JSTL features. We additionally adopted the hand-crafted LOMO feature proposed in <ref type="bibr" target="#b18">[19]</ref>. We performed PCA to produce 512-D LOMO features, and the results are shown in <ref type="table">Table 8</ref>. Among all the models, the results of Dic and ISR are the most comparable (Dic and ISR take all second places). So for clarity, we only compare CAMEL with them and L 2 distance as baseline. From the table we can see that CAMEL can outperform them.   <ref type="table">Table 8</ref>. Results using 512-D LOMO features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have shown that metric learning can be effective for unsupervised RE-ID by proposing clusteringbased asymmetric metric learning called CAMEL. CAMEL learns view-specific projections to deal with view-specific interference, and this is based on existing clustering (e.g., the k-means model demonstrated in this work) on RE-ID unlabelled data, resulting in an asymmetric metric clustering. Extensive experiments show that our model can outperform existing ones in general, especially on large-scale unlabelled RE-ID datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Original distribution (b) distribution in the common space learned by symmetric metric clustering (c) distribution in the shared space learned by asymmetric metric clustering. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 4. CMC curves. For CUHK01, CUHK03 and SYSU, we take the results under single-shot setting as examples. Similar patterns can be observed on multi-shot setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>while decrement of F obj &gt; &amp; maximum iteration unreached do • Construct Y according to Eq. (11). • Fix U and conduct k-means clustering with respect to each column of Y to update H according to Eq. (7) and (8). • Fix H and solve the eigen-decomposition problem described by Eq. (18) and (19) to update U . Overview of dataset scales. "#" means "the number of". tains 20,715 tracklets of 1,261 pedestrians. All the identities from MARS are of a subset of those from Market. We then took 20% frames (each one in every five successive frames) from the tracklets and combined them with Market to obtain an extended version of Market (ExMarket). The imbalance between the numbers of samples from the 1,261 persons and other 240 persons makes this dataset more challenging and realistic. There are 236,696 images in ExMarket in total, and 112,351 images of them are of training set. A brief overview of the dataset scales can be found in Table 1.</figDesc><table><row><cell>and</cell></row></table><note>3istic, we further combined the MARS dataset [36] with Market. MARS is a video-based RE-ID dataset which con-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>4. CMC curves. For CUHK01, CUHK03 and SYSU, we take the results under single-shot setting as examples. Similar patterns can be observed on multi-shot setting.</figDesc><table><row><cell>Model</cell><cell>SDALF</cell><cell>CPS</cell><cell>UDML</cell><cell>GL</cell><cell>GTS</cell><cell>SDC</cell><cell>CAMEL</cell></row><row><cell></cell><cell>[8]</cell><cell>[7]</cell><cell>[24]</cell><cell>[12]</cell><cell>[29]</cell><cell>[35]</cell><cell></cell></row><row><cell>VIPeR</cell><cell>19.9</cell><cell>22.0</cell><cell>31.5</cell><cell>33.5</cell><cell>25.2</cell><cell>25.8</cell><cell>30.9</cell></row><row><cell>CUHK01</cell><cell>9.9</cell><cell>-</cell><cell>27.1</cell><cell>41.0</cell><cell>-</cell><cell>26.6</cell><cell>57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>We can see that CAMEL s outperforms CAMEL by various degrees, indicating that label information can further improve CAMEL's performance. Also fromTable 4, we notice that CAMEL can be comparable to other standard supervised models on some datasets like CUHK01, and even outperform some of them. It is probably because the used JSTL model had not been fine-tuned on the target datasets: this was for a fair comparison with unsupervised models which work on completely unlabelled training data. Nevertheless, this suggests that the performance of CAMEL may not be ] 28.4 53.0/57.1 37.8/45.4 24.7/31.8 51.1(24.5) 48.0(18.3) XQDA<ref type="bibr" target="#b18">[19]</ref> 28.9 54.3/58.<ref type="bibr" target="#b1">2</ref> 36.7/43.7 25.2/31.7 50.8(24.4) 47.4(18.1) CVDCA [4] 37.6 57.1/60.9 37.0/44.6 31.1/38.9 52.6(25.3) 51.5(22.6) CAMELs 33.7 58.5/62.7 45.1/53.5 31.6/37.6 55.0(27.1) 56.1(24.1)</figDesc><table><row><cell>Dataset</cell><cell cols="3">VIPeR CUHK01 CUHK03</cell><cell>SYSU</cell><cell>Market</cell><cell>ExMarket</cell></row><row><cell>Setting</cell><cell>SS</cell><cell>SS/MS</cell><cell>SS/MS</cell><cell>SS/MS</cell><cell>MS</cell><cell>MS</cell></row><row><cell>KISSME [14CAMEL</cell><cell cols="6">30.9 57.3/61.9 31.9/39.4 30.8/36.8 54.5(26.3) 55.9(23.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>52.5/54.9 29.8/37.5 25.4/30.9 47.6(21.5) 48.7(20.0) CAMEL 30.9 57.3/61.9 31.9/39.4 30.8/36.8 54.5(26.3) 55.9(23.9)</figDesc><table><row><cell cols="4">Dataset VIPeR CUHK01 CUHK03</cell><cell>SYSU</cell><cell>Market</cell><cell>ExMarket</cell></row><row><cell>Setting</cell><cell>SS</cell><cell>SS/MS</cell><cell>SS/MS</cell><cell>SS/MS</cell><cell>MS</cell><cell>MS</cell></row><row><cell>CMEL</cell><cell>27.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .Table 7 .</head><label>567</label><figDesc>Performances of CAMEL compared to its symmetric version, denoted as CMEL. Performances of CAMEL when the number of clusters, K, varies. Measured by single-shot rank-1 accuracies (%) for CUHK01 and multi-shot for Market and ExMarket. Rate of clusters containing similar persons on CUHK01. Similar trend can be observed on other datasets. /23.6 8.6/13.4 14.2/24.4 32.8(12.2) 33.8(12.2) ISR [21] 20.8 22.2/27.1 16.7/20.7 11.7/21.6 29.7(11.0) -L2 11.6 14.0/18.6 7.6/11.6 10.8/18.9 27.4(8.3) 27.7(8.0) CAMEL 26.4 30.0/36.2 17.3/23.4 23.6/35.6 41.4(14.1) 42.2(13.7)</figDesc><table><row><cell cols="2">K</cell><cell>250</cell><cell>500</cell><cell>750</cell><cell></cell><cell>1000</cell><cell>1250</cell></row><row><cell cols="2">CUHK01</cell><cell>56.59</cell><cell>57.35</cell><cell cols="2">56.26</cell><cell>55.12</cell><cell>52.75</cell></row><row><cell cols="2">Market</cell><cell>54.48</cell><cell>54.45</cell><cell cols="2">54.54</cell><cell>54.48</cell><cell>54.48</cell></row><row><cell cols="2">ExMarket</cell><cell>55.49</cell><cell>55.87</cell><cell cols="2">56.17</cell><cell>55.93</cell><cell>55.67</cell></row><row><cell>K</cell><cell></cell><cell>250</cell><cell>500</cell><cell></cell><cell></cell><cell>750</cell><cell>1000</cell><cell>1250</cell></row><row><cell cols="2">Initial Stage</cell><cell>77.6%</cell><cell cols="2">57.0%</cell><cell cols="2">26.3%</cell><cell>11.6%</cell><cell>6.0%</cell></row><row><cell cols="2">Convergence Stage</cell><cell>55.8%</cell><cell cols="2">34.3%</cell><cell cols="2">18.2%</cell><cell>7.2%</cell><cell>4.8%</cell></row><row><cell>Dataset</cell><cell cols="6">VIPeR CUHK01 CUHK03 SYSU</cell><cell>Market</cell><cell>ExMarket</cell></row><row><cell>Setting</cell><cell>SS</cell><cell>SS/MS</cell><cell cols="2">SS/MS</cell><cell cols="2">SS/MS</cell><cell>MS</cell><cell>MS</cell></row><row><cell>Dic [13]</cell><cell>15.8</cell><cell>19.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"Asymmetric" means specific transformations for each camera view.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Demo code for the model and the ExMarket dataset can be found on https://github.com/KovenYu/CAMEL.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported partially by the National Key Research and Development Program of China (2016YFB1001002), NSFC(61522115, 61472456, 61573387, 61661130157, U1611461), the Royal Society Newton Advanced Fellowship (NA150459), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reference-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kafai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Person reidentification with reference descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kafai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An asymmetric distance model for cross-view feature mapping in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>John Wiley &amp; Sons Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised\ ell 1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dictionary learning with iterative laplacian regularisation for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching people across camera views using kernel canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDSC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Covariance descriptor based on bio-inspired features for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised neighborhood component analysis for clustering. Neurocomputing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification with correspondence structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cross-scenario transfer person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive distance metric learning for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Person re-identification by saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
