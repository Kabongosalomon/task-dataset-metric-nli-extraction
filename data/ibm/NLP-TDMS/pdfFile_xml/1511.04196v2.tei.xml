<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
							<email>zhiweid@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
							<email>hexiangh@sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relations between image entities are an important facet of higher-level visual understanding. Building relationships, such as the spatial distance between people in a scene, their relative motions, or concurrent actions can be used to drive recognition of higher-level activities. Models for interpreting such scenes require the need to accurately interpret image cues, determine relevant relations between entities, and infer the properties of these relations.</p><p>In this paper we present a general-purpose method for this task, illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The method builds upon deep networks for image analysis, endowing these networks with the ability to reason over structures and relationships. This is accomplished by building higher-level recurrent networks that equip the model with the ability to perform inference over lower-level network outputs, including learning structures that are effective for higher-level tasks.</p><p>We ground the work by developing specific models for group activity analysis. Group activity analysis involves reasoning over individual people in a scene and considering their relations. Multiple people in a scene could either be performing the same action at the same time, or have varied actions and interactions that compose a collective activity. Effective models need to jointly consider the rich relations between components of visual appearance. Standard approaches to this problem utilize graphical models to encode spatial relations and interactions. Recent work in this vein includes Choi et al. <ref type="bibr" target="#b7">[8]</ref>, who discover subgroups of interacting people. Lan et al. <ref type="bibr" target="#b25">[26]</ref> proposed a hierarchical graphical model that considers the interactions on the social role level. Hajimirsadeghi and Mori <ref type="bibr" target="#b15">[16]</ref> proposed a gradient boosting training method. Amer et al. <ref type="bibr" target="#b0">[1]</ref> adopted a HiRF model to perform recognition and detection simultaneously.</p><p>On another track, deep learning has proven successful in many computer vision applications, such as image clas-sification, object recognition, and action recognition. On the image side, seminal work by Krizhevsky et al. <ref type="bibr" target="#b23">[24]</ref> demonstrated the effectiveness of deep networks for object recognition; recent state of the art methods include GoogLeNet <ref type="bibr" target="#b33">[34]</ref>. On the video side, Simonyan and Zisserman <ref type="bibr" target="#b32">[33]</ref> proposed a two-stream convnet pipeline to apply deep learning to video analysis. Karpathy et al. <ref type="bibr" target="#b19">[20]</ref> adopted various fusion techniques in convolutional neural networks to consider temporal information in video sequences. These methods have demonstrated the power of deep networks for classification tasks.</p><p>However, these models are trained to produce a flat classification output, categorizing an image/video according to the existence of a set of object/action labels. For highly compositional tasks such as group activity recognition, models reasoning over structures can bring benefits, allowing the classification of higher-level concepts built from recognition over lower-level entities.</p><p>The main contribution of this paper centers on developing a model that bridges from low-level classifications to higher-level compositions. We contribute an end-to-end trainable deep network that (1) classifies low-level image inputs according to their content, (2) refines these classifications by passing messages between outputs, (3) performs structure learning via gating functions that determine which outputs to connect, and (4) results in effective classification of high-level concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>This paper contributes a general-purpose deep learning inference machine and demonstrates its effectiveness for group activity recognition. In this section we review relevant work on modeling structures in deep learning and specific models deployed for group activity recognition.</p><p>Deep Learning with Structures: Recently, there have been several interesting approaches to address the problem of combining graphical models and deep neural networks, primarily in the context of semantic image segmentation. Chen et al. <ref type="bibr" target="#b5">[6]</ref> proposed DeepLab, that feeds coarse responses at the final layer of a deep neural network to the CRF model proposed by Krähenbühl and Koltun <ref type="bibr" target="#b22">[23]</ref>. Zheng et al. <ref type="bibr" target="#b37">[38]</ref> proposed a CRF-RNN that trains the same model in an end-to-end fashion by transforming the approximate inference method <ref type="bibr" target="#b22">[23]</ref> into a Recurrent Neural Network (RNN). Schwing and Urtasun <ref type="bibr" target="#b30">[31]</ref> proposed an iterative procedure for end-to-end training of the CRF model. We expand on this line of work by relaxing the assumptions that the underlying graphical model i) is fully connected and/or ii) has Gaussian kernels in the pairwise potential functions. Similarly, Chen et al. <ref type="bibr" target="#b6">[7]</ref> extended to general MRFs using approximate inference. We use different inference techniques and enable structure learning.</p><p>Another line of work aims at modeling class relations in a graphical model that could be trained with a deep neural network. Deng et al. <ref type="bibr" target="#b11">[12]</ref> proposed Hierarchy and Exclusion (HEX) graphs for modeling inclusion and exclusion relations between object classes and showed how these graphs could be used for computing HEX-based marginalized distributions of labels on top of a deep neural network. Ding et al. <ref type="bibr" target="#b13">[14]</ref> extended HEX graphs to probabilistic HEX graphs modeled by Ising models and showed how standard Loopy Belief Propagation (LBP) can be used in the inference. Both works rely on a special-purpose graphical model that represents particular relations between binary variables with a pre-designed structure.</p><p>In the context of other structured problems, Bottou et al. <ref type="bibr" target="#b4">[5]</ref> proposed Graph Transformer Networks to jointly optimize subtasks. In this work, it was assumed that exact inference can be performed during a forward-backward pass. Ross et al. <ref type="bibr" target="#b28">[29]</ref> phrased structured prediction as a series of message passing steps. Tompson et al. <ref type="bibr" target="#b34">[35]</ref> proposed a feedforward neural network that mimics a single iteration of the message passing algorithm for a markov random field for the task of human body pose recognition. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> incorporate structured prediction as a loss layer in a neural network. Deng et al. <ref type="bibr" target="#b12">[13]</ref> conduct message passing to do inference over a fixed structure for group activity recognition. In contrast to these approaches, we infer structure via a gated network, allowing the model to determine the appropriate connections to use for inference.</p><p>Group Activity Recognition: Group activity recognition is typically modeled as a structured prediction problem that considers both individual actions and interactions with other people in a scene. Many previous work have used various forms of graphical models to address this problem: hierarchical graphical models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref>, AND-OR graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> and dynamic Bayesian networks <ref type="bibr" target="#b38">[39]</ref> are among the popular models. Lan et al. <ref type="bibr" target="#b27">[28]</ref> and Amer et al. <ref type="bibr" target="#b1">[2]</ref> have shown the effectiveness of adaptive structures to the group activity recognition problem. Modeled by latent structure <ref type="bibr" target="#b27">[28]</ref> or grouping nodes <ref type="bibr" target="#b1">[2]</ref>, an adaptive structure can adjust its structure to the most discriminative interactions in a scene. Khamis et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> utilize tracklevel and person-level features to determine group activity. Shu et al. <ref type="bibr" target="#b31">[32]</ref> reason about groups, roles, and events on top of noisy tracklets with a spatio-temporal AND-OR graph model. Kwak et al. <ref type="bibr" target="#b24">[25]</ref> reason over temporal logic primitives in a quadratic programming formulation. These models are trained in a sophisticated framework using shallow features and cannot easily be adopted in a deep learning framework. In this work, we propose a general framework for integrating graphical models into a deep neural network that is capable of adapting their structure in an instancebased approach.   <ref type="figure">Figure 2</ref>. The pipeline of inference in an RNN. We first use the unary scores to initialize the messages. In every iteration, new messages are computed using related message units, unary scores (x), and output predictions from the previous timestep (c (t−1) ). Note that for each timestep, a prediction layer outputs predictions (only illustrated in last layer), and in training receives loss as in a standard RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structure Inference Machine</head><p>Group activity recognition requires reasoning about structures. Interpreting an image of a scene of people involves determining what each individual person is doing and reasoning about their relations. These tasks are both challenging due to ambiguity in image features and uncertainty in determining relations between people. The ability to infer structures over the people in a scene is helpful for suppressing noise in the form of inaccurate human detections, mistaken low-level action recognition results, and spurious people not involved in a particular group activity.</p><p>Beyond group activity recognition, many other visual recognition tasks benefit from similar lines of reasoning. Detecting and classifying individual component elements can be improved by considering structured relations among them.</p><p>The question we address in this paper is how to model such structured relations. We take an approach building upon neural networks. Deep learning-based methods have benefits in highly effective low-level action recognition, and we want to utilize this effectiveness within an end-to-end trainable model for higher-level reasoning.</p><p>Two components are used to cast this problem as a neural network formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Recurrent neural networks for message passing.</head><p>Consider an individual person within an image of a scene. Ambiguity in inferring the action of the individual person is a fundamental problem. As per standard arguments around context <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>, using the actions of other people in the scene can help to disambiguate the action of this individual. We accomplish this by a recurrent neural network that aggregates cues about the actions of other people in a scene by repeatedly passing messages that refine estimates of an individual person's action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Gating functions to learn structures. Deciding who is interacting with whom in a group activity is an important part of inference. Sub-groups of people can be engaged in different activities <ref type="bibr" target="#b7">[8]</ref>; individuals can be outliers compared to the group activity <ref type="bibr" target="#b27">[28]</ref>. Reasoning over structures that determine connections between people in a scene can bring many benefits: which people are relevant to detecting the presence of an overarching group activity, which people provide context for which others. Within a neural network structure, we accomplish this by introducing trainable gating functions that can turn on and off connections between individual people in the scene. <ref type="figure">Fig. 2</ref> summarizes our structure inference machine. The following sections present its details. First, we present the use of recurrent neural networks as a tool for inference in a group activity model in Sec. 4. The use of gating functions to learn structure is presented in Sec. 5. By un-tying weights in these networks, we can relax assumptions regarding the message passing, leading to a general structure inference machine presented in Sec. 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Group Activity Recognition with an RNN</head><p>We build our model on top of a set of person detections in an image. The model includes the actions for these individual people as well as the group activity for the whole image. Given a set of M detected persons in a scene, a classifier (using a CNN) is used to provide visual classification {x i } M i=1 of each person's action based on an image window cropped at the person detection. Each x i is a probability distribution over individual action for person i. In addition, a classifier that operates on the entire image can be used to directly estimate the group activity in the scene. We denote by x s the group activity classification obtained from this whole-image classifier; x s is a distribution over possible scene-level group activities. A graphical model is built over these individual actions and the group activity, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. We use this graphical model to motivate our framework, and address structure learning in a graphical model with varied potential functions on top of a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Recurrent Networks for Refining Action/Activity Classification</head><p>On top of the individual classifications, we build a neural network structure which considers relations among entities to refine these classifications. This network structure is a recurrent neural network (RNN). The RNN passes information amongst nodes representing individual person classifications and the scene classification in order to facilitate contextual refinement of classification decisions.</p><p>Given the graph structure (i.e. probabilistic graphical model in <ref type="figure" target="#fig_3">Fig. 3</ref>), the RNN structure is built to model the connections between nodes. Distributions over values for each node i can be determined by a combination of classification results x i of local observations, along with information passed along the graph. We denote by m information coming from related nodes along the graph.</p><p>In group activity recognition, this can model contextual relations between the actions of people in the scene. In every iteration of refinement, the values of the contextual information m will be updated, while the inputs corresponding to visual observations of individual actions or scene label remain fixed.</p><p>We represent this iterative updating process as a recurrent neural network model. In general, we take as input the local observations for all nodes x = x s ∪ {x i : i = 1, . . . , M }. We output a set of refined classification scores 1 c (t) at each timestep t. These refined estimates are based on iteratively computed messages m (t) that pass information <ref type="bibr" target="#b0">1</ref> These classification scores may in general be over different quantities from, or a subset of, the inputs. between the nodes:</p><formula xml:id="formula_0">m (t) = f (W mm m (t−1) + W xm x + W cm c (t−1) + b m ) (1) c (t) = f (W mc m (t) + W xc x + b c )<label>(2)</label></formula><p>In this and all subsequent equations, variables W (·) and b (·) refer to the neural network parameters which are to be learned. Here f is an element-wise activation function for introducing non-linearity. At each time step, x, the scores from all image-based classifiers, is a static input into the recurrent neural network model. The "hidden contextual information" m (t) is updated by considering the previous contextual information m (t−1) , the local observation x, and previous classification scores c (t−1) . Each step of the RNN involves a single pass of aggregating information from contextual nodes within a graph to refine the scores of each node. Over a series of iterations, RNNs are used to allow finer refinements of these scores c (t) .</p><p>The formulation above is a general-purpose inference machine for refining estimates and producing classification outputs. In the following section, we describe connections with graphical models and use this to motivate the specific choices of weight matrices and weight sharing we use for our group activity recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Belief Propagation in an RNN</head><p>Recall again the graphical model depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>. This is a typical structure used in group activity recognition, with all people connected to each other and to the scene node corresponding to group activity <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. For a probabilistic graphical model of this type, a standard inference algorithm is belief propagation -rounds of passing messages between nodes would be conducted to obtain marginal distributions given observations.</p><p>We use the intuition from this probabilistic graphical model viewpoint to construct the specific form of weight matrices used in our structured inference machine.</p><p>In particular, in the graphical model formulation, we assume that all people are affecting each other and the scene. The message passing procedure starts by initializing all messages by unary energies from this graphical model. The unary energies are the inverse probability of an input image taking labels across action states, and are generated by a classifier (e.g. CNN). The initial messages are therefore independent probabilities without considering connections or smoothness between nodes.</p><p>To model the connections between entities in a group activity, three types of weights are used: (1) the weights to map the relations from individual actions to scene-level group activity, (2) from scene-level group activity to individual actions, and (3) amongst individual actions of different people. This is analogous to typical potential functions which describe pairwise energies between actions and actions or actions and scenes (similar to the shared factors in <ref type="bibr" target="#b12">[13]</ref>). The pairwise connections provide a data-dependent smoothing term analyzing influence between entities and are crucial in understanding a highly structured problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Messages in an RNN</head><p>In the message passing recurrent neural network, each message unit is a vector composed of a set of neurons. The message passing is conducted between the message units. The configuration of connections in the RNN is determined by the graph structure/connections between nodes. Thus, each message unit's input is its neighbouring connected messages, the static unary inputs, and outputs from the previous time step. As the message units basically correspond to the distribution of the node <ref type="bibr" target="#b28">[29]</ref>, this process, roughly speaking, can be considered as classifying one entity by other related entities and the local observation of itself. We adopted a recurrent neural network structure which shares the weights of message computation in all time steps.</p><p>Consider a message m </p><formula xml:id="formula_1">(t) i→j is: f   W mm 1 x i , c (t−1) i , k m (t−1) k→i |N i | − 1 , m (t−1) s→i T   , i ∈ V P , k ∈ Ni\j<label>(3)</label></formula><p>where W mm 1 is the concatenation of a set of weights:</p><formula xml:id="formula_2">W mm 1 = [W xm , W cm , W (aa) mm , W (sa) mm ].</formula><p>Since message m i→j corresponds to the distribution of node i, intuitively the weights W i→s is: </p><formula xml:id="formula_3">f W mm 2 xi, c (t−1) i , k m (t−1) k→i |Ni| , k ∈ Ni<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Output Prediction Layer</head><p>The message units representing information all over the graph are formulated into the recurrent unit. However, to eventually classify a node in a graphical model, all messages around each node should be collected and used to perform prediction. In our model, we use a prediction layer to collect all messages around each node and infer a scene label for the group activity and action classification for each person. Through this layer, the losses on each time step are imposed on the message unit. More precisely, the prediction function for the scene level node is:</p><formula xml:id="formula_4">c (t) s = f W hc 1 xs, k m (t) k→s |Ns| , p k ∈ Ns (6)</formula><p>where all notation is consistent with the previous message definitions. The activation function we used here is a softmax normalization. Similarly, for performing action classification on a person-level node:</p><formula xml:id="formula_5">c (t) i = f W hc 2 xi, k m (t) k→i |Ni| , m (t) s→i</formula><p>, k ∈ Ni <ref type="formula">(7)</ref> As shown above, the hidden contextual information is iteratively refined in a recurrent neural network through message passing. Similar to a standard belief propagation algorithm, the final prediction of each node is performed by collecting all related messages and is done in the prediction layer. The softmax loss imposed on the prediction results with standard mini-batch backpropagation training is used to train the model in an end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Structure Learning for Group Activity Inference</head><p>For group activity, the structure of connections between people can greatly influence performance. In general a fully connected graph, in which all people in a scene are connected to all others, and all people are related to the group activity, could model any type of relation. However, including spurious edges relating irrelevant people introduces significant noise. Instead focusing on relevant connections can lead to better models.</p><p>Beyond this, in a highly structured group activity, the connections between people or interactions between person and scene may vary according to different situation. Also, the interactions between people could be hard to explicitly capture. For example, in a crossing-the-street scene, both people crossing the street and people waiting for the lights to change are contributing to the group activity, while the people walking behind them may become irrelevant or even bring ambiguity. Hence, connectivity of the model should  In the context of a recurrent neural network, gates are widely used as a tool for selecting information on the activation level. Both long short-term memory (LSTM) / gated recurrent units (GRUs) are proven to be successful on many tasks involving iteratively learning and gating information element-wise. In our model, we introduce the concept of "instance level" gates. An instance level gate is used to modify an edge of a graphical model which models the interactions between instances, such as a person to a person, or a person to a scene. Instead of using a vector gate to select information element-wise, we instead learn a scalar value gate function to enforce sparsity on the structure of a graphical model.</p><p>Based on the previously introduced message passing RNN model, each node will receive information passed through an edge. Intuitively, the message passed could be noisy and may harm the understanding of actions of the instance. We propose to adopt an architecture similar to a LSTM gate, by taking multiple sources of information to selectively choose connectivity of nodes. To determine whether an edge is useful we compare the messages passed along this edge with other related messages. For example, to measure the gain by including the message m A→B , the message content of m A→B is compared to other messages from other edges, the previous iteration's classification results, and the unary distribution on node B. The gain of including the edge AB is the average value of gains for m A→B and m B→A . For two person nodes i and j, the mathematical definition of the gating functions for message m i→j is:</p><formula xml:id="formula_6">g (t) i→j = σ W hg 1 x j , c (t−1) j , m (t) i→j , k m k→j (t) |N j − 1| , k ∈ Nj\i (8) where W hg 1 = [W xg , W cg , W (aa) mm , W (aa) mm ].</formula><p>Here we reuse the weights W Similarly, for the scene node s connecting to a person node i, the gate value for message m s→i is calculated as:</p><formula xml:id="formula_7">g (t) s→i = σ W hg 2 xi, c (t−1) i , m (t) s→i , k m k→i (t) |Ni| , k ∈ Ni (9) where W hg 2 is [W xg , W cg , W (sa) mm , W (aa) mm ].</formula><p>And the gating function for m i→s is:</p><formula xml:id="formula_8">g (t) i→s = σ W hg 3 xs, c (t−1) s , m (t) i→s , k m k→s (t) |Ns| − 1 , k ∈ Ns (10) where W hg 3 equals [W xg , W cg , W (as) mm , W (as) mm ].</formula><p>Then the gate values for an edge between a person and the scene e &lt;i,s&gt; , and between two persons e &lt;i,j&gt; , at timestep t, are calculated as:</p><formula xml:id="formula_9">g (t) &lt;i,s&gt; = (g (t) i→s + g (t) s→i )/2 (11) g (t) &lt;i,j&gt; = (g (t) i→j + g (t)</formula><p>j→i )/2 <ref type="bibr" target="#b11">(12)</ref> After imposing the gates on relevant messages, the message units in the previous section are recalculated as:</p><formula xml:id="formula_10">m (t) A→B = (g (t) &lt;A,B&gt; m (t) A→B )<label>(13)</label></formula><p>Note that the above equation is a general form for gated messages. The symbol represents the product operation between a scalar and a vector. Nodes A and B could either be a person i or a scene node s. Further more, to enforce the sparsity of connections between nodes and learn the most discriminative structures for each graph, we use the L1 regularization on the gate values. Given a particular training data sample (i.e. labeled frame) d, a graphical model is built on top of it as shown in above sections. Let E d denote the set of edges in the graph. The total loss on gates for the data sample d is:</p><formula xml:id="formula_11">L d = λ |E d | e=1 (|g d e (·)|)<label>(14)</label></formula><p>where λ is the coefficient for this L1 regularization term to balance between the sparsity of the graph and prediction loss. g d e (·) is the gate value on edge e of the graph for data sample d, where the gate could either be a scene-person or a person-person edge, or, in general the edge between two nodes. As the whole model is trained by the standard minibatch method, the loss on each batch B is d∈B (L d ).</p><p>This structure selection is performed for each time step of message passing. An overall summary of the structure inference machine is presented in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Extension: Untying Weights as A Deep Inference Machine</head><p>As shown previously, in the recurrent neural network framework, a graphical model described by various potential functions can be represented as weight-shared message predictors. However, by untying the weights of message computation for each step, the model could be further extended to a deep inference machine with structure gates to selectively pass information. A model with high nonlinearity could be learned through this inference process.</p><p>In summary, this approach provides a general framework for both performing message passing of a RNN built from a graphical model and learning structures of a graphical model. If the weights in the message passing steps are tied over iterations, this has direct analogy to inference in a graphical model. If the assumption of tying of weights is relaxed, instead this process corresponds to a general deep inference machine with structure learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We demonstrate our learning framework on group activity recognition. We provide results on three challenging datasets: (1) Collective Activity Dataset <ref type="bibr" target="#b9">[10]</ref>; (2) Collective Activity Extended Dataset <ref type="bibr" target="#b10">[11]</ref>; and (3) Nursing Home Dataset <ref type="bibr" target="#b12">[13]</ref>.</p><p>The first two datasets are standard benchmarks widely used for group activity recognition. The Collective Activity Dataset contains 44 videos from 5 group activities (Crossing, Waiting, Queueing, Walking and Talking) and 6 individual actions (NA, Crossing, Waiting, Queueing, Walking and Talking). Collective Activity Extended omits the walking activity, due to ambiguities in its definition, and includes Jogging and Dancing categories. We follow the common protocol in <ref type="bibr" target="#b27">[28]</ref> for the Collective Activity Dataset. The scene label for a frame is defined by choosing the activity in which the most people participate.</p><p>The Nursing Home Event Dataset contains human activities captured from fixed cameras in various rooms of a nursing home. It contains 80 videos showing 6 actions (walking, standing, bending, squating, sitting, falling) and two scenes (fall, non-fall). This dataset consists of many chanllenging frames with highly cluttered scenes and large intra-class variation within actions. We adopted the same protocol used in Deng et al. <ref type="bibr" target="#b12">[13]</ref> for evaluation.</p><p>Implementation details: Our models are implemented using the Caffe library <ref type="bibr" target="#b18">[19]</ref>. To acquire the action scores for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Learning Latent Constituent <ref type="bibr" target="#b3">[4]</ref> 75.1% Latent SVM with Optimized Graph <ref type="bibr" target="#b27">[28]</ref> 79.7% Deep Struct. Model <ref type="bibr" target="#b12">[13]</ref> 80.6% Unified Tracking And Recognition <ref type="bibr" target="#b8">[9]</ref> 80.6% Cardinality Kernel <ref type="bibr" target="#b16">[17]</ref> 83.4% Our Model 81.2% each person image patch or the whole frame scene score, we fine-tuned the AlexNet architecture <ref type="bibr" target="#b23">[24]</ref> pre-trained using the ImageNet data. We assume that an image has been preprocessed by a person detector to get person image patches. The message passing recurrent neural network is trained by adding a softmax loss on top of the output from each timestep. We also found it easier to train the gates by first fixing the weights of learned predictors and then learning structures on it. The number of neurons for the RNN per layer is AB∈ε G (|S A |+|S B |), where ε G is the set of edges of graphical model G, and |S A | and |S B | are the numbers of states of nodes A and B respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Collective Activity Dataset</head><p>We compare results of four different methods introduced in our paper with standard baselines. <ref type="table" target="#tab_2">Table 1</ref> provides an ablation study examining the effects of different variants/components of our model. There is a clear benefit by adopting structure gates to adaptively capture connections between nodes. Note that all the CRFs in our experiments are tuned on the validation set. Our model improved the accuracy of nodes over the whole graph: person-level action classification is improved by ≈ 6% after three steps of gated message passing.</p><p>Our model is compared to state of the art methods in <ref type="table" target="#tab_3">Table 2</ref>. The results are superior to other deep learning and structure learning models. The method of <ref type="bibr" target="#b16">[17]</ref> achieves better results, though uses a counting kernel (cardinality kernel) which directly mimics the majority-action scene label definition of Collective Activity Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Collective Activity Extended Dataset</head><p>We also experiment with the Collective Activity Extended Dataset. As noted in Choi et al. <ref type="bibr" target="#b10">[11]</ref>, the walking <ref type="figure">Figure 4</ref>. This figure shows visualizations of our experimental results. Note that these images are all misclassified by the fully connected graphical model. We show the scene gates learned in our model after 3 iterations of message passing and structure learning. For visualization, since the gate values are not strictly 0 or 1, we consider &lt; 0.2 as irrelevant/noisy connection versus &gt; 0.7 as useful connections. The red box has the same action class as the scene level node. Labels: "Cr": Crossing, "Wk": Walking. action is ill-defined, hence we remove it, and include the new actions Jogging and Dancing. The two previous works choose to adopt leave-one-out for testing. However, this is very computationally intensive for a deep learning framework, and further makes hyper-parameter tuning a challenge. We choose to adopt a new train-test split with 2241 frames as training and 1106 as testing.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Note that on each person, action classification is improved by ≈ 10% via the structure inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Nursing Home Dataset</head><p>On the Nursing Home Dataset, our person-level action classification accuracy also improved, by ≈ 4% after the second iteration. The accuracy is superior to baselines including Deng et al. <ref type="bibr" target="#b12">[13]</ref>. Note that in the Nursing Home Dataset, there is a smaller margin of improvement by adopt- ing gating functions to learn structures. Because in each scene the irrelevant actions, such as sitting, can be identified as non-useful by simply a fully connected graphical model more easily than in the previous two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented a method for performing structure learning within a deep learning setting. An inference algorithm for refining estimates of individual nodes and determining connections between nodes is implemented using a recurrent neural network with gating functions. This approach was used to build a model for group activity recognition -connections between individual people in a scene and their relation to the overarching scene-level activity label are learned. This leads to improvements in accuracy over rounds of inference and structure learning via gating functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Structure learning in a deep network. Analyzing group activity requires reasoning about relations between the actions of individual people. Our structure inference machine iteratively reasons about which people in a scene are interacting and which are involved in a group activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A group activity scenario represented as a graphical model. Estimates of individual person actions and a group activity are refined via message passing. The squares are messages. These message units carry information from the source node and are propagated to the target node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>i→j . This message coming out of person i on the edge connected to person j corresponds to the distribution of i, and is classified by average pooled scores of neighbouring persons ( k m(t−1) k→i )/(|N i | − 1), static unary input x i , and output c (t−1) i from last time step. Denote the set of person nodes as V P , the neighbouring person nodes of i as N i , N i ⊆ V P , the scene node as s and the current time step as t. The mathematical form of m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>mm can be considered as classifying person i's action based on other people's action or the scene label respectively. And the unary input x i and previous iteration's score c (t−1) i are remapped via a linear transformation using W xm and W cm . The function f (·) denotes the non-linear activation function, in our case simply a softmax function to normalize the message.Likewise, if i is a person node and s is the scene node, then the message m (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Predicted scene label from timestep T , c (T ) s be able to adaptively change and adjust according to the particular input situation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>mm which classifies a person's action by other people's action labels. The activation function σ squeezes the gate value into [0, 1]. In our model, we used a sigmoid activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where W mm 2 equals [W xm , W cm , W Finally, if s is the scene node, and j is a person node, the message m Ns\pj<ref type="bibr" target="#b4">(5)</ref> where W mm 3 = [W xm , W cm , W</figDesc><table><row><cell>is set as: f W mm 3 xs, c (t−1) s</cell><cell>(aa) (t−1) , k m k→s |Ns| − 1</cell><cell>(t) s→j , k ∈ (as)</cell></row></table><note>mm ].mm ].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Structure Inference Machine Inputs: frame, detected person bounding boxes Pass image through CNN to get x s Pass person bounding boxes through CNN to get {x i } M i=1</figDesc><table><row><cell cols="2">Initialize m (0) s→i by x s ; m (0) i→s , m (0) i→j by {x i } M i=1</cell></row><row><cell>for each iteration t do</cell><cell></cell></row><row><cell>for edge (i, j) do</cell><cell></cell></row><row><cell cols="2">Compute messages m (t) i→j and m (t) j→i by Eq. 3-5</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>for edge (i, j) do</cell><cell></cell></row><row><cell cols="2">Compute gate value g &lt;i,j&gt; or g (t) &lt;s,i&gt; by Eq. 8-12 (t)</cell></row><row><cell>Impose gates on m</cell><cell>(t) i→j and m (t) j→i by Eq. 13</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>for each node i do</cell><cell></cell></row><row><cell cols="2">Compute node prediction c</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Results on Collective Activity Dataset. Ablation study including variants of our model.</figDesc><table><row><cell>Iterations</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>CRF + CNN</cell><cell></cell><cell>74.18%</cell><cell></cell></row><row><cell>Struct. SVM + CNN</cell><cell></cell><cell>73.87%</cell><cell></cell></row><row><cell>Tied Weights</cell><cell cols="3">73.86% 74.02% 74.02%</cell></row><row><cell>Untied Weights</cell><cell cols="3">73.86% 74.33% 74.33%</cell></row><row><cell>Gated Tied Weights</cell><cell cols="3">80.12% 80.90% 81.22%</cell></row><row><cell cols="4">Gated Untied Weights 80.12% 81.06% 81.22%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art methods on Collective Activity Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>51% 90.14% 90.14% Gated Untied Weights 89.51% 90.14% 90.23% Results on Collective Activity Extended Dataset.</figDesc><table><row><cell>Iterations</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>CRF + CNN</cell><cell></cell><cell>86.75%</cell><cell></cell></row><row><cell>Struct. SVM + CNN</cell><cell></cell><cell>87.34%</cell><cell></cell></row><row><cell>Tied Weights</cell><cell cols="3">84.45% 87.97% 87.97%</cell></row><row><cell>Untied Weights</cell><cell cols="3">84.45% 88.16% 88.16%</cell></row><row><cell>Gated Tied Weights</cell><cell>89.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hirf: Hierarchical random field for collective activity recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hirf: Hierarchical random field for collective activity recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cost-sensitive top-down/bottom-up inference for multiscale activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning latent constituents for recognition of group activities in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global training of document processing systems using graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="489" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.2538</idno>
		<title level="m">Learning deep structured models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering groups of people in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified framework for multitarget tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What are they doing? : Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 9th International Workshop on Visual Surveillance (VSWS09) in conjuction with ICCV</title>
		<meeting>of 9th International Workshop on Visual Surveillance (VSWS09) in conjuction with ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep structured models for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic label relation graphs with ising models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2012" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning ensembles of potential functions for structured prediction with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual recognition by counting instances: A multi-instance cardinality potential kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining perframe and per-track cues for multi-person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A flow model for joint action recognition and identity maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-agent event detection: Localization and role assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social roles in hierarchical models for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond actions: Discriminative models for contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning message-passing inference machines for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic representation and recognition of high-level group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context-based vision system for place and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextaware modeling and recognition of activities in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2491" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
