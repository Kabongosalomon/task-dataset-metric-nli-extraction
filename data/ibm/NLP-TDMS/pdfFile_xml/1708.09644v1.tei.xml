<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ABNORMAL EVENT DETECTION IN VIDEOS USING GENERATIVE ADVERSARIAL NETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DITEN</orgName>
								<orgName type="institution">University of Genova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DITEN</orgName>
								<orgName type="institution">University of Genova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DITEN</orgName>
								<orgName type="institution">University of Genova</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Carlos III University of Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ABNORMAL EVENT DETECTION IN VIDEOS USING GENERATIVE ADVERSARIAL NETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video analysis</term>
					<term>abnormal event detection</term>
					<term>crowd behaviour analysis</term>
					<term>Generative Adversarial Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the abnormality detection problem in crowded scenes. We propose to use Generative Adversarial Nets (GANs), which are trained using normal frames and corresponding optical-flow images in order to learn an internal representation of the scene normality. Since our GANs are trained with only normal data, they are not able to generate abnormal events. At testing time the real data are compared with both the appearance and the motion representations reconstructed by our GANs and abnormal areas are detected by computing local differences. Experimental results on challenging abnormality detection datasets show the superiority of the proposed method compared to the state of the art in both frame-level and pixel-level abnormality detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Abnormality detection in crowds is motivated by the increasing interest in video-surveillance systems for public safety. However, despite a lot of research has been done in this area in the past years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, the problem is still open.</p><p>There are two main reasons for which abnormality detection is challenging. First, existing datasets with ground truth abnormality samples are small. This limitation is particularly significant for deep-learning based methods, which have shown an impressive accuracy boost in many other recognition tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> but are data-hungry. The second reason is the lack of a clear and objective definition of abnormality. Moreover, these two problems are related to each other, because the abnormality definition subjectivity makes it harder to collect abnormality ground truth.</p><p>In order to deal with these problems, generative methods for abnormality detection focus on modeling only the normal pattern of the crowd. The advantage of the generative paradigm lies in the fact that only normal samples are needed at training time, while detection of what is abnormal is based on measuring the distance from the learned normal pattern. However, most of the existing generative approaches rely on hand-crafted features to represent visual information * Carlo Regazzoni has contributed to produce this work partially under the program "UC3M-Santander Chairs of Excellence". [4, 14, 3, 7, 2] or use Convolutional Neural Networks (CNNs) trained on external datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Recently, Xu et al. <ref type="bibr" target="#b16">[17]</ref> proposed to use stacked denoising autoencoders. However, the networks used in their work are shallow and based on small image patches. Moreover, additional one-class SVMs need to be trained on top of the learned representation. In this paper we propose a generative deep learning method applied to abnormality detection in crowd analysis. More specifically, our goal is to use deep networks to learn a representation of the normal pattern utilizing only normal training samples, which are much easier to collect. For this purpose, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b17">[18]</ref> are used, an emerging approach for training deep networks using only unsupervised data. While GANs are usually used to generate images, we propose to use GANs to learn the normality of the crowd behaviour. At testing time the trained networks are used to generate appearance and motion information. Since our networks have learned to generate only what is normal, they are not able to reconstruct appearance and motion information of the possible abnormal regions of the test frame. Exploiting this intuition, a simple difference between the real test-frame representations and the generated descriptions allows us to easily and robustly detect abnormal areas in the frame. Extensive experiments on challenging abnormality detection datasets show the superiority of the proposed approach compared to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abnormality Detection</head><p>Our method is different from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, which also focus on learning generative models on motion and/or appearance features. A key difference compared to these methods is that they employ hand-crafted features (e.g., Optical-flow, Tracklets, etc.) to model normal-activity patterns, whereas our method learns features from raw-pixels using a deep learning based approach. A deep learning-based approach has been investigated also in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Nevertheless, these works use existing CNN models trained for other tasks (e.g., object recognition) which are adapted to the abnormality detection task. For instance, Ravanbakhsh et al. <ref type="bibr" target="#b14">[15]</ref> propose a Binary Quantization Layer plugged as a final layer on top of a CNN, capturing temporal motion patterns in video frames for the task of abnormality segmentation. Differently from <ref type="bibr" target="#b14">[15]</ref>, we specifically propose to train a deep generative network directly for the task of abnormality detection.</p><p>Most related to our paper is the work of Xu et al. <ref type="bibr" target="#b16">[17]</ref>, who propose to learn motion/appearance feature representations using stacked denoising autoencoders. The networks used in their work are relatively shallow, since training deep autoencoders on small abnormality datasets is prone to overfitting. Moreover, their networks are not end-to-end trained and the learned representation need externally trained classifiers (multiple one-class SVMs) which are not optimized for the learned features. Conversely, we propose to use adversarial training for our representation learning. Intuitively, the adopted conditional GANs provide data augmentation and implicit data supervision thank to the discriminator network. As a result we can train much deeper generative networks on the same small abnormality datasets and we do not need to train external classifiers. GANs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> are based on a two-player game between two different networks, both trained with unsupervised data. One network is the generator (G), which aims at generating realistic data (e.g., images). The second network is the discriminator (D), which aims at discriminating real data from data generated from G. Specifically, the conditional GANs <ref type="bibr" target="#b17">[18]</ref>, that we use in our approach, take as input an image x and generate a new image p. D tries to distinguish x from p, while G tries to "fool" D producing more and more realistic images which are hard to be distinguished. Very recently Isola et al. <ref type="bibr" target="#b26">[27]</ref> proposed an "image-to-image translation" framework based on conditional GANs, where both G and D are conditioned on the real data. They show that a U-net encoderdecoder with skip connections can be used as the generator architecture together with a patch-based discriminator in order to transform images with respect to different representations. A similar framework is adopted here, generating optical-flow images from raw-pixel frames and vice versa. However, we do not aim at generating images which look realistic, but we use G to learn the normal pattern of an observed crowd scene. At testing time, G is used to generate appearance and motion information of the normal content of the input frame. Comparing this generated content with the real frame allows us to detect the possible abnormal areas of the frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LEARNING THE NORMAL CROWD BEHAVIOUR</head><p>We use the framework proposed by Isola et al. <ref type="bibr" target="#b26">[27]</ref> to learn the normal behaviour of the observed scene. Specifically, let F t be the t-th frame of a training video and O t the opticalflow obtained using F t and F t+1 . O t is computed using <ref type="bibr" target="#b27">[28]</ref>. We train two networks: N F →O , which generates opticalflow from frames and N O→F , which generates frames from optical-flow. In both cases, inspired by <ref type="bibr" target="#b26">[27]</ref>, our networks are composed of a conditional generator G and a conditional discriminator D (we refer to <ref type="bibr" target="#b26">[27]</ref> for the architectural details of G and D). G takes as input an image x and a noise vector z (drawn from a noise distribution Z) and outputs an image p = G(x, z) of the same dimensions of x but represented in a different channel. For instance, in case of N F →O , x is a frame (x = F t ) and p is the reconstruction of its corresponding optical-flow image y = O t . On the other hand, D takes as input two images (either (x, y) or (x, p)) and outputs a scalar representing the probability that both its input images came from the real data.</p><p>G and D are trained using both a conditional GAN loss L cGAN and a reconstruction loss L L1 . In case of N F →O , the training set is composed of pairs of frame-optical flow images</p><formula xml:id="formula_0">X = {(F t , O t )},</formula><p>where O t is represented using a standard three-channels representation of the horizontal, the vertical and the magnitude components. L L1 is given by:</p><formula xml:id="formula_1">L L1 (x, y) = ||y − G(x, z)|| 1<label>(1)</label></formula><p>while the conditional adversarial loss L cGAN is:</p><formula xml:id="formula_2">L cGAN (G, D) = E (x,y)∈X [log D(x, y)]+ (2) E x∈{Ft},z∈Z [log(1 − D(x, G(x, z)))]<label>(3)</label></formula><p>Conversely, in case of N O→F , we use X = {(O t , F t )}. We refer to <ref type="bibr" target="#b26">[27]</ref> for more details about the training procedure.</p><p>What is important to highlight here is that both {F t } and {O t } are collected using the frames of the only normal videos of the training dataset. The fact that we do not need videos showing abnormal events at training time makes it possible to train our networks with potentially very large datasets without the need of ground truth samples describing abnormality. At testing time we use only the generators (G F →O and G O→F ) corresponding to the trained networks. Since G F →O and G O→F have observed only normal scenes during training, they are not able to reconstruct an abnormal event. For instance, in <ref type="figure" target="#fig_0">Fig. 1 (top)</ref> a frame F , containing a vehicle unusually moving on a University campus, is input to G F →O and in the generated optical flow image (p O ) the abnormal area corresponding to that vehicle is not correctly reconstructed. Similarly, when the real optical flow (O) associated with F is input to G O→F , the network tries to reconstruct the area corresponding to the vehicle but the output is a set of unstructured blobs <ref type="figure" target="#fig_0">(Fig. 1, bottom)</ref>. We exploit this inability of our networks to reliably reconstruct abnormality to detect possible anomalies as explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ABNORMALITY DETECTION</head><p>At testing time we input G F →O and G O→F using each frame F of the test video and its corresponding optical-flow image O, respectively. Note that the random noise vector z is internally produced by the two networks using dropout <ref type="bibr" target="#b26">[27]</ref>, and in the following we drop z to simplify our notation. Using F , an optical-flow reconstruction can be obtained: <ref type="figure" target="#fig_0">Fig. 1</ref>). ∆ O highlights the (local) differences between the real optical flow and its reconstruction and these differences are higher in correspondence of those areas in which G F →O was not able to generate the abnormal behaviour.</p><formula xml:id="formula_3">p O = G F →O (F ), which is compared with O using a simple pixel- by-pixel difference, obtaining ∆ O = O − p O (see</formula><p>Similarly, we obtain the appearance reconstruction p F = G O→F (O). As shown in <ref type="figure" target="#fig_0">Fig. 1 (bottom)</ref>, the network generates "blobs" in the abnormal areas of p F . Even if these blobs have an appearance completely different from the corresponding area in the real image F , we empirically observed that a simple pixel-by-pixel difference between F and p F is less informative than the difference computed in the optical-flow channel. For this reason, a "semantic" difference is computed using another network, pre-trained on ImageNet <ref type="bibr" target="#b28">[29]</ref>. Specifically, we use AlexNet <ref type="bibr" target="#b7">[8]</ref>. Note that AlexNet is trained using supervised data which are pairs of images and object-labels contained in ImageNet. However, no supervision about crowd abnormal behaviour is contained in ImageNet and the network is trained to recognize generic objects. Let h(F ) be the conv 5 representation of F in this network and h(p F ) the corresponding representation of the appearance reconstruction. The fifth convolutional layer of AlexNet (before pooling) is chosen because it represents the input information in a sufficiently abstract space and is the last layer preserving geometric information. We can now compute a semantics-based difference between F and p F :</p><formula xml:id="formula_4">∆ S = h(F ) − h(p F ).</formula><p>Finally, ∆ S and ∆ O are fused in order to obtain a unique abnormality map. Specifically, we first upsample ∆ S in order to obtain ∆ S with the same resolution as ∆ O . Then, both ∆ S and ∆ O are normalized with respect to their corresponding channel-value range as follows. For each test video V we compute the maximum value m O of all the elements of ∆ O over all the input frames of V . The normalized optical-flow difference map is given by:</p><formula xml:id="formula_5">N O (i, j) = 1/m O ∆ O (i, j).<label>(4)</label></formula><p>Similarly, the normalized semantic difference map N S is obtained using m S computed over all the elements of ∆ S in all the frames of V :</p><formula xml:id="formula_6">N S (i, j) = 1/m S ∆ S (i, j).<label>(5)</label></formula><p>The final abnormality map is obtained by summing N S and N O : A = N S + λN O . In all our experiments we use λ = 2.</p><p>A is our final abnormality heatmap.  real frame generated frame generated OF abnormality heatmap <ref type="figure">Fig. 3</ref>. Some examples of abnormality localization on UCSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>In this section we evaluate our method using two well-known crowd abnormality datasets. We use both a pixel-level and a frame-level protocol under the original evaluation setup <ref type="bibr" target="#b0">[1]</ref>. The rest of this section describes the datasets, the experimental setup and the obtained results. GANs Setup. In our experiments, N F →O and N O→F are trained with the train sequences of the UCSD dataset. All frames are resized to 256 × 256 pixels. Training is based on stochastic gradient descent with momentum 0.5, batch size 1. Each network is trained for 10 epochs. Datasets and Experimental Setup. We use two standard datasets: the UCSD Anomaly Detection Dataset <ref type="bibr" target="#b2">[3]</ref> and the UMN SocialForce <ref type="bibr" target="#b3">[4]</ref>. The UCSD dataset is split into two subsets: Ped1, which contains 34 train and 16 test sequences, and Ped2, which contains 16 train and 12 test videos. This dataset is challenging due to the low-resolution images, different types of moving objects, the presence of one or more anomalies in the scene. The UMN dataset contains 11 videos in 3 different scenes, with a total amount of 7700 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results and Discussion</head><p>Frame-level abnormality detection.  <ref type="bibr" target="#b16">[17]</ref>.</p><p>Method AUC optical-flow <ref type="bibr" target="#b3">[4]</ref> 0.84 SFM <ref type="bibr" target="#b3">[4]</ref> 0.96 Sparse Reconstruction <ref type="bibr" target="#b6">[7]</ref> 0.97 Commotion <ref type="bibr" target="#b29">[30]</ref> 0.98 Plug-and-Play CNN <ref type="bibr" target="#b14">[15]</ref> 0.98 Proposed Method 0.99 <ref type="table">Table 2</ref>. Results on the UMN dataset (all but our values are taken from <ref type="bibr" target="#b29">[30]</ref>).</p><p>contains at least one predicted abnormal pixel: in this case the abnormal label is assigned to the whole frame. The procedure is applied over a range of thresholds to build a ROC curve. We compare our method with the state of the art. Quantitative results using both EER (Equal Error Rate) and AUC (Area Under Curve) are shown in Tab. 1, and the ROC curves in <ref type="figure" target="#fig_2">Fig. 2</ref>. The proposed method is also evaluated on UMN dataset using the same frame level evaluation (Tab. 2). Pixel-level abnormality localization. The goal of the pixellevel evaluation is to measure the accuracy of the abnormality localization. Following <ref type="bibr" target="#b0">[1]</ref>, a true positive prediction should cover at least 40% the ground truth abnormal pixels, otherwise the frame is counted as a false positive. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the ROC curves of the localization accuracy over USDC, and Tab. 1 reports a quantitative comparison with the state of the art. The results reported in Tab. 1-2 show that the proposed approach sharply overcomes all the other compared methods. Information fusion analysis. In order to analyze the impact on the accuracy provided by each network, N O→F and N F →O , we perform a set of experiments on UCSD Ped1. In the frame-level evaluation, N O→F obtains 84.1% AUC and N F →O 95.3% AUC, which are lower than the 97.4% obtained by the fused version. In the pixel-level evaluation, however, the performance of N O→F dropped to 30.1%, while the N F →O is 66.2%. We believe this is due to the low resolution of ∆ S (computed over the results obtained using N O→F ), which makes the pixel-level localization a hard task. By fusing appearance and motion we can refine the detected area, which leads to a better localization accuracy. Qualitative results. <ref type="figure">Fig. 3</ref> shows some results using the standard visualization protocol for abnormality localization (red pixels represent abnormal areas). The figure shows that our approach can successfully localize different abnormality types. Moreover, since the generator learned a spatial distribution of the normal motion in the scene, common perspective issues are automatically alleviated. <ref type="figure">Fig. 3</ref> also shows the intuition behind our approach. Normal objects and events (e.g., walking pedestrians) are generated with a sufficient accuracy. However, the generators are not able to reproduce abnormal objects and events (e.g., a vehicle in the first row) and this inability in reproducing abnormalities is what we exploit in order to detect abnormal areas. The last row in <ref type="figure">Fig. 3</ref> shows a failure case, miss detecting the abnormal object (a skateboard). The failure is probably due to the fact that the skateboard is very small, has a "normal" motion (the same speed of normal pedestrians), and is partially occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper we addressed the problem of abnormality detection in crowd videos. We proposed a generative deep learning method based on two conditional GANs. Since our GANs are trained using only normal data, they are not able to generate abnormal events. At testing time, a local difference between the real and the generated images is used to detect possible abnormalities. Experimental results on standard datasets show that our approach outperforms the state of the art with respect to both the frame-level and the pixel-level evaluation protocols. As future work we will investigate the use of Dynamic Images <ref type="bibr" target="#b30">[31]</ref> as an alternative to optical-flow in order to represent motion information collected from more than one frame, as suggested by an anonymous reviewer of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Top: a generator network takes as input a frame and produces a corresponding optical-flow image. Bottom: a second generator network is fed with a real optical-flow image and outputs an appearance reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Frame-level ROC curves.(b) Pixel-level ROC curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>ROC curves on Ped1 (UCSD dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state of the art on the UCSD dataset. The values of the other methods are taken from</figDesc><table><row><cell>The frame-level abnor-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Analyzing tracklets for the detection of abnormal crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Plug-and-play cnn for crowd motion analysis: An application in abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00307</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00866</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Generative adversarial nets,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes: A novel framework based on swarm optimization and social force modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained abnormal behavior understanding in crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Abnormality detection with improved histogram of oriented tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIAP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowd behavior representation: an attribute-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SpringerPlus</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1179</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detection and localization of crowd behavior using a novel tracklet-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>JMLC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Crowd activity classification using category constrained correlated topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>KSII TIIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wo</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Improved techniques for training GANs,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crowd motion monitoring using trackletbased commotion measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
