<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
							<email>luowei.zhou@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
							<email>yen-chun.chen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
							<email>siqi.sun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>yu.cheng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically w.r.t. the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Our proposed method allows information integration beyond local windows, which is especially beneficial for question answering (QA) and language modeling tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long-range contextual understanding is critical for many natural language processing (NLP) tasks. For example, the relevant information needed to correctly answer an open-domain question or generate the next word (i.e., language modeling) can arch over thousands of words. Due to limitations on time and GPU memory, encoding long sequences through neural networks is challenging and expensive. Traditional sequence modeling methods <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> encode long sequences in a chronological order, hence suffering from the high latency issue. On the other hand, recent models such as Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> avoid sequential encoding via simultaneous self-attention over the entire input, and have been successfully adopted in many NLP tasks such as textual entailment , dependency parsing <ref type="bibr" target="#b30">(Zhou and Zhao, 2019)</ref>, and summarization . A caveat with Transformer is that building full connections over long sequences leads to quadratic growth on memory demand and computational complexity with respect to the sequence length.</p><p>To process long sequences, a widely adopted solution is to first chunk a sequence into much shorter ones with a sliding window, then build connections between the shorter sequences. For example, <ref type="bibr" target="#b4">Child et al. (2019)</ref>, <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref> and <ref type="bibr" target="#b29">Zaheer et al. (2020)</ref> apply sparse attention to chunked sequences in hand-designed patterns in order to gather information from the chunks. <ref type="bibr" target="#b5">Choi et al. (2017)</ref> and <ref type="bibr" target="#b28">Wang et al. (2019)</ref> first use a simpler model to filter chunked sequences, then process selected sequences with fully-connected self-attention. <ref type="bibr" target="#b21">Rae et al. (2019)</ref> makes use of the shared memory of chunked sequences to build connections between them. However, the above methods cannot encode long-range dependencies with as much flexibility or accuracy as fully-connected self-attention, due to the dependency on hand-designed patterns or the lack of critical information.</p><p>Recently, several works <ref type="bibr" target="#b12">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b24">Tay et al., 2020)</ref> have proposed to further improve the sparse attention mechanism by hashing or sorting the hidden states into different buckets. However, these works mainly explore tasks with relatively short sequences, such as sentence-level Machine Translation (MT), where the number of hashing vectors is relatively small (less than 16 in <ref type="bibr" target="#b12">Kitaev et al. (2020)</ref>) and randomly initialized hashing vectors are good enough to hash hidden states into correct buckets. In this paper, we further explore the potential of hashing-based attention in the context of long sequences (thousands of words).</p><p>Our proposed framework for processing long sequences combines the benefits of both slidingwindow and hashing-based methods on local and Figure 1: Illustration of different methods for processing long sequences. Each square represents a hidden state. The black-dotted boxes are Transformer layers. (a) is the sliding-window-based method to chunk a long sequence into short ones with window size 3 and stride 2. (b) builds cross-sequence attention based on sliding window over pre-selected positions (red-dotted boxes). (c) hashes the hidden states into different buckets by randomlyinitialized vectors. (d) is our proposed approach to cluster the hidden states. Our final model is a combination of (a) and (d) that processes both local and global context. long-range dependency encoding. It consists of two types of encoding layer. The first one (noted as a Sliding-Window Layer) focuses on local information within a sliding window. It applies Transformer to the hidden states of each chunked sequence independently, as shown in <ref type="figure">Figure 1</ref>(a). The other one (noted as a Cluster-Former Layer) encodes global information beyond the initial chunked sequences. Specifically, we first apply clustering to the input hidden states so that similar hidden states are assigned to the same cluster, as shown in <ref type="figure">Figure 1(d)</ref>. The clustered and sorted input is then divided uniformly into chunks, each encoded by a Transformer layer. Note that to make model training more efficient, the cluster centroids are not computed online but updated periodically (every epoch or a few epochs). We accumulate the hidden states from the layer prior to the Cluster-Former layer in a memory bank, and apply the K-Means algorithm to form cluster centroids during each update cycle. Compared to previously discussed sparse attention based on pre-selected positions <ref type="figure">(Figure 1(b)</ref>) or randomly-initialized hashing vectors ( <ref type="figure">Figure 1(c)</ref>), experimental results show that our method can encode dependency across chunked sequences more effectively.</p><p>Our contributions can be summarized as follows. (i) We propose Cluster-Former, a novel approach to capturing long-range dependencies more effectively than the locality-sensitive hashing method. (ii) We propose a new Transformer-based framework to process long sequences by combining Sliding-Window and Cluster-Former layers to ex-tract both local and global contextual information.</p><p>(iii) Our model achieves the best performance on question answering datasets of Natural Questions (long answer), SearchQA, and Quasar-T. (iv) We provide fair comparison between different methods on multiple language modeling tasks, and demonstrate that our clustering-based method makes use of contextual information beyond sliding windows effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Long Sequence in Language Modeling Language modeling is one of the benchmark tasks to test models' ability on handling long sequences. As words from the same long article are likely related, a model should have the ability to detect longrange dependencies for sequence generation. <ref type="bibr" target="#b23">Sundermeyer et al. (2012)</ref> first used LSTM to address long-range dependencies beyond N-grams. With the availability of more computational resources, more complex models are proposed to encode long sequences. <ref type="bibr" target="#b19">Merity et al. (2017)</ref> released the Wiki-Text dataset that composes of full Wiki articles and facilitates the study of long context modeling. They proposed to use self-attention mechanism for encoding long-range dependencies. <ref type="bibr" target="#b9">Grave et al. (2017)</ref> proposed to save a long range of hidden states in continuous cache, which can be used for next word generation later on.</p><p>Recently, most of the best-performing models are based on Transformer <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>. <ref type="bibr" target="#b21">Rae et al. (2019)</ref> proposed to make use of the compressed memory from Transformer for long-range sequence modeling. To enable the Transformer itself to handle long sequences without additional memory or continuous cache, <ref type="bibr" target="#b4">Child et al. (2019)</ref>; <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref> introduced Sparse Attention, where attention is applied to sparse pre-selected positions across sliding windows to encode longrange dependencies. <ref type="bibr" target="#b12">Kitaev et al. (2020)</ref> proposed to re-order the hidden states by hashing similar states into the same buckets, and then apply Sparse Attention across the buckets. Our proposed method is in line with this direction. However, instead of using randomly initialized hashing vectors, we propose to cluster the hidden states based on the cluster centroids from our hidden-state memory bank, which leads to better performance on language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Sequence in Question Answering</head><p>For tasks such as open-domain question answering <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, a large volume of documents or paragraphs are usually retrieved to infer the answer, yielding extremely long context content. Despite that state-of-the-art NLP models are capable of extracting answers amid complex context, they still struggle with extremely long input sequence. Recent advances that advocate the use of large-scale pre-trained models <ref type="bibr" target="#b14">Lan et al., 2020)</ref> for question answering make this problem more prominent, due to tremendous memory consumption. Therefore, to process a long sequence, the most widelyused method is to first use a lightweight model to filter out redundant text, and then use slidingwindow-based approaches to encode the remaining sequences with a more sophisticated model. <ref type="bibr" target="#b3">Chen et al. (2017)</ref> integrated bi-gram features into Information Retrieval (IR) methods to retrieve the related documents more accurately. <ref type="bibr" target="#b27">Wang et al. (2018)</ref> trained a paragraph selector using whether the entire system can obtain the correct answer or not as the reward. <ref type="bibr" target="#b16">Lin et al. (2018)</ref> proposed to use a paragraph ranking model to curate data that are required for training reading comprehension models. <ref type="bibr" target="#b28">Wang et al. (2019)</ref> trained a ranker to merge paragraphs for multi-passage reasoning. <ref type="bibr" target="#b1">Asai et al. (2020)</ref> trained a recurrent retriever to select paragraphs for multi-hop question answering. However, all these paragraph ranking or filtering methods may risk losing important information for question answering. In this paper, we focus on directly training a large model on long sequences without any intermediate method for text filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The proposed framework to handle long sequences is centered on two types of Transformer layers: (i) Sliding-Window Layer, and (ii) Cluster-Former Layer. The former layer focuses on encoding local sequence information, while the latter is on encoding global context and always built on top of the former layer. An overview of the two layers is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sliding-Window Layer</head><p>Despite that our focus is on capturing long-range dependencies for global context, local information also plays a critical role for knowledge propagation. Therefore, in the lower part of our network, we adopt the traditional sliding-window encoding mechanism. A sliding window segments a long sequence X into short, overlapping ones with window size l and stride m, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(a). Note that for question answering tasks, we concatenate the question Q with each sequence chunked from the document (Q is not applicable in language modeling tasks). Then, we have</p><formula xml:id="formula_0">H 0 k = [Q; X [m × k : (m × k + l)]] ,<label>(1)</label></formula><p>where Q ∈ R q×d denotes question embeddings given a QA task, q is the number of tokens in the question, and X ∈ R x×d is the embeddings for all the context. k is the id of the chunked sequence, l is the window size, m is the stride of the sliding window. [idx 1 : idx 2 ] indicates selecting rows between the index of idx 1 and idx 2 of the matrix.</p><p>[·; ·] means concatenating the matrices along the row. As we expect the neighbouring sequences to share useful information in hidden states as well, we always set m &lt; l to allow overlapping between sequences. We use the mean values of the Transformer hidden states at the overlapped tokens as their final output.</p><formula xml:id="formula_1">H n+1 k = Transformer(H n k ), H n+1 k [q : q + l − m] + = H n+1 k−1 [q + m : end], H n+1 k [q : q + l − m] / = 2, H n+1 k [q + m : end] + = H n+1 k+1 [q : q + l − m], H n+1 k [q + m : end] / = 2,<label>(2)</label></formula><p>where + = is to add matrices in-place and / = is to divide a matrix by a scaler value in-place. The output of the k-th sequence in the n-th layer is H n+1 k ∈ R (q+l)×d , which merges the hidden states from both the (k−1)-th and (k+1)-th sequences. If the next layer is Cluster-Former, the output hidden states in this layer H n+1 k will be saved into memory bank for computing the cluster centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cluster-Former Layer</head><p>We introduce a new method, Cluster-Former, to add global representational power to Transformer beyond sliding windows. An in-depth visualization of the layer is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref></p><formula xml:id="formula_2">(b).</formula><p>The input of the Cluster-Former layer comes from the hidden states of the prior layer (in our case a Sliding-Window layer). After merging the overlaps between sequence chunks, the input of this layer is defined as:</p><formula xml:id="formula_3">H n = [H n 0 [0 : q + m]; ...; H n k [0 : q + m]] ,<label>(3)</label></formula><p>whereH n ∈ R (q x/m +x)×d is the hidden states to cluster, x is the number of tokens in the context. As the hidden states with larger cosine similarity are more likely to have higher attention weights, we build sparse self-attention only on the hidden states in the same cluster. In this work, we use K-Means as the chosen clustering method for simplicity. More advanced clustering algorithms have the potential of yielding better performance. Since running K-Means on the fly in each training iteration is computationally expensive, we decide to recompute the cluster centroids with low frequency (every epoch or every few epochs).</p><p>Besides, to avoid dramatic changes in the cluster centroids due to limited hidden state inputs, we maintain a memory bank for the most recent hidden states. The entire procedure is depicted in Algorithm 1. Once we have the cluster centroids, we can directly use them for hidden state clustering as follows:</p><formula xml:id="formula_4">v n = argmax H n (C n ) T ||H n || 2 ||C n || 2 ,<label>(4)</label></formula><p>where C n ∈ R p×d are the cluster centroids for layer n, and p is the pre-defined number of clusters. The function argmax(·) performs on the last dimension and assigns all the input hidden states into different clusters based on the max value of cosine similarity between the hidden states and cluster centroids. v n ∈ R (q x/m +x) is the assigned cluster IDs of all the input hidden states. As the number of hidden states in different clusters can vary substantially, padding them to the maximum length to run Transformer will significantly increase the computational time. To make global context gathering more efficient, we greedily pick the cluster centroids based on the nearest neighbour (measured by cosine similarity) as shown in the function GETCENTROIDS in Algorithm 1. Thus, the hidden states with similar cluster IDs are also close to each other. Then, we can directly sort the cluster IDs of hidden states and uniformly chunk the hidden states (same window </p><formula xml:id="formula_5">u n = argsort(v n ), a n k = u n [mk : m(k + 1)], E n k = H n [sort(a n k )],<label>(5)</label></formula><p>where the function argsort(·) is to obtain the indexes of input values sorted in order (same values sorted by the corresponding position of hidden states). a n k ∈ R m is the chunked indexes of the hidden states. Note that the function sort(·) on a n k ∈ R m is specifically designed for the language modeling task to mask words, and can be ignored for QA tasks. E n k ∈ R m×d is the k-th clustered hidden states, and we will run Transformer on top of it to build the connection beyond the words in the initial sliding window as follows:</p><formula xml:id="formula_6">E n+1 k = Transformer(E n k ).<label>(6)</label></formula><p>After updating the hidden states, we will map them back to the order before clustering:</p><formula xml:id="formula_7">H n+1 = [E n+1 0 ; E n+1 1 ;</formula><p>...; E n+1 K ], a n = [a n 0 ; a n 1 ; ...; a n K ],</p><formula xml:id="formula_8">H n+1 [ā n ] = clone(H n+1 ),<label>(7)</label></formula><p>whereH n+1 is the final output hidden state of this layer and has the same word order as the inputH n . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we introduce our experimental setting and detailed analysis of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our proposed approach on two main tasks: question answering and language modeling. For question answering, we use the following datasets, and summarize the statistics in <ref type="table">Table 1</ref>.</p><p>Quasar-T 1 <ref type="bibr" target="#b7">(Dhingra et al., 2017)</ref>: The goal of this task is to answer open-domain questions from Trivia Challenge. All the passages harvested through information retrieval can be used to answer questions. The task requires the model to generate answers in phrases. The evaluation metric on this dataset is based on Exact Match and F1 score of the bag-of-words matching. Our evaluation tool 2 comes from the SQuAD dataset. As the given document may not contain answer, we can either predict an answer or predict no answer. The evaluation metric on this dataset is the F1 score, where true positives are exactly correct answers, false positives are incorrect answer predictions, and false negatives are incorrect "no answer" predictions. As the test set is hidden, Quasart-T SearchQA NQ(long) NQ(short) EM/F1 EM/F1 F1 F1</p><p>R3 <ref type="bibr" target="#b27">(Wang et al., 2018)</ref> 35.3/41.7 49.0/55.3 --DECAPROP <ref type="bibr" target="#b25">(Tay et al., 2018)</ref> 38.6/46.9 62.2/70.8 --DS-QA <ref type="bibr" target="#b16">(Lin et al., 2018)</ref> 42.2/49.3 58.8/64.5 --Multi-passage BERT <ref type="bibr" target="#b28">(Wang et al., 2019)</ref> 51.1/59.1 65.1/70.7 --DrQA <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> 37.7/44.5 41.9/48.   we split 5% of the training set for validation, and use the original validation set for testing. We use the official tool from the dataset to evaluate our models.</p><p>To demonstrate Cluster-Former's ability to detect long-range dependencies, we also evaluate on two language modeling tasks: 6</p><p>Wikitext-103 <ref type="bibr" target="#b19">(Merity et al., 2017)</ref>: The dataset is extracted from the set of verified Good and Featured articles on Wikipedia. We use the dataset to train word-level language modeling and use perplexity as the evaluation metric.</p><p>Enwik8 <ref type="bibr" target="#b18">(Mahoney, 2011)</ref>: This also comes from Wikipedia. We train character-level language modeling on this dataset and use bit per character (bpc) as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All the models are trained on 8 Nvidia V100 GPUs. We use cluster centroids that perform the best on the validation set for test set experiments. The number of hidden states kept in memory bank is 100K.</p><p>Question Answering: We initialize our models with RoBERTa-large  that has 24 Transformer layers, 16 heads per layer and hidden state dimension of 1024. As the number of position embeddings of RoBERTa is limited to 512, we cannot assign different position embeddings to all tokens. Instead, we assign the same position embeddings to each chunked sequence. The majority of our model is made up of Sliding-Window Layers, as the local information is essential for QA tasks. We adopt the proposed Cluster-Former Layer in the randomly selected layers 15 and 20 to further capture long-range information. We set the sliding window size l to 256, stride m to 224, and change the number of clusters in {64, 256, 512} to analyze its impact on the final performance. We prepend a special token to the beginning of all the given/retrieved paragraphs and directly concatenate all the paragraphs as the final context sequence.</p><p>For Quasar-T and SearchQA, we predict the start and end positions of the answer. For Natural Question, we first identify whether the question can be answered or not based on the given document, before predicting the answer. Then, we classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short or long answers. We rank the hidden states of all the special tokens for long answer selection and predict the start and end positions of short answers. Due to memory constraints, we set the max length to be 5000 during training and 10000 during inference. During dataset finetuning, we use Adam (Kingma and Ba, 2015) to optimize the model. We set warm-up updates to 2,220, maximal updates to 22,200, dropout rate to 0.1, learning rate to 5 × 10 −5 , and batch size to 160. The model will converge in one day for all the QA datasets.</p><p>Language Modeling: All the models are trained from scratch. We set the number of layers to 16, with 8 heads per layer. Our Cluster-Former layer is used in layers 11 and 15. We segment long input into short sequences of 3072 tokens, set sliding window size l to 256, and stride m to 128. SGD is used for optimizing the models. We set clip threshold of gradients to 0.1, warm-up updates to 16,000, maximal updates to 286,000, dropout rate to 0.3, learning rate to 0.1, and batch size to 16. The model will converge in 3 days for all the LM datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline</head><p>We compare our models with several strong baselines according to their published results. • R3 <ref type="bibr" target="#b27">(Wang et al., 2018)</ref> proposes to use reinforcement learning to jointly train passage ranker and reader, considering no gold label for the passage. • DS-QA <ref type="bibr" target="#b16">(Lin et al., 2018)</ref> proposes to first use paragraph selection to filter the noisy data, so that the final answer extraction model can be trained on denoised data. • Multi-passage BERT <ref type="bibr" target="#b28">(Wang et al., 2019)</ref> proposes to filter the passages and then merge multiple useful passages into one sequence, which can be encoded by BERT. • DrQA <ref type="bibr" target="#b3">(Chen et al., 2017</ref>) makes use of attention mechanism across the question and the document for answer phrase extraction. • DecAtt and DocReader <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> is based on a pipeline approach that first uses a simpler model to select long answers and then a reading comprehension model to extract short answers from the long answers. • BERT joint  jointly trains short and long answer extraction in a single model rather than using a pipeline approach.</p><p>• BERT wwm +SQuAD2 <ref type="bibr">(Pan et al., 2019)</ref> makes use of multi-task learning to further boost performance. We also re-implement several strong baselines which have not been explored to process long context in question answering tasks. To make a fair comparison among different methods on longrange information collection, we replace several layers of the sliding window baseline with Sparse Attention, Locality-Sensitive Hashing and Cluster-Former.</p><p>• Sliding Window: This method is fully made up of Sliding-Window Layers and can only attend to local information. • Sparse Attention <ref type="bibr" target="#b4">(Child et al., 2019)</ref>: This method replaces several layers in the previous baseline by training a Transformer layer across sequences on pre-selected positions. We run this sparse Transformer on all the hidden states in the same position across sequences, so that the output of sparse Transformer can merge the information from different sequences. • Locality-Sensitive Hashing <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref>:</p><p>This method hashes hidden states into different buckets determined by randomly-initialized hashing vectors. A Transformer layer is then applied across buckets to build Sparse Attention across the whole sequence. Note that this method cannot be directly used for question answering without adding Sliding-Window layer, as our QA model is initialized by RoBERTa that only has 512 position embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>Experiments on question answering and language modeling are presented in <ref type="table" target="#tab_3">Table 2</ref> and 3. State-of-the-Art Results on QA: Our proposed method outperforms several strong baselines, thanks to its ability to encode both local and global information. Cluster-Former with 512 clusters achieves new state-of-the-art results on Quasar-T, SearchQA and Natural Question (long answer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Cluster-Former:</head><p>We also test the ability of Cluster-Former on modeling long-range dependencies. NOte that Sparse Attention <ref type="bibr" target="#b4">(Child et al., 2019)</ref> and Locality-Sensitive Hashing <ref type="bibr" target="#b12">(Kitaev et al., 2020)</ref> have never been tested on question answering tasks with long context. To make a fair comparison, we replace the same layers in our  <ref type="bibr">, 50, 51, 52, 53, 54, 55, 115, 116, 168, 273, 394, ..., 5567, 5577, 5704, 5722, 5740, 5742, 5760, 5778, 5831, 5850, 5851, 5890, 5891, 5989, 6022, 6040, 6042, 6060, 6094, 6095, 6096</ref>  baseline (sliding window only) with these methods and also our Cluster-Former. As can be seen, although Sparse Attention can boost the performance of language modeling, it hurts the performance of question answering. The loss may come from the noise introduced by pre-selected positions, the corresponding words of which may not be related.</p><p>We set the number of hashing vectors in Locality-Sensitive Hashing (LSH) to 64, the same as the number of clusters in Cluster-Former. LSH outperforms the baseline slightly on QA and consistently underperforms our Cluster-Former (#C=64).</p><p>Overall, our Cluster-Former performs the best on detecting long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Number of Cluster Centroids:</head><p>We also test the effect of different numbers of cluster centroids (C) on model performance. With the increase of C, the hidden states with dependencies tend to be assigned to the same cluster. Based on our experiments on both QA and LM, we observe that the model with 512 clusters works significantly better than the model with 64 clusters on most of the tasks. However, for Natural Questions, the improvement is marginal. Potential Improvement on LM: Although we have proven the effectiveness of our Cluster-Former on the task of language modeling, our best performance is still lagging behind state of the art <ref type="bibr" target="#b22">(Sukhbaatar et al., 2019)</ref>. We will further adapt our Cluster-Former to more different frameworks on language modeling in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>We perform qualitative analysis on how the hidden states are clustered in <ref type="table" target="#tab_6">Table 4</ref>, by visualizing the corresponding words and positions of the hidden states. From the first row of the table, we can see that the special tokens "&lt;s&gt;" tend to belong to the same cluster. Note that the special token "&lt;s&gt;" is the start token of each long answer candidate, and its hidden state is used for final long answer ranking. Therefore, the Transformer on this cluster can compare across the candidates and help make the final prediction. We further observe that the same types of tokens are more likely to appear in the same cluster. For example, words from the second row to the forth row cover the topics of time, stopwords, and organization &amp; geopolitical entities.</p><p>Finally, we randomly sample a cluster and list positions of clustered hidden states in the last row. We find that states in long distance (over 6000 tokens apart) can be in one cluster, which demonstrates the ability of Cluster-Former to detect long-range dependencies. Besides, we observe that states tend to cluster in phrases. For example, we see consecutive positions <ref type="bibr">like "49, 50, 51, 52, 53, 54, 55"</ref>, which likely result from the sliding-window encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present Cluster-Former, a new method to encode global information for long sequence modeling. We achieve new state of the art on three question answer datasets: Quasar-T, SearchQA, and Natural Questions. Further, we observe that a larger number of clusters in Cluster-Former can lead to better performance on question answering and language modeling tasks. Cluster-Former is a generic approach, and we believe that it can also potentially benefit other tasks that rely on long-range dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of two types of Transformer layer. (a): Sliding-window layer over a sequence. The question is omitted here for simplicity. (b) Cluster-Former layer over clustered hidden states from the output of (a). The cluster centroids are periodically updated based on the memory bank of the hidden states in the corresponding layer. Note that the sequence inputs in (a) and (b) usually come from two different samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SearchQA 3 (</head><label>3</label><figDesc><ref type="bibr" target="#b8">Dunn et al., 2017)</ref>: The setting of this dataset is the same as Quasar-T, except that the questions are sourced from Jeopardy! 4 instead.Natural Questions 5<ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref>: This task aims to answer questions based on a given Wikipedia document, and has two settings.(i) Long answer: select a paragraph that can answer the question based on the Wikipedia document if any. (ii) Short answer: extract an answer phrase from the document if the document contains the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on Question Answering datasets. #C: number of clusters.</figDesc><table><row><cell></cell><cell cols="2">Wikitext Enwik8</cell></row><row><cell></cell><cell>ppl</cell><cell>bpc</cell></row><row><cell>Sliding window</cell><cell>20.8</cell><cell>1.34</cell></row><row><cell>Sparse Attention</cell><cell>20.5</cell><cell>1.29</cell></row><row><cell>Locality-Sensitive Hashing</cell><cell>20.8</cell><cell>1.33</cell></row><row><cell>Cluster-Former (#C=64)</cell><cell>20.5</cell><cell>1.28</cell></row><row><cell>Cluster-Former (#C=256)</cell><cell>20.3</cell><cell>1.24</cell></row><row><cell>Cluster-Former (#C=512)</cell><cell>20.2</cell><cell>1.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on Language Modeling.</figDesc><table /><note>#C: number of clusters; Wikitext: Wikitext-103.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>QuestionWhere did the underground railroad start and finish ?Context The Underground Railroad by artist Charles T. Webber , 1893 Date Late 1700s -1865 Location Northern United States with routes to Canada , Mexico ... Special token &lt;s&gt;&lt;s&gt;&lt;s&gt;Island island in the colonies city&lt;s&gt;&lt;s&gt;&lt;s&gt;With in the in . the &lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;the South The Underground Railroad was the Railroad &lt;s&gt;The Underground Railroad Time did start and finish 1893 Date 1700 1865 Location Participants Outcome Deaths 19 1763 83 17 1821 formed in the late 1700s 1850 1860 1850 via did start and finish 1821 1700 1850 1860 1850 -1872 1793 Stopwords the the , the , , , , to , , , , the American runaway slaves of free states the , , , it to , a the the to , , , there the the the , -, , the , they a , there , to , it the the , the Federal for Deep cotton as the the , , , Entity Canada Mexico Canada is applied Florida Spanish Railroad Railroad Railroad British Canada Ontario were said Numerous are documented in the book Congress Positions 49</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>An example from Natural Question dataset. The rows in the middle section show the corresponding words of the clustered hidden states, and the bottom row shows the positions of the clustered hidden states. "&lt;s&gt;" refers to start token of long answer candidate.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/bdhingra/quasar 2 https://rajpurkar.github.io/SQuAD-explorer/ 3 https://github.com/nyu-dl/dl4ir-searchQA 4 http://j-archive.com/ 5 https://ai.google.com/research/NaturalQuestions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For both datasets, we follow the dataset split from https://github.com/salesforce/awd-lstm-lm.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08634</idno>
		<title level="m">A bert baseline for the natural questions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Coarse-to-fine question answering for long documents</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://www.mattmahoney.net/text/text.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Ferritto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05286</idno>
		<title level="m">Radu Florian, and Avirup Sil. 2019. Frustratingly easy natural question answering</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Speech Communication Association (ISCA)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11296</idno>
		<title level="m">Sparse sinkhorn attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected attention propagation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">R3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-passage bert: A globally normalized bert model for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ramesh Nallapati, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Big bird: Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Head-driven phrase structure grammar parsing on penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
