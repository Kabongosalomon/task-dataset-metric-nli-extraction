<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harvesting and Refining Question-Answer Pairs for Unsupervised QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongli</forename><surname>Li</surname></persName>
							<email>lizhongli@</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>lidong1@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
							<email>kexu@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harvesting and Refining Question-Answer Pairs for Unsupervised QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question Answering (QA) has shown great success thanks to the availability of largescale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as REFQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RE-FQA. We conduct experiments 1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets <ref type="bibr" target="#b16">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b15">(Rajpurkar et al., , 2018</ref><ref type="bibr" target="#b8">Joshi et al., 2017)</ref>, and well-designed neural models <ref type="bibr" target="#b19">(Wang and Jiang, 2016;</ref><ref type="bibr" target="#b17">Seo et al., 2016;</ref><ref type="bibr">Yu et al., 2018)</ref>. Recently, unsupervised pre-training of language models on large corpora, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, has brought further performance gains.</p><p>However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es-pecially for new domains or languages. In order to tackle the setting in which no training data available, <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> leverage unsupervised machine translation to generate synthetic contextquestion-answer triples. The paragraphs are sampled from Wikipedia. NER and noun chunkers are employed to identify answer candidates. Cloze questions are first extracted from the sentences of the paragraph, and then translated into natural questions. However, there are a lot of lexical overlaps between the generated questions and the paragraph. Similar lexical and syntactic structures render the QA model tend to predict the answer just by word matching. Moreover, the answer category is limited to the named entity or noun phrase, which restricts the coverage of the learnt model.</p><p>In this work, we present two approaches to improve the quality of synthetic context-questionanswer triples. First, we introduce the REFQA dataset, which harvests lexically and syntactically divergent questions from Wikipedia by using the cited documents. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the sentence (statement) in Wikipedia and its cited documents are semantically consistent, but written with different expressions. More informative context-question-answer triples can be created by using the cited document as the context paragraph and extracting questions from the statement in Wikipedia. Second, we propose to iteratively refine data over REFQA. Given a QA model and some REFQA examples, we first filter its predicted answers with a probability threshold. Then we refine questions based on the predicted answers, and obtain the refined question-answer pairs to continue the model training. Thanks to the pretrained linguistic knowledge in the BERTbased QA model, there are more appropriate and diverse answer candidates in the filtered predictions, some of which do not appear in the candidates extracted by NER tools. We also show that iteratively refining the data further improves model performance.</p><p>We conduct experiments on SQuAD 1.1 <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, and NewsQA <ref type="bibr" target="#b18">(Trischler et al., 2017)</ref>. Our method yields state-of-the-art results against strong baselines in the unsupervised setting. Specifically, the proposed model achieves 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using annotated data. We also evaluate our method in a few-shot learning setting. Our approach achieves 79.4 F1 on the SQuAD 1.1 dev set with only 100 labeled examples, compared to 63.0 F1 using the method of <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>.</p><p>To summarize, the contributions of this paper include: i) REFQA constructing in an unsupervised manner, which contains more informative context-question-answer triples. ii) Using the QA model to iteratively refine and augment the question-answer pairs in REFQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Extractive Question Answering Given a document and question, the task is to predict a continuous sub-span of the document to answer the question. Extractive question answering has garnered a lot of attention over the past few years. Benchmark datasets, such as SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b15">(Rajpurkar et al., , 2018</ref>, NewsQA <ref type="bibr" target="#b18">(Trischler et al., 2017)</ref> and TriviaQA <ref type="bibr" target="#b8">(Joshi et al., 2017)</ref>, play an important role in the progress. In order to improve the performance on these benchmarks, several models have been proposed, including BiDAF <ref type="bibr" target="#b17">(Seo et al., 2016)</ref>, R-NET , and QANet <ref type="bibr">(Yu et al., 2018)</ref>. Recently, unsupervised pre-training of language models such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct.</p><p>Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models <ref type="bibr" target="#b28">Zhu et al., 2019b;</ref><ref type="bibr">Alberti et al., 2019;</ref><ref type="bibr" target="#b6">Dong et al., 2019)</ref>. However, the methods require labeled data to train the sequence-to-sequence QG model. <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> propose to collect synthetic context-question-answer triples by gen-erating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner.</p><p>Unsupervised QA <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract "fill-inthe-blank" cloze-style questions given the candidate answer and context. iv) Translate cloze-style questions into natural questions by an unsupervised translator. Compared with <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref>, <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> attempt to generate natural questions by training an unsupervised neural machine translation (NMT) model. They train the NMT model on non-aligned corpora of natural questions and cloze questions. The unsupervised QA model of <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> achieves promising results, even outperforms early supervised models. However, their questions are generated from the sentences or sub-clauses of the same paragraphs, which may lead to a biased learning of word matching since its similar lexicons and syntactic structures. Besides, the category of answer candidates is limited to named entity or noun phrase, which restricts the coverage of the learnt QA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Harvesting REFQA from Wikipedia</head><p>In this section, we introduce REFQA, a question answering dataset constructed in an unsupervised manner. One drawback of <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> is that questions are produced from the paragraph sentence that contains the answer candidate. So there are considerable expression overlaps between generated questions and context paragraphs. In contrast, we harvest informative questions by taking advantage of Wikipedia's reference links, where lexical and syntactic differences exist between the article and its cited documents.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given statements in Wikipedia paragraphs and its cited documents, we use the cited documents as the context paragraphs and generate questions from the sub-clauses of statements. In order to generate question-answer pairs, we first find answer candidates that appear in both sub-clauses and context paragraphs. Next, we convert sub-clauses into the cloze questions based on the candidate answers. We then conduct cloze-to-natural-question translation by a depen-  dency tree reconstruction algorithm. We describe the details as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context and Answer Generation</head><p>Statements in Wikipedia and its cited documents often have similar content, but are written with different expressions. Informative questions can be obtained by taking the cited document as the context paragraph, and generate questions from the statement. We crawl statements with reference links from the English Wikipedia. The cited documents are obtained by parsing the contents of reference webpages.</p><p>Given a statement and its cited document, we restrict the statement to its sub-clauses, and extract answer candidates (i.e., named entities) that appear in both of them by using a NER toolkit. We then find the answer span positions in the context paragraph. If the candidate answer appears multiple times in the context, we select the position whose surrounding context has the most overlap with the statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Generation</head><p>We first generate cloze questions <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> from the sub-clauses of Wikipedia statements. Then we introduce a rule-based method to rewrite them to more natural questions, which utilizes the dependency structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cloze Generation</head><p>Cloze questions are the statements with the answer replaced to a mask token. Following <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>, we replace answers in statements with a special mask token, which depends on its answer category 2 . Using the statement and the answer (with a type label PRODUCT) from <ref type="figure" target="#fig_0">Figure 1</ref>, this leaves us with the cloze question "Guillermo crashed a Matt Damon interview, about his upcoming movie [THING]".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Translate Clozes to Natural Questions</head><p>We perform a dependency reconstruction to generate natural questions. We move answer-related words in the dependency tree to the front of the question, since answer-related words are important. The intuition is that natural questions usually start with question words and question focus <ref type="bibr" target="#b23">(Yao and Van Durme, 2014)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref>, we apply the dependency parsing to the cloze questions, and translate them to natural questions by three steps: i) We keep the right child nodes of the answer and prune its lefts. ii) For each node in the parsing tree, if the subtree of its child node contains the answer node, we move the child node to the first child node. iii) Finally, we obtain the natural question by inorder traversal on the reconstructed tree. We apply the same rule-based mapping as <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>, which replaces each answer category with the most appropriate wh* word. For example, the THING category is mapped to "What".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Iterative Data Refinement</head><p>In this section, we propose to iteratively refine data over REFQA based on the QA model. As shown in <ref type="figure">Figure 3</ref>, we use the QA model to filter REFQA data, find appropriate and diverse answer candidates, and use these answers to refine and augment REFQA examples. Filtering data can get rid of some noisy examples in REFQA, and pretrained linguistic knowledge in the BERT-based QA model finds more appropriate and diverse answers. We produce questions for the refined answers, then continue to train the QA model on the refined and filtered triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initial QA Model Training</head><p>The first step of iterative data refinement is to train an initial QA model. We use the REFQA exam-</p><formula xml:id="formula_0">ples S I = {(c i , q i , a i )} N i=1</formula><p>to train a BERT-based QA model P (a|c, q) by maximizing:</p><formula xml:id="formula_1">S I log P (a i |c i , q i )<label>(1)</label></formula><p>where the triple consists of context c i , question q i , and answer a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Refine Question-Answer Pairs</head><p>As shown in <ref type="figure">Figure 3</ref>, the QA model P (a|c, q) is used to refine the REFQA examples. We first conduct inference on the unseen data (denoted as S U ), and obtain the predicted answers and their probabilities. Then we filter the predicted answers with  <ref type="figure">Figure 3</ref>: Overview of our iterative data refinement process. "QG" is the process of question generation as described in Section 3.2. We produce new training data and iteratively train the QA model. a confidence threshold τ :</p><formula xml:id="formula_2">Z A = {a i |P (a i |c i , q i ) ≥ τ } (c i ,q i ,a i )∈S U</formula><p>where a i represents the predicted answer.</p><p>For each predicted answer a i , if it agrees with the gold answer a i , we keep the original question. For the case that a i = a i , we treat a i as our new answer candidate. Besides, we use the question generator (Section 3.2) to refine the original question q i to q i .</p><p>In this step, using the QA model for filtering helps us get rid of some noisy examples. The refined question-answer pairs (q i , a i ) can also augment the REFQA examples. The pretrained linguistic knowledge in the BERT-based QA model is supposed to find more novel answers, i.e., some candidate answers are not extracted by the NER toolkit. With the refined answer spans, we then use the question generator to produce their corresponding questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iterative QA Model Training</head><p>After refining the dataset, we concatenate them with the filtered examples whose candidate answers agree with the predictions. The new training set is then used to continue to train the QA model. The training objective is defined as:</p><formula xml:id="formula_3">max a i ∈Z A [I(a i = a i )log P (a i |c i , q i ) + I(a i = a i )log P (a i |c i , q i )],<label>(2)</label></formula><p>Algorithm 1: Iterative Data Refinement Input: synthetic context-question-answer triples S = {(c i , q i , a i )} N i=1 , a threshold τ and a decay factor γ. Sample a part of triples S I from S Update the model parameters by where I(·) is an indicator function (i.e., 1 if the condition is true).</p><formula xml:id="formula_4">maximizing S I log P (a|c, q) Split unseen triples into {S U 1 , S U 2 , ..., S U M } for k ← 1 to M do D ← φ for (c i , q i , a i ) in S U k do Z A ← {a i s.t. P (a i |c i , q i ) ≥ τ } for a i in Z A do if a i = a i then D ← D ∪ (c i , q i , a i ) else Refine question q i to q i D ← D ∪ (c i , q i , a i ) τ ← τ × γ</formula><p>Using the resulting QA model, we further refine question-answer pairs and repeat the training procedure. The process is repeated until the performance plateaus, or no new data available. Besides, in order to obtain more diverse answers during iterative training, we apply a decay factor γ for the threshold τ . The pseudo code of iterative data refinement is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our proposed method on two widely used extractive QA datasets <ref type="bibr" target="#b16">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b18">Trischler et al., 2017)</ref>. We also demonstrate the effectiveness of our approach in the few-shot learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFQA Construction</head><p>We collect the statements with references from English Wikipedia following the procedure in <ref type="bibr" target="#b27">(Zhu et al., 2019a)</ref>. We only consider the references that are HTML pages, which results in 1.4M statement-document pairs.</p><p>In order to make sure the statement is relevant to the cited document, we tokenize the text, remove stop words and discard the examples if more than half of the statement tokens are not in the cited document. The article length is limited to 1,000 words for cited documents. Besides, we compute ROUGE-2 <ref type="bibr" target="#b13">(Lin, 2004)</ref> as correlation scores between statements and context. We use the score's median (0.2013) as a threshold, i.e., half of the data with lower scores are discarded. We obtain 303K remaining data to construct our REFQA.</p><p>We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser <ref type="bibr" target="#b10">(Kitaev and Klein, 2018)</ref>. The questions are generated as in Section 3.2. We also discard sub-clauses that are less than 6 tokens, to prevent losing too much information of original sentences. Finally, we obtain 0.9M REFQA examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering Model</head><p>We adopt BERT as the backbone of our QA model. Following <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, we represent the question and passage as a single packed sequence. We apply a linear layer to compute the probability of each token being the start or end of an answer span. We use Adam (Kingma and Ba, 2015) as our optimizer with a learning rate of 3e-5 and a batch size of 24. The max sequence length is set to 384. We split the long document into multiple windows with a stride of 128. We use the uncased version of BERT-Large (Whole Word Masking). We evaluate on the dev set every 1000 training steps, and conduct early stopping when the performance plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Data Refinement</head><p>We uniformly sample 300k data from REFQA to train the initial QA model. We split the remaining 600k data into 6 parts for iterative data refinement. For each part, we use the current QA model to refine questionanswer pairs. We combine the refined data with filtered data in a 1:1 ratio to continue training the QA model. Specially, we keep the original answer if its prediction is a part of the original answer during inference. The threshold τ is set to 0.15 for filtering the model predictions. The decay factor γ is set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We conduct evaluation on the SQuAD 1.1 <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, and the NewsQA <ref type="bibr" target="#b18">(Trischler et al., 2017)</ref> datasets. We compare our proposed approach with previous unsupervised approaches and several supervised models. Performance is measured via the standard Exact Match (EM) and F1 metrics.  <ref type="bibr" target="#b26">(Yu et al., 2016)</ref> 62.5 / 71.2 62.5 / 71.0 -/ --/ -mLSTM <ref type="bibr" target="#b19">(Wang and Jiang, 2016)</ref> 64.1 / 73.9 64.7 / 73.7 34.4 / 49.6 * 34.9 / 50.0 * FastQAExt <ref type="bibr">(Weissenborn et al., 2017) 70.3 / 78.5 70.8 / 78.9</ref> 43.7 / 56.1 42.8 / 56.1 R-NET  71.1 / 79.5 71.3 / 79.7 -/ --/ -BERT-Large <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 84.2 / 91.1 85.  <ref type="table">Table 1</ref>: Results (EM / F1) of our method, various baselines and supervised models on SQuAD 1.1, and NewsQA. " * " means results taken from <ref type="bibr" target="#b18">Trischler et al. (2017)</ref>, " †" means results taken from <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>, and " ‡" means our reimplementation on BERT-Large (Whole Word Masking). <ref type="bibr" target="#b5">Dhingra et al. (2018)</ref> propose to train the QA model on the cloze-style questions. Here we take the unsupervised results that re-implemented by <ref type="bibr" target="#b12">Lewis et al. (2019)</ref> with BERT-Large. The other unsupervised QA system <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> borrows the idea of unsupervised machine translation <ref type="bibr" target="#b11">(Lample et al., 2017)</ref> to convert cloze questions into natural questions. For a fair comparison, we use their published data 3 to re-implement their approach based on BERT-Large (Whole Word Masking) model. <ref type="table">Table 1</ref> shows the main results on SQuAD 1.1 and NewsQA. Training QA model on our REFQA outperforms the previous methods by a large margin. Combining with iterative data refinement, our approach achieves new state-of-the-art results in the unsupervised setting. Our QA model attains 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using their annotated data, outperforming all of the previous unsupervised methods. In particular, the results are competitive with early supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>We conduct ablation studies on the SQuAD 1.1 dev set, in order to better understand the contributions of different components in our method.  <ref type="table">Table 2</ref>: Results (EM / F1) of REFQA and WIKI datasets with different cloze translation methods on the SDuAD 1.1 dev set. "DRC" is short for dependency reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effects of REFQA</head><p>We conduct experiments on REFQA and another synthetic dataset (named as WIKI). The WIKI dataset is constructed using the same method as in <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>, which uses Wikipedia pages as context paragraphs for QA examples. In addition to the dependency reconstruction method (Section 3.2.2), we compare three cloze translation methods proposed in <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>.</p><p>Identity Mapping generates questions by replacing the mask token in cloze questions with a relevant wh* question word.</p><p>Noise Cloze first applies a noise model, such as permutation, and word drop, as in <ref type="bibr" target="#b11">Lample et al. (2017)</ref>, and then applies the "Identity Mapping" translation.</p><p>UNMT converts cloze questions into natural questions following unsupervised neural machine translation. Here we directly use the published model of <ref type="bibr" target="#b12">Lewis et al. (2019)</ref>    <ref type="table">Table 4</ref>: Results of using filtered data, refined data, and the combination for data refinement on the SDuAD 1.1 dev set. "Iter." is short for iterative training.</p><p>For a fair comparison, we sample 300k training data for each dataset, and fine-tune BERT-Base for 2 epochs. As shown in <ref type="table">Table 2</ref>, training on our REFQA achieves a consistent gain over all cloze translation methods. Moreover, our dependency reconstruction method is also favorable compared with the "Identity Mapping" method. The improvement of DRC on WIKI is smaller than on REFQA. We argue that it is because WIKI contains too many lexical overlaps, while DRC mainly focuses on providing structural diversity.</p><p>We present the generated questions of our method (DRC) and UNMT in <ref type="table" target="#tab_7">Table 3</ref>. Most natural questions follow a similar structure: question word (what/who/how), question focus (name/money/time), question verb (is/play/take) and topic <ref type="bibr" target="#b23">(Yao and Van Durme, 2014)</ref>. Compared with UNMT, our method adjusts answer-related words in the dependency tree according to the linguistic characteristics of natural questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effects of Data Combination</head><p>We validate the effectiveness of combining refined and filtered data for our data refinement. We use only refined or filtered data to train our QA model, comparing with the combining approach.</p><p>The results are shown in <ref type="table">Table 4</ref>. We observe  <ref type="table">Table 5</ref>: Results of using different confidence thresholds during the construction of the refined data and filtered data.   <ref type="figure">Figure 4</ref>: Comparison on filtered data and refined data with different confidence thresholds. "F" is short for using filtered data, "R" is short for using refined data. "R+F" is short for the combination of refined and filtered data.</p><p>that both data can help the QA model to achieve better performance. Moreover, the combination of refined and filtered data is more useful than only using one of them. Using iterative training, our combination approach further improves the model performance to 72.6 F1 (1.6 absolute improvement). Besides, using our refined data contributes further improvement compared with filtered data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Effects of Confidence Threshold</head><p>We experiment with several thresholds (0.0, 0.1, 0.15, 0.2, 0.3, 0.5 and 0.7) to filter the predicted answers. Their QA results on SQuAD 1.1 dev set are presented in <ref type="table">Table 5</ref>. Using threshold of 0.15 achieves better performance.</p><p>We also analyze the effects of threshold on refined data and filtered data. As shown in <ref type="figure">Figure 4</ref>, for the filtered data, using a higher confidence threshold achieves better performance, suggesting that using the QA model for filtering makes our examples more credible. For the refined data and the combination, we observe that the threshold 0.15 achieves a better performance than the threshold 0.3, but the EM is greatly reduced when the threshold is set to 0.0. Besides, there are 26,257 answers that do not appear in named entities using the threshold 0.15, compared to 15,004 for the threshold 0.3. Thus, an appropriate threshold can help us improve the answer diversity and get rid of some noisy examples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Effects of Refinement Types</head><p>For brevity, we denote the original answer and predicted answer by "OA" and "PA", respectively. In order to analyze the contribution of our refined data, we categorize the data refinements into the following three types:</p><p>OA⊃PA The original answer contains the predicted answer.</p><p>OA⊂PA The predicted answer contains the original answer.</p><p>Others The remaining data except for the above two types of refinement.</p><p>For each type, we keep the original data or use refined data to train our QA model. We conduct experiments on the non-iterative setting with the data combination. <ref type="table" target="#tab_11">Table 6</ref>, our refined data improves the QA model in most types of refinement except "OA⊃PA". The results indicate that the QA model favors longer phrases as answer spans. Moreover, for the "OA⊂PA" and "Others" types, there are 47.8% answers that are not extracted by the NER toolkit. The iterative refinement extends the category of answer candidates, which in turn produces novel question-answer pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We show a few examples of our generated data in <ref type="table">Table 7</ref>. We list one example for each type. For the "OA⊃PA" refinement, the predicted answer is a sub-span of the extracted named entity, but the complete named entity is more appropriate as an answer. For the "OA⊂PA" refinement, the QA model can help us extend the original answer to be a longer span, which is more complete and appropriate. Besides, for the "Others" refinement, its prediction can be a new answer, and not appear in named entities extracted by the NER toolkit. In August 2013, Guillermo crashed a Matt Damon interview, about his upcom movie Elysium, by promoting his own movie called "Estupido", about a stupi man, which poster had an arrow pointing towards Matt Damon.</p><p>[17] At the end the interview, Matt removed the poster, revealing on the other side the name o another Guillermo movie called "Ass Face", also with an arrow pointing towa Matt. Matt accuses Guillermo of acting on Kimmel's orders and, facing the camera, starts to say "you...", … During Kimmel's 2016 post-Oscar special, Ben Affleck wore a very large coat his appearance, and Damon emerged from the coat for the interview. However was removed from the studio by an enraged Kimmel, who then moved on to interview Affleck. Later, Damon appeared in a sketch about the movie that Affleck stars in, Batman v Superman: Dawn of Justice, reprising his role as astronaut Mark Watney. <ref type="bibr">[19]</ref> e sub find answer p Statement … In the clip, Kimmel sidekick/parking lot security guard Guillermo Rodriguez interrupted an interview Damon is giving while sitting in front of a poster for "Elysium," and propped up his own movie poster for a film called "Estupido." The sign was bright yellow, with the title in big bold letters and an arrow pointed down at Damon. … </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cited Document</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Few-Shot Learning</head><p>Following the evaluation of <ref type="bibr" target="#b5">Dhingra et al., 2018)</ref>, we conduct experiments in a few-shot learning setting. We use the best configuration of our approach to train the unsupervised QA model based on BERT-Large (Whole Word Masking). Then we fine-tune the model with limited SQuAD training examples.</p><p>As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, our method obtains the best performance in the restricted setting, compared with the previous state of the art <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> and directly fine-tuning BERT. Moreover, our approach achieves 79.4 F1 (16.4 absolute gains than other models) with only 100 labeled examples. The results illustrate that our method can greatly reduce the demand of in-domain annotated data. In addition, we observe that the results of different methods become comparable when the labeled data size is greater than 10,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present two approaches to improve the quality of synthetic QA data for unsupervised question answering. We first use the Wikipedia paragraphs and its references to construct a synthetic QA data REFQA and then use the QA model to iteratively refine data over RE-FQA. Our method outperforms the previous unsupervised state-of-the-art models on SQuAD 1.1, and NewsQA, and achieves the best performance in the few-shot learning setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>find answer positionIn the clip, Kimmel… interrupted an interview Damon is giving while sitting in front of a poster for "Elysium," … Statement … In the clip, Kimmel sidekick/parking lot security guard Guillermo Rodriguez interrupted an interview Damon is giving while sitting in front of a poster for "Elysium," and propped up his own movie poster for a film called "Estupido." The sign was bright yellow, with the title in big bold letters and an arrow pointed down at Damon. … Overview of REFQA construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of translating cloze questions to natural questions. The node with light yellow color indicates that its subtree contains the answer node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Update the model parameters by maximizing D log P (a|c, q) Output: the updated QA model P (a|c, q)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>F1 score on the SQuAD 1.1 dev set with various training dataset sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>10 100 1000 10000 100000 Number of Labeled Training Data BERT-Large + Ours</head><label></label><figDesc>From Wikipedia, the free encyclopediaIn August 2013, Guillermo crashed a Matt Damon interview, about his upcoming movie Elysium, by promoting his own movie called "Estupido", about a stupid man, which poster had an arrow pointing towards Matt Damon.[17]  At the end of the interview, Matt removed the poster, revealing on the other side the name of another Guillermo movie called "Ass Face", also with an arrow pointing towards</figDesc><table><row><cell>Jimmy Kimmel Live!</cell><cell>extract sub-clause</cell><cell>Guillermo crashed a Matt Damon interview, about his upcoming movie Elysium</cell><cell>extract answer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Elysium</cell></row><row><cell></cell><cell></cell><cell>Guillermo crashed a Matt Damon interview, about his</cell><cell>replace answer</cell></row><row><cell></cell><cell></cell><cell>upcoming movie [THING]</cell><cell></cell></row><row><cell cols="2">Matt. Matt accuses Guillermo of acting on Kimmel's orders and, facing the</cell><cell></cell><cell></cell></row><row><cell>camera, starts to say "you...", …</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">During Kimmel's 2016 post-Oscar special, Ben Affleck wore a very large coat for</cell><cell></cell><cell></cell></row><row><cell cols="2">his appearance, and Damon emerged from the coat for the interview. However, he</cell><cell>What his upcoming movie</cell><cell></cell></row><row><cell cols="2">was removed from the studio by an enraged Kimmel, who then moved on to</cell><cell>about Guillermo crashed a</cell><cell></cell></row><row><cell cols="2">interview Affleck. Later, Damon appeared in a sketch about the movie that</cell><cell>Matt Damon interview</cell><cell></cell></row><row><cell cols="2">Affleck stars in, Batman v Superman: Dawn of Justice, reprising his role as</cell><cell></cell><cell></cell></row><row><cell>astronaut Mark Watney.[19]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>/ 51.6 45.1 / 53.5 43.4 / 52.0 49.2 / 58.8</figDesc><table><row><cell></cell><cell>Identity</cell><cell>Noise</cell><cell>UNMT</cell><cell>DRC</cell></row><row><cell>WIKI</cell><cell cols="4">20.8 / 30.5 36.6 / 45.6 40.5 / 49.1 26.3 / 35.7</cell></row><row><cell cols="2">REFQA 42.5</cell><cell></cell><cell></cell></row><row><cell>3 https://github.com/facebookresearch/</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UnsupervisedQA</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Examples of generated questions using UNMT and our method. "DRC" is short for our dependency reconstruction. The blue words indicate extracted answers.</figDesc><table><row><cell>Iter.</cell><cell>Size</cell><cell>EM / F1</cell></row><row><cell>Initial QA Model</cell><cell>300k</cell><cell>57.1 / 66.8</cell></row><row><cell>Training on</cell><cell></cell><cell></cell></row><row><cell>Filtered Data</cell><cell>464k</cell><cell>57.4 / 67.1</cell></row><row><cell>Refined Data</cell><cell>100k</cell><cell>61.0 / 70.7</cell></row><row><cell>Refined + Filtered Data</cell><cell>200k</cell><cell>61.8 / 71.0</cell></row><row><cell>Refined Data</cell><cell cols="2">6×15k 60.1 / 70.0</cell></row><row><cell>Refined + Filtered Data</cell><cell cols="2">6×30k 62.5 / 72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison between different types of data refinement on the SQuAD 1.1 dev set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We obtain the answer type labels by a NER toolkit, and group these labels to high-level answer categories, which are used as our mask tokens, e.g., PRODUCT corresponding to THING, LOC corresponding to PLACE.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work was partially supported by National Natural Science Foundation of China (NSFC) [Grant  No. 61421003].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Allen Petersen escaped the advancing Japanese armies by sailing a junk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oa⊃pa S</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hummel Hummel</title>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
	<note>from Shanghai to California with his wife Tani and two White Russians (Tsar loyalists</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Allen</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<title level="m">What at they would be revealing their future rally plans on February 9 OA: Chicago Auto Show PA: the 2011 Chicago Auto Show RQ: What at their future rally plans they would be revealing on February 9</title>
		<imprint/>
	</monogr>
	<note>Petersen RQ: Who escaped the advancing Japanese armies by sailing a junk OA⊂PA S: Hyundai announced they would be revealing their future rally plans at the 2011 Chicago Auto Show on February 9</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Q: What the Kanye West song on the spoken word segment re-imagines OA: Low Lights PA: That&apos;s What&apos;s Up RQ: What the track she released that re-imagines the spoken word segment on the Kanye West song</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S:</forename><surname>Others</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>In</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01" />
			<publisher>Low Lights</publisher>
		</imprint>
	</monogr>
	<note>That&apos;s What&apos;s Up&quot; that re-imagines the spoken word segment on the Kanye West song &quot;Low Lights</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PA&quot; and &quot;RQ&quot; are short for the original answer, predicted answer and the refined question. References Chris Alberti</title>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins; Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
	<note>The generated and refined question-answer pairs. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and effective semi-supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Danish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="582" to="587" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1711.00043</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised question answering by cloze translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4896" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshop</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>abs/1611.01603</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1608.07905</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making neural QA as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised QA with generative domain-adaptive nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1040" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">QANet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<idno>abs/1804.09541</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end reading comprehension with dynamic answer chunk ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1610.09996</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transforming wikipedia into augmented data for query-focused summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1911.03324</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to ask unanswerable questions for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1415</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4238" to="4248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
