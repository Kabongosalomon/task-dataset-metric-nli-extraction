<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic Association and Salient Point Clustering Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-25">25 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangru</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingquan</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">IET Research Journals Submission Template for IET Research Journal Papers</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic Association and Salient Point Clustering Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="page" from="1" to="8"/>
							<date type="published" when="2020-06-25">25 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel 3D point cloud segmentation framework named SASO, which jointly performs semantic and instance segmentation tasks. For semantic segmentation task, inspired by the inherent correlation among objects in spatial context, we propose a Multi-scale Semantic Association (MSA) module to explore the constructive effects of the semantic context information. For instance segmentation task, different from previous works that utilize clustering only in inference procedure, we propose a Salient Point Clustering Optimization (SPCO) module to introduce a clustering procedure into the training process and impel the network focusing on points that are difficult to be distinguished. In addition, because of the inherent structures of indoor scenes, the imbalance problem of the category distribution is rarely considered but severely limits the performance of 3D scene perception. To address this issue, we introduce an adaptive Water Filling Sampling (WFS) algorithm to balance the category distribution of training data. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on benchmark datasets in both semantic segmentation and instance segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene perception plays a decisive role in many applications, such as autonomous driving, robot navigation and augmented reality.</p><p>With the growth of computer technology and artificial intelligence in recent years, scene perception ability of intelligent devices has received increasing attention from both academia and industry, especially for the 3D scenes which can represent the real environment intuitively. Semantic segmentation and instance segmentation of 3D scenes are the fundamental and critical portions of 3D scene perception. Nevertheless, how to model the 3D space into digital shape to accomplish scene segmentation task is an indefinite problem. Various representations of 3D scenes have been investigated, such as depth maps, voxels, multi-views, meshes and point clouds. Based on these representations, a series of excellent works have been investigated to operate segmentation task, such as <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Among these representations, point clouds are the most compact and natural to the geometric distributions of real 3D scenes, which have been applied extensively in recent researches. In terms of semantic and instance segmentation tasks in 3D point clouds, based on the great success achieved in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> for each single task, joint learning methods for both tasks <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> have opened up a new effective way to explore the 3D scene segmentation, which improved the performance and promoted further development. Compared with the method <ref type="bibr" target="#b13">[14]</ref> exploiting similarity matrix, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> utilized clustering algorithm to generate instance segmentation result, which was proved to be more effective and flexible. Nevertheless, whether the convergence direction of the training process is consistent with the orientation of clustering algorithm was rarely considered. Additionally, the marginal points are usually harder to be distinguised than the central points, and in multiple objects case the internal points are easier to be distinguished than the boundary points across objects, as shown in <ref type="figure">Figure 3</ref>. To address this problem, we propose a Salient Point Clustering Optimization (SPCO) module to introduce clustering into the training process and saliently focus on the points that are harder to be distinguished in the clustering process. As for semantic segmentation, the spatial distribution of the semantic information has a strong association, which can be further exploited. For example, when a point comes from table, it is highly possible that there will be some neighbor points belonging to chair other than from ceiling. The most common approach to explore the semantic associations is the Conditional Random Fields (CRF) algorithm <ref type="bibr" target="#b21">[22]</ref>, which utilizes normalization based on statistical global probability and has been proved to be effective in segmentation tasks. However, CRF is complex and consumes plenty of resources, how to sufficiently exploit the semantic associations more efficiently is an indefinite problem. Consequently, we propose a Multi-scale Semantic Association (MSA) module to fine tune the semantic segmentation results, which is based on the multiple scale semantic association maps generated by statistical analysis. In addition, because of the inherent structures of indoor scenes, the imbalance problem of the category distribution badly limits the performance of 3D scene perception. For example, wall and floor certainly exist in every room while other categories may not, such as sofa, sink, bookshelf, etc.. This leads to the numbers of points from wall and floor are much more than the one from other categories. The imbalance problem is rarely considered in previous wokrs. Thus, we present an adaptive Water Filling Sampling (WFS) algorithm to address this problem by changing the sampling probabilities of each category adaptively. To summarize, our contributions are the following:</p><p>• We propose a Salient Point Clustering Optimization (SPCO) module to introduce clustering into the training process and saliently focus on the points that are harder to be distinguished in instance segmentation.</p><p>• We propose a Multi-scale Semantic Association (MSA) module based on statistical knowledge to explore the potential spatial association of the semantic information in point clouds.</p><p>• We propose an adaptive Water Filling Sampling (WFS) algorithm to balance category distribution in the point clouds, which is rarely considered but critical in 3D scene perception.</p><p>• Extensive experiments demonstrate that our SASO outperforms the state-of-the-art related methods on benchmark datasets in both semantic and instance segmentation criteria.  <ref type="figure" target="#fig_1">Fig. 1</ref>: An illustration of our joint learning framework. The input 3D point clouds are first encoded to F share by PointNet++ <ref type="bibr" target="#b3">[4]</ref>, then the common feature will be decoded separately by semantic and instance segmentation branches. In semantic segmentation branch (blue), a MSA module based on statistics knowledge is proposed to explore the semantic association, we will expound it in Sec 3.2. For the instance segmentation (green), we proposed SPCO module to introduce clustering into the training process and focus on hard-distinguished points, which will be explained in Sec 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>This section reviews recent deep learning-based techniques applied to 3D point clouds. In recent years, a series of deep learning architectures have been proposed to perform the encoding and decoding for 3D point clouds or its derived representations, which are widely utilized in many 3D vision tasks such as semantic and instance segmentation, object part segmentation and object detection. We divide these methods into four categories based on the data representations. Further more, we will introduce recent 3D semantic and instance segmentation research progress based on above techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Volumetric Methods</head><p>Due to 3D point clouds are irregular, the most simple but naive method is to voxelize the irregular point clouds to regular 3D grids so that 3D convolutions can be applied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Specifically, Wu et al. <ref type="bibr" target="#b22">[23]</ref> represented a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Maturana et al. <ref type="bibr" target="#b30">[31]</ref> proposed an architecture to efficiently deal with large amounts of point cloud data by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network. Zhou et al. <ref type="bibr" target="#b24">[25]</ref> removed the manual feature engineering for 3D point clouds and divided point clouds into equally spaced 3D voxels, then transformed a group of points within each voxel into a unified feature representation through a newly introduced voxel feature encoding layer. Wang et al. <ref type="bibr" target="#b0">[1]</ref> designed a spatial dense extraction module to preserve the spatial resolution during the feature extraction procedure, alleviating the loss of detail caused by sub-sampling operations such as max-pooling. Although volumetric data representation is the most common and simplest form, there is an obvious drawback that cubic complexity of 3D convolutions leads to a dramatic increase in the memory consumption and computing resources. To tackle this issue, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> proposed octree representation to improve efficiency of network and reduce computing resources. In addition, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> proposed sparse convolutional operations to process spatially-sparse 3D point clouds and achieved impressive results. Although these methods try to alleviate the efficiency problem, they are much more complex than volumetric CNNs and can not fundamentally solve the memory consumption problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-view Methods</head><p>Another common method for 3D point clouds are a multi-view representation. In recent years, Convolutional Neural Networks have been proved successful in a wide range of 2D visual tasks. To sufficiently take advantage of the strong extraction capability of classical CNNs, 3D point clouds are first projected into multiple pre-defined views, which are then processed by well-designed image-based CNNs to extract features, such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. Specifically, Guerry et al. <ref type="bibr" target="#b36">[37]</ref> used 3D-coherent synthesis of scene observations and mixed them in a multi-view framework for 3D labeling. Su et al. <ref type="bibr" target="#b32">[33]</ref> presented a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. Dai et al. <ref type="bibr" target="#b1">[2]</ref> encoded the sparse 3D point clouds with a compact multi-view representation, including bird's eye view and front view as well as RGB image to perform high-accuracy 3D object detection. You et al. <ref type="bibr" target="#b35">[36]</ref> proposed PVNet to integrate both the point cloud and the multi-view data towards joint 3D shape recognition. Although the multi-view representation of point cloud data is reasonable, the project process from 3D to 2D will loss the full utilization of 3D geometric information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Convolution Methods</head><p>Graph structure is a native representation of irregular data, such as 3D point clouds, which offers a compact yet rich representation of contextual relationships between points of different object parts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. Specifically, Bruna et al. <ref type="bibr" target="#b37">[38]</ref> proposed two constructions based on a hierarchical clustering of the domain and the spectrum of the graph Laplacian, to prove that for low-dimensional graphs, it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures. Wang et al. <ref type="bibr" target="#b38">[39]</ref> operated spectral graph convolution on a local graph, combined with a novel graph pooling strategy to augment the relative layout of neighboring points as well as their features. Te et al. <ref type="bibr" target="#b39">[40]</ref> treated features of points in a point cloud as signals on graph, and defined the convolution over graph by Chebyshev polynomial approximation leveraging on spectral graph theory. They also designed a graph-signal smoothness prior in the loss function to regularize the learning process. Although the graph convolutional methods have achieved significant performance, these methods constructed on Laplacian matrix, is computationally complex for Laplacian eigen-decomposition and has a large quantity of parameters to express the convolutional filters while lacks spatial localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Point clouds Methods</head><p>Point clouds are an intuitive, memory-efficient 3D representation which is well-suited for representing geometric details. How to apply deep learning techniques in point clouds directly, simply and efficiently is a critical problem. To address this challenge, Qi et al. <ref type="bibr" target="#b2">[3]</ref> designed a novel type of neural network PointNet that directly consumes point clouds and well respects the permutation invariance of points in the input. More specifically, they solved the disorder problem of the point clouds through max pooling and maintained the rotation invariance through the spatial transformation network STN. The extracted features of each point are the combination of its own information and the global information. PointNet has been proved efficient in many applications ranging from object classification, part segmentation, object detection to scene semantic parsing. However, PointNet only relies on the max-pooling layer to learn global features and does not consider local relationships. Therefore, a series of works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> were developed through investigations of the local context and hierarchical learning structures. Typically, Qi et al. <ref type="bibr" target="#b3">[4]</ref> proposed PointNet++ based on their previous work PointNet, which utilizes pointnet as a local feature extraction module to operate hierarchical feature extraction like CNNs, and finally uses upsampling to generate the final high level features. Li et al. <ref type="bibr" target="#b42">[43]</ref> proposed PointCNN which uses MLP to learn a transformation matrix to solve the disorder problem of point cloud, and then utilizes the introduced x-conv module to perform convolution on the transformed features. This method achieved similar performance as PointNet++.</p><formula xml:id="formula_0">statistics SP N × C N × C 1 × N × C N × C 1 2 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>An illustration of our MSA module. First, we create multiscale semantic association map Ms by statistics with ball query upon all the training 3D scenes. For a point i in the semantic prediction result, we also generate a vector Pcorr(i) indicating the probabilities of different categories about surrounding points with ball query. Then we calculate the similarity between this vector and each line (category) in the Ms and normalize it as a probability vector, the detail of the calculation SP is formulated in equation 7. The final prediction for each point is the fusion of original predict probability and the fine-tuned probability, as formulated in equation 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">3D semantic and instance segmentation</head><p>Recent advances in learning-based techniques have also led to various cutting-edge 3D semantic and instance segmentation approaches <ref type="bibr">[1-8, 19, 20, 26, 40, 43-45]</ref>. Volumetric representation has been adapted by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> to transfer 3D point clouds to regular grids and operate CNNs to extract features. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40</ref>] utilized graph convolutional networks to model the relationships of 3D points which offers a compact yet rich representation of context. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> transfered 3D point clouds into multiple views to sufficiently take advantage of the strong extraction capability of classical CNNs. <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b6">7]</ref> presented more efficient and flexible ways to utilize MLP directly upon point clouds and well respect the permutation invariance of points. <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref> operated segmentation task by designing novel CNNs on point clouds, Huang et al. <ref type="bibr" target="#b7">[8]</ref> and Ye et al. <ref type="bibr" target="#b8">[9]</ref> proposed new approaches by slicing the point clouds and utilizing recurrent neural networks to exploit the inherent contextual features. 3D instance segmentation is a relatively new research area and attracts more and more attention <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Specifically, Lahoud et al. <ref type="bibr" target="#b11">[12]</ref> proposed a network based on 3D voxel grids, which treats the instance segmentation task as multi-task learning problem. The network generates abstract feature embeddings for voxels and estimates instances' centers to learn instance information. Yang et al. <ref type="bibr" target="#b10">[11]</ref> introduced a framework which simultaneously generates 3D bounding boxes and predicts the binary masks for the points within each box in one stage. Recently, Wang et al. <ref type="bibr" target="#b13">[14]</ref> have opened up a framework by jointly operating semantic and instance segmentation in 3D point clouds. Inspired by the proposal mechanism in 2D FasterRcnn <ref type="bibr" target="#b45">[46]</ref>, they proposed similarity matrix indicating the similarity between each pair of points in embedded feature space to predict point grouping proposals, then the network will predict corresponding semantic class for each proposal to generate the final semantic-instance results. Although the similarity matrix is effective and natural to indicate the proposals, it will generate a large and inefficient matrix which suffers from the heavy computation and memory consumes. Some followed proposal methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> were proposed to boost the performance of similar framework while still depended on two-stage procedure and the time-consuming non-maximum suppression algorithm. More recently, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> utilized clustering algorithm to divide points into different objects, which was demonstrated to be more effective and efficient than proposal methods. Nevertheless, they did not consider whether the convergence direction of the training process is coupled with the orientation of clustering algorithm. In addition, different points have various diffculties to be devided into distinct objects, which is rarely considered. In this work, we propose a framework which take this critical problem into consideration and prove that it is significant and effective.  <ref type="figure">Fig. 3</ref>: An illustration of our SPCO module. As shown in the first line, the different points of one object have different difficulties to be distinguished, especially for the points of the joints among different objects. In terms of this problem, we introduce clustering into training procedure and saliently focus on the points that are harder to be distinguished in the clustering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we first introduce the baseline framework of our network which jointly perform semantic and instance segmentation tasks. Then we give the details of our MSA module for semantic segmentation in Sec 3.2, as depicted in <ref type="figure">Figure 2</ref>. Next, we expound our SPCO module for instance segmentation in Sec 3.3, as shown in <ref type="figure">Figure 3</ref>. The whole framework of our method can be seen in <ref type="figure" target="#fig_1">Figure  1</ref>. Finally, the adaptive Water Filling Sampling (WFS) algorithm is explained in details in Sec 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Framework</head><p>As depicted in <ref type="figure" target="#fig_1">Figure 1</ref>, the network without MSA and replaced SPCO with normal clustering is the baseline framework. First, point clouds of size N are encoded into a high-dimensional feature matrix F share ∈ R N ×D by the encoder PointNet++ <ref type="bibr" target="#b3">[4]</ref>. Next, two tasks separately decode F share for their own missions. In the semantic segmentation branch, F share is decoded into the semantic feature matrix Fsem ∈ R N ×D and then outputs the semantic predictions Psem ∈ R N ×C , where C is the semantic class number. The instance segmentation branch decodes F share into the instance feature matrix F ins ∈ R N ×D , which is utilized to predict the per-point instance embeddings E ins ∈ R N ×E , where E denotes the length of the output embedding dimensions. These embeddings are used to calculate the distances among the points for instance clustering.</p><p>During the training process, the semantic branch is supervised by cross entropy loss while the loss function for instance segmentation, inspired by <ref type="bibr" target="#b14">[15]</ref>, is formulate as follows:</p><formula xml:id="formula_1">L base = L in + L out + λLreg<label>(1)</label></formula><p>where the goal of L in is to pull the embeddings toward the mean embedding of the points in the instance, while L out guides the mean embedding of instances to repel each other. We denote Lreg as a regularization term that bounds the embedding values. The three loss terms are denoted as:</p><formula xml:id="formula_2">L in = 1 I I i=1 1 Ni Ni j=1 [||τ i − f j || 1 − ζv] 2 + (2) L out = 1 I(I−1) I ia=1 I i b =1 ia =i b [2ζ d − ||τ ia − τ i b || 1 ] 2 + (3) Lreg = 1 I I i=1 ||τ i || 1<label>(4)</label></formula><p>where I represents the number of ground-truth instances; N i is the number of points in instance i; τ i denotes the mean embedding of instance i; f j is an embedding of a point; ζv and ζ d indicate margins for the variance and distance loss respectively; ia and i b represent different instances; [x] + =max(0, x) is the hinge function; and the l 1 distance is represented by || · || 1 . For inference, we use mean-shift clustering <ref type="bibr" target="#b46">[47]</ref> on the instance embeddings to obtain the final instance labels following <ref type="bibr" target="#b14">[15]</ref> . The mode of the semantic labels for the points within the same instance is assigned as the predicted semantic class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-scale Semantic Association Module</head><p>In 3D semantic segmentation, for a point of an object, the categories of surrounding points are usually related to the category of the point itself, i.e., the spatial distribution of the semantic information has a strong association as the ensample in Sec 1, which can be further exploited. Thus, based on the semantic context information, we propose our Multi-scale Semantic Association (MSA) Module, which can be seen in <ref type="figure">Figure 2</ref>. As shown in <ref type="figure">Figure 2</ref>, on the one hand, we create multi-scale semantic association maps by statistics with ball query upon all the training 3D scenes, Ms ∈ R C×C means the map in scale s, C is the number of class. On the other hand, based on the decoded semantic output feature Psem, we can also generate the probabilities P s corr of the categories from surrounding points with ball query in scale s. Then for each point i in P s corr , we calculate the distance between P s corr (i) and each line in Ms, and transfer the result as a probability vector for this point, where the larger a bit is, the higher the probability for this point belonging to corresponding category is. Note that the MSA module will generate multiple probability vectors because of multiple scales and these probabilities are only come from surrounding points. At last, the original predicted probability vector is added by the multiple probability vectors to get the final prediction. The formula is described as equation <ref type="formula">(5)</ref></p><formula xml:id="formula_3">-(8) Osem = o(argmax(Psem)) (5) P s corr (i) = 1 |B s i | |B s i | j=1 j∈B s i Osem(j) (6) P s surr (i) = φ(1 − σ(||P s corr (i) − Ms|| 2 ))<label>(7)</label></formula><p>P out = Psem + α 1 P s1 surr + α 2 P s2 surr + α 3 P s3 surr <ref type="bibr" target="#b7">(8)</ref> where o means one hot operation, |B s i | means the number of points in the ball query of point i in scale s, Ms means the semantic association map in scale s, σ means normalization and φ means softmax operation. Note that P s corr (i) ∈ R 1×C and Ms ∈ R C×C can be operated with broadcast mechanism, and || · || 2 is operated in axis 1. The final probability output is the sum of Psem and Pcorr in different scales with different coefficients. In our experiment, we set s 1 , s 2 , s 3 equal to radius 0.2, 0.3, 0.5 m and α 1 , α 2 , α 3 equal to 0.5, 0.3, 0.2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Salient Point Clustering Optimization Module</head><p>As explained in the baseline framework, for instance segmentation, the goal of L in is to pull the embeddings toward the mean embedding of the points from the same object, while L out guides the mean embedding of instances to repel each other in the training process. In the inference time, mean shift clustering algorithm is utilized to distinguish points of different objects. However, the coupling between the convergence orientations in training and the clustering orientation in inference is not taken into consideration. In addition, the points from the same object have different difficulties in instance segmentation as the ensample in Sec 1. Thus, in this paper, we propose a Salient Point Clustering Optimization (SPCO) module, which takes mean shift clustering algorithm into the training process and saliently focuses on the points that are harder to be distinguished in the clustering process. More specifically, as shown in <ref type="figure">Figure 3</ref>, mean shift clustering algorithm is operated in training process to simulate the clustering procedure in inference. Then for the points clustered in one instance while are not belonging to this instance according to the ground truth, we generate an additional loss L cluster to repel these embeddings away from the mean embedding of this instance. The loss L cluster is formulated in equation <ref type="formula">(9)</ref>, note that the ID of the clustered instance is decided by the mode of ID in the ground truth, and to converge on a reliable model, we add L cluster into the training process from 10 epochs.</p><formula xml:id="formula_4">L cluster = 1 Nc Nc i=1 1 |W i | |W i | j=1 j∈W i [2ζ d − ||E i j − E i || 1 ] 2 + (9) L ins = L base + L cluster<label>(10)</label></formula><p>where Nc means the number of instances in clustering, |W i | means the number of wrong clustered points in instance i, E i j means the embedding of jth wrong clustered point in instance i and E i means the mean embedding of the correct clustered points in instance i. Equipped with our SPCO module, the network can simulate the clustering procedure in inference more realistically, and pay more attention to the points that are easy to be erroneously clustered, which is significant for improving the performance of instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Water Filling Sampling algorithm</head><p>In indoor scenes, there exists some inherent structures. For example, the space is always surrounded by walls and floors. When we sample point clouds from indoor scenes, points of certain categories will occupy the main proportion, which will cause serious imbalance problem between these main categories and other normal categories, especially for tiny objects. In previous works of points segmentation task, this problem is rarely discussed. Therefore, in this paper, a Water Filling Sampling algorithm is proposed to solve the imbalance problem in indoor scenes, which is adaptive to different category distribution. Specifically, for a point cloud of a scene, we first cut it into blocks along X-Y plane and store corresponding semantic and instance labels for each point in the blocks. In addition, we define an accumulative vector V B ∈ R 1×C to store the block number for each category, and generate a list SemB[i] to indicate which block contains points of category i. If the number of points in a block that belongs to category i is larger than a thresh t, the block index will be contained in SemB[i] and V B[i] will be added by 1. When we accomplished the cutting step, we can get the probabilities of block number for each category from V B. To keep the balance among the categories, we need to sample the same size of blocks from all the blocks with different probabilities. If the original probability of a category is high in the row data, the sample probability should be correspondingly low. To achieve this goal, we gradually add a small probability value δ to the category with the minimum sum of original probability and current sampling probability, until the sum of the total sampling probability values up to 1. The process is likely to fill water to the canyon consisting of original probabilities of all the categories, the details of the algorithm can be formulated as Algorithm 1. As for part segmentation datasets, such as ShapeNet, the algorithm becomes more concise because we can obtain SemB and V B for each object directly and skip the cutting step. Note that because of the characteristic of part segmentation dataset, we perform W F S algorithm on super categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this part, we will compare our method with other SOTA methods in 3D point clouds semantic and instance segmentation tasks to demonstrate that our method is effective and robust on different kind of datasets, including large scale indoor 3D dataset and part segmentation 3D dataset. Cut S i into blocks B i along X-Y plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>In each block, random sample N p points with labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for B i j in all the blocks B i do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>SLab i j ← Separate out corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for c in range [0, N c − 1] do 7:</p><formula xml:id="formula_5">pc = sum(ISLab i j == c) 8:</formula><p>if pc &gt; t then 9: </p><formula xml:id="formula_6">SemB[c] ← SemB[c] extended with [(i, j)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Details</head><p>Datasets. Followed as <ref type="bibr" target="#b14">[15]</ref>, we conduct the experiments on two benchmark datasets: Stanford 3D Indoor Semantics Dataset (S3DIS) <ref type="bibr" target="#b47">[48]</ref> and ShapeNet part segmentation Dataset <ref type="bibr" target="#b48">[49]</ref>. The specific introduction of these datasets is as follows:</p><p>• S3DIS is a real 3D point cloud dataset generated by Matterport Scanners for indoor spaces, which contains 6 areas and 272 rooms. Each point contains 9 dimensions for the input feature including XY Z, RGB and normalized coordinates. For each point, an instance ID and a semantic category ID with 13 classes are annotated. Following <ref type="bibr" target="#b2">[3]</ref>, we split the rooms into 1 m × 1 m overlapped blocks with stride 0.5 m along the X-Y plane and sample 4096 points from each block.</p><p>• ShapeNet dataset is a synthetic scene mesh for part segmentation, which consists of 16881 shape models from 16 categories. Each object is annotated with 2 to 5 parts from 50 different subcategories. We utilize the instance annotations generated by <ref type="bibr" target="#b13">[14]</ref> as the ground-truth labels and we sample 2048 points for each shape during training followed as <ref type="bibr" target="#b2">[3]</ref>. We split the dataset into training and validation followed <ref type="bibr" target="#b14">[15]</ref> and 3-dimensional vector including XY Z is fed into our network as input.</p><p>Details. For instance segmentation, we trained SASO with λ = 0.001. We use five output embeddings following <ref type="bibr" target="#b14">[15]</ref> and set α to 0.01. We select the Adam optimizer to optimize the network on a single GPU (Tesla P100) and set the momentum to 0.9 for the training process. During the inference process, we set the bandwidth to 0.6 for mean-shift clustering and apply the BlockMerging algorithm [14] to merge instances from different blocks.</p><p>Evaluation. Following <ref type="bibr" target="#b14">[15]</ref>, we evaluate the experimental results in the following metrics. For semantic segmentation, we calculate the overall accuracy (oAcc), mean accuracy (mAcc) and mean IoU (mIoU ) across all the semantic classes along with the detailed scores of the per-class IoU . To evaluate the performance of instance segmentation, we use the coverage (Cov) and weighted coverage (W Cov) <ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. Cov is the average instance-wise IoU of the prediction matched with ground truth, and W Cov is the Cov score after being weighted by the size of ground truth. For the predicted regions P and the ground-truth regions G, Cov and W Cov are defined as:</p><formula xml:id="formula_7">Cov(G, P) = |G| i=1 1 |G| max j IoU(r G i , r P j ) (11) WCov(G, P) = |G| i=1 w i max j IoU(r G i , r P j )<label>(12)</label></formula><formula xml:id="formula_8">w i = |r G i | k |r G k |<label>(13)</label></formula><p>where |r G i | is the number of points in ground-truth region i. We also measure the classical metrics of mean precision (mP rec) and mean recall (mRec) with an IoU threshold of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">S3DIS Evaluation</head><p>We conduct the experiments on the S3DIS dataset with the backbone networks PointNet++. We train the network for 50 epochs with a batch size of 12, the initial learning rate is set to 0.001 and divided by 2 every 300 k iterations.</p><p>Quantitative Results. For classical Area5 validation scenes, the quantitative results of SASO in instance and semantic segmentation tasks are shown in <ref type="table">Table 1</ref>. As we can see, SASO achieves 51.9 mW Cov and 59.5 mP rec, which dramatically outperforms the state-of-the-art method 3D-BoNet [11] by 7.3 in mW Cov and 1.9 in mP rec. As for semantic segmentation, our method significantly improves the mAcc and mIoU by 2.6 and 2.1 respectively, compared with advanced ASIS <ref type="bibr" target="#b14">[15]</ref>. For a more comprehensive comparison, we evaluate our method with 6 fold cross validation on S3DIS dataset. As shown in the table, our method achieves 58.3 mW Cov and 72.8 mAcc, which significantly outperforms the stateof-the-art methods by a large margin. The stable improvement in both semantic and instance segmentation demonstrates the effectiveness of our method. For a more detailed comparison with our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Clouds</head><p>Sem.Res Ins.Res Sem.GT Ins.GT <ref type="figure">Fig. 4</ref>: Qualitative results of our method on the S3DIS dataset. For semantic results, each color refers to a particular category and for instance results, different colors represent different objects. baseline framework and ASIS <ref type="bibr" target="#b14">[15]</ref>, <ref type="table">Table 3</ref> shows the results for specific categories in both instance and semantic segmentation based on Area5 scene in S3DIS. Note that for a fair comparison, we reproduce the result of ASIS <ref type="bibr" target="#b14">[15]</ref> with PointNet++ backbone using the author's code to get the per class results.</p><p>Qualitative Results. To intuitively present our results, we visualize the predict results and annotations on point clouds, as shown in <ref type="figure">Figure 4</ref>. For instance segmentation, different colors represent different instances. For semantic segmentation, each color refers to a particular category. It is obvious that our method has a great performance, especially at the boundaries of different objects.</p><p>Ablation Study. The ablation study results are shown in <ref type="table">Table 2</ref>.</p><p>Equipped with different modules of our method upon the baseline framework, we can find that with our SP CO module, we obtain 3.  that the semantic segmentation results are also improved with this module, we think this is because the semantic and instance segmentation tasks share the shallow features, the improvement in the instance segmentation branch can be beneficial to semantic segmentation branch. When we add M SA module to the baseline, we can find that the semantic segmentation results are improved with 1.7 in mAcc and 1.2 in mIoU . With the WFS algorithm added to the baseline framework, we obtain 3.4 gains in mP rec and 1.3 gains in mIoU , which means the balance among different categories is critical to both two tasks. Finally, compared with the baseline framework, our full method has a dramatic improvement in both two tasks, including 7.6 mP rec gains in instance segmentation task and 3.5 mIoU gains in the semantic segmentation task.</p><p>Consumption of memory and time. <ref type="table">Table 4</ref> shows a comparison of the memory cost and computation time. For a fair comparison, we conducted the experiments in the same environment, including the same GPU (GTX 1080), batch size (4) and data (Area5 including 68 rooms). Note that all the time units are minutes, and all the memory units are MB. In the training process, the result is the time and memory required for one epoch. As we can see, our method needs relatively more time for training because we introduce clustering into training process, while costs little memory because of the brief but efficient architecture. In the inference process, the results show the resource consumption for Area5. Our approach takes only 373 MB and needs 40.4 minutes while acquires better performance, which is significantly faster and more efficient than the state-of-the-art methods.  <ref type="bibr" target="#b3">[4]</ref> 84.3 ASIS <ref type="bibr" target="#b14">[15]</ref> 85.0 SGPN <ref type="bibr" target="#b13">[14]</ref> 85.8 SpiderCNN <ref type="bibr" target="#b52">[53]</ref> 85.3 SSCN <ref type="bibr" target="#b5">[6]</ref> 86.0 PointConv <ref type="bibr" target="#b44">[45]</ref> 85.7 BASE 83.5 OURS 86.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ShapeNet Evaluation</head><p>We also validate our method on part segmentation dataset ShapeNet, the semantic annotations are publicly available while the instance segmentation annotations are the generated results as <ref type="bibr" target="#b13">[14]</ref>. Because of the deficiency of ground truth for instance annotations, we only provide the qualitative results for instance segmentation in <ref type="figure" target="#fig_3">Figure 5</ref> as <ref type="bibr" target="#b14">[15]</ref>. Four lines from top to bottom in <ref type="figure" target="#fig_3">Figure 5</ref> mean semantic segmentation results, semantic annotations, instance segmentation results and instance annotations respectively. As we can see, different parts in the same object are well grouped into individual instances, especially the boundaries of different parts. The semantic segmentation results are exhibited in <ref type="table" target="#tab_4">Table 5</ref>. Our approach obviously boosts the result upon baseline framework by 2.9 mIoU and outperforms the state-of-the-art method ASIS <ref type="bibr" target="#b14">[15]</ref>, PointConv <ref type="bibr" target="#b44">[45]</ref> and SSCN <ref type="bibr" target="#b5">[6]</ref>. These results reveal that our proposed method also has the capability to improve the part segmentation performance.</p><p>To prove the effectiveness of our WFS algorithm intuitively, we show the sampling probability and the corresponding improvement for different categories, as depicted in <ref type="figure" target="#fig_4">Figure 6</ref>. In the upper graph (a), the orange color means the original frequency of different categories in the training dataset, the blue color represents the sampling probabilities for different categories. We can find that the distribution of different categories is more balanced with our WFS algorithm. The second graph (b) shows the improvement for different categories, the orange color means positive boost while the purple color represents negative influence. For the categories with low frequency existing in the raw data, the corresponding improvements are obvious, while for the categories with high frequency, the results are rarely influenced. It demonstrates that our WFS algorithm is effective and critical for alleviating the imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel framework which jointly performs semantic and instance segmentation. For the instance segmentation task, a module named SPCO is proposed to introduce clustering into the training process and saliently focus on the points that are harder to be distinguished in the clustering process. For the semantic segmentation branch, we introduce MSA module based on the statistic knowledge to exploit the potential association of spatial semantic distribution. In addition, we propose a Water Filling Sampling algorithm to address the imbalance problem of category distribution. Qualitative and quantitative experiment results on challenging benchmark datasets demonstrate the effectiveness and robustness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Details of Water Filling Sampling algorithm (W F S) Input: Training point clouds of all the scenes S with corresponding semantic-instance labels, and a series of parameters, including threshold t, number of points for each block N p and number of categories N c. Output: All the balanced blocks Blk with corresponding semantic labels SLab and instance labels ILab. initialization: SemB = [[]] * N c,V B = [0] * N c,SP = [0] * N c, SP B = [],B = [],Ω = 0,δ = 0.0001 1: for S i in all the scenes S do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>25 :</head><label>25</label><figDesc>end for 16: Get the original probability OP = V B/sum(V B) 17: while Ω &lt; 1 do 18: idx = argmin(OP ) 19: OP [idx] + = δ 20: SP [idx] + = δ 21: Ω+ = δ 22: end while 23: for c in range [0, N c − 1] do 24: Sc = SP [c] * length(B) Bc ← Random sample Sc block indicates in SemB[c] 26: SP B ← SP B extended with Bc 27: end for 28: B ← B extended with B[SP B] 29: Separate B into Blk, SLab and ILab 30: Return Blk, SLab, ILab</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results for semantic and instance segmentation on ShapeNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The sampling probability and the corresponding improvement for different categories upon ShapeNet dataset. (a). The orange color means the original frequency for different categories in the training dataset, the blue color represents the sampling probabilities for different categories. (b). The orange color means positive boost while the purple color represents negative influence. Note that for an intuitive visualization, the value are multiplied by 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Table 2</head><label>12</label><figDesc>Semantic (green) and instance (red) segmentation results on S3DIS. Ablation study on the S3DIS dataset in Area5.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="6">mCov mWCov mPrec mRec mAcc mIou oAcc</cell></row><row><cell></cell><cell cols="2">SGPN [14]</cell><cell>32.7</cell><cell>35.5</cell><cell cols="3">36.0 28.7 --------</cell></row><row><cell>Area5</cell><cell cols="3">JSIS3D [16] 32.6 3D-BoNet [11] 41.5</cell><cell>35.6 44.6</cell><cell cols="3">39.7 29.1 59.2 51.8 86.9 57.6 40.2 59.2 51.8 86.9</cell></row><row><cell></cell><cell>ASIS [15]</cell><cell></cell><cell>44.6</cell><cell>47.8</cell><cell cols="3">55.3 42.4 60.9 53.4 86.9</cell></row><row><cell></cell><cell>OURS</cell><cell></cell><cell>49.0</cell><cell>51.9</cell><cell cols="3">59.5 45.9 63.5 55.5 87.5</cell></row><row><cell></cell><cell cols="2">SGPN [14]</cell><cell>37.9</cell><cell>40.8</cell><cell cols="3">38.2 31.2 --------</cell></row><row><cell>6-Fold CV</cell><cell cols="3">JSIS3D [16] 37.3 3D-BoNet [11] 48.4</cell><cell>41.0 52.4</cell><cell cols="3">49.5 33.4 59.8 48.5 79.9 65.6 47.6 69.3 59.4 86.3</cell></row><row><cell></cell><cell>ASIS [15]</cell><cell></cell><cell>51.2</cell><cell>55.1</cell><cell cols="3">63.6 47.5 70.1 59.3 86.2</cell></row><row><cell></cell><cell>OURS</cell><cell></cell><cell>54.5</cell><cell>58.3</cell><cell cols="3">64.2 50.8 72.8 61.1 87.0</cell></row><row><cell>SPCO</cell><cell>MSA</cell><cell cols="2">WFS</cell><cell>mWCov</cell><cell>mPrec</cell><cell>mAcc</cell><cell>mIou</cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell></cell><cell>47.1</cell><cell>51.9</cell><cell>59.7</cell><cell>52.0</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell></cell><cell>50.3</cell><cell>56.0</cell><cell>61.6</cell><cell>53.6</cell></row><row><cell>×</cell><cell></cell><cell>×</cell><cell></cell><cell>47.1</cell><cell>51.9</cell><cell>61.4</cell><cell>53.2</cell></row><row><cell>×</cell><cell>×</cell><cell></cell><cell></cell><cell>49.8</cell><cell>55.3</cell><cell>61.2</cell><cell>53.3</cell></row><row><cell></cell><cell></cell><cell>×</cell><cell></cell><cell>50.3</cell><cell>56.0</cell><cell>62.7</cell><cell>54.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.9</cell><cell>59.5</cell><cell>63.5</cell><cell>55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Table 4</head><label>34</label><figDesc>Per class results on the S3DIS dataset. Comparisons of computation time, GPU memory and performance.</figDesc><table><row><cell>Metrics</cell><cell cols="2">Method</cell><cell>mean</cell><cell>ceiling</cell><cell>floor</cell><cell>wall</cell><cell>beam</cell><cell>column</cell><cell>window</cell><cell>door</cell><cell>table</cell><cell>chair</cell><cell>sofa</cell><cell>bookcase</cell><cell>board</cell><cell>clutter</cell></row><row><cell>Wcov</cell><cell cols="2">BASE ASIS  *  [15]</cell><cell>47.1 47.6</cell><cell>89.7 89.0</cell><cell>88.7 89.2</cell><cell>68.3 72.4</cell><cell>0.0 0.0</cell><cell>3.4 8.8</cell><cell>60.9 58.1</cell><cell>5.0 4.7</cell><cell>51.8 52.4</cell><cell>67.6 76.6</cell><cell>23.9 46.3</cell><cell>53.6 50.1</cell><cell>50.3 64.4</cell><cell>49.5 45.5</cell></row><row><cell></cell><cell></cell><cell>OURS</cell><cell>51.9</cell><cell>89.0</cell><cell>87.3</cell><cell>73.1</cell><cell>0.0</cell><cell>9.1</cell><cell>60.1</cell><cell>13.3</cell><cell>54.3</cell><cell>69.8</cell><cell>48.7</cell><cell>55.0</cell><cell>68.1</cell><cell>46.6</cell></row><row><cell>Sem IoU</cell><cell cols="2">BASE ASIS  *  [15]</cell><cell>52.0 53.4</cell><cell>92.8 92.4</cell><cell>97.8 98.4</cell><cell>74.8 76.7</cell><cell>0.0 0.0</cell><cell>7.9 15.6</cell><cell>51.9 49.5</cell><cell>16.1 21.4</cell><cell>72.3 72.3</cell><cell>77.9 78.7</cell><cell>35.4 38.0</cell><cell>56.1 55.9</cell><cell>42.5 45.8</cell><cell>50.8 49.7</cell></row><row><cell></cell><cell></cell><cell>OURS</cell><cell>55.5</cell><cell>92.5</cell><cell>97.7</cell><cell>77.2</cell><cell>0.0</cell><cell>11.7</cell><cell>50.8</cell><cell>29.0</cell><cell>74.2</cell><cell>80.3</cell><cell>41.3</cell><cell>60.0</cell><cell>56.6</cell><cell>50.8</cell></row><row><cell>Method</cell><cell>Metrics</cell><cell cols="5">Train time (m) memory (MB) time (m) memory (MB) Test</cell><cell>mPrec</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SGPN [14]</cell><cell>59.3</cell><cell>7549</cell><cell cols="2">209.5</cell><cell>420</cell><cell>36.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ASIS [15]</cell><cell>64.7</cell><cell>4275</cell><cell>54.2</cell><cell></cell><cell>1235</cell><cell>55.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OURS</cell><cell>75.0</cell><cell>1203</cell><cell>40.4</cell><cell></cell><cell>373</cell><cell>59.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2 gains in mW Cov and 4.1 gains in mP rec. It is interesting</figDesc><table><row><cell>Sem.Res</cell></row><row><cell>Sem.GT</cell></row><row><cell>Ins.Res</cell></row><row><cell>Ins.GT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Semantic segmentation results on ShapeNet datasets.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>PointNet++</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">© The Institution of Engineering and Technology 2015</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">IET Research Journals, pp. 1-8</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">IET Research Journals, pp. 1-8 © The Institution of Engineering and Technology 2015</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV). (</title>
		<meeting>the European Conference on Computer Vision (ECCV). (</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation: 15th European Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part VII</title>
		<meeting>Part VII<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<idno>arXiv:190601140</idno>
		<title level="m">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<idno>arXiv:190608650</idno>
		<title level="m">3d instance segmentation via multi-task metric learning&apos;, arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Masc: Multi-scale affinity with sparse convolution for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<idno>arXiv:190204478</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jsis3d: Joint semanticinstance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3d graph embedding learning with a structureaware loss function for point cloud semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv:190205247</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>arXiv:190402199</idno>
		<title level="m">3d-bevis: Birdseye-view instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sbnet: Sparse blocks network for fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8711" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3-d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A network architecture for point cloud classification via automatic depth images generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rahmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4176" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Snapnet-r: Consistent 3d multi-view semantic labeling for robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>arXiv:13126203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV). (</title>
		<meeting>the European Conference on Computer Vision (ECCV). (</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rgcnn: Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on xtransformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6656" to="6664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Indoor scene parsing with instance segmentation, semantic labeling and support relationship inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5429" to="5437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV). (</title>
		<meeting>the European Conference on Computer Vision (ECCV). (</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
