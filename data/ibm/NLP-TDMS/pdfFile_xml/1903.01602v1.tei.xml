<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">‡ University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country>§ Salesforce Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">‡ University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country>§ Salesforce Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">‡ University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country>§ Salesforce Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">‡ University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country>§ Salesforce Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">‡ University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country>§ Salesforce Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I know I came from there. Where should I go next? My estimated confidence decreased. Something went wrong. Let's learn this lesson and go back. Instruction: Exit the room. Walk past the display case and into the kitchen. Stop by the table. 20% 13% 25% 42% 60% 75% 90% 1 st step 1 st step 2 nd 5 th 5 th step 4 th 6 th 7 th <ref type="figure">Figure 1</ref>: Vision-and-Language Navigation task and our proposed regretful navigation agent. The agent leverages the selfmonitoring mechanism [14] through time to decide when to roll back to a previous location and resume the instructionfollowing task. Our code is available at https://github.com/chihyaoma/regretful-agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>As deep learning continues to make progress for challenging perception tasks, there is increased interest in combining vision, language, and decision-making. Specifically, the Vision and Language Navigation (VLN) task involves navigating to a goal purely from language instructions and visual information without explicit knowledge of the goal. Recent successful approaches have made in-roads in achieving good success rates for this task but rely on beam search, which thoroughly explores a large number of trajectories and is unrealistic for applications such as robotics. In this paper, inspired by the intuition of viewing the problem as search on a navigation graph, we propose to use a progress monitor developed in prior work as a learnable heuristic for search. We then propose two modules incorporated into an end-to-end architecture: 1) A learned mechanism to perform backtracking, which decides whether to continue moving forward or roll back to a previous state (Regret Module) and 2) A mechanism to help the agent decide which direction to go next by showing directions that are visited and their associated progress estimate (Progress * Work partially done while the author was a research intern at Salesforce Research.</p><p>Marker). Combined, the proposed approach significantly outperforms current state-of-the-art methods using greedy action selection, with 5% absolute improvement on the test server in success rates, and more importantly 8% on success rates normalized by the path length.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Vision-and-Language Navigation task and our proposed regretful navigation agent. The agent leverages the selfmonitoring mechanism <ref type="bibr" target="#b13">[14]</ref> through time to decide when to roll back to a previous location and resume the instructionfollowing task. Our code is available at https://github.com/chihyaoma/regretful-agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>As deep learning continues to make progress for challenging perception tasks, there is increased interest in combining vision, language, and decision-making. Specifically, the Vision and Language Navigation (VLN) task involves navigating to a goal purely from language instructions and visual information without explicit knowledge of the goal. Recent successful approaches have made in-roads in achieving good success rates for this task but rely on beam search, which thoroughly explores a large number of trajectories and is unrealistic for applications such as robotics. In this paper, inspired by the intuition of viewing the problem as search on a navigation graph, we propose to use a progress monitor developed in prior work as a learnable heuristic for search. We then propose two modules incorporated into an end-to-end architecture: 1) A learned mechanism to perform backtracking, which decides whether to continue moving forward or roll back to a previous state (Regret Module) and 2) A mechanism to help the agent decide which direction to go next by showing directions that are visited and their associated progress estimate (Progress</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building on the success of deep learning in solving various computer vision tasks, several new tasks and corresponding benchmarks have been proposed to combine visual perception and decision-making <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b4">5]</ref>. One such task is the Vision-and-Language Navigation task (VLN), where an agent must navigate to a goal purely from language instructions and visual input without explicit knowledge of the goal. This task has a number of applications, including service robotics where it would be preferable if humans interacted naturally with the robot by instructing it to perform various tasks.</p><p>Recently, there have been several approaches proposed to solve this task. The dominant approaches frame the navigation task as a sequence to sequence problem <ref type="bibr" target="#b1">[2]</ref>. Several enhancements such as synthetic data augmentation <ref type="bibr" target="#b11">[12]</ref>, pragmatic inference <ref type="bibr" target="#b11">[12]</ref>, and combinations of model-free and model-based reinforcement learning techniques <ref type="bibr" target="#b21">[21]</ref> have also been proposed. However, current methods are separated into two regimes: those that use beam search and obtain good success rate (with longer trajectory lengths) and those that use greedy action selection (and hence result in very low trajectory lengths) but obtain much lower success rates. In fact, there have recently been new metrics proposed that balance these two objectives <ref type="bibr" target="#b0">[1]</ref>. Intuitively, the agent should perform intelligent action selection (akin to best-first search), without exhaustively exploring the search space. For robotics application, for example, the use of beam search is unrealistic as it would require the robot to explore a large number of possible trajectories.</p><p>In this paper, we view the process of navigation as graph search across the navigation graph and employ two strategies, encoded within the neural network architecture, to enable navigation without the use of beam search. Specifically, we develop: 1) A Regret Module that provides a mechanism to allow the agent to learn when to backtrack <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref> and 2) We propose a Progress Marker mechanism that allows the agent to incorporate information from previous visits and reason about such visits and their associated progress estimates towards better action selection.</p><p>Specifically, in graph search a heuristic is used to make meaningful progress towards the goal in a manner that avoids exhaustive search but is more effective than naïve greedy search. We therefore build on recent work <ref type="bibr" target="#b13">[14]</ref> that developed a progress monitor which is a learned mechanism that was used to estimate the progress made towards the goal (with low values meaning progress has not been made and high values meaning the agent is closer to the goal). In that work, however, the focus was on the regularizing effect of the progress monitor as well as its use in beam search. Instead, we use this progress monitor effectively as a learned heuristic that can be used to determine directions that are more likely to lead towards the goal during inference.</p><p>We use the Progress Marker in two ways. First, we leverage the notion of backtracking, which is prevalent in graph search, by developing a learned rollback mechanism that decides whether to go back to the previous location or not (Regret Module). Second, we incorporate a mechanism to allow the agent to use the estimated progress it computed when visiting the viewpoints to choose the next action to perform after it has rolled back (Progress Marker). This allows the agent to know when particular directions have already been visited and the progress they resulted in, which can bias it to not re-visit states unless warranted. We do this by augmenting the visual state vectors with the progress estimates so that the agent can reduce the probability of revisiting such states (again, in a learned manner).</p><p>We demonstrate that these learned mechanisms are superior to greedy decoding. Our agent is able to achieve stateof-the-art results among published works both in terms of success rate (when beam search is not used) and more im-portantly the SPL <ref type="bibr" target="#b0">[1]</ref> metric which incorporates path length, owing to our short trajectory lengths. In summary, our contributions include: 1) A graph search perspective on the instruction-based navigation problem, and use of a learned heuristic in the form of a progress monitor to effectively explore the navigation graph, 2) an end-to-end trainable Regret Module that can learn to decide when to roll back to the previous location given the history of textual and visual grounding observed, 3) a Progress Marker that can enable effective backtracking and reduce the probability of going to a visited location accordingly, and 4) state-of-the-art results on the VLN task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision and language navigation. There are a number of benchmarks and environments for investigating the combination of vision, language, and decision-making. This includes House3D <ref type="bibr" target="#b24">[24]</ref>, Embodied QA <ref type="bibr" target="#b6">[7]</ref>, AI2-THOR <ref type="bibr" target="#b12">[13]</ref>, navigation based agents <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b14">15]</ref> (including with communication <ref type="bibr" target="#b8">[9]</ref>), and the VLN task that we focus on <ref type="bibr" target="#b1">[2]</ref>. For tasks that contain only sparse rewards, reinforcement learning approaches exist <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b7">8]</ref>, for example focusing on language grounding through guided feature transformation <ref type="bibr" target="#b27">[27]</ref> and development of a neural module approach <ref type="bibr" target="#b7">[8]</ref>. Our work, in contrast, focuses on tasks that contain language instructions that can guide the navigation process and has applications such as service robotics. Approaches to this task are dominated by a sequence-tosequence formulation, beginning with initial work introducing the task <ref type="bibr" target="#b1">[2]</ref>. Subsequent methods have used a Speaker-Follower technique to generate synthetic instructions that are used for data augmentation and pragmatic inference <ref type="bibr" target="#b11">[12]</ref>, as well as the combination of supervisedbased and RL-based approaches <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b19">20]</ref>. Recently, the Self-Monitoring navigation agent was introduced which learns to estimate progress made towards the goal using visual and language co-grounding <ref type="bibr" target="#b13">[14]</ref>. Prior work employs beam-search type techniques, though, optimizing for success rate at the expense of trajectory length and reduced applicability to robotics and other domains. Inspired by the latter work, we view the progress monitor as a learned heuristic and combine it with other techniques in graph search, namely backtracking, to use it for action selection, which was not a focus of the prior work.</p><p>Navigation and learned heuristics. Several works in vision and robotics have explored the intersection of learning and planning. In robotics, planning systems must often explore large search trees for getting from start to goal, and selection of the next state to expand must be done intelligently to reduce computation. Often fixed heuristics (e.g. distance to goal) are used, but these are static, require known goal locations, and are used for optimal A*style algorithms rather than greedy best-first search, which is what can be employed on robots when maps are not available <ref type="bibr" target="#b17">[18]</ref>. Recently, several learning-based approaches have been developed for such heuristics, including older works that learn residuals for existing heuristics <ref type="bibr" target="#b26">[26]</ref>, heuristic ranking methods that enable refinement of new ones <ref type="bibr" target="#b23">[23]</ref> as well as learning of a heuristic policy in a Markov Decision Process (MDP) formulation to directly optimize search effort by taking into account history and contextual information <ref type="bibr" target="#b3">[4]</ref>. In our work, we similarly learn to estimate a heuristic (progress monitor) and use it for action selection, showing that the resulting estimates can generalize to unseen environments. We also develop an architecture to explicitly learn when to backtrack based on this progress monitor (with a Progress Marker to reduce the chance of choosing the same action again after backtracking unless warranted), which further improves navigation performance.</p><p>Modern Reinforcement Learning. Modern Reinforcement Learning methods like Asynchronous Advantage Actor Critic (A3C) <ref type="bibr" target="#b15">[16]</ref> or Advantage Actor Critic (A2C) methods are related to the baseline Self-Monitoring agent <ref type="bibr" target="#b13">[14]</ref> and the proposed Regretful agent. Specifically, the progress monitor in the Self-Monitoring agent (our baseline) is similar to the value function in RL, and the difference between progress marker of a viewpoint and current progress estimation (denote as ∆v marker t,k , see Sec. 4.2) is conceptually similar to the advantage function. However, the advantage function in RL serves as a way to regularize and improve the training of the policy network. We instead associate the ∆v marker t,k directly to all navigable states, and this ∆v marker t,k has a direct impact on the agent deciding next action even during inference. While having an accurate value estimate for VLN with dynamic and implicit goals may reduce the need for this formulation, we however believe that this is hardly possible because of the lack of training data. On the other hand, relating to the proposed endto-end learned regret module, Leave no Trace <ref type="bibr" target="#b9">[10]</ref> learns a forward and a reset policy to reset the environment for preventing the policy entering a non-reversible state. Instead of learning to reset, we learn to rollback to a previous state and continue the navigation task with a policy network that learns to decide a better next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Baseline</head><p>Given natural language instructions, our task is to train an agent to follow these instructions and reach an (unspecified) goal in the environment (see <ref type="figure">Figure 1</ref> for an example). This requires processing both the instructions and the visual inputs, along with attentional mechanisms to ground them to the current situation. We adapt the recently introduced Self-Monitoring Visual-Textual Co-grounding agent <ref type="bibr" target="#b13">[14]</ref> as our baseline. The Self-Monitoring agent consists of two primary components: (1) A visual-textual co-grounding module that grounds to the completed instruction, the next instruction, and the subsequent navigable directions represented as visual features. (2) A progress monitor that takes the attention weights of grounded instructions as input and estimates the agent's progress towards completing the instruction. It was shown that such a progress monitor can regularize the attentional mechanism (via an additional loss), but the authors did not focus on using the progress estimates for action selection itself. In the following, we briefly introduce the Self-Monitoring agent.</p><p>Specifically, a language instruction with L words is represented via embeddings denoted as X = x 1 , x 2 , . . . , x L , where x l is the feature vector for the l-th word encoded by a Long Short-Term Memory (LSTM) language encoder. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>, we use a panoramic view as visual input. At the t-th time step, the agent perceives a set of images at each viewpoint v t = v t,1 , v t,2 , ..., v t,K , where K is the maximum number of navigable directions, and v t,k represents the image feature of direction k obtained from an ImageNet pre-trained ResNet-152. It first obtains visual and textual grounded features,v t , andx t , respectively, with hidden states from the last time step h t−1 using soft-attention (see <ref type="bibr" target="#b13">[14]</ref> for details). Conditioned on these grounded features and historical context, it then produces the hidden context of the current step h t :</p><formula xml:id="formula_0">h t , c t = LST M ([x t ,v t , a t−1 ], h t−1 , c t−1 ),</formula><p>where [, ] denotes concatenation and c t−1 denote cell states from the last time step. To further decide where to go next, the current hidden states h t are concatenated with grounded instructionsx t , yielding a representation that contains historical context and relevant parts of the instructions (for example, corresponding to parts that have just been carried out and those that have to be carried out next), to compute the correlations with visual features for each viewpoint k (v t,k ). Formally, action selection is calculated as follows:</p><formula xml:id="formula_1">o t,k = (W a [h t ,x t ]) g(v t,k ) and p t = softmax(o t )</formula><p>where W a are the learned parameters and g(·) is a Multi-Layer Perceptron (MLP).</p><p>Furthermore, we also equip the agent with a progress monitor following <ref type="bibr" target="#b13">[14]</ref> to enforce the attention weights of the textual grounding to align with the progress made toward the goal, further regularizing the grounded instructions to be relevant. The progress monitor is optimized such that the agent is required to use the attention distribution of textual grounding to predict the distance from goal. The output of progress monitor p pm t represents the completeness of instruction-following estimated by the agent.  <ref type="figure">Figure 2</ref>: Illustration of the proposed regretful navigation agent. Note that the progress monitor is based on <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_2">h pm t = σ(W h ([h t−1 ,v t ]) ⊗ tanh(c t )) p pm t = tanh(W pm ([α α α t , h pm t ]))</formula><p>where W h and W pm are the learnt parameters, c t is the cell state of the LSTM, ⊗ denotes the element-wise product, α t is the attention weights of textual grounding, and σ is the sigmoid function. Please refer to <ref type="bibr" target="#b13">[14]</ref> for further details on the baseline architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Regretful Navigation Agent</head><p>The progress monitor previously mentioned reflects the agent's progress made towards the goal, and consequently its outputs will decrease or fluctuate if the agent selects an action leading to deviation from the goal. Conversely it will increase if it moves closer to the goal by completing the instruction. We posit that such a property, while conceptually simple, provides critical feedback for action selection. To this end, we leverage the outputs of the progress monitor to allow the agent to regret and backtrack using a Regret Module and a Progress Marker (see <ref type="figure">Figure.</ref> 2). In particular, the Regret Module examines the progress made from the last step to the current step to decide whether to take a forward or rollback action. Once the agent regrets and rolls back to the previous location, the Progress Marker informs whether location(s) have been visited before and rates the visited location(s) according to the agent's confidence in completing the instruction-following task. Combining the two proposed methods, we show that the agent is able to perform a local search on the navigational graph by <ref type="formula">(1)</ref> assessing the current progress, (2) deciding when to roll back, and (3) selecting the next location after rollback occurs. In the following, we elaborate these two components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Regret Module</head><p>The Regret Module takes in the outputs of the progress monitor at different time steps and decides whether to go forward or to rollback. In particular, we use the concatenation of hidden state h t and grounded instructionx t as our forward embedding m f t , and more importantly we intro-duce a rollback embedding m r t to be the projection of the visual features for the action that leads to the previously visited location. The two vector representations are as follows:</p><formula xml:id="formula_3">m f t = W a [h t ,x t ] and m r t = g(v t,r ),</formula><p>where W a are the learned parameters,x t is the grounded instruction obtained from the textual grounding module, and v t,r is the image feature vector representing a direction that points to the previously visited location.</p><p>To decide whether to go forward or rollback, the Regret Module leverages the difference of the progress monitor outputs between the current time step and the previous time step ∆p pm t = p pm t − p pm t−1 . Intuitively, if the difference is larger than a certain threshold ∆p pm t &gt; σ, the agent should decide to take a forward action, and vice versa. Since it is hard to decide an optimal value for σ, we achieve this by computing attention weights α f r t and perform a weighted sum on both forward and rollback embeddings. If the weight on rollback is larger, the agent is likely to be biased to take an action that leads to the last visited location. Formally, the weights can be computed as:</p><formula xml:id="formula_4">α f r t = softmax(W r (∆p pm t )) m f r t = (α f r t ) [m f t , m r t ]</formula><p>, where W r are the learnt parameters, [, ] denotes concatenation between feature vectors, and m f r t represents the weighted sum of the forward and rollback embeddings. Note that to ensure the progress monitor remains focused on estimating the agent's progress and regularizing the textual grounding module, we detach the output of the progress monitor which is fed into the Regret Module and set it as a leaf in the computational graph.</p><p>Action selection. Similar to existing work, the agent determines which image features from navigable directions have the highest correlation with the movement vector m f r t by computing the inner-product, and the probability of each navigable direction is then computed as:</p><formula xml:id="formula_5">o t,k = (W f r m f r t ) g(v t,k ) and p t = softmax(o t ),</formula><p>where W f r are the learned parameters and p t is the probability distribution over navigable directions at time t. In practice, once the agent takes a rollback action, we block the action that leads to oscillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Progress Marker</head><p>The Regret Module provides a mechanism for the agent to decide when to rollback to a previous location or move forward according to the progress monitor outputs. Once the agent rolls back, it is required to select the next direction to go forward. It is thus essential for the agent to (1) know which directions it has already visited (and rolled back) and</p><p>(2) estimate if the visited locations can lead to a path which completes the given instruction.</p><p>Toward this end, we propose the Progress Marker to mark each visited location with the agent's confidence in completing the instruction (see <ref type="figure" target="#fig_1">Figure 3</ref>). More specifically, we maintain a set of memory M and store the output of the progress monitor associated with each visited location; if the location is not yet visited, the marker will be filled with a value 1:</p><formula xml:id="formula_6">v marker t,k = p pm i , if k leads to a location i ∈ M . 1, otherwise.</formula><p>where i is a unique viewpoint ID for each location. We allow the marker on each location to be updated every time the agent visits it. The marker value on each navigable direction indicates the estimated confidence that a location leads to the goal. We assign a value 1 for unvisited directions to encourage the agent to explore the environment. The navigating probabilities between unvisited directions depend on the action probabilities p t since their marker values are the same.</p><p>Action selection with Progress Marker. During action selection, in addition to the movement vector m f r t that the agent can rely on in deciding which direction to go, we propose to label the marker value to each navigation direction as indications of whether a direction is likely to lead to the goal or to unexplored (and potentially better) paths. To achieve this, we leverage the difference between the current estimated progress and the marker for each navigable direction ∆v marker</p><formula xml:id="formula_7">t,k = p pm t − v marker t,k</formula><p>. We then concatenate it to the visual feature representation for each navigable direction before action selection.</p><formula xml:id="formula_8">v marked t,k = [g(v t,k ), ∆v marker t,k ].</formula><p>The difference ∆v marker t,k indicates the chances of navigable directions leading to the goal and further inform the agent which direction to select. In our design, lower ∆v marker t,k corresponds to higher chance for action selection. For instance, in step 4 in <ref type="figure" target="#fig_1">Figure 3</ref>, the ∆v marker t,k for starting location and the last visited location are 0.08 and -0.02 respectively, whereas an unvisited location will have -0.71, which eventually leads to 0.52 estimated progress.</p><p>When using Progress Marker, the final action selection is formulated as:</p><formula xml:id="formula_9">o t,k = (W f r m f r t ) v marked t,k and p t = softmax(o t )</formula><p>In practice, we tiled the difference n times before concatenating with the projected image feature v t,k in order to account for imbalance. The marker value for the stop action is set to be 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training and Inference</head><p>We train the proposed agent with cross-entropy loss for action selection and Mean Squared Error (MSE) loss for progress monitor. In addition to these losses, we also introduce an additional entropy loss to encourage the agent to explore other actions, such that it is not biased to actions with already very high confidence. The motivation is that, after training an agent for a period of time, the agent starts to overfit and perform fairly well on the training set. As a result, the agent will not learn to properly roll back during training since the majority of the training samples do not require the agent to roll back. Introducing the entropy loss increases the chance of exploration and making incorrect actions during training.</p><formula xml:id="formula_10">L loss = λ action selection T t=1 y nv t log(p t,k ) +(1 − λ) progress monitor T t=1 (y pm t − p pm t ) 2 − β T t=1 K k=1 −p t,k log(p t,k ) entropy loss ,</formula><p>where p t,k is the action probability of each navigable direction, y nv t is the ground-truth navigable direction at step t, λ = 0.5 is the weight balancing the cross-entropy loss and MSE loss, and β = 0.01 is the weight for entropy loss.</p><p>Following existing approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2]</ref>, we perform categorical sampling during training for action selection. During inference, the agent greedily selects the action with highest action probability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dataset and Implementations</head><p>Room-to-Room dataset. We use the Room-to-Room (R2R) dataset <ref type="bibr" target="#b1">[2]</ref> for evaluating our proposed approach. The R2R dataset is built upon the Matterport3D dataset <ref type="bibr" target="#b5">[6]</ref>. It consists of 10,800 panoramic views from 194,400 RGB-D images in 90 buildings and has 7,189 paths sampled from its navigation graphs. Each path has three ground-truth navigation instructions written by humans. The whole dataset has 90 scenes: 61 for training and validation seen, 11 for validation unseen, 18 for test unseen.</p><p>Evaluation metrics. To compare to existing work, we show the same evaluation metrics used in those works: (1) Navigation Error (NE), mean of the shortest path distance in meters between the agent's final position and the goal location. (2) Success Rate (SR), the percentage of final positions less than 3m away from the goal location. (3) Oracle Success Rate (OSR), the success rate if the agent can stop at the closest point to the goal along its trajectory. However, we note the importance of a recently added metric that emphasizes the trade-off between success rate and trajectory length: Success rate weighted by (normalized inverse) Path Length (SPL) <ref type="bibr" target="#b0">[1]</ref>, which incorporates trajectory lengths and is an important consideration for real-world applications such as robotics.</p><p>Implementation Details. For fair comparison with existing work, we use the pre-trained ResNet-152 on Im-ageNet to extract image features. Following the Self-Monitoring <ref type="bibr" target="#b13">[14]</ref> and Speaker-Follower <ref type="bibr" target="#b11">[12]</ref> works, the embedded feature vector for each navigable direction is obtained by concatenating an appearance feature with a 4-d orientation feature [sinφ; cosφ; sinθ; cosθ], where φ and θ are the heading and elevation angles. Please refer to the Appendix for further implementation details. <ref type="bibr" target="#b0">1</ref> Note that both Speaker-Follower <ref type="bibr" target="#b11">[12]</ref> and Self-Monitoring <ref type="bibr" target="#b13">[14]</ref> were originally designed to optimize the success rate (SR) via beam search, and concurrently to our work, RCM <ref type="bibr" target="#b19">[20]</ref> proposed a new setting allowing the agent to explore unseen environments prior to the navigation task via Self-Supervised Imitation Learning (SIL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparison with Prior Art</head><p>We first compare the proposed regretful navigation agent with the state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method achieves significant performance improvement over the existing approaches. We achieved 37% SPL and 48% SR on the validation unseen set and outperformed all existing work. Our best performing model achieves 41% SPL and 50% SR on validation unseen set when trained with the synthetic data from the Speaker <ref type="bibr" target="#b11">[12]</ref>. We demonstrate absolute 8% SPL improvement and 5% SR improvement on the test server over the current state-of-theart method. We can also see that our regretful navigation agent without data augmentation has already outperformed the existing work on both SR and SPL metrics. <ref type="table" target="#tab_2">Table 2</ref> shows an ablation study to analyze the effect of each component. The first thing to note is that our method is significantly better than the Self-Monitoring agent which uses greedy decoding, even though it still has a progress monitor loss (although the progress monitor is not used for action selection). A second interesting point is that when the Progress Marker is available with the features of each navigable direction that have been visited before, but the Regret Module is not available, performance does not increase significantly (44% SR). Note that we also conducted an experiment with another condition, where the progress monitor estimates were attached to the forward embedding, meaning that the network could use that information to improve action selection. That condition again was only able to achieve modest gains (45% SR), compared to our Regret Module which was able to achieve 47% SR (and 48% when the Progress Marker was added). In all, this shows that the key improvement stems from the design of the Regret Module, allowing the agent to intelligently backtrack after making mistakes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Rollback Analysis</head><p>We now further analyze the behavior of the agent to verify that the source of improvement is indeed from the ability to learn when to roll back.</p><p>Does rollback lead to the performance improvement? Our proposed regretful agent relies on the ability to regret and roll back to a previous location, further exploring the unknown environment to increase the success rate. As a sanity check, we manually block all actions leading to rollback for both the state-of-the-art Self-Monitoring agent and our regretful agent 2 . The result is shown in <ref type="table" target="#tab_3">Table 3</ref>. As can be seen, blocking rollback for the Self-Monitoring agent produces mixed results, with worse NE but better metrics such as OSR. The SR, however, is unchanged. On the other hand, blocking rollback for our agent significantly reduces most metrics including NE, SR, and OSR especially on unseen environments. This shows that blocking the ability to learn when to roll back degrades a large source of performance increase, and this is especially true for unseen environments.</p><p>Number of unsuccessful examples reduced. We calculate the total number of unsuccessful examples involves rollback action for both Self-Monitoring and our proposed agent (in percentage). As demonstrated in <ref type="figure" target="#fig_2">Figure 4</ref>, our proposed regretful agent significantly reduces the unsuccessful examples from around 43% to 38%, which correlates to the 4-5% improvement on SR in <ref type="table" target="#tab_1">Table 1</ref> and 2.</p><p>Regretful agent in unfamiliar environments. The key 2 except when there is only one navigable direction to go. to the performance increase of an agent focusing on the rollback ability is not that the agent learns a better textual or visual grounding, but that the agent learns to search especially when it is not certain which direction to go. To demonstrate this, we train both the Self-Monitoring agent and our proposed regretful agent only on synthetic data and test them on the unseen validation set (real data). We expect the regretful agent to outperformed the Self-Monitoring agent across all metrics since our agent is designed to operate in an environment where the agent is likely to be uncertain on action selection. As shown in <ref type="table" target="#tab_4">Table 4</ref>, when trained using only the synthetic data, our method significantly outperformed Self-Monitoring agent. Interestingly, when compared with the Self-Monitoring agent trained with real data, our agent trained with synthetic data is slightly better on ONE, same on OSR, and marginally lower on SR.</p><p>We achieved slightly better performance on oracle metrics since stopping at the correct location is not a hard constrain. This indicates that even though our regretful agent is not yet learned how to properly stop at the goal (due to training on synthetic data only), the chance that it passes/reaches the goal is slightly higher than Self-Monitoring agent trained with real data. Further, when the regretful agent trained with real data, the performance improved across all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Qualitative Results</head><p>Figures 5 show qualitative outputs of our model during successful navigation in unseen environments. In <ref type="figure">Figure 5</ref> (a), the agent made a mistake at the first step, and the estimated progress at the second step slightly decreases. The agent then decides to rollback, after which the progress monitor significantly increases. Finally, the agent stopped correctly as instructed. <ref type="figure">Figure 5 (b)</ref> shows an example where the agent correctly goes up the stairs but incorrectly does it again rather than turning and finding the TV as instructed. Note that the progress monitor increases but only by a small amount; this demonstrates the need for learned mechanisms that can reason about the textual and visual grounding and context, as well as the resulting level of change in progress. In this case the agent then correctly decides to rollback and successfully walked into the TV room. Similarly, in <ref type="figure">Figure 5 (c)</ref>, the agent misses the stairs, resulting in a very small progress increase. The agent decides to rollback as a result. Upon reaching the goal, the agent's progress estimate is 99%. Please refer to the Appendix for the full trajectories and unsuccessful examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have proposed a end-to-end trainable regretful navigation agent for the VLN task. Inspired by the intuition of viewing this task as graph search over the navigation graph, we specifically use a progress monitor as a learned heuristic that can be trained and employed during inference to greedily select the next best action (bestfirst search). The progress monitor incorporates informa-tion from grounded language instructions and visual information, integrated across time with LSTMs. We then propose a Regret Module that is able to learn to decide when to perform backtracking depending on the progress made and state of the agent. Finally, a Progress Marker is used to allow the agent to reason about previous visits and unvisited directions, so that the agent can choose a better navigable direction by reducing action probabilities for visited locations with lower progress estimate.</p><p>The resulting framework is able to achieve state-of-theart success rates compared to existing published methods on the public leaderboard, without using beam search. We show through several extensive analyses that the source of performance improvement is the design of the learned rollback mechanism, that when blocked the performance decreases, and that this learning can occur even on purely synthetic data and generalize to real data. We also demonstrated that the total number of unsuccessful examples involve rollback reduces with our regretful agent. There is a great deal of future work possible, which extends our method. For example, other aspects of graph search can be incorporated such as elements of exploration (e.g. a search space frontier), but in a manner that is more efficient than beam search, can also be investigated. Finally, a combination of goal-driven perception and reinforcement learning would be interesting to explore, as the tasks contain less and less structured information (e.g. embodied QA). <ref type="figure">Figure 5</ref>: Successful regretful agent navigates in unseen environments. (a) The agent made a mistake at the first step, but it was able to roll back to the previous location since the output of the progress monitor was not significantly increased. It then follows the rest of the instruction correctly. (b) The agent is able to correctly follow the instruction at the beginning but made a mistake by walking up the stairs again. The agent realized that the output of the progress monitor is decreased and the next action take a right is not feasible and decides to rollback rollback at step 4. The agent was then able to follow the rest of the instruction and stop with estimated progress 0.95. (c) The agent made a mistake by missing the stairs at step 1. It was however able to decide to rollback at step 2 and moves down stairs as instructed and successfully stops near the bamboo plant with estimated progress 0.99. Please see Appendix for the full trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with Beam Search Methods</head><p>We compare our method using greedy action selection with existing beam search approaches, e.g., Pragmatic Inference in Speaker-Follower <ref type="bibr" target="#b11">[12]</ref> and progressed integrated beam search in Self-Monitoring agent <ref type="bibr" target="#b13">[14]</ref>. We can see in <ref type="table" target="#tab_5">Table 5</ref> that, while beam search methods perform well on success rate (SR), their trajectory lengths are significantly longer, achieving low success rate weighted by Path Length (SPL) scores and therefore are impractical for real-world applications. On the other hand, our proposed method significantly improved both SR and SPL when not using beam search. We show the complete trajectory of the agents successfully deciding when to roll back and reach the goal in unseen environments in <ref type="figure">Figure 6</ref>, 7, 8, and 9.</p><p>In <ref type="figure">Figure 6</ref>, we demonstrate that the agent is capable of performing a local search on the navigation graph. Specifically, from step 0 to step 3, the agent searched two possible directions and decided to move with one particular direction at step 4. Once it reached step 5, the agent decides to continue to move forward, and we observed that the progress estimate significantly increased to 45% at step 7. Interestingly, unlike other examples we have shown, the agent did not decide to roll back despite the progress estimate slightly decreased from 45% to 40%. We reckon that this is one of the advantages of using a learning-based regret module, where a learned and dynamically changing threshold decides when to rollback. Finally, the agent successfully stopped in front of the microwave.</p><p>In <ref type="figure">Figure 7</ref>, the agent is instructed to walk across living room. It is ambiguous since both directions seem like a living room. Our agent first decides to move into the direction that leads to a room with a kitchen and living room. It then decided to roll back with the progress monitor output slightly decreased. The agent then followed the rest of the instruction successfully with the progress monitor steadily increased at each step after that. Finally, the agent decides to stop with the progress estimate 99%.</p><p>In <ref type="figure">Figure 8</ref>, the agent first moved out of the room and walked up the stairs as instructed, but the second set of stairs makes the instruction ambiguous. The agent continued to walk up the stairs for one more step and then decided to go down the stairs at step 4. As the agent decided to turn right at step 6, we can see the progress estimate significantly increased from 51% to 66%. Once the agent entered the TV room, the progress estimate increased again to 82%. Finally, the agent successfully stopped with the progress monitor output 95%.</p><p>In <ref type="figure">Figure 9</ref>, the agent failed to walk down the stairs at step 1. Because of the proposed Regret Module and Progress Marker, the agent was able to discover the correct path to go downstairs. Once walking down, the progress estimate increased to 39% immediately, and as the agent goes further down, the progress estimate reached 98% by the time the agent reached the bottom of the stairs. Finally, the agent decided to wait by the bamboo plant with progress estimate 99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Failed examples</head><p>We have shown how the agent can successfully utilize the rollback mechanism to reach the goal, even though it is not familiar with the environment and likely to be uncertain about some actions it took. Intuitively, the rollback mechanism can increase the chance that the agent reaches the goal as long as the agent can correctly decide when to stop.</p><p>We now discuss two failed examples of our proposed regretful agent in unseen environments that highly resemble the successful examples in terms of the given instruction and ground-truth path. Both examples demonstrate that the agent successfully rolled back to the correct path towards the goal but failed to stop at the goal.</p><p>Specifically, in <ref type="figure">Figure 10</ref>, the agent reaches the room with the white cabinet as instructed but decided to move one step forward. The agent then decided to roll back to the room correctly at step 5. However, this does not help the agent to stop at the goal resulting in a failed run.</p><p>On the other hand, in <ref type="figure">Figure 11</ref>, we can see that the progress estimate at step 5 significantly dropped by 21%, and the agent correctly decided to roll back. The agent then successfully reached the refrigerator but did not stop immediately. It continued to move forward after step 8, resulting in an unsuccessful run.</p><p>Lastly, we discuss a failed example when the agent incorrectly decided when to roll back. In <ref type="figure">Figure 12</ref>, the agent first followed the instruction to go down the hallway and tried to find the second door to turn right. As the agent reached the end of the hallway at step 4, it decided to roll back since there is no available navigable direction that leads to turn right. The agent then decided to go down the hallway again with completely opposite direction. However, the agent decided to roll back again at step 7 with the progress estimate dropped to 18%. Although the agent eventually was able to escape from the hallway leading to the dead end, it ends up unsuccessful. <ref type="figure">Figure 6</ref>: The first part of the instruction walk past the glass doors is ambiguous since there are multiple directions that lead to glass doors, and naturally the agent is confused and uncertain where to go. Our agent is able to perform local search on the navigation graph and decides to roll back multiple times at the beginning of the navigation. At step 6, the agent performs an action turn right. Consequently, the progress estimate at step 7 significantly increased to 45%. Interestingly, the agent continues to move forward even though the progress estimate slightly decreased from step 7 to step 8. We reckon that this as one of the advantage of using a learning-based regret module as opposed to using a hard-coded threshold. The agent then successfully follows the instruction and stops in front of the microwave with progress estimate 89%. <ref type="figure">Figure 7</ref>: The agent first walk across living room, but decides to move into the direction that leads to kitchen and dinning room. At step 1, the agent decides to roll back due to a decreasing of the progress monitor output. The agent then followed the rest of the instruction successfully with the progress monitor steadily increased at each step. Finally, the agent decides to stop with the progress estimate 99%. <ref type="figure">Figure 8</ref>: The agent walked up the stairs as instructed at step 1, but the second set of stairs makes the instruction ambiguous. The agent continues to walk up stairs but soon realized that it needs to go down the stairs and turn right from step 4 -6. When the agent decides to turn right, we can see the progress estimate significantly increased from 51% to 66%. As the agent turned right to the TV room, the progress estimate increased again to 82%. Finally, the agent stops with the progress monitor output 95%. <ref type="figure">Figure 9</ref>: The agent walks down the hall way to the stairs but failed to walk down the stairs at step 1. With a small increase on the progress monitor output, the agent then decides to roll back and take the action to walk down the stairs. Once walking down, we can see the progress estimate increased to 39%, and as the agent goes further down, the progress estimate reached 98% at the bottom of the stairs. Finally, the agent decides to stop near by the bamboo plant with progress estimate 99%. <ref type="figure">Figure 10</ref>: Failed example. The agent starts to navigate through the unseen environment by following the given instruction. It was able to successfully follow the instruction and correctly reach the goal at step 4. The agent then decided to move forward towards the kitchen and correctly decided to roll back to the goal. However, the agent did not stop and continue to explore the environment and eventually stopped a bit further from the goal. <ref type="figure">Figure 11</ref>: The agent correctly followed the first parts of the instruction until step 4, but it decided to move forward towards the hall. At step 5, the agent correctly decided to roll back with the progress estimate decreased from 56% to 35%. The agent was then able to follow the rest of the instruction successfully and reach the refrigerator at step 8. However, the agent did not stop nearby the refrigerator and continued to take another two forward steps. <ref type="figure">Figure 12</ref>: The agent followed the first part of instruction to go down the hallway. As the agent reached the end of the hallway, it was not able to find the second door to turn left. The agent then decided to roll back at step 4 with progress estimate decreased from 65% to 61%. The agent continued to go back towards the hallway but decided to roll back again at step 7. Although the agent was able to correct its errors made at the first few steps and escape from the hallway leading to the dead end, it ends up unsuccessful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Concept of the proposed Progress Marker (red flags). The agent marks each visited location with estimated progress made towards the goal. The changes on the estimated progress determines whether the agent should rollback or forward, and the difference between the current estimated progress and the markers on the next navigable directions helps the agent decide which direction to go.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Percentage of unsuccessful examples involving rollback reduced by our proposed regretful agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Instruction: Exit the room. Walk past the display case and into the kitchen. Stop by the table. 20% 13% 25% 42% 60% 75% 90%</head><label></label><figDesc></figDesc><table><row><cell>1 st step</cell><cell></cell></row><row><cell></cell><cell>4 th</cell><cell>5 th step</cell></row><row><cell></cell><cell>5 th</cell></row><row><cell></cell><cell>1 st step</cell></row><row><cell></cell><cell>2 nd</cell></row><row><cell>7 th</cell><cell>6 th</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state of the arts with greedy decoding for action selections 1 . *: with data augmentation.SR  ↑ OSR ↑ SPL ↑ NE ↓ SR ↑ OSR ↑ SPL ↑ NE ↓ SR ↑ OSR ↑ SPL ↑</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Validation-Seen</cell><cell></cell><cell></cell><cell cols="2">Validation-Unseen</cell><cell></cell><cell></cell><cell cols="2">Test (unseen)</cell><cell></cell></row><row><cell cols="2">Method NE ↓ Random 9.45</cell><cell>0.16</cell><cell>0.21</cell><cell>-</cell><cell>9.23</cell><cell>0.16</cell><cell>0.22</cell><cell>-</cell><cell>9.77</cell><cell>0.13</cell><cell>0.18</cell><cell>0.12</cell></row><row><cell>Student-forcing [2]</cell><cell>6.01</cell><cell>0.39</cell><cell>0.53</cell><cell>-</cell><cell>7.81</cell><cell>0.22</cell><cell>0.28</cell><cell>-</cell><cell>7.85</cell><cell>0.20</cell><cell>0.27</cell><cell>0.18</cell></row><row><cell>RPA [21]</cell><cell>5.56</cell><cell>0.43</cell><cell>0.53</cell><cell>-</cell><cell>7.65</cell><cell>0.25</cell><cell>0.32</cell><cell>-</cell><cell>7.53</cell><cell>0.25</cell><cell>0.33</cell><cell>0.23</cell></row><row><cell>Speaker-Follower [12]*</cell><cell>3.36</cell><cell>0.66</cell><cell>0.74</cell><cell>-</cell><cell>6.62</cell><cell>0.36</cell><cell>0.45</cell><cell>-</cell><cell>6.62</cell><cell>0.35</cell><cell>0.44</cell><cell>0.28</cell></row><row><cell>RCM [20]*</cell><cell>3.37</cell><cell>0.67</cell><cell>0.77</cell><cell>-</cell><cell>5.88</cell><cell>0.43</cell><cell>0.52</cell><cell>-</cell><cell>6.01</cell><cell>0.43</cell><cell>0.51</cell><cell>0.35</cell></row><row><cell>Self-Monitoring [14]*</cell><cell>3.22</cell><cell>0.67</cell><cell>0.78</cell><cell>0.58</cell><cell>5.52</cell><cell>0.45</cell><cell>0.56</cell><cell>0.32</cell><cell>5.99</cell><cell>0.43</cell><cell>0.55</cell><cell>0.32</cell></row><row><cell>Regretful</cell><cell>3.69</cell><cell>0.65</cell><cell>0.72</cell><cell>0.59</cell><cell>5.36</cell><cell>0.48</cell><cell>0.61</cell><cell>0.37</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Regretful*</cell><cell>3.23</cell><cell>0.69</cell><cell>0.77</cell><cell>0.63</cell><cell>5.32</cell><cell>0.50</cell><cell>0.59</cell><cell>0.41</cell><cell>5.69</cell><cell>0.48</cell><cell>0.56</cell><cell>0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study showing the effect of each proposed components compared to the prior arts. All methods here trained without data augmentation. SR ↑ OSR ↑ SPL ↑ NE ↓ SR ↑ OSR ↑ SPL ↑</figDesc><table><row><cell></cell><cell>Regret</cell><cell>Progress</cell><cell cols="2">Validation-Seen</cell><cell></cell><cell></cell><cell cols="2">Validation-Unseen</cell><cell></cell></row><row><cell cols="3">Method NE ↓ Speaker-Follower [12] # Module Marker 4.86</cell><cell>0.52</cell><cell>0.63</cell><cell>-</cell><cell>7.07</cell><cell>0.31</cell><cell>0.41</cell><cell>-</cell></row><row><cell>Self-Monitoring [14]</cell><cell></cell><cell>3.72</cell><cell>0.63</cell><cell>0.75</cell><cell>0.56</cell><cell>5.98</cell><cell>0.44</cell><cell>0.58</cell><cell>0.30</cell></row><row><cell></cell><cell>1</cell><cell>3.88</cell><cell>0.64</cell><cell>0.70</cell><cell>0.58</cell><cell>5.65</cell><cell>0.47</cell><cell>0.59</cell><cell>0.37</cell></row><row><cell>Regretful</cell><cell>2</cell><cell>3.76</cell><cell>0.63</cell><cell>0.73</cell><cell>0.57</cell><cell>5.74</cell><cell>0.44</cell><cell>0.59</cell><cell>0.32</cell></row><row><cell></cell><cell>3</cell><cell>3.69</cell><cell>0.65</cell><cell>0.72</cell><cell>0.59</cell><cell>5.36</cell><cell>0.48</cell><cell>0.61</cell><cell>0.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Sanity check for verifying that the source of performance improvement is from the agent's ability to decide when to roll back. SR ↑ OSR ↑ SPL ↑ NE ↓ SR ↑ OSR ↑ SPL ↑</figDesc><table><row><cell>Blocking</cell><cell></cell><cell cols="2">Validation-Seen</cell><cell></cell><cell></cell><cell cols="2">Validation-Unseen</cell><cell></cell></row><row><cell cols="2">Method Rollback NE ↓ Self-Monitoring [14] 3.72 3.85</cell><cell>0.63 0.64</cell><cell>0.75 0.75</cell><cell>0.56 0.58</cell><cell>5.98 6.02</cell><cell>0.44 0.44</cell><cell>0.58 0.60</cell><cell>0.30 0.34</cell></row><row><cell>Regretful</cell><cell>3.69 3.91</cell><cell>0.65 0.64</cell><cell>0.72 0.68</cell><cell>0.59 0.60</cell><cell>5.36 5.80</cell><cell>0.48 0.46</cell><cell>0.61 0.55</cell><cell>0.37 0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study when trained using only the synthetic or real training data. Oracle Navigation Error (ONE): the navigation error if the agent can stop at the closest point to the goal along its trajectory.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Validation-Unseen</cell></row><row><cell>Method</cell><cell>Synthetic</cell><cell>Real</cell><cell>ONE ↓</cell><cell>SR ↑</cell><cell>OSR ↑</cell></row><row><cell>Self-Monitoring [14]</cell><cell></cell><cell></cell><cell>4.09 3.62</cell><cell>0.35 0.44</cell><cell>0.49 0.58</cell></row><row><cell>Regretful</cell><cell></cell><cell></cell><cell>3.47 3.34</cell><cell>0.41 0.48</cell><cell>0.58 0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our regretful agent using greedy action selection with beam search.</figDesc><table><row><cell></cell><cell>Beam</cell><cell></cell><cell cols="2">Test set (leaderboard)</cell><cell></cell></row><row><cell>Method</cell><cell>search</cell><cell>NE ↓</cell><cell>SR ↑</cell><cell>Length ↓</cell><cell>SPL ↑</cell></row><row><cell>Speaker-Follower [12]</cell><cell></cell><cell>6.62 4.87</cell><cell>0.35 0.53</cell><cell>14.82 1257.38</cell><cell>0.28 0.01</cell></row><row><cell>Self-Monitoring [14]</cell><cell></cell><cell>5.99 4.48</cell><cell>0.43 0.61</cell><cell>17.11 373.09</cell><cell>0.32 0.02</cell></row><row><cell>Regretful</cell><cell></cell><cell>5.69</cell><cell>0.48</cell><cell>13.69</cell><cell>0.40</cell></row><row><cell cols="3">C. Qualitative Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">C.1. Successful examples</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by DARPAs Lifelong Learning Machines (L2M) program, under Cooperative Agreement HR0011-18-2-001. We thank Chia-Jung Hsu for her valuable and artistic suggestions on the figures.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The embedding dimension of the instruction encoder is 256, followed by a dropout layer with ratio 0.5.</p><p>We encode the instruction using a regular LSTM, and the hidden state is 512 dimensional. The MLP g used for projecting the raw image feature is</p><p>The FC layer projects the 2176-d input vector to a 1024-d vector, and the dropout ratio is set to be 0.5. The hidden state of the LSTM which allows integration of information across time is 512. When using the progress marker, the markers are tiled n = 32 times. The dimension of the learnable matrices are:</p><p>without progress marker, and W f r ∈ R 1024×1056 with progress marker.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On evaluation of embodied navigation agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">van den Hengel. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Robot motion planning: A distributed representation approach. The International Journal of Robotics Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barraquand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Latombe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="628" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning heuristic search via imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03034</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Home: A household multimodal environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brodeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Celotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11017</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Modular Control for Embodied Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning (CoRL)</title>
		<meeting>the Conference on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03367</idno>
		<title level="m">Talk the walk: Navigating new york city through grounded dialogue</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leave no trace: Learning to reset for safe and autonomous reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning and executing generalized robot plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="288" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-monitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06066</idno>
		<title level="m">Visual representations for semantic target driven navigation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Artificial intelligence: a modern approach. Malaysia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<idno>2016. 3</idno>
		<imprint>
			<publisher>Pearson Education Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03931</idno>
		<title level="m">Multimodal indoor simulator for navigation in complex environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for visionlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised predictive memory in a goaldirected agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10760</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a heuristic for greedy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Wilt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ruml</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Annual Symposium on Combinatorial Search</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Building generalizable agents with a realistic and rich 3d environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02209</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gibson env: Real-world perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9068" to="9079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative learning of beam-search heuristics for planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2041" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Guided feature transformation (gft): A neural language grounding module for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08329</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
