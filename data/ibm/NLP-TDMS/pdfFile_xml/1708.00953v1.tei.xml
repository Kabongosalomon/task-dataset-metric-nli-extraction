<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
							<email>vishwanath.sindagi@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vishal.m.patel@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixellevel Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.</p><p>Recent CNN-based methods using different multi-scale architectures <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">29]</ref> have achieved significant success in addressing some of the above issues, especially in the high-density complex crowded scenes. However, these methods tend to under-estimate or over-estimate count in the presence of high-density and low-density crowd images, respectively (as shown in <ref type="figure">Fig. 2)</ref>. A potential solution is to use contextual information during the learning process. Several recent works for semantic segmentation [21], scene parsing [51] and visual saliency <ref type="bibr" target="#b52">[52]</ref> have demonstrated that incorporating contextual information can provide significant improvements in the results. Motivated by their success, we believe that availability of global context shall aid the learning process and help us achieve better</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With ubiquitous usage of surveillance cameras and advances in computer vision, crowd scene analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">43]</ref> has gained a lot of interest in the recent years. In this paper, we focus on the task of estimating crowd count and high-quality density maps which has wide applications in video surveillance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">41]</ref>, traffic monitoring, public safety, urban planning <ref type="bibr" target="#b43">[43]</ref>, scene understanding and flow monitoring. Also, the methods developed for crowd counting can be extended to counting tasks in other fields such as cell microscopy <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>, vehicle counting <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">34]</ref>, environmental survey <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">43]</ref>, etc. The task of crowd counting and density estimation has seen a significant progress in the recent years. However, due to the presence of various complexities such as occlusions, high clutter, non-uniform distribution of people, non-uniform illumination, intra-scene and inter-scene variations in appearance, scale and perspective, the resulting accuracies are far from optimal. Figure 2: Average estimation errors across various density levels. Current state-of-the-art method <ref type="bibr" target="#b50">[50]</ref> overestimates/underestimates count in the presence of lowdensity/high-density crowd. count estimation. In addition, existing approaches employ max-pooling layers to achieve minor translation invariance resulting in low-resolution and hence low-quality density maps. Also, to the best of our knowledge, most existing methods concentrate only on the quality of count rather than that of density map. Considering these observations, we propose to incorporate global context into the learning process while improving the quality of density maps.</p><p>To incorporate global context, a CNN-based Global Context Estimator (GCE) is trained to encode the context of an input image that is eventually used to aid the density map estimation process. GCE is a CNN-based on VGG-16 architecture. A Density Map Estimator (DME), which is a multi-column architecture-based CNN with appropriate max-pooling layers, is used to transform the image into high-dimensional feature maps. Furthermore, we believe that use of local context in the image will guide the DME to estimate better quality maps. To this effect, a Local Context Estimator CNN (LCE) is trained on input image patches to encode local context information. Finally, the contextual information obtained by LCE and GCE is combined with the output of DME using a Fusion-CNN (F-CNN). Noting that the use of max-pooling layers in DME results in lowresolution density maps, F-CNN is constructed using a set of fractionally-strided convolutions <ref type="bibr" target="#b21">[22]</ref> to increase the output resolution, thereby generating high-quality maps. In a further attempt to improve the quality of density maps, the F-CNN is trained using a weighted combination of pixelwise Euclidean loss and adversarial loss <ref type="bibr" target="#b9">[10]</ref>. The use of adversarial loss helps us combat the widely acknowledge issue of blurred results obtained by minimizing only the Euclidean loss <ref type="bibr" target="#b12">[13]</ref>.</p><p>The proposed method uses CNN networks to estimate context at various levels for achieving lower count error and better quality density maps. It can be considered as a set of CNNs to estimate pyramid of contexts, hence, the proposed method is dubbed as Contextual Pyramid CNN (CP-CNN).</p><p>To summarize, the following are our main contributions:</p><p>• We propose a novel Contextual Pyramid CNN (CP-CNN) for crowd count and density estimation that encodes local and global context into the density estimation process. • To the best of our knowledge, ours is the first attempt to concentrate on generating high-quality density maps. Also, in contrast to the existing methods, we evaluate the quality of density maps generated by the proposed method using different quality measures such as PSNR/SSIM and report state-of-the-art results. • We use adversarial loss in addition to Euclidean loss for the purpose of crowd density estimation.</p><p>• Extensive experiments are conducted on three highly challenging datasets ( <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b11">12]</ref>) and comparisons are performed against several recent state-of-the-art approaches. Further, an ablation study is conducted to demonstrate the improvements obtained by including contextual information and adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Various approaches have been proposed to tackle the problem of crowd counting in images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50]</ref> and videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b6">7]</ref>. Initial research focussed on detection style <ref type="bibr" target="#b16">[17]</ref> and segmentation framework <ref type="bibr" target="#b35">[35]</ref>. These methods were adversely affected by the presence of occlusions and high clutter in the background. Recent approaches can be broadly categorized into regression-based, density estimation-based and CNN-based methods. We briefly review various methods among these cateogries as follows: Regression-based approaches. To overcome the issues of occlusion and high background clutter, researchers attempted to count by regression where they learn a mapping between features extracted from local image patches to their counts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b5">6]</ref>. These methods have two major components: low-level feature extraction and regression modeling. Using a similar approach, Idrees et al. <ref type="bibr" target="#b11">[12]</ref> fused count from multiple sources such as head detections, texture elements and frequency domain analysis. Density estimation-based approaches. While regressionbased approaches were successful in addressing the issues of occlusion and clutter, they ignored important spatial information as they were regressing on the global count. Lempitsky et al. <ref type="bibr" target="#b15">[16]</ref> introduced a new approach of learning a linear mapping between local patch features and corresponding object density maps using regression. Observing that it is difficult to learn a linear mapping, Pham et al. in <ref type="bibr" target="#b24">[24]</ref> proposed to learn a non-linear mapping between local patch features and density maps using a random forest framework. Many recent approaches have proposed methods based on density map regression <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b40">40]</ref>. A more comprehensive survey of different crowd counting methods The network incorporates global and local context using GCE and LCE respectively. The context maps are concatenated with the output of DME and further processed by F-CNN to estimate high-quality density maps.</p><p>can be found in <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">28]</ref>. CNN-based methods. Recent success of CNN-based methods in classification and recognition tasks has inspired researchers to employ them for the purpose of crowd counting and density estimation <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b30">30]</ref>. Walach et al. <ref type="bibr" target="#b36">[36]</ref> used CNNs with layered training approach. In contrast to the existing patch-based estimation methods, Shang et al. <ref type="bibr" target="#b30">[30]</ref> proposed an end-to-end estimation method using CNNs by simultaneously learning local and global count on the whole sized input images. Zhang et al. <ref type="bibr" target="#b50">[50]</ref> proposed a multi-column architecture to extract features at different scales. Similarly, Onoro-Rubio and López-Sastre in <ref type="bibr" target="#b22">[23]</ref> addressed the scale issue by proposing a scale-aware counting model called Hydra CNN to estimate the object density maps. Boominathan et al. in <ref type="bibr" target="#b0">[1]</ref> proposed to tackle the issue of scale variation using a combination of shallow and deep networks along with an extensive data augmentation by sampling patches from multi-scale image representations. Marsden et al. explored fully convolutional networks <ref type="bibr" target="#b18">[19]</ref> and multi-task learning <ref type="bibr" target="#b19">[20]</ref> for the purpose of crowd counting.</p><p>Inspired by cascaded multi-task learning <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b3">4]</ref>, Sindagi et al. <ref type="bibr" target="#b32">[32]</ref> proposed to learn a high-level prior and perform density estimation in a cascaded setting. In contrast to <ref type="bibr" target="#b32">[32]</ref>, the work in this paper is specifically aimed at reducing overestimation/underestimation of count error by systemically leveraging context in the form of crowd density levels at various levels using different networks. Additionally, we incorporate several elements such as local context and adversarial loss aimed at improving the quality of density maps. Most recently, Sam et al. <ref type="bibr" target="#b29">[29]</ref> proposed a Switching-CNN network that intelligently chooses the most optimal regressor among several independent regressors for a particular input patch. A comprehensive survey of recent cnnbased methods for crowd counting can be found in <ref type="bibr" target="#b33">[33]</ref>. Recent works using multi-scale and multi-column architectures <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">36]</ref> have demonstrated considerable success in achieving lower count errors. We make the following observations regarding these recent state-of-the-art approaches:</p><p>1. These methods do not explicitly incorporate contextual information which is essential for achieving further improvements. 2. Though existing approaches regress on density maps, they are more focussed on improving count errors rather than quality of the density maps, and 3. Existing CNN-based approaches are trained using a pixel-wise Euclidean loss which results in blurred density maps. In view of these observations, we propose a novel method to learn global and local contextual information from images for achieving better count estimates and high-quality density maps. Furthermore, we train the CNNs in a Generative Adversarial Network (GAN) based framework <ref type="bibr" target="#b9">[10]</ref> to exploit the recent success of adversarial loss to achieve highquality and sharper density maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method (CP-CNN)</head><p>The proposed CP-CNN method consists of a pyramid of context estimators and a Fusion-CNN as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. It consists of four modules: GCE, LCE, DME, and F-CNN. GCE and LCE are CNN-based networks that encode global and local context present in the input image respectively. DME is a multi-column CNN that performs the initial task of transforming the input image to high-dimensional feature maps. Finally, F-CNN combines contextual information from GCE and LCE with highdimensional feature maps from DME to produce highresolution and high-quality density maps. These modules are discussed in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Context Estimator (GCE)</head><p>As discussed in Section 1, though recent state-of-the-art multi-column or multi-scale methods <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">36]</ref> achieve significant improvements in the task of crowd count estimation, they either underestimate or overestimate counts in high-density and low-density crowd images respectively (as explained in <ref type="figure">Fig. 2</ref>). We believe it is important to explicilty model context present in the image to reduce the estimation error. To this end, we associate global context with the level of density present in the image by considering the task of learning global context as classifying the input image into five different classes: extremely low-density (ex-lo), lowdensity (lo), medium-density (med), high-density (hi) and extremely high-density (ex-hi). Note that the number of classes required is dependent on the crowd density variation in the dataset. A dataset containing large variations may require higher number of classes. In our experiments, we obtained significant improvements using five categories of density levels.</p><p>In order to learn the classification task, a VGG-16 <ref type="bibr" target="#b31">[31]</ref> based network is fine-tuned with the crowd training data. Network used for GCE is as shown in <ref type="figure">Fig. 4</ref>. The convolutional layers from the VGG-16 network are retained, however, the last three fully connected layers are replaced with a different configuration of fully connected layers in order to cater to our task of classification into five categories. Weights of the last two convolutional layers are finetuned while keeping the weights fixed for the earlier layers. The use of pre-trained VGG network results in faster convergence as well as better performance in terms of context estimation. <ref type="figure">Figure 4</ref>: Global context estimator based on VGG-16 architecture. The network is trained to classify the input images into various density levels thereby encoding the global context present in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Context Estimator (LCE)</head><p>Existing methods for crowd density estimation have primarily focussed on achieving lower count errors rather than estimating better quality density maps. As a result, these methods produce low-quality density maps as shown in <ref type="figure" target="#fig_0">Fig.  1</ref>. After an analysis of these results, we believe that some kind of local contextual information can aid us to achieve better quality maps. To this effect, similar to GCE, we propose to learn an image's local context by learning to classify it's local patches into one of the five classes: {ex-lo, lo, med, hi, ex-hi}. The local context is learned by the LCE whose architecture shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. It is composed of a set of convolutional and max-pooling layers followed by 3 fully connected layers with appropriate drop-out layers after the first two fully connected layers. Every convolutional and fully connected layer is followed by a ReLU layer except for the last fully connected layer which is followed by a sigmoid layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Density Map Estimator (DME)</head><p>The aim of DME is to transform the input image into a set of high-dimensional feature maps which will be concatenated with the contextual information provided by GCE and LCE. Estimating density maps from high-density crowd images is especially challenging due to the presence of heads with varying sizes in and across images. Previous works on multi-scale <ref type="bibr" target="#b22">[23]</ref> or multi-column <ref type="bibr" target="#b50">[50]</ref> architectures have demonstrated abilities to handle the presence of considerably large variations in object sizes by achieving significant improvements in such scenarios. Inspired by the success of these methods, we use a multi-column architecture similar to <ref type="bibr" target="#b50">[50]</ref>. However, notable differences compared to their work are that our columns are much deeper and have different number of filters and filter sizes that are optimized for lower count estimation error. Also, in this work, the multicolumn architecture is used to transform the input into a set of high-dimensional feature map rather than using them directly to estimate the density map. Network details for DME are illustrated in <ref type="figure">Fig. 6</ref>.</p><p>It may be argued that since the DME has a pyramid of filter sizes, one may be able to increase the filter sizes and number of columns to address larger variation in scales. However, note that addition of more columns and the filter sizes will have to be decided based on the scale variation present in the dataset, resulting in new network designs that cater to different datasets containing different scale variations. Additionally, deciding the filter sizes will require time consuming experiments. With our network, the design remains consistent across all datasets, as the context estimators can be considered to perform the task of coarse crowd counting. <ref type="figure">Figure 6</ref>: Density Map Estimator: Inspired by Zhang et al. <ref type="bibr" target="#b50">[50]</ref>, DME is a multi-column architecture. In contrast to <ref type="bibr" target="#b50">[50]</ref>, we use slightly deeper columns with different number of filters and filter sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fusion-CNN (F-CNN)</head><p>The contextual information from GCE and LCE are combined with the high-dimensional feature maps from DME using F-CNN. The F-CNN automatically learns to incorporate the contextual information estimated by context estimators. The presence of max-pooling layers in the DME network (which are essential to achieve translation invariance) results in down-sampled feature maps and loss of details.</p><p>Since, the aim of this work is to estimate high-resolution and high-quality density maps, F-CNN is constructed using a set of convolutional and fractionally-strided convolutional layers. The set of fractionally-strided convolutional layers help us to restore details in the output density maps. The following structure is used for F-CNN: CR(64,9)-CR(32,7)-TR(32)-CR(16,5)-TR(16)-C <ref type="figure" target="#fig_0">(1,1)</ref>, where, C is convolutional layer, R is ReLU layer, T is fractionally-strided convolution layer and the first number inside every brace indicates the number of filters while the second number indicates filter size. Every fractionally-strided convolution layer increases the input resolution by a factor of 2, thereby ensuring that the output resolution is the same as that of input.</p><p>Once the context estimators are trained, DME and F-CNN are trained in an end-to-end fashion. Existing methods for crowd density estimation use Euclidean loss to train their networks. It has been widely acknowledged that minimization of L 2 error results in blurred results especially for image reconstruction tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref>. Motivated by these observations and the recent success of GANs for overcoming the issues of L2-minimization <ref type="bibr" target="#b12">[13]</ref>, we attempt to further improve the quality of density maps by minimizing a weighted combination of pixel-wise Euclidean loss and adversarial loss. The loss for training F-CNN and DME is defined as follows:</p><formula xml:id="formula_0">L T = L E + λ a L A ,<label>(1)</label></formula><formula xml:id="formula_1">L E = 1 W H W w=1 H h=1 φ(X w,h ) − (Y w,h ) 2 ,<label>(2)</label></formula><formula xml:id="formula_2">L A = − log(φ D (φ(X)),<label>(3)</label></formula><p>where, L T is the overall loss, L E is the pixel-wise Euclidean loss between estimated density map and it's corresponding ground truth, λ a is a weighting factor, L A is the adversarial loss, X is the input image of dimensions W × H, Y is the ground truth density map, φ is the network consisting of DME and F-CNN and φ D is the discriminator sub-network for calculating the adversarial loss. Following structure is used for the dis- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and evaluation details</head><p>In this section, we discuss details of the training and evaluation procedures. GCE is trained using the dataset D dme . The corresponding ground truth categories for each image is determined based on the number of people present in it. Note that the images are resized to 224 × 224 before feeding them into the VGG-based GCE network. The network is then trained using the standard cross-entropy loss. LCE is trained using the 64 × 64 patches in D lc . The ground truth categories of the training patches is determined based on the number of people present in them. The network is then trained using the standard cross-entropy loss.</p><p>Next, the DME and F-CNN networks are trained in an end-to-end fashion using input training images from D dme and their corresponding global and local contexts 1 . The global context (F i gc ) for an input training image X i is obtained in the following way. First, an empty global</p><formula xml:id="formula_3">context F i gc of dimension 5 × W i /4 × H i /4 is created, where W i × H i is the dimension of X i .</formula><p>Next, a set of classification scores y i,j gc (j = 1...5) is obtained by feeding X i to GCE. Each feature map in global context F i,j gc is then filled with the corresponding classification score y i,j g . The local context (F i lc ) for X i is obtained in the following way. An empty local context F i lc of dimension 5 × W i × H i is first created. A sliding window classifier (LCE) of size 64 × 64 is run on X i to obtain the classification score y i,j,w lc (j = 1...5) where w is the window location. The classification scores y i,j,w lc are used to fill the corresponding window location w in the respective local context map F i,j gc . F i,j gc is then resized to a size of W i /4 × H i /4. After the context maps are estimated, X i is fed to DME to obtain a high-dimensional feature map F i dme which is concatenated with F i gc and F i lc . These concatenated feature maps are then fed into F-CNN. The two CNNs (DME and F-CNN) are trained in an end-toend fashion by minimizing the weighted combination of pixel-wise Euclidean loss and adversarial loss (given by (1)) between the estimated and ground truth density maps.</p><p>Inference details: Here, we describe the process to estimate the density map of a test image X t i . First, the global context map F i tgc for X t i is calculated in the following way. The test image X t i is divided into non-overlapping blocks of size W t i /4 × H t i /4. All blocks are then fed into GCE to obtain their respective classification scores. As in training, the classification scores are used to build the context maps for each block to obtain the final global context feature map F i tgc . Next, the local context map F i tlc for X t i is <ref type="bibr" target="#b0">1</ref> Once GCE and LCE are trained, their weights are frozen. calculated in the following way: A sliding window classifier (LCE) of size 64 × 64 is run across X t i and the classification scores from every window are used to build the local context F i tlc . Once the context information is obtained, X t i is fed into DME to obtain high-dimensional feature maps F i tdme . F i tdme is concatenated with F i tgc and F i tlc and fed into F-CNN to obtain the output density map. Note that due to additional context processing, inference using the proposed method is computationally expensive as compared to earlier methods such as <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b29">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>In this section, we present the experimental details and evaluation results on three publicly available datasets. First, the results of an ablation study conducted to demonstrate the effects of each module in the architecture is discussed. Along with the ablation study, we also perform a detailed comparison of the proposed method against a recent stateof-the-art-method <ref type="bibr" target="#b50">[50]</ref>. This detailed analysis contains comparison of count metrics defined by (4), along with qualitative and quantitative comparison of the estimated density maps. The quality of density maps is measured using two standard metrics: PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity in Image <ref type="bibr" target="#b39">[39]</ref>). The count error is measured using Mean Absolute Error (MAE) and Mean Squared Error (MSE):</p><formula xml:id="formula_4">M AE = 1 N N i=1 |y i − y i |, M SE = 1 N N i=1 |y i − y i | 2 ,<label>(4)</label></formula><p>where N is number of test samples, y i is the ground truth count and y i is the estimated count corresponding to the i th sample. The ablation study is followed by a discussion and comparison of proposed method's results against several recent state-of-the-art methods on three datasets: Shang-haiTech <ref type="bibr" target="#b50">[50]</ref>, WorldExpo '10 <ref type="bibr" target="#b44">[44]</ref> and UCF CROWD 50 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation study using ShanghaiTech Part A</head><p>In this section, we perform an ablation study to demonstrate the effects of different modules in the proposed method. Each module is added sequentially to the network and results for each configuration are compared. Following four configurations are evaluated: (1) DME: The highdimensional feature maps of DME are combined using 1×1 conv layer whose output is used to estimate the density map. L E loss is minimized to train the network. (2) DME with only GCE and F-CNN: The output of DME is concatenated with the global context. DME and F-CNN are trained to estimate the density maps by minimizing L E loss. (3) DME with GCE, LCE and F-CNN. In addition to the third configuration, local context is also used in this case and the  <ref type="table">Table 1</ref>: Estimation errors for different configurations of the proposed network on ShanghaiTech Part A <ref type="bibr" target="#b50">[50]</ref>. Addition of contextual information and the use of adversarial loss progressively improves the count error and the quality of density maps.</p><p>network is trained using L E loss. (4) DME with GCE, LCE and F-CNN with L A + L E (entire network). These results are compared with a fifth configuration: Zhang et al. <ref type="bibr" target="#b50">[50]</ref> (which is a recent state-of-the-art method) in order to gain a perspective of the improvements achieved by the proposed method and its various modules. The evaluation is performed on Part A of ShanghaiTech <ref type="bibr" target="#b50">[50]</ref> dataset which contains 1198 annotated images with a total of 330,165 people. This dataset consists of two parts: Part A with 482 images and Part B with 716 images. Both parts are further divided into training and test datasets with training set of Part A containing 300 images and that of Part B containing 400 images. Rest of the images are used as test set. Due to the presence of large variations in density, scale and appearance of people across images in the Part A of this dataset, estimating the count with high degree of accuracy is difficult. Hence, this dataset was chosen for the detailed analysis of performance of the proposed architecture.</p><p>Count estimation errors and quality metrics of the estimated density images for the various configurations are tabulated in <ref type="table">Table 1</ref>. We make the following observations: (1) The network architecture for DME used in this work is different from Zhang et al. <ref type="bibr" target="#b50">[50]</ref> in terms of column depths, number of filters and filter sizes. These changes improve the count estimation error as compared to <ref type="bibr" target="#b50">[50]</ref>. However, no significant improvements are observed in the quality of density maps. (2) The use of global context in (DME + GCE + F-CNN) greatly reduces the count error from the previous configurations. Also, the use of F-CNN (which is composed of fractionally-strided convolutional layers), results in considerable improvement in the quality of density maps. (3) The addition of local context and the use of adversarial loss progressively reduces the count error while achieving better quality in terms of PSNR and SSIM.</p><p>Estimated density maps from various configurations on sample input images are shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. It can be observed that the density maps generated using Zhang et al. <ref type="bibr" target="#b50">[50]</ref> and DME (which regress on low-resolution maps) suffer from loss of details. The use of global context information and fractionally-strided convolutional layers results in better estimation quality. Additionally, the use of local context and minimization over a weighted combination of L A and L E further improves the quality and reduces the estimation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluations and comparisons</head><p>In this section, the results of the proposed method are compared against recent state-of-the-art methods on three challenging datasets. ShanghaiTech. The proposed method is evaluated against four recent approaches: Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, MCNN <ref type="bibr" target="#b50">[50]</ref>, Cascaded-MTL <ref type="bibr" target="#b32">[32]</ref> and Switching-CNN <ref type="bibr" target="#b29">[29]</ref> on Part A and Part B of the ShanghaiTech dataset are shown in <ref type="table" target="#tab_2">Table  2</ref>. The authors in <ref type="bibr" target="#b44">[44]</ref> proposed a switchable learning function where they learned their network by alternatively training on two objective functions: crowd count and density estimation. They made use of perspective maps for appropriate ground truth density maps. In another approach, Zhang et al. <ref type="bibr" target="#b50">[50]</ref> proposed a multi-column convolutional network (MCNN) to address scale issues and a sophisticated ground truth density map generation technique. Instead of using the responses of all the columns, Sam et al. <ref type="bibr" target="#b29">[29]</ref> proposed a switching-CNN classifier that chooses the optimal regressor. Sindagi et al. <ref type="bibr" target="#b32">[32]</ref> incorporate high-level prior in the form of crowd density levels and perform a cascaded multi-task learning of estimating prior and density map. It can be observed from <ref type="table" target="#tab_2">Table 2</ref>, that the proposed method is able to achieve superior results as compared to the other methods, which highlights the importance of contextual processing in our framework.   <ref type="bibr" target="#b44">[44]</ref>. Also, similar to <ref type="bibr" target="#b44">[44]</ref>, ROI maps are considered for post processing the output density map generated by the network. The proposed method is evaluated against five recent state-of-the-art approaches: Chen et al. <ref type="bibr" target="#b4">[5]</ref>, Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, MCNN <ref type="bibr" target="#b50">[50]</ref>, Shang et al. <ref type="bibr" target="#b30">[30]</ref> and Switching-CNN <ref type="bibr" target="#b29">[29]</ref> is presented in <ref type="table">Table 3</ref>. The authors in <ref type="bibr" target="#b4">[5]</ref> introduced cumulative attributive concept for learning a regression model for crowd density and age estimation. Shang et al. <ref type="bibr" target="#b30">[30]</ref> proposed an end-to-end CNN architecture consisting of three parts: pre-trained GoogLeNet model for feature generation, long short term memory (LSTM) decoders for local count and fully connected layers for the final count. It can be observed from <ref type="table">Table 3</ref> that the proposed method outperforms existing approaches on an average while achieving comparable performance in individual scene estimations.  <ref type="table">Table 3</ref>: Average estimation errors on the WorldExpo'10 dataset.</p><p>UCF CC 50. The UCF CC 50 is an extremely challenging dataset introduced by Idrees et al. <ref type="bibr" target="#b11">[12]</ref>. The dataset contains 50 annotated images of different resolutions and aspect ratios crawled from the internet. There is a large variation in densities across images. Following the standard protocol discussed in <ref type="bibr" target="#b11">[12]</ref>, a 5-fold cross-validation was performed for evaluating the proposed method. Results are compared with seven recent approaches: Idrees et al. <ref type="bibr" target="#b11">[12]</ref>, Zhang et al. <ref type="bibr" target="#b44">[44]</ref>, MCNN <ref type="bibr" target="#b50">[50]</ref>, Onoro et al. <ref type="bibr" target="#b22">[23]</ref>, Walach et al. <ref type="bibr" target="#b36">[36]</ref>, Cascaded-MTL <ref type="bibr" target="#b32">[32]</ref> and Switching-CNN <ref type="bibr" target="#b29">[29]</ref>. The authors in <ref type="bibr" target="#b11">[12]</ref> proposed to combine information from multiple sources such as head detections, Fourier analysis and texture features (SIFT). Onoro et al. in <ref type="bibr" target="#b22">[23]</ref> proposed a scale-aware CNN to learn a multi-scale non-linear regression model using a pyramid of image patches extracted at multiple scales. Walach et al. <ref type="bibr" target="#b36">[36]</ref> proposed a layered approach of learning CNNs for crowd counting by iteratively adding CNNs where every new CNN is trained on residual error of the previous layer. It can be observed from <ref type="table" target="#tab_5">Table 4</ref> that our network achieves the lowest MAE and MSE count errors. This experiment clearly shows the significance of using context especially in images with widely varying densities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented contextual pyramid of CNNs for incorporating global and local contextual information in an image to generate high-quality crowd density maps and lower count estimation errors. The global and local contexts are obtained by learning to classify the input images and its patches into various density levels. This context information is then fused with the output of a multi-column DME by a Fusion-CNN. In contrast to the existing methods, this work focuses on generating better quality density maps in addition to achieving lower count errors. In this attempt, the Fusion-CNN is constructed with fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion by optimizing a weighted combination of adversarial loss and pixel-wise Euclidean loss. Extensive experiments performed on challenging datasets and comparison with recent state-of-the-art approaches demonstrated the significant improvements achieved by the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Density estimation results. Top Left: Input image (from the ShanghaiTech dataset [50]). Top Right: Ground truth. Bottom Left: Zhang et al. [50] (PSNR: 22.7 dB SSIM: 0.68). Bottom Right: CP-CNN (PSNR: 26.8 dB SSIM: 0.91).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed CP-CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Local context estimator: The network is trained to classify local input patches into various density levels thereby encoding the local context present in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>criminator sub-network: CP(64)-CP(128)-M-CP(256)-M-CP(256)-CP(256)-M-C(1)-Sigmoid, where C represents convolutional layer, P represents PReLU layer and M is max-pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Training details: Let D be the original training dataset. Patches 1/4 th the size of original image are cropped from 100 random locations from every image in D. Other augmentation techniques like horizontal flipping and noise addition are used to create another 200 patches. The random cropping and augmentation resulted in a total of 300 patches per image in the training dataset. Let this set of images be called as D dme . Another training set D lc is formed by cropping patches of size 64 × 64 from 100 random locations in every training image in D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of results from different configurations of the proposed network along with Zhang et al. [50]. Top Row: Sample input images from the ShanghaiTech dataset. Second Row: Ground truth. Third Row: Zhang et al. [50]. (Loss of details can be observed). Fourth Row: DME. Fifth Row: DME + GCE + F-CNN. Sixth Row:DME + GCE + LCE + F-CNN. Bottom Row: DME + GCE + LCE + F-CNN with adversarial loss. Count estimates and the quality of density maps improve after inclusion of contextual information and adversarial loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Estimation errors on the ShanghaiTech dataset.</figDesc><table><row><cell>WorldExpo'10. The WorldExpo'10 dataset was introduced</cell></row><row><cell>by Zhang et al. [44] and it contains 3,980 annotated frames</cell></row><row><cell>from 1,132 video sequences captured by 108 surveillance</cell></row><row><cell>cameras. The frames are divided into training and test sets.</cell></row><row><cell>The training set contains 3,380 frames and the test set con-</cell></row><row><cell>tains 600 frames from five different scenes with 120 frames</cell></row><row><cell>per scene. They also provided Region of Interest (ROI) map</cell></row><row><cell>for each of the five scenes. For a fair comparison, perspec-</cell></row><row><cell>tive maps were used to generate the ground truth maps sim-</cell></row><row><cell>ilar to the work of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Estimation errors on the UCF CC 50 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by US Office of Naval Research (ONR) Grant YIP N00014-16-1-3134.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This section contains some additional results of the proposed method for the three datasets (Shanghai Tech <ref type="bibr" target="#b50">[50]</ref>, UCF CC 50 dataset <ref type="bibr" target="#b11">[12]</ref> and WorldExpo '10 <ref type="bibr" target="#b44">[44]</ref>) on which the evaluations were performed. Results on sample images from these datasets are shown in <ref type="figure">Fig. 8</ref> to <ref type="figure">Fig. 11</ref>. Sample images were chosen carefully to be representative of various density levels present in the respective datasets.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A cascaded convolutional neural network for age estimation of unconstrained faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on BTAS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person count localization in videos from noisy foreground and detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1364" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for counting fish in fisheries surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Needle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference Workshop</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2913" to="2920" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-R</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10118</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowded scene analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="386" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguiness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00220</idno>
		<title level="m">Fully convolutional crowd counting on highly congested scenes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Resnetcrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10698</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2423" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications, 2009. DICTA&apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recent survey on crowd density estimation and counting for visual surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Suandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end crowd counting via joint learning local and global count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1215" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multitask learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<publisher>AVSS</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic flow from a low frame rate city camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Toropov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3802" to="3806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unified crowd segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krahnstoever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="691" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast visual object counting via example-based density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3653" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Block-coordinate frank-wolfe optimization for counting objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatiotemporal modeling for crowd counting in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Crowd density estimation based on rich features and random projection forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Crowd analysis: a survey. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="345" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-style generative network for real-time transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Joint transmission map estimation and dehazing using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding traffic density from large-scale web camera data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fcnrlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
