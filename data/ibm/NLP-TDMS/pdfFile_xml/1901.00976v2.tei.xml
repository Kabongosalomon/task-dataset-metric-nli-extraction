<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Adaptation Network for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<addrLine>2 Google AI, 3 Baidu Research</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
							<email>lujiang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<addrLine>2 Google AI, 3 Baidu Research</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Adaptation Network for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations are only available in the source domain. Previous methods minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance. To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a new metric which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy. We design an alternating update strategy for training CAN in an end-to-end manner. Experiments on two real-world benchmarks Office-31 and VisDA-2017 demonstrate that CAN performs favorably against the state-of-the-art methods and produces more discriminative features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advancements in deep neural networks have successfully improved a variety of learning problems <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. For supervised learning, however, massive labeled training data is still the key to learning an accurate deep model. Although abundant labels may be available for a few pre-specified domains, such as ImageNet <ref type="bibr" target="#b6">[7]</ref>, manual labels often turn out to be difficult or expensive to obtain for every ad-hoc target domain or task. The absence of indomain labeled data hinders the application of data-fitting models in many real-world problems.</p><p>In the absence of labeled data from the target domain, Unsupervised Domain Adaptation (UDA) methods have emerged to mitigate the domain shift in data distributions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref>. It relates to unsupervised learning as it requires manual labels only from the source domain and zero labels from the target domain. Among the recent work on UDA, a seminal line of work proposed by Long et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> aims at minimizing the discrepancy between the source and target domain in the deep neural network, where the domain discrepancy is measured by Maximum <ref type="bibr">Figure 1</ref>. Comparison between previous domain-discrepancy minimization methods and ours. Left: The domain shift exists between the source and target data before adaptation. Middle: Classagnostic adaptation aligns source and target data at the domainlevel, neglecting the class label of the sample, and hence may lead to sub-optimal solutions. Consequently, the target samples of one label may be misaligned with source samples of a different label. Right: Our method performs class-aware alignment across domains. To avoid the misalignment, only the intra-class domain discrepancy is minimized. The inter-class domain discrepancy is maximized to enhance the model's generalization ability.</p><p>Mean Discrepancy (MMD) <ref type="bibr" target="#b21">[22]</ref> and Joint MMD (JMMD) <ref type="bibr" target="#b24">[25]</ref>. MMD and JMMD have proven effective in many computer vision problems and demonstrated the state-of-the-art results on several UDA benchmarks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Despite the success of previous methods based on MMD and JMMD, most of them measure the domain discrepancy at the domain level, neglecting the class from which the samples are drawn. These class-agnostic approaches, hence, do not discriminate whether samples from two domains should be aligned according to their class labels ( <ref type="figure">Fig. 1</ref>). This can impair the adaptation performance due to the following reasons. First, samples of different classes may be aligned incorrectly, e.g. both MMD and JMMD can be minimized even when the target-domain samples are misaligned with the source-domain samples of a different class. Second, the learned decision boundary may generalize poorly for the target domain. There exist many suboptimal solutions near the decision boundary. These solutions may overfit the source data well but are less discriminative for the target.</p><p>To address the above issues, we introduce a new Contrastive Domain Discrepancy (CDD) objective to enable class-aware UDA. We propose to minimize the intra-class discrepancy, i.e. the domain discrepancy within the same class, and maximize the inter-class margin, i.e. the domain discrepancy between different classes. Considering the toy example in <ref type="figure">Fig. 1</ref>, CDD will draw closer the source and target samples of the same underlying class (e.g. the blue and red triangles), while pushing apart the samples from different classes (e.g. the blue triangle and the red star).</p><p>Unfortunately, to estimate and optimize with CDD, we may not train a deep network out-of-the-box as we need to overcome the following two technical issues. First, we need labels from both domains to compute CDD, however, target labels are unknown in UDA. A straightforward way, of course, is to estimate the target labels by the network outputs during training. However, because the estimation can be noisy, we find it can harm the adaptation performance (see Section 4.3). Second, during the mini-batch training, for a class C, the mini-batch may only contain samples from one domain (source or target), rendering it infeasible to estimate the intra-class domain discrepancy of C. This can result in a less efficient adaptation. The above issues require special design of the network and the training paradigm.</p><p>In this paper, we propose Contrastive Adaptation Network (CAN) to facilitate the optimization with CDD. During training, in addition to minimizing the cross-entropy loss on labeled source data, CAN alternatively estimates the underlying label hypothesis of target samples through clustering, and adapts the feature representations according to the CDD metric. After clustering, the ambiguous target data (i.e. far from the cluster centers) and ambiguous classes (i.e. containing few target samples around the cluster centers) are zeroed out in estimating the CDD. Empirically we find that during training, an increasing amount of samples will be taken into account. Such progressive learning can help CAN capture more accurate statistics of data distributions. Moreover, to facilitate the mini-batch training of CAN, we employ the class-aware sampling for both source and target domains, i.e. at each iteration, we sample data from both domains for each class within a randomly sampled class subset. Class-aware sampling can improve the training efficiency and the adaptation performance.</p><p>We validate our method on two public UDA benchmarks: Office-31 <ref type="bibr" target="#b29">[30]</ref> and VisDA-2017 <ref type="bibr" target="#b28">[29]</ref>. The experimental results show that our method performs favorably against the state-of-the-art UDA approaches, i.e. we achieve the best-published result on the Office-31 benchmark and very competitive result on the challenging VisDA-2017 benchmark. Ablation studies are presented to verify the contribution of each key component in our framework.</p><p>In a nutshell, our contributions are as follows,</p><p>• We introduce a new discrepancy metric Contrastive Domain Discrepancy (CDD) to perform class-aware alignment for unsupervised domain adaptation.</p><p>• We propose a network Contrastive Adaptation Network to facilitate the end-to-end training with CDD.</p><p>• Our method achieves the best-published result on the Office-31 benchmark <ref type="bibr" target="#b29">[30]</ref> and competitive performance compared to the state-of-the-art on the challenging VisDA-2017 benchmark <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Class-agnostic domain alignment. A common practice for UDA is to minimize the discrepancy between domains to obtain domain-invariant features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b20">21]</ref>. For example, Tzeng et al. <ref type="bibr" target="#b37">[38]</ref> proposed a kind of domain confusion loss to encourage the network to learn both semantically meaningful and domain invariant representations. Long et al. proposed DAN <ref type="bibr" target="#b21">[22]</ref> and JAN <ref type="bibr" target="#b24">[25]</ref> to minimize the MMD and Joint MMD distance across domains respectively, over the domain-specific layers. Ganin et al. <ref type="bibr" target="#b9">[10]</ref> enabled the network to learn domain invariant representations in adversarial way by back-propagating the reverse gradients of the domain classifier. Unlike these domaindiscrepancy minimization methods, our method performs class-aware domain alignment. Discriminative domain-invariant feature learning. Some previous works pay efforts to learn more disciminative features while performing domain alignment <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. Adversarial Dropout Regularization (ADR) <ref type="bibr" target="#b30">[31]</ref> and Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b31">[32]</ref> were proposed to train a deep neural network in adversarial way to avoid generating non-discriminative features lying in the region near the decision boundary. Similar to us, Long et al. <ref type="bibr" target="#b22">[23]</ref> and Pei et al. <ref type="bibr" target="#b27">[28]</ref> take the class information into account while measuring the domain discrepancy. However, our method differs from theirs mainly in two aspects. Firstly, we explicitly model two types of domain discrepancy, i.e. the intra-class domain discrepancy and the interclass domain discrepancy. The inter-class domain discrepancy, which has been ignored by most previous methods, is proved to be beneficial for enhancing the model adaptation performance. Secondly, in the context of deep neural networks, we treat the training process as an alternative optimization over target label hypothesis and features. Intra-class compactness and inter-class separability modeling. This paper is also related to the work that explicitly models the intra-class compactness and the inter-class separability, e.g. the contrastive loss <ref type="bibr" target="#b11">[12]</ref> and the triplet loss <ref type="bibr" target="#b32">[33]</ref>. These methods have been used in various applications, e.g. face recognition <ref type="bibr" target="#b5">[6]</ref>, person re-identification <ref type="bibr" target="#b15">[16]</ref>, etc. Different from these methods designed for a single domain, our work focuses on adaptation across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Unsupervised Domain Adaptation (UDA) aims at improving the model's generalization performance on target domain by mitigating the domain shift in data distribution of the source and target domain. Formally, given a set of source domain samples S = {(x s 1 , y s 1 ), · · · , (x s Ns , y s Ns )}, and target domain samples T = {x t 1 , · · · , x t Nt }, x s , x t represent the input data, and y s ∈ {0, 1, · · · , M − 1} denote the source data label of M classes. The target data label y t ∈ {0, 1, · · · , M − 1} is unknown. Thus, in UDA, we are interested in training a network using labeled source domain data S and unlabeled target domain data T to make accurate predictions {ŷ t } on T .</p><p>We discuss our method in the context of deep neural networks. In deep neural networks, a sample owns hierarchical features/representations denoted by the activations of each layer l ∈ L. In the following, we use φ l (x) to denote the outputs of layer l in a deep neural network Φ θ for the input x, where φ(·) denotes the mapping defined by the deep neural network from the input to a specific layer.</p><p>In the rest of this section, we start our discussions by briefly reviewing the relevant concepts in MMD in Section 3.1. Section 3.2 introduces a new domain discrepancy metric. Finally, Section 3.3 and Section 3.4 discuss the objective and the training procedure of proposed deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Maximum Mean Discrepancy Revisit</head><p>In Maximum Mean Discrepancy (MMD), {x s i } and {x t i } are i.i.d. sampled from the marginal distributions P (X s ) and Q(X t ) respectively. Based on the observed samples, MMD <ref type="bibr" target="#b33">[34]</ref> performs a kernel two-sample test to determine whether to accept the null hypothesis P = Q or not. MMD is motivated by the fact that if two distributions are identical, all of their statistics should be the same. Formally, MMD defines the difference between two distributions with their mean embeddings in the reproducing kernel Hilbert space (RKHS), i.e.</p><formula xml:id="formula_0">D H (P, Q) sup f ∼H (E X s [f (X s )] − E X t [f (X t )]) H , (1)</formula><p>where H is class of functions.</p><p>In practice, for a layer l, the squared value of MMD is estimated with the empirical kernel mean embeddingŝ</p><formula xml:id="formula_1">D mmd l = 1 n 2 s ns i=1 ns j=1 k l (φ l (x s i ), φ l (x s j )) + 1 n 2 t nt i=1 nt j=1 k l (φ l (x t i ), φ l (x t j )) − 2 n s n t ns i=1 nt j=1 k l (φ l (x s i ), φ l (x t j )),<label>(2)</label></formula><p>where x s ∈ S ⊂ S, x t ∈ T ⊂ T , n s = |S |, n t = |T |. The S and T represent the mini-batch source and target data sampled from S and T respectively. And k l denotes the kernel selected for the l-th layer of deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Domain Discrepancy</head><p>We propose to explicitly take the class information into account and measure the intra-class and inter-class discrepancy across domains. The intra-class domain discrepancy is minimized to compact the feature representations of samples within a class, whereas the inter-class domain discrepancy is maximized to push the representations of each other further away from the decision boundary. The intra-class and inter-class discrepancies are jointly optimized to improve the adaptation performance.</p><p>The proposed Contrastive Domain Discrepancy (CDD) is established on the difference between conditional data distributions across domains. Without any constraint on the type (e.g. marginal or conditional) of data distributions, MMD is convenient to measure such difference be-</p><formula xml:id="formula_2">tween P (φ(X s )|Y s ) and Q(φ(X t )|Y t ), i.e. D H (P, Q) sup f ∼H (E X s [f (φ(X s )|Y s )] − E X t [f (φ(X t )|Y t )]) H . Supposing µ cc (y, y ) = 1 if y = c, y = c ; 0 otherwise. , for</formula><p>two classes c 1 , c 2 (which can be same or different), the kernel mean embedding estimation for squared D H (P, Q) iŝ</p><formula xml:id="formula_3">D c1c2 (ŷ t 1 ,ŷ t 2 , · · · ,ŷ t nt , φ) = e 1 + e 2 − 2e 3<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">e 1 = ns i=1 ns j=1 µ c1c1 (y s i , y s j )k(φ(x s i ), φ(x s j )) ns i=1 ns j=1 µ c1c1 (y s i , y s j ) e 2 = nt i=1 nt j=1 µ c2c2 (ŷ t i ,ŷ t j )k(φ(x t i ), φ(x t j )) nt i=1 nt j=1 µ c2c2 (ŷ t i ,ŷ t j ) e 3 = ns i=1 nt j=1 µ c1c2 (y s i ,ŷ t j )k(φ(x s i ), φ(x t j )) ns i=1 nt j=1 µ c1c2 (y s i ,ŷ t j )</formula><p>.</p><p>Note that Eq. (3) defines two kinds of class-aware domain discrepancy, 1) when c 1 = c 2 = c, it measures intraclass domain discrepancy; 2) when c 1 = c 2 , it becomes the inter-class domain discrepancy. To compute the mask µ c2c2 (ŷ t i ,ŷ t j ) and µ c1c2 (y s i ,ŷ t j ), we need to estimate target labels {ŷ t i }, which will be discussed in Section 3.4. Based on the above definitions, the CDD is calculated as (Theŷ t 1 ,ŷ t 2 , · · · ,ŷ t nt is abbreviated asŷ t 1:nt )</p><formula xml:id="formula_6">D cdd = 1 M M c=1D cc (ŷ t 1:nt , φ) intra − 1 M (M − 1) M c=1 M c =1 c =cD cc (ŷ t 1:nt , φ) inter ,<label>(5)</label></formula><p>where the intra-and inter-class domain discrepancies will be optimized in the opposite direction. Note although the estimation of the labels {ŷ t i } can be noisy, the CDD (which is established on MMD) in itself is robust the the noise to an extent. Because MMD is determined by the mean embeddings of distributions in the RKHS, the sufficient statistics is less likely to be severely affected by the label noise, especially when the amount of data is large. We will discuss and verify this in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contrastive Adaptation Network</head><p>Deep convolutional neural networks (CNNs) is able to learn more transferable features than shallow methods. However, the discrepancy still exists for domain-specific layers. Specifically, the convolutional layers extracting general features are more transferable, while the fullyconnected (FC) layers which exhibit abstract and domainspecific features should be adapted <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In this paper, we start from ImageNet <ref type="bibr" target="#b6">[7]</ref> pretrained networks, e.g. ResNet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and replace the last FC layer with task-specific ones. We follow the general practice that minimizes the domain discrepancy of last FC layers and fine-tunes the convolutional layers through backpropagation. Then our proposed CDD can be readily incorporated into the objective as an adaptation module over the activations of FC layers. We name our network Contrastive Adaptation Network (CAN).</p><p>The overall objective. In a deep CNN, we need to minimize CDD over multiple FC layers, i.e. minimizinĝ</p><formula xml:id="formula_7">D cdd L = L l=1D cdd l .<label>(6)</label></formula><p>Besides, we train the network with labeled source data through minimizing the cross-entropy loss,</p><formula xml:id="formula_8">ce = − 1 n s n s i =1 log P θ (y s i |x s i )<label>(7)</label></formula><p>where y s ∈ {0, 1, · · · , M − 1} is the ground-truth label of sample x s . P θ (y|x) denotes the predicted probability of label y with the network parameterized by θ, given input x. Therefore, the overall objective can be formulated as</p><formula xml:id="formula_9">min θ = ce + βD cdd L<label>(8)</label></formula><p>where β is the weight of the discrepancy penalty term. Through minimizingD cdd L , the intra-class domain discrepancy is minimized and the inter-class domain discrepancy is maximized to perform class-aware domain alignment.</p><p>Note that we independently sample the labeled source data to minimize the cross-entropy loss ce and those to estimate the CDDD cdd L . In this way, we are able to design more efficient sampling strategy (see Section 3.4) to facilitate the mini-batch stochastic optimization with CDD, while not disturbing the conventional optimization with cross-entropy loss on labeled source data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimizing CAN</head><p>The framework of CAN is illustrated in <ref type="figure">Fig. 2</ref>. In this section, we mainly focus on discussing how to minimize CDD loss in CAN.</p><p>Alternative optimization (AO). As shown in Eq. <ref type="formula" target="#formula_6">(5)</ref>, we need to jointly optimize the target label hypothesisŷ t 1:nt and the feature representations φ 1:L . We adopt alternative steps to perform such optimization. In detail, at each loop, given current feature representations, i.e. fixing θ, we update target labels through clustering. Then, based on the updated target labelsŷ t , we estimate and minimize CDD to adapt the features, i.e. update θ through back-propagation.</p><p>We employ the input activations φ 1 (·) of the first taskspecific layer to represent a sample. For example, in ResNet, each sample can be represented as the outputs of the global average pooling layer, which are also the inputs of the following task-specific layer. Then the spherical Kmeans is adopted to perform the clustering of target samples and attach corresponding labels. The number of clusters is the same as the number of underlying classes M . For each class, the target cluster center O tc is initialized as the source cluster center O sc , i.e. O tc ← O sc , where</p><formula xml:id="formula_10">O sc = Ns i=1 1 y s i =c φ1(x s i ) φ1(x s i ) , 1 y s i =c 1 if y s i = c 0 otherwise and c = {0, 1, · · · , M − 1}.</formula><p>For the metric measuring the distance between points a and b in the feature space, we apply the cosine dissimilarity, i.e. dist(a, b) = 1 2 (1 − a,b a b ). Then the clustering process is iteratively 1) attaching labels for each target samples:</p><formula xml:id="formula_11">ŷ t i ← argmin c dist(φ 1 (x t i )</formula><p>, O tc ), and 2) updating the cluster centers:</p><formula xml:id="formula_12">O tc ← Nt i=1 1ŷt i =c φ1(x t i ) φ1(x t i )</formula><p>, till convergence or reaching the maximum clustering steps.</p><p>After clustering, each target sample x t i is assigned a labelŷ t i same as its affiliated clusters. Moreover, ambiguous data, which is far from its affiliated cluster center, is discarded, i.e. we select a subsetT =</p><formula xml:id="formula_13">{(x t ,ŷ t )|dist(φ 1 (x t ), O t(ŷ t ) ) &lt; D 0 , x t ∈ T }, where D 0 ∈ [0, 1] is a constant.</formula><p>Moreover, to give a more accurate estimation of the distribution statistics, we assume that the minimum number of samples inT assigned to each class, should be guaran-  <ref type="figure">Figure 2</ref>. The training process of CAN. To minimize CDD, we perform alternative optimization between updating the target label hypothesis through clustering and adapting feature representations through back-propagation. For the clustering, we apply spherical Kmeans clustering of target samples based on their current feature representations. The number of clusters equal to that of underlying classes and the initial center of each class cluster is set to the center of source data within the same class. Then ambiguous data (i.e. far from the affiliated cluster centers) and ambiguous classes (i.e. containing few target samples around affiliated cluster centers) are discarded. For the feature adaptation, the labeled target samples provided by the clustering stage , together with the labeled source samples, pass through the network to achieve their multi-layer feature representations. The features of domain-specific FC layers are adopted to estimate CDD (Eq. <ref type="formula" target="#formula_6">(5)</ref>). Besides, we apply cross-entropy loss on independently sampled source data. Back-propagating with minimizing CDD and cross-entropy loss (Eq. (8)) adapts the features and provides class-aware alignment. Detailed descriptions can be found in Section 3.4.</p><p>teed. The class which doesn't satisfy such condition will not be considered in current loop, i.e. at loop T e , the selected subset of classes C Te = {c|</p><formula xml:id="formula_14">|T | i 1ŷt i =c &gt; N 0 , c ∈ {0, 1, · · · , M − 1}}, where N 0 is a constant.</formula><p>At the start of training, due to the domain shift, it is more likely to exclude partial classes. However, as training proceeds, more and more classes are included. The reason is two folds: 1) as training proceeds, the model becomes more accurate and 2) benefiting from the CDD penalty, the intraclass domain discrepancy becomes smaller, and the interclass domain discrepancy becomes larger, so that the hard (i.e. ambiguous) classes are able to be taken into account.</p><p>Class-aware Sampling (CAS). In the conventional training of deep neural networks, a mini-batch of data is usually sampled at each iteration without being differentiated by their classes. However, it will be less efficient for computing the CDD. For example, for class C, there may only exist samples from one domain (source or target) in the mini-batch, thus the intra-class discrepancy could not be estimated.</p><p>We propose to use class-aware sampling strategy to enable the efficient update of network with CDD. It is easy to implement. We randomly select a subset of classes C Te from C Te , and then sample source data and target data for each class in C Te . Consequently, in each mini-batch of data during training, we are able to estimate the intra-class dis-crepancy for each selected class.</p><p>Algorithm. Algorithm 1 shows one loop of the AO procedure, i.e. alternating between a clustering phase (Step 1-4), and a K-step network update phase (Step 5-11). The loop of AO is repeated multiple times in our experiments. Because the feature adapting process is relatively slower, we asynchronously update the target labels and the network parameters to make the training more stable and efficient.    <ref type="bibr" target="#b28">[29]</ref> is a challenging testbed for UDA with the domain shift from synthetic data to real imagery. In this paper, we validate our method on its classification task. In total there are ∼280k images from 12 categories. The images are split into three sets, i.e. a training set with 152,397 synthetic images, a validation set with 55,388 real-world images, and a test set with 72,372 real-world images. The gallery of two datasets is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> Baselines: We compare our method with class-agnostic discrepancy minimization methods: RevGrad <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, DAN <ref type="bibr" target="#b21">[22]</ref>, and JAN <ref type="bibr" target="#b24">[25]</ref>. Moreover, we compare our method with the ones which explicitly or implicitly take the class information or decision boundary into consideration to learn more discriminative features: MADA <ref type="bibr" target="#b27">[28]</ref>, MCD <ref type="bibr" target="#b31">[32]</ref>, and ADR <ref type="bibr" target="#b30">[31]</ref>.The descriptions of these methods can be found in Section 2. We implement DAN and JAN using the released code 1 . For a comparison under optimal parameter setting, we cite the performance of MADA, RevGrad, MCD and ADR reported in their corresponding papers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. Implementation details: We use ResNet-50 and ResNet-101 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> pretrained on ImageNet <ref type="bibr" target="#b6">[7]</ref> as our backbone networks. We replace the last FC layer with the taskspecific FC layer, and finetune the model with labeled source domain data and unlabeled target domain data. All 1 https://github.com/thuml/Xlearn the network parameters are shared between the source domain and target domain data other than those of the batch normalization layers which are domain-specific. The hyperparameters are selected following the same protocol as described in <ref type="bibr" target="#b21">[22]</ref>, i.e. we train a domain classifier and perform selection on a validation set (of labeled source samples and unlabeled target samples) by jointly evaluating the test errors of the source classifier and the domain classifier. We use mini-batch stochastic gradient descent (SGD) with momentum of 0.9 to train the network. We follow the same learning rate schedule as described in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>, i.e. the learning rate η p is adjusted following η p = η0 (1+ap) b , where p linearly increases from 0 to 1. The η 0 is the initial learning rate, i.e. 0.001 for the convolutional layers and 0.01 for the task-specific FC layer. For Office-31, a = 10 and b = 0.75, while for VisDA-2017, a = 10 and b = 2.25. The β selected is 0.3. The thresholds (D 0 , N 0 ) are set to (0.05, 3) for Office-31 tasks A→W and A→D. And we don't filter target samples and classes for other tasks during training. <ref type="table" target="#tab_0">Table 1</ref> shows the classification accuracy on six tasks of Office-31. All domain adaptation methods yield notable improvement over the ResNet model (first row) which is fine-tuned on labeled source data only. CAN outperforms other baseline methods across all tasks, achieving the stateof-the-art performance. On average, it boosts the accuracy of JAN by a absolute 6.3% and that of MADA by 5.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state-of-the-art</head><p>We visualize the distribution of learned features by t-SNE <ref type="bibr" target="#b26">[27]</ref>. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates a representative task W → A. Compared to JAN, as expected, the target data representations learned by CAN demonstrate higher intra-class compactness and much larger inter-class margin. This suggests that our CDD produces more discriminative features for the target domain and substantiates our improvement in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table" target="#tab_2">Table 2</ref> lists the accuracy over 12 classes on VisDA-2017 with the validation set as the target domain. Our method outperforms the other baseline methods. The mean accuracy of our model (87.2%) outperforms the self-ensembling (SE) method <ref type="bibr" target="#b8">[9]</ref> (84.3%) which wins the first place in the VisDA-2017 competition, by 2.9%. It is worth noting that SE mainly deals with UDA by ensemble and data augmentation, which is orthogonal to the topic of this paper and thus can be easily combined to boost the performance further.</p><p>Moreover, we also perform adaptation on the VisDA-2017 test set (as the target domain), and submit our predictions to official evaluation server. Our goal is to evaluate the effectiveness of our proposed technique based on a vanilla backbone (ResNet-101). We choose not to use ensemble or additional data augmentation which is commonly used to boost the performance in the competition. Anyhow, our single model achieves a very competitive accuracy of 87.4%, which is comparable to the method which ranks at  <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Our methods named "intra only" and "CAN" are trained with intra-class domain discrepancy and contrastive domain discrepancy, respectively. the second place on the leaderboard (87.7%). From <ref type="table" target="#tab_0">Table 1</ref> and 2, we have two observations: 1) Taking class information/decision boundary into account is beneficial for the adaptation. It can be seen that MADA, MCD, ADR and our method achieve better performance than class-agnostic methods, e.g. RevGrad, DAN, JAN, etc.</p><formula xml:id="formula_15">Method A → W D → W W → D A → D D → A W → A</formula><p>2) Our way of exploiting class information is more effective. We achieve better accuracy than MADA (+5.4%), ADR (+12.4%), and MCD (+15.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>Effect of inter-class domain discrepancy. We compare our method ("CAN") with that trained using intra-class discrepancy only ("intra only"), to verify the merits of intro-ducing inter-class domain discrepancy measure. The results are shown in the last two rows in <ref type="table" target="#tab_0">Table 1</ref> and 2. It can be seen that introducing the inter-class domain discrepancy improves the adaptation performance. We believe the reason is that it is impossible to completely eliminate the intra-class domain discrepancy, maximizing the inter-class domain discrepancy may alleviate the possibility of the model overfitting to the source data and benefits the adaptation.</p><p>Effect of alternative optimization and class-aware sampling. <ref type="table">Table 3</ref> examines two key components of CAN, i.e. alternative optimization (or "AO"), and classaware sampling (or "CAS"). We perform ablation study by leaving-one-component-out of our framework at a time. In <ref type="table">Table 3</ref>, the method "w/o. AO" directly employs the outputs of the network at each iteration as pseudo target labels to estimate CDD and back-propagates to update the network. It can be regarded as updating the feature representations and pseudo target labels simultaneously. The method "w/o. CAS" uses conventional class-agnostic sampling instead of CAS. The comparisons to these two special cases verify the contributions of AO and CAS in our method.</p><p>Interestingly, even without alternative optimization, the method "w/o. AO" improves over class-agnostic methods, e.g. DAN, JAN, etc. This suggests our proposed CDD in itself is robust to the label noise to some extent, and MMD is a suitable metric to establish CDD (see Section 3.2).   <ref type="table">Table 4</ref>. Comparison with different ways of utilizing pseudo target labels.The "pseudo0" means training with pseudo target labels (achieved by our initial clustering) directly. The "pseudo1" is to alternatively update target labels through clustering and minimize the cross-entropy loss on pseudo labeled target data. In "pseudo1", the cross-entropy loss on source data is also minimized.</p><p>Ways of using pseudo target labels. The estimates for the target labels can be achieved through clustering, which enables various ways to train a model. In <ref type="table">Table 4</ref>, we compare our method with two different ways of training with pseudo target labels achieved by the clustering. One way ("pseudo 0 ") is to fix these pseudo labels to train a model directly. The other ("pseudo 1 ") is to update the pseudo target labels during training, which is the same as CAN, but to train the model based on the cross-entropy loss over pseudo labeled target data rather than estimating the CDD.</p><p>As shown in <ref type="table">Table 4</ref>, "pseudo 0 " leads to a model whose accuracy exactly matches with that of the initial clustering, due to the large capacity of deep neural networks. The "pseudo 1 " achieves significantly better results than "pseudo 0 ", but is still worse than our CAN, which verifies that our way of explicitly modeling the class-aware domain discrepancy makes the model better adapted and less likely to be affected by the label noise.</p><p>CDD value during training. In our training, we generate target label hypothesis to estimate CDD. We expect that the underlying metric computed with the ground-truth target labels would decrease steadily during training until convergence. To do so, during training, we evaluate the ground-truth CDD (denoted by CDD-G) for JAN and CAN with the ground-truth target labels. The trend of CDD and the test accuracy during training are plotted in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>As we see, for JAN (the blue curve), the ground-truth CDD rapidly becomes stable at a high level after a short decrease. This indicates that JAN cannot minimize the contrastive domain discrepancy effectively. For CAN (the red curve), although we can only estimate the CDD using inaccurate target label hypothesis, its CDD value steadily decreases along training. The result illustrates our estimation works as a good proxy of ground-truth contrastive domain discrepancy. And from the accuracy curve illustrated in <ref type="figure" target="#fig_3">Fig.  5</ref>, we see that minimizing CDD leads to notable accuracy improvement of CAN, compared to JAN.</p><p>Hyper-parameter sensitivity. We study the sensitivity of CAN to the important balance weight β on two example tasks A → D and D → A in <ref type="figure" target="#fig_3">Fig. 5</ref>. Generally, our model is less sensitive to the change of β. In a vast range, the performance of CAN outperforms the baseline method with a large margin (the blue dashed curve). As the β gets larger, the accuracy steadily increases before decreasing. The bellshaped curve illustrates the regularization effect of CDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed Contrastive Adaptation Network to perform class-aware alignment for UDA. The intraclass and inter-class domain discrepancy are explicitly modeled and optimized through end-to-end mini-batch training. Experiments on real-world benchmarks demonstrate the superiority of our model compared with the strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The gallery of Office-31 and VisDA-2017 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization with t-SNE for different adaptation methods (bested viewed in color). Left: t-SNE of JAN. Right: CAN. The input activations of the last FC layer are used for the computation of t-SNE. The results are on Office-31 task W → A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>(a-b) The curve of CDD and accuracy during training on task A → D of the Office-31 dataset. The "CDD-G" denotes the contrastive domain discrepancy computed with ground-truth target labels. (c-d) The sensitivity of accuracy of CAN to β. The results for A → D (Left) and D → A (Right) are illustrated as examples. The trends for other tasks are similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Optimization of CAN at loop T e . Forward S and compute the M cluster centers O sc ; 2 Initialize O tc : O tc ← O sc ; 3 Cluster target samples T using spherical K-means; 4 Filter the ambiguous target samples and classes; 5 for (k ← 1; k ≤ K; k ← k + 1) do Class-aware sampling based on C Te ,T , and S; Sample from S and compute ce using Eq. (7);</figDesc><table><row><cell>Amazon</cell><cell>Dslr</cell><cell>Webcam</cell><cell>Synthetic</cell><cell>Real</cell></row><row><cell></cell><cell>Office-31</cell><cell></cell><cell>VisDA-2017</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>source data: S = {(x s 1 , y s 1 ), · · · , (x s Ns , y s Ns )}, target data: T = {x t 1 , · · · , x t Nt }</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Procedure:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 7</cell><cell>ComputeD cdd L using Eq. (6);</cell></row></table><note>689 Back-propagate with the objective (Eq.(8));10 Update network parameters θ.11 end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>± 0.2 96.7 ± 0.1 99.3 ± 0.1 68.9 ± 0.2 62.5 ± 0.3 60.7 ± 0.3 76.1 RevGrad [10, 11] 82.0 ± 0.4 96.9 ± 0.2 99.1 ± 0.1 79.7 ± 0.4 68.2 ± 0.4 67.4 ± 0.5 82.2 DAN [22] 80.5 ± 0.4 97.1 ± 0.2 99.6 ± 0.1 78.6 ± 0.2 63.6 ± 0.3 62.8 ± 0.2 80.4 JAN [25] 85.4 ± 0.3 97.4 ± 0.2 99.8 ± 0.2 84.7 ± 0.3 68.6 ± 0.3 70.0 ± 0.4 84.3 MADA [28] 90.0 ± 0.2 97.4 ± 0.1 99.6 ± 0.1 87.8 ± 0.2 70.3 ± 0.3 66.4 ± 0.3</figDesc><table><row><cell>Average</cell></row></table><note>. Classification accuracy (%) on the VisDA-2017 validation set based on ResNet-101</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>± 1.6 92.5 ± 0.4 75.7 ± 0.2 75.3 ± 0.6 83.4 CAN 94.5 ± 0.3 95.0 ± 0.3 78.0 ± 0.3 77.0 ± 0.3 86.1</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="3">w/o. AO w/o. CAS CAN</cell><cell></cell></row><row><cell cols="2">Office-31</cell><cell>88.1</cell><cell>89.1</cell><cell>90.6</cell><cell></cell></row><row><cell cols="2">VisDA-2017</cell><cell>77.5</cell><cell>81.6</cell><cell>87.2</cell><cell></cell></row><row><cell cols="6">Table 3. The effect of alternative optimization (AO) and CAS. The</cell></row><row><cell cols="6">mean accuracy over six tasks on Office-31 and the mean accuracy</cell></row><row><cell cols="6">over 12 classes on VisDA-2017 validation set are reported.</cell></row><row><cell cols="2">Method A → W</cell><cell>A → D</cell><cell>D → A</cell><cell cols="2">W → A Average</cell></row><row><cell>pseudo0</cell><cell>85.8</cell><cell>86.3</cell><cell>74.9</cell><cell>72.3</cell><cell>79.8</cell></row><row><cell cols="2">pseudo1 90.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00340. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation/herein. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S.Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A dasvm classification technique and a circular validation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-ensembling for domain adaptation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Shakeout: A new approach to regularized deep neural network training. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1245" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">Visda: The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarial dropout regularization. ICLR</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and rkhs-based statistics in hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2110" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2653" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
