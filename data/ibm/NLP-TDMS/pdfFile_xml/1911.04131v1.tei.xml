<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
							<email>wei.peng@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
							<email>xiaopeng.hong@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Electronic and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country>PRC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
							<email>chen.haoyu@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
							<email>guoying.zhao@oulu.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatialtemporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling-and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human action recognition is a valuable but challenging research area with widespread potential applications, say security surveillance, human computer interaction and autonomous driving. Nowadays, as an alternative to the appearance and depth data, skeleton data is popularly used in action recognition. One important reason is that skeleton data conveys compact information of body movement, thus it is robust to the complex circumstances like the variations of the viewpoints, occlusion and self-occlusion. Previous works reorganize the skeleton data into a kind of grid-shape structure so that the traditional recurrent neural networks (RNN) and convolutional neural networks (CNN) can be implemented. Though substantial improvements have been seen in action recognition, it does not fully benefit from the superior representing capability of deep learning, as the skeleton data lies in a non-Euclidean geometric space. Currently, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b8">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b1">Defferrard, Bresson, and Vandergheynst 2016)</ref> has been introduced to skeleton-based action recognition and achieved many encouraging results <ref type="bibr" target="#b21">Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b10">Li et al. 2019a;</ref><ref type="bibr" target="#b3">Gao et al. 2019;</ref><ref type="bibr" target="#b18">Shi et al. 2019;</ref><ref type="bibr" target="#b11">Li et al. 2019b)</ref>. Nonetheless, most GCN methods are based on a pre-defined graph with fixed topology constraint, which ignores implicit joint correlations. Work in <ref type="bibr" target="#b18">(Shi et al. 2019</ref>) turns to replace the fixed graph with an adaptive one based on the node similarity. However, it provides a shared mechanism through the entire network and the spatial-temporal correlations are barely discussed. We argue that different layers contain different semantic information thus a layer-specific mechanism should be involved to construct a dynamic graph. Besides, mainstream GCN tends to one-order Chebyshev polynomials approximation <ref type="bibr" target="#b8">(Kipf and Welling 2016)</ref> to reduce the computational expense, meanwhile high-order connections are not well involved so that the representational ability is limited. Current works, like <ref type="bibr" target="#b3">(Gao et al. 2019)</ref>, introduce high-order approximation to have GCN with a bigger receptive filed. Nonetheless, the contribution of the each component in the approximation is not discussed. It is apparent that designing such different function modules for different tasks requires lots of efforts and exhausted try-and-error tests.</p><p>To address this problem, in this paper, we focus on reducing the manual efforts in designing better graph convolutional architecture. We replace the fixed graph structure with dynamic ones by Automatic Neural Architecture Search (NAS) <ref type="bibr" target="#b22">(Zoph and Le 2016)</ref> and explore different graph generating mechanisms at different semantic levels. NAS is designed to obtain superior neural network structures with less or without human assists under a reasonable computational budgets. However, it is not straightforward to apply NAS to GCN. Graph data like skeleton has no locality and order information as required by convolution operations, while current NAS methods focus on the design of neural opera-tions. Besides, GCN itself is a relative new research area thus existing operations are very limited, e.g., GCN does not even have a general pooling operation. Therefore, we propose to search in a GCN space built with multiple graph function modules. Moreover, a high sample-efficient deep neuro-evolution strategy (ES) <ref type="bibr" target="#b0">(Angeline, Saunders, and Pollack 1994;</ref><ref type="bibr" target="#b13">Miller, Todd, and Hegde 1989)</ref> is provided to explore an optimal GCN architecture by estimating the architecture distribution. It could be conducted in both continuous and discrete search space. Thus, one can just activate one function module at each iteration to search by a memory-efficient fashion. With our NAS for GCN, we automatically build a graph convolutional network for action recognition from skeleton data. To evaluate the proposed method, we perform comprehensive experiments on two large-scaled datasets, NTU <ref type="bibr">RGB+D (Shahroudy et al. 2016)</ref> and Kinetcis-Skeleton <ref type="bibr" target="#b7">(Kay et al. 2017;</ref><ref type="bibr" target="#b21">Yan, Xiong, and Lin 2018)</ref>. Results show that our model is robust to the subject and view variations and achieves the state-of-the-art performance. The contributions of this paper are manifold:</p><p>• We break the limitation of GCN caused by its fixed graph and, for the first time, determine the graph convolution architecture with NAS for skeleton-based action recognition.</p><p>• We enrich the search space for GCN from the following two aspects. Firstly, we provide multiple dynamic graph substructures on the basis of various spatial-temporal graphs modules. Secondly, we enlarge the receptive field of GCN convolution by building higher-order connections with Chebyshev polynomial approximation.</p><p>• To improve the search efficiency, we devise a novel evolution based NAS search strategy, which is both samplingand memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Skeleton-based Action Recognition In human action recognition, as an alternative data source for RGB and depth data, skeleton data is increasingly attracted attention thanks to its robustness property against changes in body scales, viewpoints and backgrounds. Different from the grid data, the graph constructed by the human skeleton lies in a non-Euclidean space. To benefit from the great representation ability of deep learning, conventional methods tend to rearrange the skeleton data into grid-shape structure and feed it directly into the classical <ref type="bibr">RNN (Shahroudy et al. 2016;</ref><ref type="bibr" target="#b19">Song et al. 2017;</ref><ref type="bibr" target="#b21">Zhang et al. 2017)</ref>   <ref type="bibr" target="#b18">(Shi et al. 2019</ref>). Our method is also based on GCN and we will fully explore the influence of the graph topology for this task. Neural Architecture Search As an important part of automated machine learning (AutoML), Neural Architecture Search (NAS) <ref type="bibr" target="#b22">(Zoph and Le 2016)</ref> is to automatically build a neural network under a low cost of computational resources. Numerous approaches for NAS already exist in the literature, including black-box optimization based on reinforcement learning <ref type="bibr" target="#b22">(Zoph and Le 2016)</ref>, evolutionary search <ref type="bibr" target="#b17">(Real et al. 2018)</ref>, and gradient-based method <ref type="bibr" target="#b13">(Liu, Simonyan, and Yang 2018)</ref>. Besides, promising progresses are also seen in aspects such as searching space design <ref type="bibr" target="#b13">(Liu, Simonyan, and Yang 2018)</ref>, and architecture performance evaluation (Saxena and Verbeek 2016; <ref type="bibr" target="#b17">Real et al. 2018)</ref>. Automatically designed architectures have already got superior performances against the famous manual ones in the fields like image classification tasks , and semantic image segmentation ). There are also some attempts about NAS on action recognition <ref type="bibr" target="#b15">(Peng, Hong, and Zhao 2019)</ref>   <ref type="bibr" target="#b20">Vaswani et al. 2017)</ref>. One benefit of attention mechanisms is that they select information which is relatively critical from all inputs. Inspired by this, Velickovic et al.leveraged attention mechanism for graph node classification and achieved state-of-the-art performance <ref type="bibr" target="#b20">(Veličković et al. 2018)</ref>. Work in (Sankar et al. 2019) employs self-attention along both spatial and temporal dimensions and get superior results on link prediction tasks. Nonetheless, our work is different since we compute the interaction between nodes based on various semantic information to build a dynamic graph, while others is to compute the importance weights either for frames or different feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we detail our search-based GCN for action recognition from skeleton data. To make the paper selfcontained, we briefly review how to model a spatial graph with GCN first.</p><p>Consider an undirected graph G = {V, E, A} composed of n = |V| nodes, which are connected by |E| edges and the node connections are encoded in the adjacency matrix A ∈ R n×n . Let X ∈ R n be the input representation of G and{x i , ∀i ∈ V} be its n elements. Then to model the representation of G, the graph is performed a graph Fourier transform so that the transformed signal, as in the Euclidiean space, could then perform formulation of fundamental operations such as filtering. Therefore, the graph Laplacian L, of which the normalized definition is L = I n −D −1/2 AD −1/2 and D ii = j A ij , is used for Fourier transform. Then a graph filtered by operator g θ , parameterized by θ, can be formulated as</p><formula xml:id="formula_0">Y = g θ (L)X = U g θ (Λ)U T X,<label>(1)</label></formula><p>where Y is the extracted feature of node. U is the Fourier basis and it is a set of orthonormal eigenvectors for L so that L = U ΛU T with the Λ as its corresponding eigenvalues. However, multiplication with the eigenvectors matrix is expensive. The computational burden of this non-parametric filter is O(n 2 ) (Defferrard, Bresson, and Vandergheynst 2016). Suggested by <ref type="bibr" target="#b5">(Hammond, Vandergheynst, and Gribonval 2011)</ref>, the filter g θ can be well-approximated by a Chebyshev polynomials with R-th order.</p><formula xml:id="formula_1">Y = R r=0 θ r T r (L)X,<label>(2)</label></formula><p>of which θ r denotes Chebyshev coefficients. Chebyshev polynomial T r (L) is recursively defined as</p><formula xml:id="formula_2">T r (L) = 2LT r−1 (L) − T r−2 (L)<label>(3)</label></formula><p>with T 0 = 1 and T 1 =L. HereL = 2L/λ max − I n is normalized to <ref type="bibr">[-1,1]</ref>. For Eq (2), work in (Kipf and Welling 2016) sets R = 1, λ max = 2 and makes the network adapt to this change. In this way, a first-order approximation of spectral graph convolutions is formed. Therefore,</p><formula xml:id="formula_3">Y = θ 0 X + θ 1 (L − I n )X = θ 0 X − θ 1 (D −1/2 AD −1/2 )X.</formula><p>(4) Likewise, θ r can also be approximated with an unified parameter θ, which means θ = θ 0 = −θ 1 , and let the training process adapt the approximation error, then</p><formula xml:id="formula_4">Y = θ(I n + D −1/2 AD −1/2 )X.<label>(5)</label></formula><p>The computational expense is O(|E|). One can stack multiple GCN layers to get high-level graph feature. To make it simple, in the following sections, we set L = I n + D −1/2 AD −1/2 , and generally, X ∈ R n×C is with multichannels. Thus Y = LXθ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searched Graph Convolutional Network</head><p>Here we consider the human action recognition problem from skeleton data as a graph classification task from a sequence of graphs G = {G 1 , G 2 , ..., G T }. Each graph denotes a skeleton at a certain time step and its nodes and edges represent the skeleton joints and bones, respectively. Then, this task can be framed as supervised learning problem on graph data, in which the goal is to learn a robust representation of G with GCN and thus to give a better prediction of action classes. To this end, we propose to construct this GCN with neural architecture search, which automatically assembles graph generating modules for layers at different semantic levels. Firstly, we will detail the GCN search space built with different graph modules. Then, we present a samplingand memory-efficient search strategy.</p><p>GCN search space In NAS, a neural search space determines what and how neural operations a searching strategy could take to build a neural network. Here we search in space built with multiple GCN modules to explore the optimal module combination for dynamic graph at different representation levels. Work in (Yan, Xiong, and Lin 2018) presents a ST-GCN block, which takes skeleton data and a fixed graph as inputs, to extract the spatial-temporal representation of nodes. Our GCN-block is also a spatialtemporal block, while instead of providing a pre-defined graph, we generate dynamic graphs based on the node correlations captured by different function modules. There are mainly two kinds of correlations being captured to construct the dynamic graph. is the element-wise summation. There are eight function modules for generating graphs. The top part is a implementation of Chebyshev polynomial based on Eq (3). We also add its separate components to the graphs and let the network choose the final ones. The bottom part contains three dynamic graph modules. All the graphs are added together according to Eq (8). The contribution of each module works as the architecture parameters. Note that there is a softmax function before the summation operation for dynamic graphs.</p><p>Structure representation Correlation. Structure correlation is computed based on the spatial node connections. To determine how strong the connection is between two nodes, like in <ref type="bibr" target="#b18">(Shi et al. 2019</ref>), a normalized Gaussian function is applied on the graph nodes and the similarity score works as the correlation. That is</p><formula xml:id="formula_6">∀i, j ∈ V, A D (i, j) = e φ(h(xi)) ψ(h(xj ))</formula><p>n j=1 e φ(h(xi)) ψ(h(xj )) . <ref type="formula">(7)</ref> Algorithm if Ineq (10) satisfied then 17:</p><formula xml:id="formula_7">S new ← α i o .</formula><p>18:</p><p>Draw a sample α i n with π ∼ N (µ, Σ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>if Ineq (11) satisfied then 20:</p><p>S new ← α i n .</p><p>21: if j == epochs then return best α ∈ S new .</p><formula xml:id="formula_8">i = i + 1.</formula><p>This module is named as 'Spatial m' in <ref type="figure" target="#fig_0">Figure 1</ref>. Here, we compute the correlation score A D (i, j) between node i and node j based on their corresponding representations h(x i ) and h(x j ). The represents matrix multiplication. The φ(·) and ψ(·) are two projection functions, referred as conv s in <ref type="figure" target="#fig_0">Figure 1</ref> and can be implemented by channel-wise convolution filters. In this way, the similarity between nodes are captured to build the dynamic graph. Temporal representation Correlation. Structure correlation definitely contains most intuitive cues for the topology of the graph. However, ignoring the temporal correlation can loss implicit joint correlations. We take an example from NTU RGB+D dataset. Without the temporal information, it is hard to tell a person is to touch his head or just to wave his hand. As from the physical structure perspective, little connection could be captured between head node and hand node during the action of touch head. But there should be a correlation between them in this action. Including the tem-poral information will make it much easy. Therefore, we introduce two temporal convolutions to extract the temporal information of each node before computing node correlations with Eq (7). In this way, the node interactions between neighbor frames are involved when we calculate the node connections. Note that, temporal representation correlation here is different from temporal attention mechanism, which is to give higher weights to the relative important frames. In contrast, we capture temporal information for a better generation of the spatial graph. To this regard, we also introduce a Gaussian function, as in Eq <ref type="formula">(7)</ref>, to compute the node correlation. The functions φ(·) and ψ(·) are implemented by temporal convolutions, referred as conv t in <ref type="figure" target="#fig_0">Figure 1</ref> and this module is referred as 'Temporal m'. It is also worth mentioning that, for the structure correlation, even the T frames of representation are all involved when compute the graph, the interaction is limited to the features from same dimension at the same time step. While our temporal module could involve interactions beyond the same frames.</p><p>With these two modules: 'Spatial m' and 'Temporal m', it is straightforward to build a spatial-temporal function module for dynamic graph. Thus we build the 'Spatio-Temporal m' in <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore, as in <ref type="figure" target="#fig_0">Fig 1,</ref> there are three kinds of modules for dynamic graphs.</p><p>Furthermore, we also want to explore the contribution of each component in the Chebyshev polynomials, and thus benefit from high-order hop connections. As we know, the work in <ref type="bibr" target="#b8">(Kipf and Welling 2016)</ref> gives a well-approximation of the spectral filter with the order-one Chebyshev polynomials. Instead, as illustrated in <ref type="figure" target="#fig_0">Fig 1,</ref> in our search space, we build Chebyshev polynomials functions with different orders at different layers and let the network determine which order and polynomial components each layer prefers. The function module can be constructed by Eq (3), and here the biggest order is R = 4. Since all the dynamic graphs are normalized, here we also add a normalized one for the order-4 approximation. Therefore, there are totally eight function modules, as illustrated in the <ref type="figure" target="#fig_0">Fig 1,</ref> in this search space.</p><p>With these eight modules, we could search for the best architecture. Previous NAS methods would search a single block to reduce the computational burden. However, we argue that different feature layers contain different level of semantic content and thus a layer-specific mechanism is preferred to build a graph. So we search for an entire GCN network instead of a single block. To improve the efficiency, a high computation-and memory-efficient search strategy will be provided.</p><p>Let us formally define the search space first. Here we redefine X as a sequence of graphs. Given a fixed graph L and the feature h k (X) from the k-th layer, we extract the output representation h k+1 (X) at k + 1 layer, with the function modules we choose. Inspired by one-shot NAS and DARTS <ref type="bibr" target="#b13">(Liu, Simonyan, and Yang 2018)</ref>, all the function modules are paralleled and the weighted sum of their out-puts are the output h k+1 (X), that is</p><formula xml:id="formula_9">h k+1 (X) = M i=1 α k+1,i M j α k+1,j M i (h k (X), L)h k (X)Θ k .</formula><p>(8) Here, Θ k is the network weights for the k-th layer. M i denotes the i-th function module, and α k+1,i , which works as the architecture parameter, is its corresponding parameter at the k + 1 layer. Then the problem here is to search a set of parameters α ∈ R K×M for a network with K layers so that α minimizes the loss L valid on the validation data. That is</p><formula xml:id="formula_10">α * = argmin α L valid (Θ(α), α)<label>(9)</label></formula><p>Here, Θ is the network parameters shared by all subnetworks and it will be learned on the training dataset. Previous works search on a small proxy dataset to evade the expensive computational burden. Instead, here we search directly on the target dataset to avoid introducing extra domain adaption problem.</p><p>GCN search strategy Inspired by <ref type="bibr" target="#b16">(Pou. and S. 2019)</ref>, we propose to search with a high sampling-efficient ES-based method, denoted CEIM. This method explore an optimal architecture by estimating the architecture distribution. Thus it is not limited in a differentiable search space. One could improve the memory-efficiency by only activating one function module at each searching step. Specifically, this search strategy combines Cross-Entropy method (Larrañaga and Lozano 2001) with Importance-Mixing (CEIM) to improve the sampling efficiency. In CEIM, architecture parameters α is treated as a population and the distribution of architecture is modeled by a Gaussian distribution. Then CEIM samples a group of architectures and with their performances, important samples are selected to update the architecture distribution. Thus an optimal architecture could be finally sampled from the architecture distribution. In total, there are three steps in our CEIM algorithm, sampling populations, selecting populations, and updating architecture distribution. Firstly, we model the architecture distribution with a Gaussian distribution π ∼ N (µ, Σ) and sample N architecture samples S new as the populations for CEIM, that is S new = {α i n } N i=1 . Secondly, combining S new with historical selected populations S old , we employ an importance mixing method on all these populations to choose architecture samples. Finally, the newly selected samples are used to update the architecture distribution π.</p><p>Here, we detailed the last two steps. Assume samples from previous iteration is</p><formula xml:id="formula_11">S old = {α i o } N i=1 .</formula><p>In the selecting step, for each population in S old and S new , we compare its probability density (pd) in both current (π new ) and old (π old ) probability density functions (pdf). Generally, for the old population α i o , we keep it once it is with a bigger pd in the new distribution than that in the old one. That is</p><formula xml:id="formula_12">min(1, p(α i o ; π new ) p(α i o ; π old ) ) &gt; r1<label>(10)</label></formula><p>Here r1 is a threshold randomly got from rang [0, 1] and p(·; π) is a pdf with specific distribution π. Likewise, for new sample α i n drawn from the current distribution, if its pd in the new pdf is bigger than that in the old one, we will also keep it. Therefore, when max(0, 1 − p(α i n ; π old ) p(α i n ; π new ) ) &gt; r2 <ref type="formula" target="#formula_0">(11)</ref> we save it. Here r2 is another threshold in [0, 1].</p><p>For the updating step, the samples selected in previous step are used to update mean µ and convariance Σ. Before that, the Θ of the network is updated on the training data with current architecture α = µ. Then, the Θ is fixed and every selected sample is set as the current architecture. Its corresponding fitness is evaluated on the validation data. With their performances, all the selected samples are sorted. Based on the performance order, an importance weight λ i is assigned to the i-th sample. That is</p><formula xml:id="formula_13">λ i = log(1 + N )/i N i=1 log(1 + N )/i .<label>(12)</label></formula><p>In this way, the sample with better performance will be given a bigger weight, thus it contributes more to the updating of the distribution. Finally, the weighted samples is applied to update the architecture distribution. That is</p><formula xml:id="formula_14">µ new = N i=1 λ i α i ,<label>(13)</label></formula><formula xml:id="formula_15">Σ new = N i=1 λ i (α i − µ) 2 + I.<label>(14)</label></formula><p>Here, I is a noise term for better exploring of the neural architecture. Since, in practice, Σ is too large to compute and update, here we constrain it to be a diagonal one. Note that in Eq. <ref type="formula" target="#formula_0">(14)</ref>, different with the original cross-entropy method, which updates Σ with the new mean µ new , we use the mean of last iteration to update Σ since convariance matrix adaption evolution strategy (CMA-ES) shows it is more efficient <ref type="bibr">(H. 2016)</ref>. More details about CEIM please refer to Algorithm. 1. One could improve the memory-efficiency by only activating one function module at each searching step. That means for the output h k+1 (X), it can be a single output from the activated module.</p><formula xml:id="formula_16">h k+1 (X) =      M 1 (h k (X), L)h k (X)Θ k+1 , p = α k+1,1 M j α k+1,j · · · M M (h k (X), L)h k (X)Θ k+1 , p = α k+1,M M j α k+1,j<label>(15)</label></formula><p>Here, each module is activated by a multinomial distribution with the probability p ∼ α k+1,i and Θ k+1 is the activated weight of the (k + 1)-th layer. In the following section, we will evaluate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To evaluate the performance of our model, we carry out comparative experiments on two large-scale skeleton datasets, NTU RGB+D (Shahroudy et al. 2016) and Kenitics-Skeleton <ref type="bibr" target="#b7">(Kay et al. 2017;</ref><ref type="bibr" target="#b21">Yan, Xiong, and Lin 2018)</ref>, for action recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset &amp; Evaluation Metrics</head><p>NTU RGB+D NTU RGB+D is currently the most widely used and the largest multi-modality indoor-captured action recognition dataset. There are RGB videos, depth sequences, infrared videos and 3D skeleton data in it. The skeleton data, which is captured by the Microsoft Kinect v2 camera, is the one we use here. There are totally 56,880 video clips captured from three cameras at different heights with different horizontal angles. These actions cover 60 human action classes including single-actor action, which is from class 1 to 49, and two-actor action, which is from class 50 to 60. There are 25 3D joints coordinates for each actor. We follow the benchmark evaluations in the original work <ref type="bibr" target="#b17">(Shahroudy et al. 2016)</ref>, which are Cross-subject (CS) and the Crossview (CV) evaluations. In the CS evaluation, the training set contains 40,320 videos from 20 subjects, and the rest 16,560 video clips are used for testing. In the CV evaluation, videos captured from camera two and three, contains 37,920 videos, are used in the training and the videos from camera one, contains 18,960 videos, are used for testing. In the comparison, Top-1 accuracy is reported on both of the two benchmarks. Kinetics-Skeleton Kinetics-Skeleton is based on the very large scale action dataset Kinetics <ref type="bibr" target="#b7">(Kay et al. 2017)</ref>, in which there are approximately 300 000 video clips collected from YouTube. This dataset covers 400 kinds of human actions. However, the original Kinectics dataset has no skeleton data. <ref type="bibr">Yan et al.</ref> employed the open source toolbox OpenPose <ref type="bibr" target="#b1">(Cao et al. 2017)</ref> to estimate the 2D joints location of each frame and then built this huge dataset Kinetics-Skeleton <ref type="bibr" target="#b21">(Yan, Xiong, and Lin 2018)</ref>. For each person, coordinates (X, Y) form 18 joints are estimated. For the frames which contain more than two persons, only the top-2 persons are selected based on the average joint confidence. The released data pads every clips to 300 frames. During comparison, both the Top-1 and Top-5 recognition accuracy are reported since this task is much harder due to its great variety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Our framework is implemented on the PyTorch <ref type="bibr" target="#b14">(Paszke et al. 2017</ref>) and the code is released at here 1 . To keep consistent with the current state-of-the-art GCN methods <ref type="bibr" target="#b21">(Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b18">Shi et al. 2019</ref>) 2 , we introduce ten GCN blocks into our network for both searching and training steps. Each of them is based on the block in <ref type="figure" target="#fig_0">Figure 1</ref>. Like the previous works, each block is followed with a temporal convolution with the kernel size 9×1 to capture the temporal information. The first GCN block projects the graph into a feature space with the channel number of 64. Then there will be three layers outputting 64 channels for the outputs. After that, the following three layers double the output channels. And the last three layers have 256 channels for outputs. Just like <ref type="bibr" target="#b21">(Yan, Xiong, and Lin 2018)</ref>, the resnet mechanism is applied on each GCN block. Finally, the extracted features are fed into a fully-connected layer for the final prediction. <ref type="table">Table 1</ref>: Searched modules at each layer. Here, each row refers to a block layer. There are eight module options, including dynamic graph modules (M(S), M(T ), M(ST )) with various spatialtemporal cues and Chebshev approximation with different orders ( L, L 4 n,L 4 ,L 3 , L 2 ). The modules marked with represent modules selected by CEIM.</p><formula xml:id="formula_17">M L L 4 n L 4 L 3 L 2 M(S) M(T ) M(ST ) K1 K2 K3 K4 k5 k6 k7 k8 k9 k10</formula><p>For each GCN block, the spatial modules conv s are channel-wise convolution filters and the temporal filters conv t are convolution filters with kernel size 9 × 1 performing along the temporal dimension. During searching, we conduct the experiments on the NTU RGB+D Joint data to find the optimal architecture. We share the same architecture for all the aforementioned datasets to keep consistent with the current state-of-the-art methods.</p><p>For the training process, a stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization algorithm for the network. The cross-entropy loss is selected as the loss function for the recognition task. The weight decay is set to 0.0001 and 0.0006 for searching and training, respectively. For the NTU RGB+D dataset, there are at most two people in each sample of the dataset. If the number of bodies in the sample is less than 2, we pad the second body with 0. The max number of frames in each sample is 300. For samples with less than 300 frames, we repeat the samples until it reaches 300 frames. The learning rate is set as 0.1 and is divided by 10 at the 30th, 45th, and 60th epoch. The training process is ended at the 70th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture search analysis</head><p>We conduct 70 epochs for searching. For the first 20 epochs, we randomly update each module in the network without evaluating any architectures. After that, we sample N = 50 architectures and update the architecture distribution with our CEIM algorithm. When the searching is done, we choose the modules of which the architecture parameters α &gt; 0.1. Like <ref type="bibr" target="#b18">(Shi et al. 2019</ref>), a complementary dynamic graph is added, as we do not involve the weight α for each module in the final implementation. The searched architecture is listed in Table 1. The result shows that different layers prefer different mechanisms to generate graphs, which is consistent with our expectation since high level representation contains more semantic information. Concretely, as in <ref type="table">Table 1</ref>, the lower layers, like layer K 1 to K 4 , count in all dynamic function modules to capture richer information. For higher layers, the temporal representation correlations are more preferred. It is interesting that temporal graph module M(T ) is selected through the entire network while the spatial function module M(S) is limited to the lower layers, which proves the effectiveness of the proposed temporal function module. For the higher-order connections, we found that 2order hop connection is much welcomed than any other one.</p><p>And surprisingly we found the L, which encodes the physical structure of the skeleton data, is not involved at any layer. This founding gives us a new inspiration about how to build a GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Here, we explore the effectiveness of the graph modules and also our searched GCN. Therefore, we perform the following experiments on NTU RGB+D with the benchmark of cross-view. Here we compare with six baselines, which are with different mechanism to build the dynamic graphs. Specifically, the modules used to generate graph are based on: 1) Structure representation correlations (S, here it is 2S-ACGN (Shi et al. 2019)); 2) Temporal representation correlations (Ours(T)); 3) Spatial-Temporal representation correlations(Ours(ST)); 4) Temporal correlations with 4-order Chebyshev approximation (Ours(T+Cheb)); 5) Spatial-Temporal representation correlations with 4-order Chebyshev approximation (Ours(ST+Cheb)); and 6) Combine all aforementioned modules(Ours(S+T+ST+Cheb)). Inspired by <ref type="bibr" target="#b18">(Shi et al. 2019)</ref>, we evaluate these models on both joints and the bones (the second order information of skeleton joints) data, thus a fusion result from score-level is also reported. For these six methods, the same block is shared through the whole network. Instead, our searched method explores the best modules for different layers. The comparison results are listed in the <ref type="table" target="#tab_4">Table 2</ref>. It shows that temporal information do helps for GCN (Ours(T) and Ours(ST)) and involving all modules can not make sure a better performance (Ours(S+T+ST+Cheb)). Besides, higher-order also helps for GCN (Ours( +Cheb)).The superior performance of the NAS-based GCN (Ours(NAS)) proves the effectiveness of our method. When compared to the current best result <ref type="bibr" target="#b18">(Shi et al. 2019)</ref>, shows in the first row, we improve the accuracy 0.9%, 1.5%, and 0.6% on the Joint, Bone and Combine, respectively. This verifies the effectiveness of our SGCN method.   <ref type="bibr" target="#b21">Yan, Xiong, and Lin 2018;</ref><ref type="bibr" target="#b10">Li et al. 2019a;</ref><ref type="bibr" target="#b3">Gao et al. 2019;</ref><ref type="bibr" target="#b11">Li et al. 2019b;</ref><ref type="bibr" target="#b18">Shi et al. 2019</ref>), on NTU RGB+D and Kinetics-Skeleton datasets. <ref type="table" target="#tab_6">Table 3</ref> and <ref type="table" target="#tab_7">Table 4</ref> show the results on these two datasets, respectively. Here we report the best result after performing the score-level fusion on joints and bones. It can be seen from <ref type="table" target="#tab_6">Table 3</ref> and 4 that the searched model achieves the best performance on both of the two datasets in terms of all evaluation metrics. This proves the effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose to build graph convolutional network for skeleton-based action recognition with neural architecture search (NAS). To enrich the NAS search space, firstly, three dynamic graph generating modules are constructed on the basis of various spatial-temporal correlations of nodes. Secondly, modules with higher-order connections are introduced to enlarge the receptive field of GCN convolution. Besides, we devise a novel search strategy by combining cross-entropy evolution strategy with importancemixing (CEIM), which is both sampling-and memoryefficient. Based on the proposed NAS method, we explore the optimal GCN architecture in this space for skeleton action recognition. The searched model proves the effective-ness of our temporal-based dynamic graph module. Comprehensive experiments on two very large-scale datasets show its overwhelming performance when compared to the stateof-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the search space. Here, denotes matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Euclidean space.Yan et al. and Li et al.  are the first to use GCN for skeleton-based action recognition<ref type="bibr" target="#b21">(Yan, Xiong, and Lin 2018;</ref>.Gao et al. proposed a sparsi-  </figDesc><table><row><cell>strategy, it gets the current best result</cell></row><row><cell>or CNN (Kim and Re-</cell></row><row><cell>iter 2017; Liu, Liu, and Chen 2017) architectures. However,</cell></row><row><cell>as mentioned in (Monti et al. 2017), one can not express a</cell></row><row><cell>meaningful operator in the vertex domain. Therefore, cur-</cell></row><row><cell>rent works tend to GCN and prefer to build operators in the</cell></row><row><cell>non--order</cell></row><row><cell>information (bones) are both used. With a score-level fusion</cell></row></table><note>fied graph regression based GCN (Gao et al. 2019) to exploit the dependencies of each joints. Shi et al. gave a two-stream GCN architecture, in which the joints and the second</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation Study. Performance comparison on NTU RGB+D with CV evaluation.</figDesc><table><row><cell>Methods</cell><cell cols="3">Joint(%) Bone(%) Combine(%)</cell></row><row><cell>2S-AGCN (Shi et al. 2019)</cell><cell>93.7</cell><cell>93.2</cell><cell>95.1</cell></row><row><cell>Ours(T)</cell><cell>93.8</cell><cell>93.7</cell><cell>95.1</cell></row><row><cell>Ours(ST)</cell><cell>94.0</cell><cell>93.8</cell><cell>95.2</cell></row><row><cell>Ours(T+Cheb)</cell><cell>94.0</cell><cell>93.9</cell><cell>95.2</cell></row><row><cell>Ours(ST+Cheb)</cell><cell>94.2</cell><cell>93.9</cell><cell>95.3</cell></row><row><cell>Ours(S+T+ST+Cheb)</cell><cell>93.9</cell><cell>93.6</cell><cell>95.1</cell></row><row><cell>Ours(NAS)</cell><cell>94.6</cell><cell>94.7</cell><cell>95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Comparison with State-of-the-arts (SOTA)</figDesc><table><row><cell>To evaluate the performance of our searched model, we com-</cell></row><row><cell>pared it with 14 SOTA skeleton-based action recognition ap-</cell></row><row><cell>proaches, including hand-crafted methods (Hu et al. 2015),</cell></row><row><cell>CNN-based methods (Kim and Reiter 2017; Liu, Liu, and</cell></row><row><cell>Chen 2017), LSTM-based methods (Shahroudy et al. 2016;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on NTU RGB+D with 14 current state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell cols="3">CS(%) CV(%) Conference</cell></row><row><cell>Joint (Hu et al. 2015)</cell><cell>60.2</cell><cell>65.2</cell><cell>CVPR2015</cell></row><row><cell>P-LSTM (Shahroudy et al. 2016)</cell><cell>62.9</cell><cell>70.3</cell><cell>CVPR2016</cell></row><row><cell>STA-LSTM (Song et al. 2017)</cell><cell>73.4</cell><cell>81.2</cell><cell>AAAI2017</cell></row><row><cell>TCN (Kim and Reiter 2017)</cell><cell>74.3</cell><cell>83.1</cell><cell>CVPRW2017</cell></row><row><cell>VA-LSTM (Zhang et al. 2017)</cell><cell>79.2</cell><cell>87.7</cell><cell>CVPR2017</cell></row><row><cell>SynCNN (Liu, Liu, and Chen 2017)</cell><cell>80.0</cell><cell>87.2</cell><cell>PR2017(Jou.)</cell></row><row><cell>Deep STGCK (Li et al. 2018)</cell><cell>74.9</cell><cell>86.3</cell><cell>AAAI2018</cell></row><row><cell>ST-GCN (Yan et al. 2018)</cell><cell>81.5</cell><cell>88.3</cell><cell>AAAI2018</cell></row><row><cell>DPRL (Tang et al. 2018)</cell><cell>83.5</cell><cell>89.8</cell><cell>CVPR2018</cell></row><row><cell>SR-TSL (Si et al. 2018)</cell><cell>84.8</cell><cell>92.4</cell><cell>ECCV2018</cell></row><row><cell>STGR-GCN (Li et al. 2019a)</cell><cell>86.9</cell><cell>92.3</cell><cell>AAAI2019</cell></row><row><cell>GR-GCN (Gao et al. 2019)</cell><cell>87.5</cell><cell>94.3</cell><cell>ACMM2019</cell></row><row><cell>AS-GCN (Li et al. 2019b)</cell><cell>86.8</cell><cell>94.2</cell><cell>CVPR2019</cell></row><row><cell>2S-AGCN (Shi et al. 2019)</cell><cell>88.5</cell><cell>95.1</cell><cell>CVPR2019</cell></row><row><cell>Ours(Joint+Bone)</cell><cell>89.4</cell><cell>95.7</cell><cell>-</cell></row><row><cell cols="4">Song et al. 2017; Zhang et al. 2017; Si et al. 2018), rein-</cell></row><row><cell cols="4">forcement learning based method (Tang et al. 2018), and</cell></row><row><cell cols="3">current promising GCN-based methods</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on Kinetics with eight current state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell cols="3">Top-1(%) Top-5(%) Conference</cell></row><row><cell>Feature (Fernando et al. 2015)</cell><cell>14.9</cell><cell>25.8</cell><cell>CVPR2015</cell></row><row><cell>P-LSTM (Shahroudy et al. 2016)</cell><cell>16.4</cell><cell>35.3</cell><cell>CVPR2016</cell></row><row><cell>TCN (Kim and Reiter 2017)</cell><cell>20.3</cell><cell>40.0</cell><cell>CVPRW2017</cell></row><row><cell>ST-GCN (Yan et al. 2018)</cell><cell>30.7</cell><cell>52.8</cell><cell>AAAI2018</cell></row><row><cell>AS-GCN (Li et al. 2019b)</cell><cell>34.8</cell><cell>56.5</cell><cell>CVPR2019</cell></row><row><cell>2S-AGCN(Joint) (Shi et al. 2019)</cell><cell>35.1</cell><cell>57.1</cell><cell>CVPR2019</cell></row><row><cell>2S-AGCN(Bone) (Shi et al. 2019)</cell><cell>33.3</cell><cell>55.7</cell><cell>CVPR2019</cell></row><row><cell>2S-AGCN (Shi et al. 2019)</cell><cell>36.1</cell><cell>58.7</cell><cell>CVPR2019</cell></row><row><cell>Ours(Joint)</cell><cell>35.5</cell><cell>57.9</cell><cell>-</cell></row><row><cell>Ours(Bone)</cell><cell>34.9</cell><cell>57.1</cell><cell>-</cell></row><row><cell>Ours(Joint+Bone)</cell><cell>37.1</cell><cell>60.1</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code will be released after the paper published 2 Note, these two SOTA methods claims nine blocks in the paper while their released codes shows they implemented the network with ten blocks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An evolutionary algorithm that constructs recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saunders</forename><surname>Angeline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Pollack ; Angeline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
		<idno>IEEE CVPR 7291-7299</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>Realtime multi-person 2d pose estimation using part affinity fields</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM International Conference on Multimedia</title>
		<meeting>the 2019 ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cma evolution strategy: A tutorial. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>and Gribonval</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of distribution algorithms: A new tool for evolutionary computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">Springer Science &amp; Business Media</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semi-supervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph routing for skeleton-based action recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>Monti et al. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">Darts: Differentiable architecture search. arXiv</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video action recognition via neural architecture searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhao ; Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04632</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient neural architecture search via parameter sharing</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cem-rl: Combining evolutionary and gradient-based methods for policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search. arXiv</title>
	</analytic>
	<monogr>
		<title level="m">Convolutional neural fabrics. In NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<idno>Tang et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Thirtyfirst AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
	<note>IEEE CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Veličković et al. 2018. Graph attention networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Lin ; Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
	<note>IEEE ICCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
	<note>Neural architecture search with reinforcement learning. ArXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
